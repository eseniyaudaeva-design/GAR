import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
try:
    from googlesearch import search
    USE_SEARCH = True
except ImportError:
    USE_SEARCH = False
    
import re
from collections import Counter
import math
import inspect
import concurrent.futures
from urllib.parse import urlparse
import time 

# --- –§–ò–ù–ê–õ–¨–ù–´–ô –ë–†–û–ù–ï–ë–û–ô–ù–´–ô –ü–ê–¢–ß –î–õ–Ø PYMORPHY2 ---
# (–ü–∞—Ç—á –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ —Å—Ä–µ–¥–∞–º–∏)
try:
    if not hasattr(inspect, 'getargspec'):
        def getargspec(func):
            spec = inspect.getfullargspec(func)
            return spec.args, spec.varargs, spec.varkw, spec.defaults
        inspect.getargspec = getargspec

    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_MORPH = True
except ImportError:
    USE_MORPH = False
except Exception:
    USE_MORPH = False


# ==========================================
# 1. –ù–ê–°–¢–†–û–ô–ö–ê –°–¢–†–ê–ù–ò–¶–´ –ò –°–¢–ò–õ–ò
# ==========================================

st.set_page_config(
    page_title="SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# –í–Ω–µ–¥—Ä—è–µ–º CSS —Å—Ç–∏–ª–∏ (—à—Ä–∏—Ñ—Ç Inter)
st.markdown("""
    <style>
        /* –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —à—Ä–∏—Ñ—Ç–∞ Inter */
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap');

        /* –û—Å–Ω–æ–≤–Ω–æ–π —à—Ä–∏—Ñ—Ç –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è */
        html, body, [class*="css"] {
            font-family: 'Inter', sans-serif;
            color: #171717;
        }
        
        /* –ó–∞–≥–æ–ª–æ–≤–∫–∏ */
        h1, h2, h3 {
            font-weight: 700;
            color: #0F172A;
        }

        /* –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è */
        .block-container {
            padding-top: 2rem;
            padding-bottom: 2rem;
            max-width: 1400px;
        }

        /* –ó–∞–≥–æ–ª–æ–≤–æ–∫ H1 */
        h1 {
            color: #1E40AF; /* –°–∏–Ω–∏–π –¥–ª—è –∞–∫—Ü–µ–Ω—Ç–∞ */
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }

        /* –ö–∞—Ä—Ç–æ—á–∫–∏ –≤–≤–æ–¥–∞ */
        .stTextInput > div > div > input, .stTextArea > div > textarea, .stSelectbox > div > button {
            border-radius: 0.5rem;
            border: 1px solid #E5E7EB;
            padding: 0.75rem 1rem;
            box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
        }

        /* –ö–Ω–æ–ø–∫–∏ */
        .stButton>button {
            background-color: #1E40AF;
            color: white;
            font-weight: 600;
            border-radius: 0.5rem;
            padding: 0.75rem 1.5rem;
            transition: all 0.2s;
            border: none;
        }
        .stButton>button:hover {
            background-color: #1D4ED8;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -2px rgba(0, 0, 0, 0.1);
        }
        
        /* –¢–∞–±–ª–∏—Ü—ã Pandas */
        .stDataFrame {
            border-radius: 0.75rem;
            overflow: hidden;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -4px rgba(0, 0, 0, 0.1);
        }

        /* –ü–æ–¥—Å–≤–µ—Ç–∫–∞ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π */
        .dataframe td {
             vertical-align: middle !important;
        }

    </style>
""", unsafe_allow_html=True)


# ==========================================
# 2. –§–£–ù–ö–¶–ò–ò –ë–≠–ö–ï–ù–î–ê
# ==========================================

# 2.1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
def fetch_content(url):
    """–ü–æ–ª—É—á–∞–µ—Ç HTML-–∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –∞–Ω–∫–æ—Ä—ã."""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç 10 —Å–µ–∫—É–Ω–¥
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—à–∏–±–∫–∏ HTTP

        soup = BeautifulSoup(response.content, 'lxml')
        
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∏–ª–∏, –∏ –¥—Ä—É–≥–∏–µ –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for script_or_style in soup(["script", "style", "header", "footer", "nav", "aside", "form"]):
            script_or_style.decompose()

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–Ω–∫–æ—Ä–æ–≤
        anchors = [a.get_text(separator=' ', strip=True).lower() for a in soup.find_all('a') if a.get_text(strip=True)]
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (—Ç–µ–≥–∏ p, h1-h6, li, td, span, div)
        main_tags = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'td', 'span', 'div'])
        text_content = ' '.join([tag.get_text(separator=' ', strip=True) for tag in main_tags])
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫
        text_content = re.sub(r'\s+', ' ', text_content).strip()

        return text_content, ' '.join(anchors)

    except requests.exceptions.RequestException:
        return None, None
    except Exception:
        return None, None

# 2.2. –û—á–∏—Å—Ç–∫–∞ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
def process_text(text):
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç, —É–¥–∞–ª—è–µ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç."""
    if not text:
        return []
        
    # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–±—É–∫–≤–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø–µ—Ä–µ–≤–æ–¥ –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
    text = re.sub(r'[^–∞-—èa-z\s]', ' ', text.lower())
    words = text.split()
    
    # –û—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
    stopwords = set([
        '–∏', '–≤', '–Ω–∞', '–ø–æ', '—Å', '–∫', '–æ—Ç', '–¥–æ', '–¥–ª—è', '–∏–∑', '–∑–∞', '–ø–æ–¥', 
        '–Ω–µ', '–¥–∞', '—ç—Ç–æ', '—Ç–æ', '–∫–∞–∫', '—Ç–∞–∫', '–∂–µ', '–º—ã', '–≤—ã', '–æ–Ω', '–æ–Ω–∞', 
        '–æ–Ω–æ', '–æ–Ω–∏', '–∏—Ö', '–≤—Å–µ', '—á—Ç–æ', '–∫–æ—Ç–æ—Ä—ã–π', '–ø—Ä–∏', '—É', '—è', '–Ω–æ'
    ])
    
    lemmas = []
    if USE_MORPH:
        for word in words:
            if word in stopwords or len(word) < 3:
                continue
            p = morph.parse(word)[0]
            # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ, –≥–ª–∞–≥–æ–ª—ã, –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ, –Ω–∞—Ä–µ—á–∏—è
            if p.tag.POS in ('NOUN', 'VERB', 'ADJF', 'ADJS', 'ADVB'):
                lemmas.append(p.normal_form)
    else:
        # –ï—Å–ª–∏ pymorphy2 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ—Å—Ç–æ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º
        lemmas = [word for word in words if word not in stopwords and len(word) >= 3]
            
    return lemmas

# 2.3. –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ (TF, IDF, BM25)
def calculate_metrics(word_freqs, N, idf_db, D, avg_D, k1, b):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ TF, TF-IDF, BM25 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞.
    """
    metrics = {}
    
    for word, freq in word_freqs.items():
        if word not in idf_db:
            # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ IDF –±–∞–∑–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π IDF (min_doc_freq = 1)
            idf_value = math.log(N / 1) if N > 0 else 0 
        else:
            idf_value = idf_db[word]
            
        # 1. Term Frequency (TF)
        tf = freq / D if D > 0 else 0

        # 2. TF-IDF
        tfidf = tf * idf_value

        # 3. BM25
        # –†–∞—Å—á–µ—Ç K
        K = k1 * ( (1 - b) + b * (D / avg_D) )
        # –§–æ—Ä–º—É–ª–∞ BM25
        bm25 = idf_value * ( (freq * (k1 + 1)) / (freq + K) )

        metrics[word] = {
            'tf': tf,
            'tfidf': tfidf,
            'bm25': bm25,
            'idf': idf_value,
            'count': freq, # —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—É—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ
        }
    return metrics

# 2.4. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞
def run_analysis(my_url, competitors_urls, settings):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.
    """
    # 1. –°–±–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ
    all_data = {}
    
    urls_to_fetch = [my_url] + competitors_urls
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_url = {executor.submit(fetch_content, url): url for url in urls_to_fetch}
        
        fetch_status = st.empty()
        fetch_status.info("‚è≥ –ò–¥–µ—Ç —Å–±–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...")
        
        for i, future in enumerate(concurrent.futures.as_completed(future_to_url)):
            url = future_to_url[future]
            
            try:
                body_content, anchor_content = future.result()
            except Exception:
                body_content, anchor_content = None, None
            
            if body_content:
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ 1 —Å–µ–∫—É–Ω–¥–∞, —á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ —Ü–µ–ª–µ–≤—ã–µ —Å–∞–π—Ç—ã (–∫—Ä–æ–º–µ —Å–≤–æ–µ–≥–æ)
                if url != my_url:
                    time.sleep(1) 
                
                lemmas = process_text(body_content)
                anchor_lemmas = process_text(anchor_content)
                
                all_data[url] = {
                    'body_lemmas': lemmas,
                    'anchor_lemmas': anchor_lemmas,
                    'D_body': len(lemmas),
                    'D_anchor': len(anchor_lemmas),
                    'domain': urlparse(url).netloc
                }
                
                fetch_status.text(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i+1}/{len(urls_to_fetch)} —Å—Ç—Ä–∞–Ω–∏—Ü. –¢–µ–∫—É—â–∞—è: {urlparse(url).netloc}")
            else:
                fetch_status.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")


    if my_url not in all_data:
        st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å –≤–∞—à–µ–≥–æ URL. –ê–Ω–∞–ª–∏–∑ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.")
        return None

    my_data = all_data.pop(my_url)
    comp_data = all_data

    N_comps = len(comp_data)
    if N_comps == 0:
        st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ —Å –æ–¥–Ω–æ–≥–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞. –ê–Ω–∞–ª–∏–∑ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.")
        return None

    # 2. –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (DF) –∏ IDF
    
    all_comp_words = set()
    for data in comp_data.values():
        all_comp_words.update(set(data['body_lemmas']))

    doc_freq = Counter()
    for word in all_comp_words:
        for data in comp_data.values():
            if word in data['body_lemmas']:
                doc_freq[word] += 1
    
    idf_db = {
        word: math.log(N_comps / count) for word, count in doc_freq.items() if N_comps > 0 and count > 0
    }
    
    comp_lengths = [data['D_body'] for data in comp_data.values()]
    avg_D = np.mean(comp_lengths) if comp_lengths else 1

    # 3. –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
    
    all_word_metrics = {}
    
    # --- 3.1. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ ---
    for data in comp_data.values():
        word_freqs = Counter(data['body_lemmas'])
        
        metrics = calculate_metrics(
            word_freqs, N_comps, idf_db, data['D_body'], avg_D, 
            settings['bm25_k1'], settings['bm25_b']
        )

        for word, m in metrics.items():
            if word not in all_word_metrics:
                all_word_metrics[word] = {
                    'tfidf_comp': [], 'bm25_comp': [], 'count_comp': [], 'count_sites': doc_freq.get(word, 0)
                }
            all_word_metrics[word]['tfidf_comp'].append(m['tfidf'])
            all_word_metrics[word]['bm25_comp'].append(m['bm25'])
            all_word_metrics[word]['count_comp'].append(m['count'])

    # --- 3.2. –ê–Ω–∞–ª–∏–∑ –Ω–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ ---
    my_body_freqs = Counter(my_data['body_lemmas'])
    my_anchor_freqs = Counter(my_data['anchor_lemmas'])
    
    my_body_metrics = calculate_metrics(
        my_body_freqs, N_comps, idf_db, my_data['D_body'], avg_D, 
        settings['bm25_k1'], settings['bm25_b']
    )

    # 4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
    
    final_data = []
    
    all_words = set(all_word_metrics.keys()) | set(my_body_metrics.keys())
    
    for word in all_words:
        
        comp_data_word = all_word_metrics.get(word, {'tfidf_comp': [], 'bm25_comp': [], 'count_comp': [], 'count_sites': 0})
        my_m = my_body_metrics.get(word, {})

        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–º–µ–¥–∏–∞–Ω—ã)
        tfidf_top_median = np.median(comp_data_word['tfidf_comp']) if comp_data_word['tfidf_comp'] else 0
        bm25_top_median = np.median(comp_data_word['bm25_comp']) if comp_data_word['bm25_comp'] else 0
        count_top_avg = np.mean(comp_data_word['count_comp']) if comp_data_word['count_comp'] else 0
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞—à–µ–≥–æ —Å–∞–π—Ç–∞
        tfidf_my = my_m.get('tfidf', 0)
        bm25_my = my_m.get('bm25', 0)
        idf_val = my_m.get('idf', comp_data_word.get('idf', 0)) # –ë–µ—Ä–µ–º IDF –∏–∑ –Ω–∞—à–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è –∏–ª–∏ –∏–∑ IDF –±–∞–∑—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        count_my = my_m.get('count', 0)
        
        # –ê–Ω–∫–æ—Ä–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä—ã
        anchor_my = my_anchor_freqs.get(word, 0)
        anchor_top_avg = 0 # –ê–Ω–∫–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è

        # 4.1. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –†–∞—Å—á–µ—Ç—ã
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —á–∞—Å—Ç–æ—Ç–µ —É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        if comp_data_word['count_sites'] < settings['min_sites']:
             continue

        # –õ–æ–≥–∏–∫–∞ –¥–ª—è –ø–æ–¥—Å–≤–µ—Ç–∫–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        # 1. –ü–æ–≤—Ç–æ—Ä—ã –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
        if count_my == 0 and count_top_avg > 0:
            rec_text = f"–î–æ–±–∞–≤–∏—Ç—å {math.ceil(count_top_avg):.0f} (avg) - {comp_data_word['count_sites']}/{N_comps} —Å–∞–π—Ç–æ–≤"
        elif count_my > count_top_avg * settings['max_spam_factor'] and count_top_avg > 0:
            rec_text = f"–£–±—Ä–∞—Ç—å {math.ceil(count_my - count_top_avg):.0f} (spam)"
        else:
            rec_text = "OK"

        # 2. –ü–æ–≤—Ç–æ—Ä—ã –≤ –∞–Ω–∫–æ—Ä–∞—Ö
        if anchor_my == 0 and anchor_top_avg > 0:
             rec_anchor = f"–î–æ–±–∞–≤–∏—Ç—å {math.ceil(anchor_top_avg):.0f} (avg)"
        elif anchor_my > anchor_top_avg * settings['max_spam_factor'] and anchor_top_avg > 0:
             rec_anchor = f"–£–±—Ä–∞—Ç—å {math.ceil(anchor_my - anchor_top_avg):.0f} (spam)"
        else:
             rec_anchor = "OK"
             
        # 3. –û–±—â–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è (–Ω–∞ –æ—Å–Ω–æ–≤–µ BM25)
        # –£—Å–∏–ª–∏—Ç—å BM25, –µ—Å–ª–∏ —Å–∏–ª—å–Ω–æ –æ—Ç—Å—Ç–∞–µ—Ç, –∏ —Å–ª–æ–≤–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ —É –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞
        if (bm25_my < bm25_top_median * 0.5) and (comp_data_word['count_sites'] >= N_comps * 0.5):
            rec_total = f"–î–æ–±–∞–≤–∏—Ç—å (BM25: {bm25_my:.2f} < {bm25_top_median:.2f})"
        # –£–±—Ä–∞—Ç—å, –µ—Å–ª–∏ —Å–∏–ª—å–Ω–æ –ø–µ—Ä–µ—Å–ø–∞–º–ª–µ–Ω–æ –ø–æ BM25
        elif bm25_my > bm25_top_median * settings['max_spam_factor'] and bm25_top_median > 0:
            rec_total = f"–£–±—Ä–∞—Ç—å (BM25: {bm25_my:.2f} > {bm25_top_median:.2f})"
        else:
            rec_total = "OK"

        
        final_data.append({
            '–°–ª–æ–≤–æ': word,
            'TF-IDF –¢–û–ü': tfidf_top_median,
            'TF-IDF –≤–∞—à —Å–∞–π—Ç': tfidf_my,
            'BM25 –¢–û–ü': bm25_top_median,
            'BM25 –≤–∞—à —Å–∞–π—Ç': bm25_my,
            'IDF': idf_val,
            '–ö–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤': comp_data_word['count_sites'],
            '–ú–µ–¥–∏–∞–Ω–∞': np.median(comp_data_word['count_comp']) if comp_data_word['count_comp'] else 0, # –ú–µ–¥–∏–∞–Ω–∞ –ø–æ–≤—Ç–æ—Ä–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
            '–ü–µ—Ä–µ—Å–ø–∞–º': rec_total, # –ò—Ç–æ–≥–æ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
            '–°—Ä–µ–¥–Ω–µ–µ –ø–æ –¢–û–ü—É (–ø–æ–≤—Ç.)': count_top_avg,
            '–í–∞—à —Å–∞–π—Ç (–ø–æ–≤—Ç.)': count_my,
            '<a/> –ø–æ –¢–û–ü—É (–ø–æ–≤—Ç.)': anchor_top_avg,
            '<a/> –≤–∞—à —Å–∞–π—Ç (–ø–æ–≤—Ç.)': anchor_my,
            '–¢–µ–∫—Å—Ç –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å': rec_text,
            '–¢–µ–≥ A –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å': rec_anchor,
        })


    df = pd.DataFrame(final_data)
    
    # 5. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
    
    if df.empty:
        return None
        
    numeric_cols = [
        'TF-IDF –¢–û–ü', 'TF-IDF –≤–∞—à —Å–∞–π—Ç', 'BM25 –¢–û–ü', 'BM25 –≤–∞—à —Å–∞–π—Ç', 'IDF', 
        '–ú–µ–¥–∏–∞–Ω–∞', '–°—Ä–µ–¥–Ω–µ–µ –ø–æ –¢–û–ü—É (–ø–æ–≤—Ç.)', '–í–∞—à —Å–∞–π—Ç (–ø–æ–≤—Ç.)', '<a/> –ø–æ –¢–û–ü—É (–ø–æ–≤—Ç.)', '<a/> –≤–∞—à —Å–∞–π—Ç (–ø–æ–≤—Ç.)'
    ]
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)


    df = df[df['TF-IDF –¢–û–ü'] > 0]
    
    df['–†–∞–∑–Ω–∏—Ü–∞ BM25'] = df['BM25 –¢–û–ü'] - df['BM25 –≤–∞—à —Å–∞–π—Ç']
    
    df_sorted = df.sort_values(
        by=['BM25 –¢–û–ü', '–†–∞–∑–Ω–∏—Ü–∞ BM25'], 
        ascending=[False, False]
    ).drop(columns=['–†–∞–∑–Ω–∏—Ü–∞ BM25'])

    for col in numeric_cols:
        df_sorted[col] = df_sorted[col].round(3)
        
    return df_sorted


# ==========================================
# 3. –ò–ù–¢–ï–†–§–ï–ô–° STREAMLIT
# ==========================================

st.title("üíé –ì–∏–±—Ä–∏–¥–Ω—ã–π –ê–Ω–∞–ª–∏–∑ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ PRO")
st.markdown("""
    –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π SEO-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ (TF-IDF, BM25) 
    –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏ –∏–∑ –¢–û–ü–∞.
""")

# --- 3.1. –ë–õ–û–ö –í–í–û–î–ê –î–ê–ù–ù–´–• ---
with st.container(border=True):
    col1, col2 = st.columns([3, 1])

    with col1:
        my_url = st.text_input("üöÄ –í–∞—à URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", 
                               placeholder="https://vash-site.ru/stranitsa",
                               help="–í–≤–µ–¥–∏—Ç–µ URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∫–æ—Ç–æ—Ä—É—é –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å.")
    with col2:
        mode = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤", ["Google –¢–û–ü", "–í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é"], 
                        index=0, horizontal=True)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    st.markdown("---")
    st.subheader("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    
    col_set1, col_set2, col_set3 = st.columns(3)

    with col_set1:
        query = st.text_input("–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–¥–ª—è Google –¢–û–ü–∞)", 
                              placeholder="–∫—É–ø–∏—Ç—å –¥–æ–º –≤ –º–æ—Å–∫–≤–µ",
                              disabled=(mode != "Google –¢–û–ü"))
        
        min_sites = st.slider("–ú–∏–Ω. –∫–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤ –≤ –¢–û–ü–µ –¥–ª—è —Å–ª–æ–≤–∞", 
                              min_value=1, max_value=10, value=2, step=1,
                              help="–°–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è –º–∏–Ω–∏–º—É–º –Ω–∞ N —Å–∞–π—Ç–∞—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –ø–æ–ø–∞—Å—Ç—å –≤ –∞–Ω–∞–ª–∏–∑.")

    with col_set2:
        top_n = st.slider("–ö–æ–ª-–≤–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –∏–∑ –¢–û–ü–∞", 
                          min_value=5, max_value=20, value=10, step=1,
                          disabled=(mode != "Google –¢–û–ü"),
                          help="–°–∫–æ–ª—å–∫–æ —Å–∞–π—Ç–æ–≤ –∏–∑ –¢–û–ü–∞ Google —É—á–∏—Ç—ã–≤–∞—Ç—å –≤ —Ä–∞—Å—á–µ—Ç–µ.")
        
        max_spam_factor = st.slider("–ö–æ—ç—Ñ. –ø–µ—Ä–µ—Å–ø–∞–º–∞", 
                                    min_value=1.0, max_value=5.0, value=2.0, step=0.1,
                                    help="–í–æ —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤ –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ –¥–æ–ª–∂–Ω–æ –ø—Ä–µ–≤—ã—à–∞—Ç—å —Å—Ä–µ–¥–Ω–µ–µ –¢–û–ü–∞, —á—Ç–æ–±—ã —Å—á–∏—Ç–∞—Ç—å —ç—Ç–æ –ø–µ—Ä–µ—Å–ø–∞–º–æ–º.")

    with col_set3:
        excludes = st.text_area("–°–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π (–ø–æ –¥–æ–º–µ–Ω—É/URL)", 
                                placeholder="yandex.ru\nwikipedia.org\nprofi.ru", height=100,
                                help="–£–∫–∞–∂–∏—Ç–µ —á–∞—Å—Ç–∏ URL –∏–ª–∏ –¥–æ–º–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –∏—Å–∫–ª—é—á–∏—Ç—å –∏–∑ —Å–ø–∏—Å–∫–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.")
        
        st.caption("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã BM25 (k1=1.2, b=0.75)")


    if mode == "–í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é":
        manual_urls = st.text_area("–°–ø–∏—Å–æ–∫ URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–∫–∞–∂–¥—ã–π —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", 
                                   placeholder="https://comp1.ru/page\nhttps://comp2.ru/page\n...", 
                                   height=150)
    else:
        manual_urls = ""

# --- 3.2. –ë–õ–û–ö –ó–ê–ü–£–°–ö–ê –ò –ê–ù–ê–õ–ò–ó–ê ---

if st.button("üìà –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑", type="primary", use_container_width=True):
    
    if not my_url:
        st.error("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ –í–∞—à URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")
        st.stop()
        
    if mode == "Google –¢–û–ü" and not query:
        st.error("‚ö†Ô∏è –î–ª—è –ø–æ–∏—Å–∫–∞ –Ω—É–∂–µ–Ω –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å!")
        st.stop()
        
    if mode == "–í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é" and not manual_urls:
        st.error("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ —Å–ø–∏—Å–æ–∫ URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤—Ä—É—á–Ω—É—é!")
        st.stop()


    st.markdown("---")
    st.subheader("üîç –ü—Ä–æ—Ü–µ—Å—Å –∞–Ω–∞–ª–∏–∑–∞")

    settings = {
        'exclude': [x.strip() for x in excludes.split() if x.strip()],
        'min_sites': min_sites,
        'max_spam_factor': max_spam_factor,
        'bm25_k1': 1.2,
        'bm25_b': 0.75,
    }
    
    # 1. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    comps = []
    
    if mode == "Google –¢–û–ü":
        if not USE_SEARCH:
            st.error("‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ googlesearch-python –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä—É—á–Ω–æ–π –≤–≤–æ–¥.")
            st.stop()
            
        with st.spinner(f"–ò—â–µ–º {top_n} –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}' –≤ Google..."):
            try:
                excl_list = settings['exclude']
                # –ò—â–µ–º –±–æ–ª—å—à–µ, —á—Ç–æ–±—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –Ω–µ–Ω—É–∂–Ω—ã–µ
                found = search(query, num_results=top_n * 2, lang="ru")
                count = 0
                for u in found:
                    if u == my_url: continue 
                    if any(x in u for x in excl_list): continue 
                    comps.append(u)
                    count += 1
                    if count >= top_n: break
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ Google: {e}. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫.")
                st.stop()
    else:
        if manual_urls:
            comps_raw = [u.strip() for u in manual_urls.split('\n') if u.strip()]
            for u in comps_raw:
                if u == my_url: continue
                if any(x in u for x in settings['exclude']): continue
                comps.append(u)
        
    if not comps:
        st.error("‚ùå –°–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ.")
        st.stop()
    else:
        st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã ({len(comps)} URL):")
        st.dataframe(pd.DataFrame({'URL': comps}), use_container_width=True, height=200)

        # 2. –ó–ê–ü–£–°–ö –ë–≠–ö–ï–ù–î–ê
        with st.spinner("üöÄ –ó–∞–ø—É—Å–∫ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞..."):
            df_res = run_analysis(my_url, comps, settings)
        
        if df_res is not None and not df_res.empty:
            st.markdown("### üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞")
            
            # --- –õ–æ–≥–∏–∫–∞ –¥–ª—è –ø–æ–¥—Å–≤–µ—Ç–∫–∏ —Å—Ç—Ä–æ–∫ ---
            def highlight_rec(val):
                val_str = str(val)
                # –ó–µ–ª–µ–Ω—ã–π - –¥–ª—è "–î–æ–±–∞–≤–∏—Ç—å"
                if "–î–æ–±–∞–≤–∏—Ç—å" in val_str: 
                    return 'color: #166534; font-weight: bold; background-color: #DCFCE7' 
                # –ö—Ä–∞—Å–Ω—ã–π - –¥–ª—è "–£–±—Ä–∞—Ç—å"
                if "–£–±—Ä–∞—Ç—å" in val_str: 
                    return 'color: #991B1B; font-weight: bold; background-color: #FEE2E2' 
                return ''
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–∏–ª–∏ –∫ –∫–æ–ª–æ–Ω–∫–∞–º —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
            styled_df = df_res.style.applymap(highlight_rec, subset=['–ü–µ—Ä–µ—Å–ø–∞–º', '–¢–µ–∫—Å—Ç –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å', '–¢–µ–≥ A –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å'])
            
            st.dataframe(styled_df, use_container_width=True)
            
            csv = df_res.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV",
                data=csv,
                file_name=f'seo_relevance_analysis_{urlparse(my_url).netloc}.csv',
                mime='text/csv',
                type="secondary"
            )

        else:
            st.warning("‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑ –Ω–µ –¥–∞–ª –∑–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –í–∞—à URL –∏ —Å–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.")
