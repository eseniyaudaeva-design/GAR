import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin
import inspect
import time
import json
import os # –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –µ—Å–ª–∏ –Ω–µ –±—ã–ª–æ

# ==========================================
# 0. –ü–ê–¢–ß –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò (–î–ª—è NLP)
# ==========================================
if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO", page_icon="üìä")

# ==========================================
# 2. –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø
# ==========================================
def check_password():
    if st.session_state.get("authenticated"):
        return True
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown("""
            <style>
            .auth-container {
                display: flex; flex-direction: column; align-items: center;
                justify-content: center; padding: 2rem; background-color: white;
                border-radius: 10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                margin-top: 5rem;
            }
            </style>
            <div class="auth-container">
                <h3>üìä GAR PRO</h3>
                <h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3>
            </div>
        """, unsafe_allow_html=True)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª—é—á –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —Å –¥—Ä—É–≥–∏–º–∏ –∏–Ω–ø—É—Ç–∞–º–∏
        if 'password_input_auth' not in st.session_state:
            st.session_state.password_input_auth = ""
            
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input_auth", label_visibility="collapsed")
        
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "jfV6Xel-Q7vp-_s2UYPO":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

# ==========================================
# 3. –ù–ê–°–¢–†–û–ô–ö–ò API –ò –†–ï–ì–ò–û–ù–û–í
# ==========================================
# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω –∞–∫—Ç—É–∞–ª–µ–Ω!
ARSENKIN_TOKEN = "43acbbb60cb7989c05914ff21be45379"

# –°–ª–æ–≤–∞—Ä—å —Ä–µ–≥–∏–æ–Ω–æ–≤ (–ù–∞–∑–≤–∞–Ω–∏–µ -> {yandex_id, google_id})
REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956},
    "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931},
    "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–ö–∏–µ–≤ (UA)": {"ya": 143, "go": 1012852},
    "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493},
    "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601}
}

DEFAULT_EXCLUDE_DOMAINS = [
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "ebay.com",
    "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", "pandao.ru",
    "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", "banki.ru", 
    "regmarkets.ru", "zoon.ru", "pulscen.ru", "prodoctorov.ru", "blizko.ru", 
    "domclick.ru", "satom.ru", "quto.ru", "edadeal.ru", "cataloxy.ru", 
    "irr.ru", "onliner.by", "shop.by", "deal.by", "yell.ru", "profi.ru", 
    "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by", "market.yandex.ru", 
    "youtube.com", "gosuslugi.ru", "dzen.ru", "2gis.by", "wildberries.ru", 
    "rutube.ru", "vk.com", "facebook.com", "lemanapro.ru" # <-- –î–û–ë–ê–í–õ–ï–ù–û –ü–û –ó–ê–ü–†–û–°–£
]
DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n–∫—É–ø–∏—Ç—å\n—Ü–µ–Ω–∞\n—à—Ç\n—Å–º\n–º–º\n–∫–≥\n–∫–≤\n–º2\n—Å—Ç—Ä\n—É–ª"

# –¶–≤–µ—Ç–∞
PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE" 

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif;
        color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important;
        border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important;
        }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important;
            color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important;
        border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important;
            color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] div[role="gridcell"] {{
            background-color: #FFFFFF !important;
            color: {TEXT_COLOR} !important; border-bottom: 1px solid {ROW_BORDER_COLOR} !important;
        }}
        .legend-box {{ padding: 10px;
        background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px;
        }}
        .text-red {{ color: #D32F2F; font-weight: bold;
        }}
        .text-bold {{ font-weight: 600;
        }}
        .sort-container {{ background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 8px; margin-bottom: 10px;
        border: 1px solid {BORDER_COLOR}; }}
        section[data-testid="stSidebar"] {{ background-color: #FFFFFF !important;
        border-left: 1px solid {BORDER_COLOR} !important; }}
        
        /* –°—Ç–∏–ª—å –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ –ø—Ä–æ–≤–µ—Ä–æ–∫ - –≤—ã–¥–µ–ª–µ–Ω–∏–µ (–í–∫–ª–∞–¥–∫–∞) */
        .stTabs [data-baseweb="tab-list"] button:nth-child(2) {{
            background-color: #ffe0b2; /* –°–≤–µ—Ç–ª–æ-–æ—Ä–∞–Ω–∂–µ–≤—ã–π —Ñ–æ–Ω */
            font-weight: bold;
        }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 4. –õ–û–ì–ò–ö–ê (–ë–≠–ö–ï–ù–î)
# ==========================================

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP
try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except Exception as e:
    morph = None
    USE_NLP = False
    # st.sidebar.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ NLP: {e}") # –£–±—Ä–∞–Ω–æ, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å, –µ—Å–ª–∏ –Ω–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏

if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False
if 'history' not in st.session_state: # <-- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏
    st.session_state.history = []
if 'comp_table_data' not in st.session_state: # <-- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã —Å—Ç–∞—Ç—É—Å–∞
    st.session_state.comp_table_data = []


# --- –§–£–ù–ö–¶–ò–Ø –†–ê–ë–û–¢–´ –° API ARSENKIN (–û—Å—Ç–∞–≤–ª–µ–Ω–æ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---
def get_arsenkin_urls(query, engine_type, region_name, depth_val=10):
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check" 
    url_get = "https://arsenkin.ru/api/tools/get"    
    
    # ... (–û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ get_arsenkin_urls –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
    headers = {
        "Authorization": f"Bearer {ARSENKIN_TOKEN}",
        "Content-type": "application/json"
    }
    
    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []
    
    if "–Ø–Ω–¥–µ–∫—Å" in engine_type:
        se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type:
        se_params.append({"type": 11, "region": reg_ids['go']})
        
    payload = {
        "tools_name": "check-top",
        "data": {
            "queries": [query],
            "is_snippet": False,
            "noreask": True,
            "se": se_params,
            "depth": depth_val
        }
    }
    
    # 1. –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏
    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        resp_json = r.json()
        
        if "error" in resp_json or "task_id" not in resp_json:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ API (—Å—Ç–∞—Ä—Ç): {resp_json}")
            return []
            
        task_id = resp_json["task_id"]
        st.toast(f"–ó–∞–¥–∞—á–∞ ID {task_id} –∑–∞–ø—É—â–µ–Ω–∞")
        
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–µ –∑–∞–¥–∞—á–∏: {e}")
        return []
    
    # 2. –û–∂–∏–¥–∞–Ω–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ (—á–µ—Ä–µ–∑ /check)
    status = "process"
    attempts = 0
    max_attempts = 40 
    
    progress_info = st.empty()
    bar = st.progress(0)
    res_check_data = {}
    
    while status == "process" and attempts < max_attempts:
        time.sleep(5) 
        attempts += 1
        bar.progress(attempts / max_attempts)
        progress_info.text(f"–û–∂–∏–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ API... ({attempts*5} —Å–µ–∫)")
        
        try:
            r_check = requests.post(url_check, headers=headers, json={"task_id": task_id})
            res_check_data = r_check.json()
            
            if res_check_data.get("status") == "finish":
                status = "done"
                break
            
            if str(res_check_data.get("code")) == "429":
                continue 
                    
        except Exception:
            pass 
            
    bar.empty()
    progress_info.empty()
        
    if status != "done":
        st.error(f"‚è≥ –í—Ä–µ–º—è –≤—ã—à–ª–æ. –°—Ç–∞—Ç—É—Å: {res_check_data.get('status', 'Unknown')}")
        st.write("JSON-–æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å):")
        st.json(res_check_data)
        return []
        
    # 3. –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (—á–µ—Ä–µ–∑ /get)
    res_data = {}
    try:
        st.info("–°—Ç–∞—Ç—É—Å 'finish' –ø–æ–ª—É—á–µ–Ω. –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç...")
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id}, timeout=30)
        res_data = r_final.json()
      
        if res_data.get("code") != "TASK_RESULT":
            st.error(f"‚ùå –û—à–∏–±–∫–∞: API –Ω–µ –≤–µ—Ä–Ω—É–ª —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (TASK_RESULT).")
            st.write("JSON-–æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞:")
            st.json(res_data)
            return []
          
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
        st.write("JSON-–æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞:")
        st.json(res_data)
        return []

    # 4. –§–ò–ù–ê–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì: 
    results_list = []
    try:
        if 'result' in res_data and 'result' in res_data['result'] and 'collect' in res_data['result']['result']:
            collect = res_data['result']['result']['collect']
        else:
            st.error("‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ 'collect' –≤ –æ—Ç–≤–µ—Ç–µ API.")
            st.json(res_data)
            return []

        final_url_list = []
        
        if collect and isinstance(collect, list) and len(collect) > 0 and \
           collect[0] and isinstance(collect[0], list) and len(collect[0]) > 0 and \
           collect[0][0] and isinstance(collect[0][0], list):
             
            final_url_list = collect[0][0]
        else:
             unique_urls = set()
             for engine_data in collect:
                if isinstance(engine_data, dict):
                    for engine_id, serps in engine_data.items():
                        if isinstance(serps, list):
                            for item in serps:
                                url = item.get('url')
                                pos = item.get('pos')
                                
                                if url and pos:
                                    if url not in unique_urls:
                                        results_list.append({'url': url, 'pos': pos})
                                    unique_urls.add(url)
                                    else:
                                        for res in results_list:
                                            if res['url'] == url and pos < res['pos']:
                                                res['pos'] = pos
                            return results_list 

        if final_url_list:
            for index, url in enumerate(final_url_list):
                pos = index + 1
                results_list.append({'url': url, 'pos': pos})

    except Exception as e:
        st.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ JSON-–æ—Ç–≤–µ—Ç–∞: {e}")
        st.write("JSON, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å:")
        st.json(res_data) 
        return []
        
    return results_list


def process_text_detailed(text, settings, n_gram=1):
    if settings['numbers']:
        pattern = r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+' 
    else:
        pattern = r'[–∞-—è–ê-–Ø—ë–Åa-zA-Z]+'
        
    words = re.findall(pattern, text.lower())
    stops = set(w.lower() for w in settings['custom_stops'])
    
    lemmas = []
    forms_map = defaultdict(set)
    
    for w in words:
        if len(w) < 2: continue
        if w in stops: continue
        
        lemma = w
        if USE_NLP and n_gram == 1: 
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            lemma = p.normal_form
        
        lemmas.append(lemma)
        forms_map[lemma].add(w)
    
    if n_gram > 1:
        ngrams = []
        for i in range(len(lemmas) - n_gram + 1):
            phrase = " ".join(lemmas[i:i+n_gram])
            ngrams.append(phrase)
        return ngrams, {}
        
    return lemmas, forms_map

# --- –£–°–ò–õ–ï–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì (–ó–∞–ø—Ä–æ—Å 1) ---
def parse_page_robust(url, settings, retries=3, timeout=30):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
    headers = {'User-Agent': settings['ua']}
    for attempt in range(retries):
        try:
            r = requests.get(url, headers=headers, timeout=timeout)
            r.raise_for_status() # –í—ã–∑—ã–≤–∞–µ—Ç HTTPError, –µ—Å–ª–∏ —Å—Ç–∞—Ç—É—Å 4xx –∏–ª–∏ 5xx
            
            # –ï—Å–ª–∏ 200 OK, –ø—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å
            soup = BeautifulSoup(r.text, 'html.parser')
            
            tags_to_remove = ['script', 'style', 'head']
            if settings['noindex']:
                tags_to_remove.extend(['noindex', 'nav', 'footer', 'header', 'aside'])
            
            comments = soup.find_all(string=lambda text: isinstance(text, Comment))
            for c in comments: c.extract()
            for t in soup.find_all(tags_to_remove): t.decompose()
                
            anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
            anchor_text = " ".join(anchors_list)
            
            extra_text = []
            if settings['alt_title']:
                for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
                for t in soup.find_all(title=True): extra_text.append(t['title'])
                
            body_text_raw = soup.get_text(separator=' ') + " " + " ".join(extra_text)
            body_text = re.sub(r'\s+', ' ', body_text_raw).strip()
            
            if not body_text:
                return {'url': url, 'domain': urlparse(url).netloc, 'body_text': '', 'anchor_text': '', 'error': '–ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏'}
            
            return {'url': url, 'domain': urlparse(url).netloc, 'body_text': body_text, 'anchor_text': anchor_text, 'error': None}
        
        except requests.exceptions.HTTPError as e:
            error_msg = f"HTTP –û—à–∏–±–∫–∞: {e.response.status_code}"
            if attempt == retries - 1:
                return {'url': url, 'domain': urlparse(url).netloc, 'body_text': '', 'anchor_text': '', 'error': error_msg}
        except requests.exceptions.RequestException as e:
            error_msg = f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏/—Ç–∞–π–º–∞—É—Ç: {e.__class__.__name__}"
            if attempt == retries - 1:
                return {'url': url, 'domain': urlparse(url).netloc, 'body_text': '', 'anchor_text': '', 'error': error_msg}
        except Exception as e:
            error_msg = f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {e}"
            if attempt == retries - 1:
                return {'url': url, 'domain': urlparse(url).netloc, 'body_text': '', 'anchor_text': '', 'error': error_msg}
        
        time.sleep(2 ** attempt) 
    
    return {'url': url, 'domain': urlparse(url).netloc, 'body_text': '', 'anchor_text': '', 'error': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫'}

# –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è parse_page —É–¥–∞–ª–µ–Ω–∞/–∑–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ parse_page_robust

def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    all_forms_map = defaultdict(set)
    
    # 1. –í–∞—à —Å–∞–π—Ç
    if not my_data or not my_data.get('body_text'):
        my_lemmas, my_forms, my_anchors, my_len = [], {}, [], 0
    else:
        my_lemmas, my_forms = process_text_detailed(my_data['body_text'], settings)
        my_anchors, _ = process_text_detailed(my_data['anchor_text'], settings)
        my_len = len(my_lemmas)
        for k, v in my_forms.items():
            all_forms_map[k].update(v)

    # 2. –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã (—Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã–µ)
    comp_docs = []
    for p in comp_data_full:
        body, c_forms = process_text_detailed(p['body_text'], settings)
        anchor, _ = process_text_detailed(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor, 'url': p['url'], 'pos': p['pos']})
        for k, v in c_forms.items():
            all_forms_map[k].update(v)
    
    if not comp_docs:
        # –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –Ω–∞–º –Ω—É–∂–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å, –∫—Ç–æ –±—ã–ª –≤ –¢–û–ü–µ
        table_rel_fallback = []
        for item in original_results:
            domain = urlparse(item['url']).netloc
            table_rel_fallback.append({
                "–î–æ–º–µ–Ω": domain, 
                "–ü–æ–∑–∏—Ü–∏—è": item['pos'],
                "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": 0, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": 0
            })
        
        if my_data and my_data.get('domain'):
            my_label = f"{my_data['domain']} (–í—ã)"
        else:
            my_label = "–í–∞—à —Å–∞–π—Ç"
        
        table_rel_fallback.append({
            "–î–æ–º–µ–Ω": my_label, 
            "–ü–æ–∑–∏—Ü–∏—è": my_serp_pos if my_serp_pos > 0 else len(original_results) + 1,
            "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": 0, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": 0
        })
        
        table_rel_df = pd.DataFrame(table_rel_fallback).sort_values(by='–ü–æ–∑–∏—Ü–∏—è', ascending=True).reset_index(drop=True)
        
        return {"depth": pd.DataFrame(), "hybrid": pd.DataFrame(), "ngrams": pd.DataFrame(), "relevance_top": table_rel_df, "my_score": {"width": 0, "depth": 0}}

    # –î–∞–ª—å—à–µ —Ä–∞—Å—á–µ—Ç—ã –∏–¥—É—Ç —Ç–æ–ª—å–∫–æ –ø–æ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã–º comp_docs
    avg_len = np.mean([len(d['body']) for d in comp_docs])
    norm_k = (my_len / avg_len) if (settings['norm'] and my_len > 0 and avg_len > 0) else 1.0
    
    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    N = len(comp_docs) 
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
        
    table_depth, table_hybrid = [], []
    for word in vocab:
        df = doc_freqs[word]
        if df < 2 and word not in my_lemmas: continue 
        
        my_tf_total = my_lemmas.count(word)        
        my_tf_anchor = my_anchors.count(word)      
        my_tf_text = max(0, my_tf_total - my_tf_anchor) 
        
        forms_set = all_forms_map.get(word, set())
        forms_str = ", ".join(sorted(list(forms_set))) if forms_set else word
        
        c_total_tfs = [d['body'].count(word) for d in comp_docs]
        c_anchor_tfs = [d['anchor'].count(word) for d in comp_docs]
        
        sum_in_top = sum(c_total_tfs)
        mean_total = np.mean(c_total_tfs)
        med_total = np.median(c_total_tfs)
        max_total = np.max(c_total_tfs)

        med_anchor = np.median(c_anchor_tfs)
        
        rec_min = int(round(min(mean_total, med_total) * norm_k))
        rec_max = int(round(max_total * norm_k))
        rec_anchor = int(round(med_anchor * norm_k)) 
        
        diff_total = 0
        if my_tf_total < rec_min: diff_total = rec_min - my_tf_total 
        elif my_tf_total > rec_max: diff_total = rec_max - my_tf_total 
        
        diff_anchor = rec_anchor - my_tf_anchor
        rec_text_min = max(0, rec_min - rec_anchor)
        rec_text_max = max(0, rec_max - rec_anchor)
        diff_text = 0
        if my_tf_text < rec_text_min: diff_text = rec_text_min - my_tf_text
        elif my_tf_text > rec_text_max: diff_text = rec_text_max - my_tf_text

        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        idf = max(0.1, idf) 
        spam_percent = 0
        if my_tf_total > rec_max and rec_max > 0:
            spam_percent = round(((my_tf_total - rec_max) / rec_max) * 100, 1)
        elif my_tf_total > 0 and rec_max == 0:
            spam_percent = 100 
        spam_idf = round(spam_percent * idf, 1)
        abs_diff = abs(diff_total)

        if med_total > 0.5 or my_tf_total > 0:
            table_depth.append({
                "–°–ª–æ–≤–æ": word, "–°–ª–æ–≤–æ—Ñ–æ—Ä–º—ã": forms_str, "–ü–æ–≤—Ç–æ—Ä—ã —É –≤–∞—Å": my_tf_total,
                "–ü–æ–≤—Ç–æ—Ä–æ–≤ –≤ –¢–û–ü–µ": sum_in_top, "–ú–∏–Ω–∏–º—É–º (—Ä–µ–∫)": rec_min, "–ú–∞–∫—Å–∏–º—É–º (—Ä–µ–∫)": rec_max,
                "–î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": diff_total, "–¢–µ–≥ A —É –≤–∞—Å": my_tf_anchor, "–¢–µ–≥ A (—Ä–µ–∫)": rec_anchor,
                "–¢–µ–≥ A +/-": diff_anchor, "–¢–µ–∫—Å—Ç —É –≤–∞—Å": my_tf_text, "–¢–µ–∫—Å—Ç (—Ä–µ–∫)": rec_text_min,
                "–¢–µ–∫—Å—Ç +/-": diff_text, "–ü–µ—Ä–µ—Å–ø–∞–º %": spam_percent, "–ü–µ—Ä–µ—Å–ø–∞–º*IDF": spam_idf,
                "diff_abs": abs_diff, "is_missing": (my_tf_total == 0)
            })
            table_hybrid.append({
                "–°–ª–æ–≤–æ": word, "TF-IDF –¢–û–ü": round(med_total * idf, 2), "TF-IDF —É –≤–∞—Å": round(my_tf_total * idf, 2),
                "–°–∞–π—Ç–æ–≤": df, "–ü–µ—Ä–µ—Å–ø–∞–º": max_total
            })

    # --- N-–≥—Ä–∞–º–º—ã (–§—Ä–∞–∑—ã) - –ò–°–ü–†–ê–í–õ–ï–ù–û (–ó–∞–ø—Ä–æ—Å 3) ---
    table_ngrams = []
    if comp_docs and my_data:
        try:
            N_GRAM = 2
            my_ngrams, _ = process_text_detailed(my_data['body_text'], settings, N_GRAM)
            comp_ngrams_list = [process_text_detailed(p['body_text'], settings, N_GRAM)[0] for p in comp_docs]
            
            all_ngrams = set(my_ngrams)
            for c in comp_ngrams_list: all_ngrams.update(c)
            
            ngram_doc_freqs = Counter()
            for c in comp_ngrams_list: 
                for ng in set(c): ngram_doc_freqs[ng] += 1
                
            for ng in all_ngrams:
                df = ngram_doc_freqs[ng]
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –∫–∞–∫ –º–∏–Ω–∏–º—É–º 2 —Å–∞–π—Ç–∞ –≤ –¢–û–ü–µ –∏–ª–∏ –µ—Å—Ç—å —É –Ω–∞—Å
                if df < 2 and ng not in my_ngrams: continue
                
                my_c = my_ngrams.count(ng)
                comp_c = [c.count(ng) for c in comp_ngrams_list]
                
                sum_in_top = sum(comp_c)
                
                med_c = np.median(comp_c) if comp_c else 0
                max_c = np.max(comp_c) if comp_c else 0
                
                rec_min = int(round(med_c * norm_k))
                rec_max = int(round(max_c * norm_k))
                
                diff_ngram = 0
                if my_c < rec_min: diff_ngram = rec_min - my_c
                elif my_c > rec_max: diff_ngram = rec_max - my_c
                
                is_missing = (my_c == 0)
                
                if sum_in_top > 0 or my_c > 0:
                    table_ngrams.append({
                        "–°–ª–æ–≤–æ/–§—Ä–∞–∑–∞": ng, 
                        "–ß–∞—Å—Ç–æ—Ç–∞ (–°—É–º–º–∞)": sum_in_top,
                        "–ú–∏–Ω. (—Ä–µ–∫)": rec_min, 
                        "–ú–∞–∫—Å. (—Ä–µ–∫)": rec_max,
                        "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_c,
                        "–î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": diff_ngram,
                        "–°–∞–π—Ç–æ–≤": df,
                        "is_missing": is_missing
                    })
        except Exception as e:
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Ä–∞—Å—á–µ—Ç n-–≥—Ä–∞–º–º –Ω–µ —É–¥–∞–ª—Å—è
            st.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ N-–≥—Ä–∞–º–º: {e}") 
            table_ngrams = []


    # 3. –†–∞—Å—á–µ—Ç —à–∏—Ä–∏–Ω—ã –∏ –≥–ª—É–±–∏–Ω—ã (–±–∞–ª–ª—ã)
    competitor_stats_raw = []
    
    # –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ–ª—å–∫–æ –ø–æ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã–º
    for p in comp_docs: 
        p_lemmas = p['body']
        domain = p['domain']
        pos = p['pos']
        
        relevant_lemmas = [w for w in p_lemmas if w in vocab]
        raw_width = len(set(relevant_lemmas))
        raw_depth = len(relevant_lemmas)
        competitor_stats_raw.append({
            "domain": domain, "pos": pos, "raw_w": raw_width, "raw_d": raw_depth
        })

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–∫—Å–∏–º—É–º—ã —Ç–æ–ª—å–∫–æ –ø–æ **—É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã–º –∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º** –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º
    max_width_top = max([c['raw_w'] for c in competitor_stats_raw]) if competitor_stats_raw else 1
    max_depth_top = max([c['raw_d'] for c in competitor_stats_raw]) if competitor_stats_raw else 1
    
    table_rel = []
    
    # 3.1. –ë–∞–ª–ª—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (—Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ –≤—Å–µ–º, –∫—Ç–æ –±—ã–ª –≤ original_results)
    for c in competitor_stats_raw:
        score_w = int(round((c['raw_w'] / max_width_top) * 100))
        score_d = int(round((c['raw_d'] / max_depth_top) * 100))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–¢–û–ü)
        table_rel.append({
            "–î–æ–º–µ–Ω": c['domain'], "–ü–æ–∑–∏—Ü–∏—è": c['pos'],
            "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": score_w, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": score_d
        })
        
    # 3.2. –ë–∞–ª–ª—ã –¥–ª—è –í–ê–®–ï–ì–û —Å–∞–π—Ç–∞
    my_relevant = [w for w in my_lemmas if w in vocab]
    my_raw_w = len(set(my_relevant))
    my_raw_d = len(my_relevant)
    my_score_w = int(round((my_raw_w / max_width_top) * 100))
    my_score_d = int(round((my_raw_d / max_depth_top) * 100))

    # –î–æ–±–∞–≤–ª—è–µ–º –í–ê–® —Å–∞–π—Ç –≤ —Ç–∞–±–ª–∏—Ü—É
    if my_data and my_data.get('domain'):
        my_label = f"{my_data['domain']} (–í—ã)"
    else:
        my_label = "–í–∞—à —Å–∞–π—Ç"
        
    table_rel.append({
        "–î–æ–º–µ–Ω": my_label, "–ü–æ–∑–∏—Ü–∏—è": my_serp_pos if my_serp_pos > 0 else len(original_results) + 1, 
        "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": my_score_w, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": my_score_d
    })

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ –ø–æ–∑–∏—Ü–∏–∏ 
    table_rel_df = pd.DataFrame(table_rel)
    table_rel_df = table_rel_df.sort_values(by='–ü–æ–∑–∏—Ü–∏—è', ascending=True).reset_index(drop=True)

    return {
        "depth": pd.DataFrame(table_depth), 
        "hybrid": pd.DataFrame(table_hybrid), 
        "ngrams": pd.DataFrame(table_ngrams), 
        "relevance_top": table_rel_df, 
        "my_score": {"width": my_score_w, "depth": my_score_d}
    }

# ==========================================
# 5. –§–£–ù–ö–¶–ò–Ø –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø (FINAL)
# ==========================================

# --- –§—É–Ω–∫—Ü–∏–∏ –ò—Å—Ç–æ—Ä–∏–∏ (–ó–∞–ø—Ä–æ—Å 2) ---

def save_analysis_to_history(my_url, successful_urls, results, comp_data):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é —Å–µ—Å—Å–∏–∏."""
    if 'history' not in st.session_state:
        st.session_state.history = []
    
    # –ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç
    history_entry = {
        'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
        'my_url': my_url,
        'successful_urls': successful_urls,
        'width': results['my_score']['width'],
        'depth': results['my_score']['depth'],
        'full_results': {
            'results': results,
            'comp_table_data': comp_data,
            'my_url_input': my_url,
            'competitors_input': "\n".join(successful_urls)
        }
    }
    st.session_state.history.insert(0, history_entry) 

def load_analysis_from_history(entry):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –≤ —Ç–µ–∫—É—â—É—é —Å–µ—Å—Å–∏—é –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    
    # –°–±—Ä–æ—Å –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    for key in list(st.session_state.keys()): 
        if key.endswith('_page'): st.session_state[key] = 1 
        
    st.session_state.analysis_results = entry['full_results']['results']
    st.session_state.comp_table_data = entry['full_results']['comp_table_data']
    st.session_state.my_url_input = entry['full_results']['my_url_input']
    st.session_state.manual_urls_ui = entry['full_results']['competitors_input']
    st.session_state.analysis_done = True
    st.toast(f"–ó–∞–≥—Ä—É–∂–µ–Ω –∞–Ω–∞–ª–∏–∑ –æ—Ç {entry['timestamp']}.")
    st.rerun()


def render_paginated_table(df, title_text, key_prefix, default_sort_col=None, use_abs_sort_default=False):
    if df.empty:
        st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")
        return
    st.markdown(f"### {title_text}")
    
    # –ë–õ–û–ö –°–û–†–¢–ò–†–û–í–ö–ò 
    if f'{key_prefix}_sort_col' not in st.session_state:
        st.session_state[f'{key_prefix}_sort_col'] = default_sort_col if default_sort_col in df.columns else df.columns[0]
    if f'{key_prefix}_sort_dir' not in st.session_state:
        st.session_state[f'{key_prefix}_sort_dir'] = 'desc' if use_abs_sort_default else 'asc'

    col_sort, col_dir = st.columns([1, 1], key=f"{key_prefix}_sort_cols")
    
    with col_sort:
        sort_col = st.selectbox("–°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∫–æ–ª–æ–Ω–∫–µ", options=df.columns, index=df.columns.get_loc(st.session_state[f'{key_prefix}_sort_col']), key=f"{key_prefix}_sort_col_select")
        if sort_col != st.session_state[f'{key_prefix}_sort_col']:
            st.session_state[f'{key_prefix}_sort_col'] = sort_col
            st.rerun()
            
    with col_dir:
        sort_dir = st.selectbox("–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", options=['desc', 'asc'], index=['desc', 'asc'].index(st.session_state[f'{key_prefix}_sort_dir']), key=f"{key_prefix}_sort_dir_select")
        if sort_dir != st.session_state[f'{key_prefix}_sort_dir']:
            st.session_state[f'{key_prefix}_sort_dir'] = sort_dir
            st.rerun()
            
    ascending = sort_dir == 'asc'
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π –≤–µ–ª–∏—á–∏–Ω–µ (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è)
    if use_abs_sort_default and sort_col in ['–î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å', 'diff_abs']:
        df = df.sort_values(by=sort_col, ascending=ascending, key=lambda x: np.abs(x) if np.issubdtype(x.dtype, np.number) else x).copy()
    else:
        df = df.sort_values(by=sort_col, ascending=ascending).copy()

    df = df.reset_index(drop=True)
    df.index = df.index + 1
    
    ROWS_PER_PAGE = 20
    if f'{key_prefix}_page' not in st.session_state:
        st.session_state[f'{key_prefix}_page'] = 1
        
    total_rows = len(df)
    total_pages = math.ceil(total_rows / ROWS_PER_PAGE)
    current_page = st.session_state[f'{key_prefix}_page']
    
    if current_page > total_pages: current_page = total_pages
    if current_page < 1: current_page = 1
    
    start_idx = (current_page - 1) * ROWS_PER_PAGE
    end_idx = start_idx + ROWS_PER_PAGE
    df_view = df.iloc[start_idx:end_idx]
    
    # –ü–û–ö–†–ê–°–ö–ê –Ø–ß–ï–ï–ö
    def highlight_rows(row):
        base_style = 'background-color: #FFFFFF; color: #3D4858; border-bottom: 1px solid #DBEAFE;'
        styles = [base_style] * len(row)
        
        # –ò–Ω–¥–µ–∫—Å –¥–ª—è is_missing
        try:
            is_missing_idx = row.index.get_loc("is_missing")
        except KeyError:
            is_missing_idx = -1

        if is_missing_idx != -1 and row['is_missing']:
            # –°—Ç–∏–ª—å –¥–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤
            styles[0] += 'color: #D32F2F; font-weight: bold;'
        
        # –°—Ç–∏–ª—å –¥–ª—è –¥—Ä—É–≥–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ (—É–±—Ä–∞–Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã, –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ)
        # else:
        #     styles = [base_style + 'font-weight: 600;' if col_name not in ["diff_abs", "is_missing"] else base_style for col_name in row.index]

        return styles
    
    cols_to_hide = ["diff_abs", "is_missing"]
    
    # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ '–°–ª–æ–≤–æ/–§—Ä–∞–∑–∞' —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–µ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∏–ª—è
    col_config = {}
    if '–°–ª–æ–≤–æ' in df_view.columns:
        col_config['–°–ª–æ–≤–æ'] = st.column_config.TextColumn("–°–ª–æ–≤–æ", help="–°–ª–æ–≤–æ –∏–ª–∏ –ª–µ–º–º–∞")
    elif '–°–ª–æ–≤–æ/–§—Ä–∞–∑–∞' in df_view.columns:
         col_config['–°–ª–æ–≤–æ/–§—Ä–∞–∑–∞'] = st.column_config.TextColumn("–°–ª–æ–≤–æ/–§—Ä–∞–∑–∞", help="–°–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑–∞")

    styled_df = df_view.style.apply(highlight_rows, axis=1) 

    # –í–´–í–û–î –¢–ê–ë–õ–ò–¶–´
    dynamic_height = (len(df_view) * 35) + 40
    st.dataframe(
        styled_df,
        use_container_width=True,
        height=dynamic_height,
        column_config={c: None for c in cols_to_hide}
    )
    
    # –ö–ù–û–ü–ö–ò –ü–ï–†–ï–ö–õ–Æ–ß–ï–ù–ò–Ø
    c_spacer, c_btn_prev, c_info, c_btn_next = st.columns([6, 1, 1, 1])
    with c_btn_prev:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_prev", disabled=(current_page <= 1), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] -= 1
            st.rerun()
    with c_info:
        st.markdown(f"<div style='text-align: center; margin-top: 10px; color:{TEXT_COLOR}'><b>{current_page}</b> / {total_pages}</div>", unsafe_allow_html=True)
    with c_btn_next:
        if st.button("‚û°Ô∏è", key=f"{key_prefix}_next", disabled=(current_page >= total_pages), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] += 1
            st.rerun()
    st.markdown("---") 

# --- –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –¢–ê–ë–õ–ò–¶–´ –°–¢–ê–¢–£–°–ê (–ó–∞–ø—Ä–æ—Å 5) ---
def render_competitor_status_table(comp_data):
    """
    –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Å—Ç–∞—Ç—É—Å–æ–≤ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.
    –î–æ–º–µ–Ω—ã —Ç–µ–ø–µ—Ä—å —è–≤–ª—è—é—Ç—Å—è –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π URL.
    """
    st.markdown("### 2. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (—Å—Ç–∞—Ç—É—Å)")
    
    if not comp_data:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö.")
        return

    df = pd.DataFrame(comp_data)
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã (–ó–∞–ø—Ä–æ—Å 5)
    def make_clickable_domain(row):
        url = row['URL']
        domain = row['–î–æ–º–µ–Ω']
        status = row['–°—Ç–∞—Ç—É—Å']
        if "OK" in status:
            return f'<a href="{url}" target="_blank">{domain}</a>'
        return domain
        
    df['–î–æ–º–µ–Ω'] = df.apply(make_clickable_domain, axis=1)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ DF –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    display_df = df[['–î–æ–º–µ–Ω', '–°—Ç–∞—Ç—É—Å', '–û—à–∏–±–∫–∞']]
    
    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å HTML-–∫–æ–ª–æ–Ω–∫–∞–º–∏
    st.markdown(display_df.to_html(escape=False, index=False), unsafe_allow_html=True)


# ==========================================
# 6. –ò–ù–¢–ï–†–§–ï–ô–°
# ==========================================

st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")

# --- –í–ö–õ–ê–î–ö–ò (–ó–∞–ø—Ä–æ—Å 2) ---
tab_analysis, tab_history = st.tabs(["üìä –ê–Ω–∞–ª–∏–∑ –°–µ–º–∞–Ω—Ç–∏–∫–∏", "üìö –ò–°–¢–û–†–ò–Ø –ü–†–û–í–ï–†–û–ö"]) 

with tab_analysis:
    col_main, col_sidebar = st.columns([65, 35])
    
    with col_main:
        st.markdown("### URL –∏–ª–∏ –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –í–∞—à–µ–≥–æ —Å–∞–π—Ç–∞")
        my_input_type = st.radio("–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], horizontal=True, label_visibility="collapsed", key="my_page_source_radio")
        my_url = ""
        my_page_content = ""
        
        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            my_url = st.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã", placeholder="https://site.ru/catalog/tovar", label_visibility="collapsed", key="my_url_input")
        elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            my_page_content = st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥...", height=300, placeholder="<html>...</html>", label_visibility="collapsed", key="my_content_input")

        st.markdown("### –ò—Å—Ç–æ—á–Ω–∏–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        source_type = st.radio("–¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞", ["API (–ø–æ –∑–∞–ø—Ä–æ—Å—É)", "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"], horizontal=True, label_visibility="collapsed", key="source_type_radio")
        
        query = ""
        if source_type == "API":
            query = st.text_input("–ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å (–∫–ª—é—á)", placeholder="–∫—É–ø–∏—Ç—å –¥–∏–≤–∞–Ω –≤ –º–æ—Å–∫–≤–µ", key="query_input")
            
        if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º session_state.manual_urls_ui –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª–Ω—ã—Ö URL (–ó–∞–ø—Ä–æ—Å 6)
            manual_urls_ui = st.text_area(
                "–°–ø–∏—Å–æ–∫ URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–∫–∞–∂–¥—ã–π —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏):", 
                height=300, 
                placeholder="https://comp1.ru/page/\nhttps://comp2.com/item/", 
                key="manual_urls_ui" 
            )

        # –ö–Ω–æ–ø–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
        st.markdown("---")
        if st.button("üöÄ –ù–∞—á–∞—Ç—å –ê–Ω–∞–ª–∏–∑", type="primary", use_container_width=True):
            # –°–±—Ä–æ—Å –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –∏ —Ñ–ª–∞–≥–∞
            for key in list(st.session_state.keys()): 
                if key.endswith('_page'): st.session_state[key] = 1
            st.session_state.start_analysis_flag = True
            st.rerun()

    with col_sidebar:
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        ua = st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
        search_engine = st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
        region = st.selectbox("–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞", list(REGION_MAP.keys()), key="settings_region")
        device = st.selectbox("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", ["Desktop", "Mobile"], key="settings_device")
        top_n = st.selectbox("–ì–ª—É–±–∏–Ω–∞ —Å–±–æ—Ä–∞ (–¢–û–ü)", [10, 20, 30], index=0, key="settings_top_n")
        st.markdown("---")
        st.selectbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ url", ["–í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", "–ì–ª–∞–≤–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", "–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], key="settings_url_type")
        col_c1, col_c2 = st.columns(2)
        with col_c1:
            st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å noindex/script", True, key="settings_noindex")
            st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", False, key="settings_alt")
            st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", False, key="settings_numbers")
        with col_c2:
            st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True, key="settings_norm")
            st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã", True, key="settings_agg")
        
        st.markdown("---")
        st.markdown("##### ‚õî –°—Ç–æ–ø-—Å–ª–æ–≤–∞")
        st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–∫–∞–∂–¥–æ–µ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", DEFAULT_STOPS, height=150, key="settings_stops")
    

# ==========================================
# 7. –í–´–ü–û–õ–ù–ï–ù–ò–ï (–°–ö–û–†–†–ï–ö–¢–ò–†–û–í–ê–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –°–ë–û–†–ê)
# ==========================================

if st.session_state.get('start_analysis_flag'):
    st.session_state.start_analysis_flag = False

    # ... (–ü—Ä–æ–≤–µ—Ä–∫–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö) ...
    if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ" and not st.session_state.get('my_url_input'):
        st.error("–í–≤–µ–¥–∏—Ç–µ URL!")
        st.stop()
    if my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç" and not st.session_state.get('my_content_input', '').strip():
        st.error("–í–≤–µ–¥–∏—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥!")
        st.stop()
    if source_type == "API" and not st.session_state.get('query_input'):
        st.error("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å!")
        st.stop()
    if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫" and not st.session_state.get("manual_urls_ui", "").strip():
        st.error("–í–≤–µ–¥–∏—Ç–µ —Å–ø–∏—Å–æ–∫ URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤!")
        st.stop()
        
    settings = {
        'noindex': st.session_state.settings_noindex,
        'alt_title': st.session_state.settings_alt,
        'numbers': st.session_state.settings_numbers,
        'norm': st.session_state.settings_norm,
        'ua': st.session_state.settings_ua,
        'custom_stops': st.session_state.settings_stops.split()
    }
    
    target_urls_raw = [] # –°–ø–∏—Å–æ–∫ URL:pos, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ—à–ª–∏ –ø–µ—Ä–≤–∏—á–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
    my_data = None
    my_domain = ""
    my_serp_pos = 0 
    
    # 1. –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ –í–ê–®–ï–ú —Å–∞–π—Ç–µ –∏ –¥–æ–º–µ–Ω–µ
    if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
        with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
            my_url_input = st.session_state.my_url_input
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º robust-—Ñ—É–Ω–∫—Ü–∏—é
            my_data = parse_page_robust(my_url_input, settings) 
            
            if my_data['error']:
                st.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –í–∞—à URL: {my_data['error']}")
                st.stop()
            
            my_domain = my_data['domain']
            
    # 2. –°–±–æ—Ä URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    found_results = []
    if source_type == "API":
        # ... (API logic remains the same) ...
        with st.spinner(f"–ó–∞–ø—Ä–æ—Å –¢–û–ü-{st.session_state.settings_top_n} –≤ {st.session_state.settings_search_engine} / {st.session_state.settings_region} –ø–æ –∑–∞–ø—Ä–æ—Å—É '{st.session_state.query_input}'..."):
            found_results = get_arsenkin_urls(st.session_state.query_input, st.session_state.settings_search_engine, st.session_state.settings_region, st.session_state.settings_top_n)

        # 2.1. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Ç—Ä–µ–∫–∏–Ω–≥ –ø–æ–∑–∏—Ü–∏–∏ 
        filtered_results_all = []
        excl = set(DEFAULT_EXCLUDE_DOMAINS)
        
        if st.session_state.settings_agg:
            # –ï—Å–ª–∏ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã –∏—Å–∫–ª—é—á–∞—é—Ç—Å—è, –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö –∫ —Å—Ç–æ–ø-–¥–æ–º–µ–Ω–∞–º
            excl.update(DEFAULT_EXCLUDE_DOMAINS)

        for result in found_results:
            url = result['url']
            pos = result['pos']
            domain = urlparse(url).netloc
            
            # 1. –¢—Ä–µ–∫–∏–Ω–≥ –Ω–∞—à–µ–≥–æ —Å–∞–π—Ç–∞
            if my_domain and my_domain == domain:
                if my_serp_pos == 0 or pos < my_serp_pos:
                    my_serp_pos = pos
                continue
            
            # 2. –ò—Å–∫–ª—é—á–∞–µ–º –¥–æ–º–µ–Ω—ã –∏–∑ —Å–ø–∏—Å–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
            if any(x in domain for x in excl):
                continue
            
            # –ï—Å–ª–∏ –ø—Ä–æ—à–µ–ª —Ñ–∏–ª—å—Ç—Ä—ã, –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —á–∏—Å—Ç—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
            filtered_results_all.append(result)

        # 2.2. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ TARGET_COMPETITORS (–í–¢–û–†–´–ú –®–ê–ì–û–ú)
        # TARGET_COMPETITORS –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –∫–æ–¥–µ, 
        # –∏—Å–ø–æ–ª—å–∑—É–µ–º 10 –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.
        TARGET_COMPETITORS = st.session_state.settings_top_n
        target_urls_raw = filtered_results_all[:TARGET_COMPETITORS]
        collected_competitors_count = len(target_urls_raw)
        
        st.info(f"–ü–æ–ª—É—á–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL –æ—Ç API: {len(found_results)}. –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ **–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–æ–≤ –∏ —Å—Ç–æ–ø-–¥–æ–º–µ–Ω–æ–≤**, –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—ã–±—Ä–∞–Ω–æ **{collected_competitors_count}** —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (—Ü–µ–ª—å {TARGET_COMPETITORS}). –í–∞—à —Å–∞–π—Ç –≤ –¢–û–ü–µ: **{'–î–∞ (–ü–æ–∑. ' + str(my_serp_pos) + ')' if my_serp_pos > 0 else '–ù–µ—Ç'}**.")
        
    else: # –†—É—á–Ω–æ–π —Ä–µ–∂–∏–º
        raw_urls = st.session_state.get("manual_urls_ui", "")
        if raw_urls:
            urls = [u.strip() for u in raw_urls.split('\n') if u.strip()]
            target_urls_raw = [{'url': u, 'pos': i+1} for i, u in enumerate(urls)]
        else:
            target_urls_raw = []
        st.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ **{len(target_urls_raw)}** URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤—Ä—É—á–Ω—É—é.")
        
    if not target_urls_raw and my_input_type != "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã":
        st.error("–ù–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")
        st.stop()
        
    # 3. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–æ–±–Ω–æ–≤–ª–µ–Ω–æ –¥–ª—è robust parsing)
    comp_data_full_raw = [] 
    
    with st.spinner(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {len(target_urls_raw)} –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏..."):
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            url_to_pos = {item['url']: item['pos'] for item in target_urls_raw}
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º robust –ø–∞—Ä—Å–∏–Ω–≥
            future_to_url = {executor.submit(parse_page_robust, url, settings): url for url in url_to_pos.keys()}
            
            for future in concurrent.futures.as_completed(future_to_url):
                result = future.result() 
                result['pos'] = url_to_pos[result['url']]
                comp_data_full_raw.append(result)

    # 4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ —Ç–∞–±–ª–∏—Ü—ã
    comp_data_full = []
    comp_table_data = []
    successful_urls = []
    
    for item in target_urls_raw: 
        url = item['url']
        pos = item['pos']
        
        parsed_result = next((res for res in comp_data_full_raw if res['url'] == url), None)
        
        if parsed_result and parsed_result.get('body_text'):
            comp_data_full.append(parsed_result)
            comp_table_data.append({
                "URL": url,
                "–î–æ–º–µ–Ω": parsed_result['domain'],
                "–°—Ç–∞—Ç—É—Å": "OK (2)",
                "–û—à–∏–±–∫–∞": "",
                "–ü–æ–∑–∏—Ü–∏—è": pos 
            })
            successful_urls.append(url)
        else:
            error = parsed_result['error'] if parsed_result and parsed_result.get('error') else "–ù–µ —Å–∫–∞—á–∞–Ω/–ò—Å–∫–ª—é—á–µ–Ω"
            comp_table_data.append({
                "URL": url,
                "–î–æ–º–µ–Ω": urlparse(url).netloc,
                "–°—Ç–∞—Ç—É—Å": f"–û—à–∏–±–∫–∞/–ò—Å–∫–ª—é—á–µ–Ω", # –£–±—Ä–∞–ª 0/1, —Ç.–∫. –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—à–∏–±–∫–∞, –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –∏–ª–∏ –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
                "–û—à–∏–±–∫–∞": error,
                "–ü–æ–∑–∏—Ü–∏—è": pos
            })
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª—è –≤–≤–æ–¥–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –ø–æ–ª–Ω—ã–º–∏ URL-–∞–¥—Ä–µ—Å–∞–º–∏ (–ó–∞–ø—Ä–æ—Å 6)
    st.session_state.manual_urls_ui = "\n".join(successful_urls)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    st.session_state.comp_table_data = comp_table_data

    # 5. –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
    with st.spinner("–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö..."):
        results = calculate_metrics(
            comp_data_full, 
            my_data, 
            settings, 
            my_serp_pos, 
            target_urls_raw
        )
    st.session_state.analysis_results = results
    st.session_state.analysis_done = True
    
    # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é (–ó–∞–ø—Ä–æ—Å 2)
    save_analysis_to_history(st.session_state.my_url_input, successful_urls, results, comp_table_data)
    
    st.rerun()

# --- –ë–õ–û–ö –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ---
if st.session_state.analysis_done and st.session_state.analysis_results:
    with tab_analysis:
        results = st.session_state.analysis_results
        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        
        # 0. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ë–∞–ª–ª—ã)
        st.markdown(f"""
            <div style='background-color: {LIGHT_BG_MAIN}; padding: 15px; border-radius: 8px; border: 1px solid {BORDER_COLOR}; margin-bottom: 20px;'>
                <h4 style='margin:0; color: {PRIMARY_COLOR};'>–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ (–≤ –±–∞–ª–ª–∞—Ö –æ—Ç 0 –¥–æ 100)</h4>
                <p style='margin:5px 0 0 0;'>–®–∏—Ä–∏–Ω–∞ (–æ—Ö–≤–∞—Ç —Å–µ–º–∞–Ω—Ç–∏–∫–∏): <b>{results['my_score']['width']}</b> | –ì–ª—É–±–∏–Ω–∞ (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è): <b>{results['my_score']['depth']}</b></p>
            </div>
            <div class="legend-box">
                <span class="text-red">–ö—Ä–∞—Å–Ω—ã–π</span>: —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç —É –≤–∞—Å. <span class="text-bold">–ñ–∏—Ä–Ω—ã–π</span>: —Å–ª–æ–≤–∞, —É—á–∞—Å—Ç–≤—É—é—â–∏–µ –≤ –∞–Ω–∞–ª–∏–∑–µ.<br>
                –ú–∏–Ω–∏–º—É–º: min(—Å—Ä–µ–¥–Ω–µ–µ, –º–µ–¥–∏–∞–Ω–∞). –ü–µ—Ä–µ—Å–ø–∞–º: % –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –º–∞–∫—Å. –¥–∏–∞–ø–∞–∑–æ–Ω–∞. <br>
                ‚ÑπÔ∏è –î–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –≤—Å–µ–≥–æ —Å–ø–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—é –Ω–∞–¥ —Ç–∞–±–ª–∏—Ü–µ–π.
            </div>
        """, unsafe_allow_html=True)

        render_paginated_table(results['depth'], "1. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≥–ª—É–±–∏–Ω–µ", "tbl_depth_1", default_sort_col="–î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å", use_abs_sort_default=True)
        
        # 2. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (—Å—Ç–∞—Ç—É—Å) - —Å –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ (–ó–∞–ø—Ä–æ—Å 5)
        render_competitor_status_table(st.session_state.comp_table_data) 
        
        render_paginated_table(results['hybrid'], "3. –ì–∏–±—Ä–∏–¥–Ω—ã–π –¢–û–ü (TF-IDF)", "tbl_hybrid", default_sort_col="TF-IDF –¢–û–ü", use_abs_sort_default=False)
        
        # 4. N-–≥—Ä–∞–º–º—ã (–§—Ä–∞–∑—ã) - —Ç–µ–ø–µ—Ä—å –¥–æ–ª–∂–Ω—ã —Ä–∞–±–æ—Ç–∞—Ç—å (–ó–∞–ø—Ä–æ—Å 3)
        render_paginated_table(results['ngrams'], "4. N-–≥—Ä–∞–º–º—ã (–§—Ä–∞–∑—ã)", "tbl_ngrams", default_sort_col="–ß–∞—Å—Ç–æ—Ç–∞ (–°—É–º–º–∞)", use_abs_sort_default=False)
        
        render_paginated_table(results['relevance_top'], "5. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¢–û–ü–∞", "tbl_relevance_top", default_sort_col="–ü–æ–∑–∏—Ü–∏—è", use_abs_sort_default=False)


# --- –í–ö–õ–ê–î–ö–ê –ò–°–¢–û–†–ò–Ø (–ó–∞–ø—Ä–æ—Å 2) ---
with tab_history:
    st.header("üìö –ò—Å—Ç–æ—Ä–∏—è –ü—Ä–æ–≤–µ—Ä–æ–∫")
    
    if not st.session_state.history:
        st.info("–ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–µ—Ä–æ–∫ –ø—É—Å—Ç–∞. –ù–∞—á–Ω–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–ê–Ω–∞–ª–∏–∑ –°–µ–º–∞–Ω—Ç–∏–∫–∏'.")
    else:
        for i, entry in enumerate(st.session_state.history):
            
            col_ts, col_btn = st.columns([4, 1])
            
            with col_ts:
                st.markdown(f"""
                    <div style='background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 5px; border: 1px solid {BORDER_COLOR}; margin-bottom: 10px;'>
                        <p style='margin:0; font-size: 1.1em; color: {PRIMARY_COLOR};'>
                            <b>{entry['timestamp']}</b>
                        </p>
                        <p style='margin:5px 0 0 0;'>
                            üîó URL: <span style='word-break: break-all;'>{entry['my_url']}</span>
                        </p>
                        <p style='margin:5px 0 0 0;'>
                            –®–∏—Ä–∏–Ω–∞: <b>{entry['width']}</b> | –ì–ª—É–±–∏–Ω–∞: <b>{entry['depth']}</b>
                        </p>
                    </div>
                """, unsafe_allow_html=True)
            
            with col_btn:
                st.markdown("<div style='height: 10px;'></div>", unsafe_allow_html=True)
                # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                if st.button(f"–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å", key=f"load_history_{i}", use_container_width=True):
                    load_analysis_from_history(entry)
