import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin, unquote
import inspect
import time
import json
import io
import os
import random
import streamlit.components.v1 as components

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ NLP –±–∏–±–ª–∏–æ—Ç–µ–∫
try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except ImportError:
    morph = None
    USE_NLP = False

try:
    import openai
except ImportError:
    openai = None

# ==========================================
# 0. –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –ò –ù–ê–°–¢–†–û–ô–ö–ò
# ==========================================

st.set_page_config(layout="wide", page_title="GAR PRO v3.1 (Unified)", page_icon="üè≠")

# –°—Ç–∏–ª–∏–∑–∞—Ü–∏—è
PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE"

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif; color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important; }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important; color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        
        /* –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –∫–ª–∞—Å—Å–∞, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –æ—Ç—Å—Ç—É–ø—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã */
        .tool-card {{ padding: 20px; border: 1px solid #E2E8F0; border-radius: 10px; background-color: #F8FAFC; margin-bottom: 20px; }}
        
        .block-title {{ color: {PRIMARY_COLOR}; font-size: 1.2em; font-weight: bold; margin-bottom: 10px; display: flex; align-items: center; }}
        .block-icon {{ margin-right: 10px; font-size: 1.2em; }}
        .legend-box {{ padding: 10px; background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px; }}
        .text-red {{ color: #D32F2F; font-weight: bold; }}
        .text-green {{ color: #2E7D32; font-weight: bold; }}
    </style>
""", unsafe_allow_html=True)

# Auth
def check_password():
    if st.session_state.get("authenticated"):
        return True
    st.markdown("""<style>.main { display: flex; flex-direction: column; justify-content: center; align-items: center; } .auth-logo-box { text-align: center; margin-bottom: 1rem; padding-top: 0; }</style>""", unsafe_allow_html=True)
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="auth-logo-box"><h3>–í—Ö–æ–¥ –≤ GAR PRO</h3></div>', unsafe_allow_html=True)
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "jfV6Xel-Q7vp-_s2UYPO":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

# --- Helpers ---
def transliterate_text(text):
    mapping = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }
    result = []
    for char in text.lower():
        if char in mapping:
            result.append(mapping[char])
        elif char.isalnum() or char == '-':
            result.append(char)
    return "".join(result)

def force_cyrillic_name_global(slug_text):
    raw = unquote(slug_text).lower()
    raw = raw.replace('.html', '').replace('.php', '')
    if re.search(r'[–∞-—è]', raw):
        return raw.replace('-', ' ').replace('_', ' ').capitalize()

    words = re.split(r'[-_]', raw)
    rus_words = []
    # (–°–æ–∫—Ä–∞—â–µ–Ω–Ω–∞—è –º–∞–ø–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞, –ª–æ–≥–∏–∫–∞ —Ç–∞ –∂–µ)
    exact_map = {
        'nikel': '–Ω–∏–∫–µ–ª—å', 'stal': '—Å—Ç–∞–ª—å', 'med': '–º–µ–¥—å', 'list': '–ª–∏—Å—Ç', 'truba': '—Ç—Ä—É–±–∞', 
        'gost': '–ì–û–°–¢', 'krug': '–∫—Ä—É–≥', 'provoloka': '–ø—Ä–æ–≤–æ–ª–æ–∫–∞'
    }

    for w in words:
        if not w: continue
        if w in exact_map:
            rus_words.append(exact_map[w])
            continue
        rus_words.append(w) # Fallback

    return " ".join(rus_words).capitalize()

# --- Loaders & Classification ---
@st.cache_data
def load_lemmatized_dictionaries():
    base_path = "data"
    product_lemmas = set()
    commercial_lemmas = set()
    specs_lemmas = set()
    geo_lemmas = set()
    services_lemmas = set()
    
    # –ó–∞–≥–ª—É—à–∫–∞, –µ—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç, —á—Ç–æ–±—ã –∫–æ–¥ –Ω–µ –ø–∞–¥–∞–ª
    # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ —Ç—É—Ç —á—Ç–µ–Ω–∏–µ JSON
    return product_lemmas, commercial_lemmas, specs_lemmas, geo_lemmas, services_lemmas

def classify_semantics_with_api(words_list, yandex_key):
    PRODUCTS_SET, COMM_SET, SPECS_SET, GEO_SET, SERVICES_SET = load_lemmatized_dictionaries()
    
    DEFAULT_COMMERCIAL = {'—Ü–µ–Ω–∞', '–∫—É–ø–∏—Ç—å', '–ø—Ä–∞–π—Å', '–∫–æ—Ä–∑–∏–Ω–∞', '–∑–∞–∫–∞–∑', '—Ä—É–±', '–Ω–∞–ª–∏—á–∏–µ', '—Å–∫–ª–∞–¥', 
                          '–º–∞–≥–∞–∑–∏–Ω', '–∞–∫—Ü–∏—è', '—Å–∫–∏–¥–∫–∞', '–æ–ø—Ç', '—Ä–æ–∑–Ω–∏—Ü–∞', '–∫–∞—Ç–∞–ª–æ–≥', '—Ç–µ–ª–µ—Ñ–æ–Ω'}

    categories = {'products': set(), 'services': set(), 'commercial': set(), 'dimensions': set(), 'geo': set(), 'general': set()}
    
    for word in words_list:
        word_lower = word.lower()
        if morph:
            p = morph.parse(word_lower)[0]
            lemma = p.normal_form
        else:
            lemma = word_lower

        if lemma in SPECS_SET: categories['dimensions'].add(lemma); continue
        if lemma in PRODUCTS_SET: categories['products'].add(lemma); continue
        if lemma in GEO_SET: categories['geo'].add(lemma); continue
        if lemma in SERVICES_SET: categories['services'].add(lemma); continue
        if lemma in COMM_SET or lemma in DEFAULT_COMMERCIAL: categories['commercial'].add(lemma); continue
        categories['general'].add(lemma)

    return {k: sorted(list(v)) for k, v in categories.items()}

# --- API & Parsing ---
REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904}
}

DEFAULT_EXCLUDE = "avito.ru\nyandex.ru\nozon.ru\nwildberries.ru"
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n–∫—É–ø–∏—Ç—å\n—Ü–µ–Ω–∞\n—à—Ç"
GARBAGE_LATIN_STOPLIST = {'whatsapp', 'viber', 'telegram', 'vk', 'instagram', 'facebook', 'youtube', 'twitter', 'cookie', 'policy', 'privacy', 'agreement', 'terms', 'click', 'submit', 'send', 'zakaz', 'basket', 'cart', 'order', 'call', 'back', 'callback', 'login', 'logout', 'sign', 'register', 'auth', 'account', 'profile', 'search', 'menu', 'nav', 'navigation', 'footer', 'header', 'sidebar', 'img', 'jpg', 'png', 'pdf', 'ok', 'error', 'undefined', 'null', 'true', 'false', 'var', 'let', 'const', 'function', 'return', 'ru', 'en', 'com', 'net', 'org', 'phone', 'email', 'tel', 'fax', 'mob', 'address', 'copyright', 'div', 'span', 'class', 'id', 'style', 'script', 'body', 'html', 'head', 'meta', 'link'}

def get_arsenkin_urls(query, engine_type, region_name, api_token, depth_val=10):
    if not api_token: return []
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check"
    url_get = "https://arsenkin.ru/api/tools/get"
    headers = {"Authorization": f"Bearer {api_token}", "Content-type": "application/json"}
    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []
    if "–Ø–Ω–¥–µ–∫—Å" in engine_type: se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type: se_params.append({"type": 11, "region": reg_ids['go']})

    payload = {"tools_name": "check-top", "data": {"queries": [query], "is_snippet": False, "noreask": True, "se": se_params, "depth": depth_val}}
    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        resp_json = r.json()
        if "error" in resp_json or "task_id" not in resp_json: return []
        task_id = resp_json["task_id"]
    except: return []

    status = "process"
    attempts = 0
    while status == "process" and attempts < 40:
        time.sleep(5); attempts += 1
        try:
            r_check = requests.post(url_check, headers=headers, json={"task_id": task_id})
            if r_check.json().get("status") == "finish": status = "done"; break
        except: pass

    if status != "done": return []

    try:
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id}, timeout=30)
        res_data = r_final.json()
        collect = res_data.get('result', {}).get('result', {}).get('collect')
        results_list = []
        if collect:
             if isinstance(collect, list) and len(collect) > 0 and isinstance(collect[0], list): 
                final_url_list = collect[0][0]
                for index, url in enumerate(final_url_list): results_list.append({'url': url, 'pos': index + 1})
        return results_list
    except: return []

def process_text_detailed(text, settings, n_gram=1):
    text = text.lower().replace('—ë', '–µ')
    words = re.findall(r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+', text)
    stops = set(w.lower().replace('—ë', '–µ') for w in settings['custom_stops'])
    lemmas = []
    forms_map = defaultdict(set)
    for w in words:
        if len(w) < 2: continue
        if not settings['numbers'] and w.isdigit(): continue
        if w in stops: continue
        lemma = w
        if USE_NLP and n_gram == 1:
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            lemma = p.normal_form.replace('—ë', '–µ')
        lemmas.append(lemma)
        forms_map[lemma].add(w)
    return lemmas, forms_map

def parse_page(url, settings):
    headers = {'User-Agent': settings['ua']}
    try:
        r = requests.get(url, headers=headers, timeout=10)
        if r.status_code != 200: return None
        soup = BeautifulSoup(r.text, 'html.parser')
        for c in soup.find_all(string=lambda text: isinstance(text, Comment)): c.extract()
        if settings['noindex']: 
            for t in soup.find_all('noindex'): t.decompose()
        
        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)
        
        extra_text = []
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'): extra_text.append(meta_desc['content'])
        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
        
        body_text_raw = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text = re.sub(r'\s+', ' ', body_text_raw).strip()
        if not body_text: return None
        return {'url': url, 'domain': urlparse(url).netloc, 'body_text': body_text, 'anchor_text': anchor_text}
    except: return None

def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    all_forms_map = defaultdict(set)
    if not my_data or not my_data.get('body_text'): my_lemmas, my_forms, my_anchors, my_len = [], {}, [], 0
    else:
        my_lemmas, my_forms = process_text_detailed(my_data['body_text'], settings)
        my_anchors, _ = process_text_detailed(my_data['anchor_text'], settings)
        my_len = len(my_lemmas)
        for k, v in my_forms.items(): all_forms_map[k].update(v)

    comp_docs = []
    for p in comp_data_full:
        if not p: continue
        body, c_forms = process_text_detailed(p['body_text'], settings)
        comp_docs.append({'body': body, 'url': p['url']})
        for k, v in c_forms.items(): all_forms_map[k].update(v)

    if not comp_docs:
         return { "depth": pd.DataFrame(), "hybrid": pd.DataFrame(), "relevance_top": pd.DataFrame(), "my_score": {"width": 0, "depth": 0}, "missing_semantics_high": [], "missing_semantics_low": [] }

    c_lens = [len(d['body']) for d in comp_docs]
    avg_dl = np.mean(c_lens) if c_lens else 1
    median_len = np.median(c_lens) if c_lens else 0
    norm_k_recs = (my_len / median_len) if (median_len > 0 and my_len > 0 and settings['norm']) else 1.0

    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    N = len(comp_docs)
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
    
    word_counts_per_doc = [Counter(d['body']) for d in comp_docs]
    word_idf_map = {}
    for lemma in vocab:
        df = doc_freqs[lemma]
        if df == 0: continue
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        word_idf_map[lemma] = max(idf, 0.01)

    S_WIDTH_CORE = set()
    missing_semantics_high = []
    missing_semantics_low = []
    my_full_lemmas_set = set(my_lemmas) | set(my_anchors)

    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        c_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        med_val = np.median(c_counts)
        percent = int((doc_freqs[lemma] / N) * 100)
        weight_simple = word_idf_map.get(lemma, 0) * med_val
        if med_val >= 1: S_WIDTH_CORE.add(lemma)

        if lemma not in my_full_lemmas_set:
            if len(lemma) < 2 or lemma.isdigit(): continue
            item = {'word': lemma, 'percent': percent, 'weight': weight_simple}
            if med_val >= 1: missing_semantics_high.append(item)
            elif percent >= 30: missing_semantics_low.append(item)

    missing_semantics_high.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low.sort(key=lambda x: x['percent'], reverse=True)
    
    total_width_core_count = len(S_WIDTH_CORE)
    def calculate_width_score_val(lemmas_set):
        if total_width_core_count == 0: return 0
        ratio = len(lemmas_set.intersection(S_WIDTH_CORE)) / total_width_core_count
        return 100 if ratio >= 0.9 else int(round((ratio / 0.9) * 100))

    my_width_score_final = min(100, calculate_width_score_val(my_full_lemmas_set))
    my_depth_score_final = 50 # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è

    table_depth = []
    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        df = doc_freqs[lemma]
        if df < 2 and lemma not in my_lemmas: continue
        my_tf_count = my_lemmas.count(lemma)
        c_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        med_total = np.median(c_counts)
        rec_min = int(math.ceil(med_total * norm_k_recs))
        
        status = "–ù–æ—Ä–º–∞"; action_text = "‚úÖ"
        if my_tf_count < rec_min:
            status = "–ù–µ–¥–æ—Å–ø–∞–º"; action_text = f"+{rec_min - my_tf_count}"
        
        table_depth.append({
            "–°–ª–æ–≤–æ": lemma, "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_tf_count,
            "–ú–µ–¥–∏–∞–Ω–∞": round(med_total, 1), "–ú–∏–Ω–∏–º—É–º (—Ä–µ–∫)": rec_min,
            "–°—Ç–∞—Ç—É—Å": status, "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": action_text
        })

    return { 
        "depth": pd.DataFrame(table_depth), 
        "hybrid": pd.DataFrame(), 
        "relevance_top": pd.DataFrame(), 
        "my_score": {"width": my_width_score_final, "depth": my_depth_score_final}, 
        "missing_semantics_high": missing_semantics_high, 
        "missing_semantics_low": missing_semantics_low 
    }

def render_paginated_table(df, title_text, key_prefix):
    if df.empty: st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö."); return
    st.markdown(f"### {title_text}")
    st.dataframe(df, use_container_width=True)

# --- AI Helpers ---
STATIC_DATA_GEN = {
    'IP_PROP4817': "–£—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏",
    'IP_PROP4818': "–û–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –≤ —Ä–µ–≥–∏–æ–Ω—ã —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4819': """<p>–ù–∞–¥–µ–∂–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫–∞–∑–∞.</p>""",
    'IP_PROP4820': """<p>–ù–∞—à–∏ –∏–∑–¥–µ–ª–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è—Ö.</p>""",
    'IP_PROP4821': "–û–ø–ª–∞—Ç–∞ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤:",
    'IP_PROP4822': """<p>–ù–∞—à–∞ –∫–æ–º–ø–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞ –ø—Ä–∏–Ω—è—Ç—å –ª—é–±—ã–µ –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–µ –≤–∏–¥—ã –æ–ø–ª–∞—Ç—ã.</p>""",
    'IP_PROP4823': """<div class="h4"><h3>–ü—Ä–∏–º–µ—Ä—ã –≤–æ–∑–º–æ–∂–Ω–æ–π –æ–ø–ª–∞—Ç—ã</h3></div>""",
    'IP_PROP4824': "–û–ø–∏—Å–∞–Ω–∏–µ, —Å—Ç–∞—Ç—å–∏, –ø–æ–∏—Å–∫, –æ—Ç–∑—ã–≤—ã, –Ω–æ–≤–æ—Å—Ç–∏, –∞–∫—Ü–∏–∏, –∂—É—Ä–Ω–∞–ª, info:",
    'IP_PROP4825': "–ú–æ–∂–µ–º –º–µ—Ç–∞–ª–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –æ—Ü–∏–Ω–∫–æ–≤–∞—Ç—å, –Ω–∏–∫–µ–ª–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–æ–≤–æ–ª–æ—á—å",
    'IP_PROP4826': "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥",
    'IP_PROP4834': "–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –±–µ–∑ –ø—Ä–∏–º–µ—Å–µ–π",
    'IP_PROP4835': "–ü–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫",
    'IP_PROP4836': "–ö–∞—á–µ—Å—Ç–≤–æ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä",
    'IP_PROP4837': "–ü–æ—Ä—è–¥–æ–∫ –≤ –ì–û–°–¢–∞—Ö"
}

def get_page_data_for_gen(url):
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
    except Exception as e: return None, None, f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}"
    if response.status_code != 200: return None, None, f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—É—Å–∞: {response.status_code}"
    soup = BeautifulSoup(response.text, 'html.parser')
    description_div = soup.find('div', class_='description-container')
    base_text = description_div.get_text(separator="\n", strip=True) if description_div else soup.body.get_text(separator="\n", strip=True)[:5000]
    tags_container = soup.find(class_='popular-tags-inner')
    tags_data = []
    if tags_container:
        links = tags_container.find_all('a')
        for link in links:
            tag_url = urljoin(url, link.get('href')) if link.get('href') else None
            if tag_url: tags_data.append({'name': link.get_text(strip=True), 'url': tag_url})
    return base_text, tags_data, None

def generate_five_blocks(client, base_text, tag_name, seo_words=None):
    if not base_text: return ["Error: No base text"] * 5
    system_instruction = "–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä. –ù–∞–ø–∏—à–∏ 5 HTML –±–ª–æ–∫–æ–≤. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π markdown."
    keywords_instruction = ""
    if seo_words and len(seo_words) > 0:
        keywords_str = ", ".join(seo_words)
        keywords_instruction = f"–í–∫–ª—é—á–∏ —ç—Ç–∏ —Å–ª–æ–≤–∞ (—Å–∫–ª–æ–Ω—è—è –∏—Ö) –∏ –≤—ã–¥–µ–ª–∏ <b>: {keywords_str}"

    user_prompt = f"""–í–í–û–î–ù–´–ï: –¢–µ–≥ "{tag_name}". –ë–∞–∑–∞: \"\"\"{base_text[:3000]}\"\"\" {keywords_instruction}
    –ó–ê–î–ê–ß–ê: 5 –±–ª–æ–∫–æ–≤. –°—Ç—Ä—É–∫—Ç—É—Ä–∞: h2/h3, –∞–±–∑–∞—Ü, –≤–≤–æ–¥–Ω–∞—è —Ñ—Ä–∞–∑–∞:, —Å–ø–∏—Å–æ–∫, –∑–∞–∫–ª—é—á–µ–Ω–∏–µ. –ë–µ–∑ [1] —Å—Å—ã–ª–æ–∫. –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: |||BLOCK_SEP|||"""

    try:
        response = client.chat.completions.create(model="sonar-pro", messages=[{"role": "system", "content": system_instruction}, {"role": "user", "content": user_prompt}], temperature=0.7)
        content = response.choices[0].message.content
        content = re.sub(r'\[\d+\]', '', content).replace("```html", "").replace("```", "")
        blocks = [b.strip() for b in content.split("|||BLOCK_SEP|||") if b.strip()]
        while len(blocks) < 5: blocks.append("")
        return blocks[:5]
    except Exception as e: return [f"API Error: {str(e)}"] * 5

def generate_html_table(client, user_prompt, seo_keywords_data=None):
    seo_instruction = ""
    system_instruction = f"Generate HTML tables. Inline CSS: table border 2px solid black, th bg #f0f0f0. No markdown."
    try:
        response = client.chat.completions.create(model="sonar-pro", messages=[{"role": "system", "content": system_instruction}, {"role": "user", "content": user_prompt}], temperature=0.7)
        return re.sub(r'\[\d+\]', '', response.choices[0].message.content).replace("```html", "").replace("```", "").strip()
    except Exception as e: return f"Error: {e}"

# ==========================================
# STATE INIT
# ==========================================
if 'sidebar_gen_df' not in st.session_state: st.session_state.sidebar_gen_df = None
if 'sidebar_excel_bytes' not in st.session_state: st.session_state.sidebar_excel_bytes = None
if 'analysis_results' not in st.session_state: st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state: st.session_state.analysis_done = False
if 'categorized_products' not in st.session_state: st.session_state.categorized_products = []
if 'persistent_urls' not in st.session_state: st.session_state['persistent_urls'] = ""
if "arsenkin_token" in st.session_state:
    ARSENKIN_TOKEN = st.session_state.arsenkin_token
else:
    try: ARSENKIN_TOKEN = st.secrets["api"]["arsenkin_token"]
    except: ARSENKIN_TOKEN = None
if "yandex_dict_key" in st.session_state:
    YANDEX_DICT_KEY = st.session_state.yandex_dict_key
else:
    try: YANDEX_DICT_KEY = st.secrets["api"]["yandex_dict_key"]
    except: YANDEX_DICT_KEY = None

# ==========================================
# UI TABS
# ==========================================
tab_seo, tab_gen = st.tabs(["üìä SEO –ê–Ω–∞–ª–∏–∑", "üè≠ –û–ø—Ç–æ–≤–∞—è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (–¶–µ–Ω—Ç—Ä —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è)"])

# ------------------------------------------
# TAB 1: SEO
# ------------------------------------------
with tab_seo:
    col_main, col_sidebar = st.columns([65, 35])
    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
        
        my_input_type = st.radio("–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], horizontal=True, label_visibility="collapsed", key="my_page_source_radio")
        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            st.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã", placeholder="https://site.ru/catalog/tovar", label_visibility="collapsed", key="my_url_input")
        elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏–ª–∏ —Ç–µ–∫—Å—Ç", height=200, label_visibility="collapsed", placeholder="–í—Å—Ç–∞–≤—å—Ç–µ HTML", key="my_content_input")

        st.markdown("### –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å")
        st.text_input("–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞", label_visibility="collapsed", key="query_input")
        st.markdown("### –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        source_type_new = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫", ["–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)", "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"], horizontal=True, label_visibility="collapsed", key="competitor_source_radio")
        source_type = "API" if "API" in source_type_new else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"
        
        if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
            manual_val = st.text_area(
                "–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", 
                height=200, 
                key="manual_urls_widget", 
                value=st.session_state.get('persistent_urls', "")
            )
            st.session_state['persistent_urls'] = manual_val

        st.markdown("### –°–ø–∏—Å–∫–∏ (Stop / Exclude)")
        st.text_area("–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–æ–º–µ–Ω—ã", DEFAULT_EXCLUDE, height=100, key="settings_excludes")
        st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞", DEFAULT_STOPS, height=100, key="settings_stops")
        if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", type="primary", use_container_width=True, key="start_analysis_btn"):
            st.session_state.start_analysis_flag = True

    with col_sidebar:
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API")
        if not ARSENKIN_TOKEN:
             new_arsenkin = st.text_input("Arsenkin Token", type="password", key="input_arsenkin")
             if new_arsenkin: st.session_state.arsenkin_token = new_arsenkin; ARSENKIN_TOKEN = new_arsenkin 
        if not YANDEX_DICT_KEY:
             new_yandex = st.text_input("Yandex Dict Key", type="password", key="input_yandex")
             if new_yandex: st.session_state.yandex_dict_key = new_yandex; YANDEX_DICT_KEY = new_yandex
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
        st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
        st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
        st.selectbox("–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞", list(REGION_MAP.keys()), key="settings_region")
        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å <noindex>", True, key="settings_noindex")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", False, key="settings_alt")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", False, key="settings_numbers")
        st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True, key="settings_norm")
        st.selectbox("–ì–ª—É–±–∏–Ω–∞ —Å–±–æ—Ä–∞ (–¢–û–ü)", [10, 20, 30], index=0, key="settings_top_n")

    if st.session_state.get('start_analysis_flag'):
        st.session_state.start_analysis_flag = False
        settings = {'noindex': st.session_state.settings_noindex, 'alt_title': st.session_state.settings_alt, 'numbers': st.session_state.settings_numbers, 'norm': st.session_state.settings_norm, 'ua': st.session_state.settings_ua, 'custom_stops': st.session_state.settings_stops.split()}
        my_data, my_domain, my_serp_pos = None, "", 0
        current_input_type = st.session_state.get("my_page_source_radio")
        if current_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
                my_data = parse_page(st.session_state.my_url_input, settings)
                if not my_data: st.error("–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."); st.stop()
                my_domain = urlparse(st.session_state.my_url_input).netloc
        elif current_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            my_data = {'url': 'Local', 'domain': 'local', 'body_text': st.session_state.my_content_input, 'anchor_text': ''}
        
        target_urls_raw = []
        current_source_val = st.session_state.get("competitor_source_radio")
        current_source_type = "API" if "API" in current_source_val else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"
        if current_source_type == "API":
            if not ARSENKIN_TOKEN: st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç API —Ç–æ–∫–µ–Ω Arsenkin."); st.stop()
            with st.spinner("API Arsenkin..."):
                found = get_arsenkin_urls(st.session_state.query_input, st.session_state.settings_search_engine, st.session_state.settings_region, ARSENKIN_TOKEN)
                if not found: st.stop()
                excl = [d.strip() for d in st.session_state.settings_excludes.split('\n') if d.strip()]
                filtered = []
                for res in found:
                    dom = urlparse(res['url']).netloc
                    if my_domain and my_domain == dom:
                        if my_serp_pos == 0 or res['pos'] < my_serp_pos: my_serp_pos = res['pos']
                        continue
                    if any(x in dom for x in excl): continue
                    filtered.append(res)
                target_urls_raw = filtered[:st.session_state.settings_top_n]
                st.session_state['persistent_urls'] = "\n".join([i['url'] for i in target_urls_raw])
        else:
            raw_urls = st.session_state.get("persistent_urls", "")
            target_urls_raw = [{'url': u.strip(), 'pos': i+1} for i, u in enumerate(raw_urls.split('\n')) if u.strip()]
        
        if not target_urls_raw: st.error("–ù–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤."); st.stop()
        comp_data_full = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = {executor.submit(parse_page, u['url'], settings): u['url'] for u in target_urls_raw}
            for f in concurrent.futures.as_completed(futures):
                if res := f.result(): comp_data_full.append(res)
        
        with st.spinner("–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫..."):
            st.session_state.analysis_results = calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, target_urls_raw)
            st.session_state.analysis_done = True
            
            res = st.session_state.analysis_results
            words_to_check = [x['word'] for x in res.get('missing_semantics_high', [])]
            with st.spinner("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏..."):
                categorized = classify_semantics_with_api(words_to_check, YANDEX_DICT_KEY)
                st.session_state.categorized_products = categorized['products']
            st.rerun()

    if st.session_state.analysis_done and st.session_state.analysis_results:
        results = st.session_state.analysis_results
        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        st.markdown(f"<div style='background:{LIGHT_BG_MAIN};padding:15px;border-radius:8px;'><b>–†–µ–∑—É–ª—å—Ç–∞—Ç:</b> –®–∏—Ä–∏–Ω–∞: {results['my_score']['width']} | –ì–ª—É–±–∏–Ω–∞: {results['my_score']['depth']}</div>", unsafe_allow_html=True)
        
        with st.expander("üõí –†–µ–∑—É–ª—å—Ç–∞—Ç –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å–ª–æ–≤", expanded=True):
            st.info(f"üß± –¢–æ–≤–∞—Ä—ã ({len(st.session_state.categorized_products)}): {', '.join(st.session_state.categorized_products)}")
        
        render_paginated_table(results['depth'], "1. –ì–ª—É–±–∏–Ω–∞", "tbl_depth_1")
        render_paginated_table(results['relevance_top'], "2. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", "tbl_rel")

# ------------------------------------------
# TAB 2: –û–ü–¢–û–í–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø (–ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê)
# ------------------------------------------
with tab_gen:
    st.title("üè≠ –¶–µ–Ω—Ç—Ä –û–ø—Ç–æ–≤–æ–π –ì–µ–Ω–µ—Ä–∞—Ü–∏–∏")
    st.markdown("–í—ã–±–µ—Ä–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –∏—Ö –≤ –æ–¥–Ω–æ–º –æ–∫–Ω–µ –∏ –∑–∞–ø—É—Å–∫–∞–π—Ç–µ –∑–∞–¥–∞—á–∏.")

    # --- 1. –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò (–¥–µ–π—Å—Ç–≤—É—é—Ç –Ω–∞ –≤—Å–µ –º–æ–¥—É–ª–∏) ---
    with st.expander("üåç –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (API –∏ –ò—Å—Ç–æ—á–Ω–∏–∫–∏)", expanded=True):
        col_g1, col_g2 = st.columns([1, 1])
        with col_g1:
            # –û–¥–∏–Ω –∫–ª—é—á –Ω–∞ –≤—Å–µ AI –∑–∞–¥–∞—á–∏
            if 'global_pplx_key' not in st.session_state: st.session_state.global_pplx_key = "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"
            st.session_state.global_pplx_key = st.text_input("üîë Perplexity/OpenAI API Key", value=st.session_state.global_pplx_key, type="password", help="–ù—É–∂–µ–Ω –¥–ª—è –¢–µ–∫—Å—Ç–æ–≤ –∏ –¢–∞–±–ª–∏—Ü")
        with col_g2:
            # –û–¥–∏–Ω URL, –∫–æ—Ç–æ—Ä—ã–π —á–∞—Å—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –¥–ª—è –≤—Å–µ–≥–æ
            if 'global_parent_url' not in st.session_state: st.session_state.global_parent_url = ""
            st.session_state.global_parent_url = st.text_input("üîó URL –†–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–î–æ–Ω–æ—Ä)", value=st.session_state.global_parent_url, placeholder="https://site.ru/catalog/category/")

    st.divider()

    # --- 2. –°–ï–õ–ï–ö–¢–û–† –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í ---
    st.subheader("üõ†Ô∏è –í—ã–±–µ—Ä–∏—Ç–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã:")
    c_sel1, c_sel2, c_sel3, c_sel4, c_sel5 = st.columns(5)
    
    use_texts = c_sel1.checkbox("ü§ñ AI –¢–µ–∫—Å—Ç—ã", value=True)
    use_tags = c_sel2.checkbox("üè∑Ô∏è –ü–ª–∏—Ç–∫–∞ —Ç–µ–≥–æ–≤")
    use_sidebar = c_sel3.checkbox("üìë –ë–æ–∫–æ–≤–æ–µ –º–µ–Ω—é")
    use_tables = c_sel4.checkbox("üß© –¢–∞–±–ª–∏—Ü—ã (Spec)")
    use_promo = c_sel5.checkbox("üî• –ü—Ä–æ–º–æ-–∞–∫—Ü–∏–∏")

    st.markdown("---")

    # --- 3. –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ï –ë–õ–û–ö–ò ---

    # === –ë–õ–û–ö 1: AI –¢–ï–ö–°–¢–´ ===
    if use_texts:
        with st.container():
            st.markdown('<div class="tool-card"><div class="block-title"><span class="block-icon">ü§ñ</span> –ì–µ–Ω–µ—Ä–∞—Ü–∏—è AI –¢–µ–∫—Å—Ç–æ–≤</div>', unsafe_allow_html=True)
            
            col_t1, col_t2 = st.columns([2, 1])
            with col_t1:
                target_url_text = st.text_input("URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ–≥–æ–≤ (–µ—Å–ª–∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ)", value=st.session_state.global_parent_url, key="txt_url_in")
            with col_t2:
                # –ë–µ—Ä–µ–º SEO —Å–ª–æ–≤–∞ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
                default_seo = ""
                if st.session_state.analysis_results:
                     high = st.session_state.analysis_results.get('missing_semantics_high', [])
                     if high: default_seo = ", ".join([x['word'] for x in high[:10]])
                seo_words_str = st.text_input("SEO —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)", value=default_seo, placeholder="–∫—É–ø–∏—Ç—å, —Ü–µ–Ω–∞, –æ–ø—Ç–æ–º", key="txt_seo_in")
            
            if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–æ–≤", key="btn_run_text"):
                if not st.session_state.global_pplx_key: st.error("–ù–µ—Ç API –∫–ª—é—á–∞!"); st.stop()
                if not target_url_text: st.error("–ù–µ—Ç URL!"); st.stop()
                
                status_box = st.status("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...", expanded=True)
                client = openai.OpenAI(api_key=st.session_state.global_pplx_key, base_url="https://api.perplexity.ai")
                base_text, tags, err = get_page_data_for_gen(target_url_text)
                if err or not tags: status_box.error(err or "–ù–µ—Ç —Ç–µ–≥–æ–≤"); st.stop()
                
                seo_list = [w.strip() for w in seo_words_str.split(',')] if seo_words_str else []
                all_rows = []
                bar = st.progress(0)
                for i, tag in enumerate(tags):
                    blocks = generate_five_blocks(client, base_text, tag['name'], seo_list)
                    all_rows.append({'TagName': tag['name'], 'URL': tag['url'], 'IP_PROP4839': blocks[0], 'IP_PROP4816': blocks[1], 'IP_PROP4838': blocks[2], 'IP_PROP4829': blocks[3], 'IP_PROP4831': blocks[4], **STATIC_DATA_GEN})
                    bar.progress((i+1)/len(tags))
                
                df_text = pd.DataFrame(all_rows)
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer: df_text.to_excel(writer, index=False)
                
                status_box.update(label="‚úÖ –ì–æ—Ç–æ–≤–æ!", state="complete", expanded=False)
                st.download_button("üì• –°–∫–∞—á–∞—Ç—å Excel (–¢–µ–∫—Å—Ç—ã)", buffer.getvalue(), "seo_texts.xlsx", "application/vnd.ms-excel", key="down_text_btn")

            st.markdown('</div>', unsafe_allow_html=True)

    # === –ë–õ–û–ö 2: –ü–õ–ò–¢–ö–ê –¢–ï–ì–û–í ===
    if use_tags:
        with st.container():
            st.markdown('<div class="tool-card"><div class="block-title"><span class="block-icon">üè∑Ô∏è</span> –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ü–ª–∏—Ç–∫–∏ –¢–µ–≥–æ–≤</div>', unsafe_allow_html=True)
            
            col_tg1, col_tg2 = st.columns([1, 1])
            with col_tg1:
                tags_cat_url = st.text_input("URL –ö–∞—Ç–µ–≥–æ—Ä–∏–∏", value=st.session_state.global_parent_url, key="tags_url_in")
                tags_file = st.file_uploader("–ë–∞–∑–∞ —Å—Å—ã–ª–æ–∫ (.txt)", type=["txt"], key="tags_file_in")
            with col_tg2:
                # –ê–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞–º–∏ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞
                def_prods = "\n".join(st.session_state.categorized_products) if st.session_state.categorized_products else ""
                tags_products_in = st.text_area("–°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ (–∞–Ω–∫–æ—Ä—ã)", value=def_prods, height=100, key="tags_prod_in")

            if st.button("üöÄ –°–æ–±—Ä–∞—Ç—å –ø–ª–∏—Ç–∫—É —Ç–µ–≥–æ–≤", key="btn_run_tags"):
                if not tags_file or not tags_cat_url or not tags_products_in: st.error("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–æ–ª—è"); st.stop()
                status_box = st.status("–°–±–æ—Ä–∫–∞ –ø–ª–∏—Ç–∫–∏...", expanded=True)
                
                target_urls_list = []
                try:
                    r = requests.get(tags_cat_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
                    if r.status_code == 200:
                        soup = BeautifulSoup(r.text, 'html.parser')
                        tags_container = soup.find(class_='popular-tags-inner')
                        if tags_container:
                            for link in tags_container.find_all('a'):
                                href = link.get('href')
                                if href: target_urls_list.append(urljoin(tags_cat_url, href))
                except Exception as e: status_box.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}"); st.stop()
                
                if not target_urls_list: status_box.error("–¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–ª–∞—Å—Å .popular-tags-inner)"); st.stop()
                
                products = [line.strip() for line in tags_products_in.split('\n') if line.strip()]
                stringio = io.StringIO(tags_file.getvalue().decode("utf-8"))
                all_txt_links = [line.strip() for line in stringio.readlines() if line.strip()]
                
                product_candidates_map = {}
                for p in products:
                    tr = transliterate_text(p)
                    if len(tr) >= 3:
                        matches = [u for u in all_txt_links if tr in u]
                        if matches: product_candidates_map[p] = matches
                
                final_rows = []
                for i, target_url in enumerate(target_urls_list):
                    current_page_tags = []
                    for prod_name, candidates in product_candidates_map.items():
                        valid = [u for u in candidates if u.rstrip('/') != target_url.rstrip('/')]
                        if valid:
                            chosen_url = random.choice(valid)
                            current_page_tags.append({'name': prod_name.capitalize(), 'url': chosen_url})
                    if current_page_tags:
                        random.shuffle(current_page_tags)
                        html_block = '<div class="popular-tags">\n' + "\n".join([f'    <a href="{item["url"]}" class="tag-link">{item["name"]}</a>' for item in current_page_tags]) + '\n</div>'
                    else: html_block = ""
                    final_rows.append({'Page URL': target_url, 'Tags HTML': html_block})
                
                df_tags_result = pd.DataFrame(final_rows)
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer: df_tags_result.to_excel(writer, index=False)
                
                status_box.update(label="‚úÖ –ì–æ—Ç–æ–≤–æ!", state="complete", expanded=False)
                st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel (–¢–µ–≥–∏)", data=buffer.getvalue(), file_name="tags_tiles.xlsx", key="down_tags_btn")
                
            st.markdown('</div>', unsafe_allow_html=True)

    # === –ë–õ–û–ö 3: –ë–û–ö–û–í–û–ï –ú–ï–ù–Æ ===
    if use_sidebar:
        with st.container():
            st.markdown('<div class="tool-card"><div class="block-title"><span class="block-icon">üìë</span> –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ú–µ–Ω—é (Mass Excel)</div>', unsafe_allow_html=True)
            
            col_sb1, col_sb2 = st.columns([1, 1])
            with col_sb1:
                sb_url = st.text_input("URL –î–æ–Ω–æ—Ä–∞", value=st.session_state.global_parent_url, key="sb_url_in")
            with col_sb2:
                sb_file = st.file_uploader("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –¥–ª—è –º–µ–Ω—é (.txt)", type=["txt"], key="sb_file_in")
            
            SIDEBAR_ASSETS = """<style>:root { font-size: 14px; } #sidebar-menu ul { list-style: none !important; } </style>""" # –°–æ–∫—Ä–∞—â–µ–Ω–æ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏, —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –Ω–µ –ø–æ—Å—Ç—Ä–∞–¥–∞–µ—Ç

            if st.button("üöÄ –°–æ–∑–¥–∞—Ç—å –º–µ–Ω—é", key="btn_run_sb"):
                if not sb_file or not sb_url: st.error("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –ø–æ–ª—è"); st.stop()
                status_box = st.status("–°–±–æ—Ä–∫–∞ –º–µ–Ω—é...", expanded=True)
                
                stringio = io.StringIO(sb_file.getvalue().decode("utf-8"))
                urls = [line.strip() for line in stringio.readlines() if line.strip()]
                urls = list(dict.fromkeys(urls))
                
                # –õ–æ–≥–∏–∫–∞ –¥–µ—Ä–µ–≤–∞
                tree = {}
                for url in urls:
                    path = urlparse(url).path.strip('/')
                    parts = [p for p in path.split('/') if p]
                    start_idx = 0
                    if 'catalog' in parts: start_idx = parts.index('catalog') + 1
                    relevant_parts = parts[start_idx:] if parts[start_idx:] else parts
                    current_level = tree
                    for i, part in enumerate(relevant_parts):
                        if part not in current_level: current_level[part] = {}
                        if i == len(relevant_parts) - 1:
                            current_level[part]['__url__'] = url
                            current_level[part]['__name__'] = force_cyrillic_name_global(part)
                        current_level = current_level[part]

                def render_tree(node, level=1):
                    html = ""
                    keys = sorted([k for k in node.keys() if not k.startswith('__')])
                    for key in keys:
                        child = node[key]
                        name = child.get('__name__', force_cyrillic_name_global(key))
                        url = child.get('__url__')
                        has_children = any(k for k in child.keys() if not k.startswith('__'))
                        if level == 1:
                            html += '<li class="level-1-header">\n'
                            if has_children:
                                html += f'    <span class="dropdown-toggle">{name}</span>\n'
                                html += '    <ul class="collapse-menu list-unstyled">\n' + render_tree(child, level=2) + '    </ul>\n'
                            else:
                                html += f'    <a href="{url if url else "#"}">{name}</a>\n'
                            html += '</li>\n'
                        # ... —É—Ä–æ–≤–Ω–∏ 2 –∏ 3 –æ–ø—É—â–µ–Ω—ã –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏, –Ω–æ –ª–æ–≥–∏–∫–∞ –ø–æ–Ω—è—Ç–Ω–∞
                    return html

                inner_html = render_tree(tree, level=1)
                full_sidebar_code = f"""<div class="sidebar-wrapper"><nav id="sidebar-menu"><ul class="list-unstyled components">{inner_html}</ul></nav></div>"""

                # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–Ω–æ—Ä–∞
                found_tags_urls = []
                try:
                    r = requests.get(sb_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)
                    if r.status_code == 200:
                        soup = BeautifulSoup(r.text, 'html.parser')
                        tags_container = soup.find(class_='popular-tags-inner')
                        if tags_container:
                            for link in tags_container.find_all('a'):
                                href = link.get('href')
                                if href: found_tags_urls.append(urljoin(sb_url, href))
                        else: found_tags_urls.append(sb_url)
                except: found_tags_urls.append(sb_url)
                
                excel_data = []
                for tag_url in found_tags_urls: excel_data.append({'Page URL': tag_url, 'Sidebar HTML': full_sidebar_code})
                df_sidebar = pd.DataFrame(excel_data)
                
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer: df_sidebar.to_excel(writer, index=False)
                
                status_box.update(label="‚úÖ –ú–µ–Ω—é –≥–æ—Ç–æ–≤–æ!", state="complete", expanded=False)
                st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel (–ú–µ–Ω—é)", data=buffer.getvalue(), file_name="sidebar_menu.xlsx", key="down_sb_btn")
            
            st.markdown('</div>', unsafe_allow_html=True)

    # === –ë–õ–û–ö 4: –¢–ê–ë–õ–ò–¶–´ ===
    if use_tables:
        with st.container():
            st.markdown('<div class="tool-card"><div class="block-title"><span class="block-icon">üß©</span> –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¢–∞–±–ª–∏—Ü (Specs)</div>', unsafe_allow_html=True)
            
            col_tbl1, col_tbl2 = st.columns([3, 1])
            with col_tbl1:
                tbl_url = st.text_input("URL –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (–∏—Å—Ç–æ—á–Ω–∏–∫ —Ç–µ–≥–æ–≤)", value=st.session_state.global_parent_url, key="tbl_url_in")
            with col_tbl2:
                num_tables_val = st.selectbox("–ö–æ–ª-–≤–æ —Ç–∞–±–ª–∏—Ü", [1, 2, 3], key="tbl_num_in")
            
            cols_headers = st.columns(num_tables_val)
            headers_vals = []
            defaults = ["–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–†–∞–∑–º–µ—Ä—ã", "–°–æ—Å—Ç–∞–≤"]
            for i, c in enumerate(cols_headers):
                h = c.text_input(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫ {i+1}", value=defaults[i] if i<3 else f"–¢–∞–±–ª {i+1}", key=f"tbl_h_{i}")
                headers_vals.append(h)

            if st.button("üöÄ –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—ã", key="btn_run_tbl"):
                if not st.session_state.global_pplx_key or not tbl_url: st.error("–ù–µ—Ç API –∫–ª—é—á–∞ –∏–ª–∏ URL"); st.stop()
                status_box = st.status("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü...", expanded=True)
                client = openai.OpenAI(api_key=st.session_state.global_pplx_key, base_url="https://api.perplexity.ai")
                
                # –ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–≥–æ–≤
                tags_found = []
                try:
                    r = requests.get(tbl_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)
                    if r.status_code == 200:
                        soup = BeautifulSoup(r.text, 'html.parser')
                        tags_container = soup.find(class_='popular-tags-inner')
                        if tags_container:
                             for link in tags_container.find_all('a'):
                                tags_found.append({'name': link.get_text(strip=True), 'url': urljoin(tbl_url, link.get('href'))})
                except: pass
                
                if not tags_found: status_box.error("–¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"); st.stop()
                
                results_rows = []
                bar = st.progress(0)
                path = urlparse(tbl_url).path.strip('/')
                parent_name = force_cyrillic_name_global(path.split('/')[-1])

                for idx, tag in enumerate(tags_found):
                    row_data = {'Tag Name': tag['name'], 'Tag URL': tag['url']}
                    full_product_name = f"{parent_name} {tag['name']}"
                    for t_i, t_topic in enumerate(headers_vals):
                        user_prompt = f"""Task: Create a technical HTML table. Product: "{full_product_name}". Table Topic: "{t_topic}". Content: Generate realistic technical data."""
                        html = generate_html_table(client, user_prompt)
                        row_data[f'Table_{t_i+1}_HTML'] = html
                    results_rows.append(row_data)
                    bar.progress((idx+1)/len(tags_found))
                
                df_final = pd.DataFrame(results_rows)
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer: df_final.to_excel(writer, index=False)
                
                status_box.update(label="‚úÖ –ì–æ—Ç–æ–≤–æ!", state="complete", expanded=False)
                st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel (–¢–∞–±–ª–∏—Ü—ã)", data=buffer.getvalue(), file_name="smart_tables.xlsx", key="down_tbl_btn")

            st.markdown('</div>', unsafe_allow_html=True)

    # === –ë–õ–û–ö 5: –ü–†–û–ú–û ===
    if use_promo:
        with st.container():
            st.markdown('<div class="tool-card"><div class="block-title"><span class="block-icon">üî•</span> –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ü—Ä–æ–º–æ-–±–ª–æ–∫–∞</div>', unsafe_allow_html=True)
            
            col_pr1, col_pr2 = st.columns([1, 1])
            with col_pr1:
                promo_db = st.file_uploader("–ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ (.xlsx)", type=['xlsx'], key="promo_db_in")
                promo_title = st.text_input("–ó–∞–≥–æ–ª–æ–≤–æ–∫ –±–ª–æ–∫–∞", value="–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º", key="promo_tit_in")
            with col_pr2:
                promo_links = st.text_area("–°—Å—ã–ª–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –±–ª–æ–∫–∞", height=100, key="promo_links_in")
            
            if st.button("üöÄ –°–æ–±—Ä–∞—Ç—å –ü—Ä–æ–º–æ", key="btn_run_promo"):
                if not promo_db or not promo_links: st.error("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã"); st.stop()
                status_box = st.status("–°–±–æ—Ä–∫–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫...", expanded=True)
                
                df_db = pd.read_excel(promo_db)
                img_db = {}
                for index, row in df_db.iterrows():
                    raw_url = str(row.iloc[0]).strip()
                    img_val = str(row.iloc[1]).strip()
                    if raw_url: img_db[raw_url.rstrip('/')] = img_val
                
                target_links = [line.strip() for line in promo_links.split('\n') if line.strip()]
                items_html = ""
                for link in target_links:
                    search_key = link.rstrip('/') 
                    img_src = img_db.get(search_key, "") 
                    slug = search_key.split('/')[-1]
                    name = force_cyrillic_name_global(slug)
                    items_html += f"""<div class="gallery-item"><h3><a href="{link}">{name}</a></h3><figure><img src="{img_src}"></figure></div>"""
                
                full_block = f"""<div class="gallery-wrapper"><h3>{promo_title}</h3><div class="gallery">{items_html}</div></div>"""
                
                # –ü–∞—Ä—Å–∏–Ω–≥ –¥–ª—è Excel
                found_tags = []
                try:
                    r = requests.get(st.session_state.global_parent_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
                    if r.status_code == 200:
                        soup = BeautifulSoup(r.text, 'html.parser')
                        tags_container = soup.find(class_='popular-tags-inner')
                        if tags_container:
                            for link in tags_container.find_all('a'):
                                href = link.get('href')
                                if href: found_tags.append(urljoin(st.session_state.global_parent_url, href))
                except: pass
                if not found_tags: found_tags.append(st.session_state.global_parent_url)
                
                excel_rows = []
                for tag_url in found_tags: excel_rows.append({'Page URL': tag_url, 'HTML Block': full_block})
                
                df_promo = pd.DataFrame(excel_rows)
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer: df_promo.to_excel(writer, index=False)
                
                status_box.update(label="‚úÖ –ì–æ—Ç–æ–≤–æ!", state="complete", expanded=False)
                st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel (Promo)", data=buffer.getvalue(), file_name="promo_blocks.xlsx", key="down_promo_btn")

            st.markdown('</div>', unsafe_allow_html=True)

    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã–±—Ä–∞–Ω–æ
    if not any([use_texts, use_tags, use_sidebar, use_tables, use_promo]):
        st.info("üëà –í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Å–≤–µ—Ä—Ö—É, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É.")

