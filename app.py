import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin
import inspect
import time
import json
import io 
import os

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ openai
try:
    import openai
except ImportError:
    openai = None

# ==========================================
# 0. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–û–°–¢–û–Ø–ù–ò–Ø (SESSION STATE)
# ==========================================
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False

if 'ai_generated_df' not in st.session_state:
    st.session_state.ai_generated_df = None
if 'ai_excel_bytes' not in st.session_state:
    st.session_state.ai_excel_bytes = None

if 'tags_html_result' not in st.session_state:
    st.session_state.tags_html_result = None

if 'table_html_result' not in st.session_state:
    st.session_state.table_html_result = None

if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´ –ò –°–ü–ò–°–ö–ò
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO", page_icon="üìä")

GARBAGE_LATIN_STOPLIST = {
    'whatsapp', 'viber', 'telegram', 'skype', 'vk', 'instagram', 'facebook', 'youtube', 'twitter',
    'cookie', 'cookies', 'policy', 'privacy', 'agreement', 'terms',
    'click', 'submit', 'send', 'zakaz', 'basket', 'cart', 'order', 'call', 'back', 'callback',
    'login', 'logout', 'sign', 'register', 'auth', 'account', 'profile',
    'search', 'menu', 'nav', 'navigation', 'footer', 'header', 'sidebar',
    'img', 'jpg', 'png', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'svg',
    'ok', 'error', 'undefined', 'null', 'true', 'false', 'var', 'let', 'const', 'function', 'return',
    'ru', 'en', 'com', 'net', 'org', 'biz', 'shop', 'store',
    'phone', 'email', 'tel', 'fax', 'mob', 'address', 'copyright', 'all', 'rights', 'reserved',
    'div', 'span', 'class', 'id', 'style', 'script', 'body', 'html', 'head', 'meta', 'link'
}

# ==========================================
# 2. –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø
# ==========================================
def check_password():
    if st.session_state.get("authenticated"):
        return True
    
    st.markdown("""
        <style>
        .main {
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
        }
        .auth-logo-box {
            text-align: center;
            margin-bottom: 1rem;
            padding-top: 0; 
        }
        .login-box h3 {
            margin-top: 0;
            text-align: center;
        }
        </style>
    """, unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="auth-logo-box">', unsafe_allow_html=True)
        try:
            st.image("logo.png", width=250) 
        except Exception:
            st.markdown("<h3 style='color: #D32F2F; font-size: 14px; margin-top: 0;'>LOGO (–ù–µ –Ω–∞–π–¥–µ–Ω)</h3>", unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)
        
        st.markdown('<div class="login-box">', unsafe_allow_html=True)
        st.markdown("<h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3>", unsafe_allow_html=True)
        
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "jfV6Xel-Q7vp-_s2UYPO":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
        st.markdown('</div>', unsafe_allow_html=True)
        
    return False

if not check_password():
    st.stop()

# ==========================================
# 3. –ù–ê–°–¢–†–û–ô–ö–ò API –ò –†–ï–ì–ò–û–ù–û–í
# ==========================================
ARSENKIN_TOKEN = "43acbbb60cb7989c05914ff21be45379"

REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956},
    "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931},
    "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–ö–∏–µ–≤ (UA)": {"ya": 143, "go": 1012852},
    "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493},
    "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601}
}

DEFAULT_EXCLUDE_DOMAINS = [
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "ebay.com",
    "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", "pandao.ru",
    "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", "banki.ru", 
    "regmarkets.ru", "zoon.ru", "pulscen.ru", "prodoctorov.ru", "blizko.ru", 
    "domclick.ru", "satom.ru", "quto.ru", "edadeal.ru", "cataloxy.ru", 
    "irr.ru", "onliner.by", "shop.by", "deal.by", "yell.ru", 
    "profi.ru", "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by", 
    "market.yandex.ru", "youtube.com", "gosuslugi.ru", "dzen.ru", 
    "2gis.by", "wildberries.ru", "rutube.ru", "vk.com", "facebook.com"
]
DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n–∫—É–ø–∏—Ç—å\n—Ü–µ–Ω–∞\n—à—Ç\n—Å–º\n–º–º\n–∫–≥\n–∫–≤\n–º2\n—Å—Ç—Ä\n—É–ª"

# –¶–≤–µ—Ç–∞
PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE" 

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif; color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important; }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important; color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] div[role="gridcell"] {{
            background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; border-bottom: 1px solid {ROW_BORDER_COLOR} !important;
        }}
        .legend-box {{ padding: 10px; background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px; }}
        .text-red {{ color: #D32F2F; font-weight: bold; }}
        .text-green {{ color: #2E7D32; font-weight: bold; }}
        .text-bold {{ font-weight: 600; }}
        .sort-container {{ background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 8px; margin-bottom: 10px; border: 1px solid {BORDER_COLOR}; }}
        section[data-testid="stSidebar"] {{ background-color: #FFFFFF !important; border-left: 1px solid {BORDER_COLOR} !important; }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 4. –õ–û–ì–ò–ö–ê (–ë–≠–ö–ï–ù–î)
# ==========================================

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP
try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except Exception as e:
    morph = None
    USE_NLP = False
    st.sidebar.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ NLP: {e}")

# --- –§–£–ù–ö–¶–ò–Ø –†–ê–ë–û–¢–´ –° API ARSENKIN ---
def get_arsenkin_urls(query, engine_type, region_name, depth_val=10):
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check" 
    url_get = "https://arsenkin.ru/api/tools/get"
    
    headers = {
        "Authorization": f"Bearer {ARSENKIN_TOKEN}",
        "Content-type": "application/json"
    }
    
    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []
    
    if "–Ø–Ω–¥–µ–∫—Å" in engine_type:
        se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type:
        se_params.append({"type": 11, "region": reg_ids['go']})
        
    payload = {
        "tools_name": "check-top",
        "data": {
            "queries": [query],
            "is_snippet": False,
            "noreask": True,
            "se": se_params,
            "depth": depth_val
        }
    }
    
    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        resp_json = r.json()
        if "error" in resp_json or "task_id" not in resp_json:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ API (—Å—Ç–∞—Ä—Ç): {resp_json}")
            return []
        task_id = resp_json["task_id"]
        st.toast(f"–ó–∞–¥–∞—á–∞ ID {task_id} –∑–∞–ø—É—â–µ–Ω–∞")
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–µ –∑–∞–¥–∞—á–∏: {e}")
        return []
    
    status = "process"
    attempts = 0
    max_attempts = 40
    progress_info = st.empty()
    bar = st.progress(0)
    res_check_data = {}
    
    while status == "process" and attempts < max_attempts:
        time.sleep(5)
        attempts += 1
        bar.progress(attempts / max_attempts)
        progress_info.text(f"–û–∂–∏–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ API... ({attempts*5} —Å–µ–∫)")
        
        try:
            r_check = requests.post(url_check, headers=headers, json={"task_id": task_id})
            res_check_data = r_check.json()
            if res_check_data.get("status") == "finish":
                status = "done"
                break
            if str(res_check_data.get("code")) == "429":
                continue 
        except Exception:
            pass
            
    bar.empty()
    progress_info.empty()
        
    if status != "done":
        st.error(f"‚è≥ –í—Ä–µ–º—è –≤—ã—à–ª–æ. –°—Ç–∞—Ç—É—Å: {res_check_data.get('status', 'Unknown')}")
        st.write("JSON-–æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞:")
        st.json(res_check_data)
        return []
        
    res_data = {}
    try:
        st.info("–°—Ç–∞—Ç—É—Å 'finish' –ø–æ–ª—É—á–µ–Ω. –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç...")
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id}, timeout=30)
        res_data = r_final.json()
        if res_data.get("code") != "TASK_RESULT":
            st.error(f"‚ùå –û—à–∏–±–∫–∞: API –Ω–µ –≤–µ—Ä–Ω—É–ª —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.")
            st.json(res_data)
            return []
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
        st.json(res_data)
        return []

    results_list = []
    try:
        if 'result' in res_data and 'result' in res_data['result'] and 'collect' in res_data['result']['result']:
            collect = res_data['result']['result']['collect']
        else:
            st.error("‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: –ù–µ—Ç –ø–æ–ª—è 'collect'.")
            st.json(res_data)
            return []

        final_url_list = []
        if collect and isinstance(collect, list) and len(collect) > 0 and \
           collect[0] and isinstance(collect[0], list) and len(collect[0]) > 0 and \
           collect[0][0] and isinstance(collect[0][0], list):
            final_url_list = collect[0][0]
        else:
             unique_urls = set()
             for engine_data in collect:
                if isinstance(engine_data, dict):
                    for engine_id, serps in engine_data.items():
                        if isinstance(serps, list):
                            for item in serps:
                                url = item.get('url')
                                pos = item.get('pos')
                                if url and pos:
                                    if url not in unique_urls:
                                        results_list.append({'url': url, 'pos': pos})
                                        unique_urls.add(url)
                                    else:
                                        for res in results_list:
                                            if res['url'] == url and pos < res['pos']:
                                                res['pos'] = pos
             return results_list 

        if final_url_list:
            for index, url in enumerate(final_url_list):
                pos = index + 1
                results_list.append({'url': url, 'pos': pos})
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        st.json(res_data) 
        return []
    return results_list

def process_text_detailed(text, settings, n_gram=1):
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ –º–µ–Ω—è–µ–º '—ë' –Ω–∞ '–µ' –ü–ï–†–ï–î –≤—Å–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–º
    text = text.lower().replace('—ë', '–µ')
    
    pattern = r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+' 
    words = re.findall(pattern, text)
    
    # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ —Ç–æ–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º (—ë -> –µ)
    stops = set(w.lower().replace('—ë', '–µ') for w in settings['custom_stops'])
    
    lemmas = []
    forms_map = defaultdict(set)
    
    for w in words:
        if len(w) < 2: continue
        
        if not settings['numbers'] and w.isdigit():
            continue
            
        if w in stops: continue
        
        lemma = w
        if USE_NLP and n_gram == 1: 
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            
            # !FIX: –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –ó–ê–ú–ï–ù–ê –Å –ù–ê –ï –í –õ–ï–ú–ú–ï
            lemma = p.normal_form.replace('—ë', '–µ')
        
        lemmas.append(lemma)
        forms_map[lemma].add(w)
    
    if n_gram > 1:
        ngrams = []
        for i in range(len(lemmas) - n_gram + 1):
            phrase = " ".join(lemmas[i:i+n_gram])
            ngrams.append(phrase)
        return ngrams, {}
        
    return lemmas, forms_map

def parse_page(url, settings):
    headers = {'User-Agent': settings['ua']}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code != 200: return None
        soup = BeautifulSoup(r.text, 'html.parser')
        
        tags_to_remove = []
        
        if settings['noindex']:
            tags_to_remove.append('noindex') 
        
        comments = soup.find_all(string=lambda text: isinstance(text, Comment))
        for c in comments: c.extract()
        
        if tags_to_remove:
            for t in soup.find_all(tags_to_remove): t.decompose()
            
        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)
        
        extra_text = []
        
        # –í—Å–µ–≥–¥–∞ —Å–æ–±–∏—Ä–∞–µ–º Meta Description –∏ Keywords
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            extra_text.append(meta_desc['content'])
            
        meta_kw = soup.find('meta', attrs={'name': 'keywords'})
        if meta_kw and meta_kw.get('content'):
            extra_text.append(meta_kw['content'])
            
        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])
            
        body_text_raw = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text = re.sub(r'\s+', ' ', body_text_raw).strip()
        
        if not body_text:
            return None 

        return {'url': url, 'domain': urlparse(url).netloc, 'body_text': body_text, 'anchor_text': anchor_text}
    except: 
        return None

def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    all_forms_map = defaultdict(set)
    
    # --- 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ ---
    if not my_data or not my_data.get('body_text'):
        my_lemmas, my_forms, my_anchors, my_len = [], {}, [], 0
    else:
        my_lemmas, my_forms = process_text_detailed(my_data['body_text'], settings)
        my_anchors, _ = process_text_detailed(my_data['anchor_text'], settings)
        my_len = len(my_lemmas)
        for k, v in my_forms.items():
            all_forms_map[k].update(v)

    # --- 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ ---
    comp_data_parsed = [d for d in comp_data_full if d.get('body_text')]
    
    comp_docs = []
    for p in comp_data_parsed:
        body, c_forms = process_text_detailed(p['body_text'], settings)
        anchor, _ = process_text_detailed(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor})
        for k, v in c_forms.items():
            all_forms_map[k].update(v)
    
    # –ó–∞–≥–ª—É—à–∫–∞, –µ—Å–ª–∏ –Ω–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    if not comp_docs:
        table_rel_fallback = []
        for item in original_results:
            domain = urlparse(item['url']).netloc
            table_rel_fallback.append({
                "–î–æ–º–µ–Ω": domain, 
                "–ü–æ–∑–∏—Ü–∏—è": item['pos'],
                "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": 0, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": 0
            })
        
        if my_data and my_data.get('domain'):
            my_label = f"{my_data['domain']} (–í—ã)"
        else:
            my_label = "–í–∞—à —Å–∞–π—Ç"
        
        table_rel_fallback.append({
            "–î–æ–º–µ–Ω": my_label, 
            "–ü–æ–∑–∏—Ü–∏—è": my_serp_pos if my_serp_pos > 0 else len(original_results) + 1,
            "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": 0, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": 0
        })
        
        table_rel_df = pd.DataFrame(table_rel_fallback).sort_values(by='–ü–æ–∑–∏—Ü–∏—è', ascending=True).reset_index(drop=True)
        return {
            "depth": pd.DataFrame(), 
            "hybrid": pd.DataFrame(), 
            "relevance_top": table_rel_df, 
            "my_score": {"width": 0, "depth": 0}, 
            "missing_semantics_high": [], 
            "missing_semantics_low": []
        }

    # –î–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤
    c_lens = [len(d['body']) for d in comp_docs]
    median_len = np.median(c_lens)
    
    # AvgL (–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤) –¥–ª—è BM25
    avg_dl = np.mean(c_lens) if c_lens else 0
    
    if median_len > 0 and my_len > 0 and settings['norm']:
        norm_k = my_len / median_len
    else:
        norm_k = 1.0
    
    # –ü–æ–ª–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    N = len(comp_docs) 
    
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
        
    word_counts_per_doc = []
    for d in comp_docs:
        word_counts_per_doc.append(Counter(d['body']))

    # --- –≠–¢–ê–ü 1: TF-IDF (–≤–µ—Å —Å–ª–æ–≤) ---
    word_idf_map = {}
    for lemma in vocab:
        df = doc_freqs[lemma]
        if df == 0: continue
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π IDF
        idf = math.log((N + 1) / (df + 1)) + 1
        word_idf_map[lemma] = idf

    # --- –≠–¢–ê–ü 2: –Ø–î–†–û (S_WIDTH_CORE) ---
    S_WIDTH_CORE = set()
    missing_semantics_high = []
    missing_semantics_low = []
    
    my_full_lemmas_set = set(my_lemmas) | set(my_anchors)
    lsi_candidates_weighted = [] # –î–ª—è —Ç–∞–±–ª–∏—Ü

    # –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ —Å–ª–æ–≤
    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        
        c_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        med_val = np.median(c_counts)
        percent = int((doc_freqs[lemma] / N) * 100)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å —Å–ª–æ–≤–∞ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
        # –î–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –≤–µ—Å: IDF * Median_TF
        # (–≤ —Ç–∞–±–ª–∏—Ü–µ Hybrid –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–ª–æ–∂–Ω—ã–π TF-IDF, –∑–¥–µ—Å—å —É–ø—Ä–æ—Å—Ç–∏–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å–±–æ—Ä–∞ —è–¥—Ä–∞)
        weight_simple = word_idf_map.get(lemma, 0) * med_val
        
        if med_val > 0:
            lsi_candidates_weighted.append((lemma, weight_simple))

        # === –£–°–õ–û–í–ò–ï –Ø–î–†–ê (Strict) ===
        is_width_word = False
        if med_val >= 1: 
            S_WIDTH_CORE.add(lemma)
            is_width_word = True
        
        # –°–ø–∏—Å–∫–∏ —É–ø—É—â–µ–Ω–Ω–æ–≥–æ
        if lemma not in my_full_lemmas_set:
            if len(lemma) < 2: continue
            if lemma.isdigit(): continue
            
            item = {'word': lemma, 'percent': percent, 'weight': weight_simple}
            
            if is_width_word:
                missing_semantics_high.append(item)
            elif percent >= 30:
                 missing_semantics_low.append(item)

    missing_semantics_high.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low.sort(key=lambda x: x['percent'], reverse=True)
    
    # –î–ª—è —Ç–∞–±–ª–∏—Ü—ã Depth —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–µ—Å—É
    lsi_candidates_weighted.sort(key=lambda x: x[1], reverse=True)
    S_DEPTH_TOP70 = set([x[0] for x in lsi_candidates_weighted[:70]])

    # --- –≠–¢–ê–ü 3: –†–ê–°–ß–ï–¢ BM25 (–ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê –ì–õ–£–ë–ò–ù–´) ---
    
    def calculate_bm25_for_doc(doc_tokens, doc_len):
        """
        –°—á–∏—Ç–∞–µ—Ç '—Å—ã—Ä–æ–π' BM25 –¥–æ–∫—É–º–µ–Ω—Ç–∞, —Å—É–º–º–∏—Ä—É—è –≤–µ—Å–∞ —Ç–æ–ª—å–∫–æ –ø–æ —Å–ª–æ–≤–∞–º –∏–∑ S_WIDTH_CORE.
        –§–æ—Ä–º—É–ª–∞: Sum( IDF * (TF * 2.2) / (TF + 1.2 * (0.25 + 0.75 * L/AvgL)) )
        """
        if avg_dl == 0 or doc_len == 0: return 0
        
        score = 0
        counts = Counter(doc_tokens)
        
        # –ó–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å K, –∑–∞–≤–∏—Å—è—â–∏–π –æ—Ç –¥–ª–∏–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
        # 1.2 * (0.25 + 0.75 * L/AvgL)
        K = 1.2 * (0.25 + 0.75 * (doc_len / avg_dl))
        
        # –°—É–º–º–∏—Ä—É–µ–º –¢–û–õ–¨–ö–û –ø–æ –∑–Ω–∞—á–∏–º—ã–º —Å–ª–æ–≤–∞–º (S_WIDTH_CORE)
        # –ï—Å–ª–∏ —Å—É–º–º–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ, –º—É—Å–æ—Ä –∑–∞–±—å–µ—Ç —Å–∏–≥–Ω–∞–ª.
        # –ï—Å–ª–∏ S_WIDTH_CORE –ø—É—Å—Ç–æ–π, –±–µ—Ä–µ–º S_DEPTH_TOP70
        target_words = S_WIDTH_CORE if S_WIDTH_CORE else S_DEPTH_TOP70
        
        for word in target_words:
            if word not in counts: continue
            
            tf = counts[word]
            idf = word_idf_map.get(word, 0)
            
            # –§–æ—Ä–º—É–ª–∞ BM25 (–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ TF)
            # (TF * 2.2) / (TF + K)
            term_weight = (tf * 2.2) / (tf + K)
            
            score += idf * term_weight
            
        return score

    # 3.1. BM25 –¥–ª—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    comp_bm25_scores = []
    for i in range(N):
        s = calculate_bm25_for_doc(comp_docs[i]['body'], c_lens[i])
        comp_bm25_scores.append(s)
        
    # 3.2. –ú–µ–¥–∏–∞–Ω–∞ BM25 –¢–û–ü–∞
    if comp_bm25_scores:
        median_bm25_top = np.median(comp_bm25_scores)
    else:
        median_bm25_top = 0
        
    # 3.3. –õ–∏–º–∏—Ç —Å–ø–∞–º–∞ (100 –±–∞–ª–ª–æ–≤)
    # –ï—Å–ª–∏ –º–µ–¥–∏–∞–Ω–∞ 0, —Å—Ç–∞–≤–∏–º –∑–∞–≥–ª—É—à–∫—É 1, —á—Ç–æ–±—ã –Ω–µ –¥–µ–ª–∏—Ç—å –Ω–∞ 0
    spam_limit = median_bm25_top * 1.25
    if spam_limit == 0: spam_limit = 1 

    # 3.4. BM25 –¥–ª—è –í–ê–°
    my_bm25_raw = calculate_bm25_for_doc(my_lemmas, my_len)
    
    # 3.5. –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª –≥–ª—É–±–∏–Ω—ã –¥–ª—è –í–ê–°
    my_depth_score_final = int(round((my_bm25_raw / spam_limit) * 100))

    # --- –≠–¢–ê–ü 4: –¢–ê–ë–õ–ò–¶–´ –î–ï–¢–ê–õ–ò–ó–ê–¶–ò–ò ---
    table_depth, table_hybrid = [], []
    words_in_range_depth = 0
    total_important_words_depth = 0
    
    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        
        df = doc_freqs[lemma]
        if df < 2 and lemma not in my_lemmas: continue 
        
        my_tf_count = my_lemmas.count(lemma)        
        forms_set = all_forms_map.get(lemma, set())
        forms_str = ", ".join(sorted(list(forms_set))) if forms_set else lemma
        
        c_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        
        med_total = np.median(c_counts)
        max_total = np.max(c_counts)
        mean_total = np.mean(c_counts)
        
        base_min = min(mean_total, med_total)
        
        # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –í–í–ï–†–• (ceil)
        rec_min = int(math.ceil(base_min * norm_k))
        rec_max = int(round(max_total * norm_k)) # Max –º–æ–∂–Ω–æ –æ–±—ã—á–Ω–æ –æ–∫—Ä—É–≥–ª—è—Ç—å
        if rec_max < rec_min: rec_max = rec_min # –ó–∞—â–∏—Ç–∞
        
        rec_median = med_total * norm_k 
        
        status = "–ù–æ—Ä–º–∞"
        action_diff = 0
        action_text = "‚úÖ"
        
        if my_tf_count < rec_min:
            status = "–ù–µ–¥–æ—Å–ø–∞–º"
            action_diff = int(round(rec_min - my_tf_count))
            if action_diff == 0: action_diff = 1
            action_text = f"+{action_diff}"
        elif my_tf_count > rec_max:
            status = "–ü–µ—Ä–µ—Å–ø–∞–º"
            action_diff = int(round(my_tf_count - rec_max))
            if action_diff == 0: action_diff = 1
            action_text = f"-{action_diff}"
        
        # –†–∞—Å—á–µ—Ç "–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–π –≥–ª—É–±–∏–Ω—ã" –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞ (–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ —Ç–∞–±–ª–∏—Ü–µ)
        depth_percent = 0
        if rec_median > 0.1:
            depth_percent = int(round((my_tf_count / rec_median) * 100))
        else:
            depth_percent = 0 if my_tf_count == 0 else 100
        depth_percent = min(100, depth_percent)

        # –î–ª—è —Ç–∞–±–ª–∏—Ü—ã –≥–∏–±—Ä–∏–¥ –Ω—É–∂–µ–Ω –≤–µ—Å
        weight_hybrid = word_idf_map.get(lemma, 0) * (my_tf_count / my_len if my_len > 0 else 0)

        table_depth.append({
            "–°–ª–æ–≤–æ": lemma, 
            "–°–ª–æ–≤–æ—Ñ–æ—Ä–º—ã": forms_str, 
            "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_tf_count,
            "–ú–µ–¥–∏–∞–Ω–∞": round(med_total, 1), 
            "–ú–∏–Ω–∏–º—É–º (—Ä–µ–∫)": rec_min, 
            "–ú–∞–∫—Å–∏–º—É–º (—Ä–µ–∫)": rec_max, 
            "–ì–ª—É–±–∏–Ω–∞ %": depth_percent,
            "–°—Ç–∞—Ç—É—Å": status,
            "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": action_text,
            "is_missing": (status == "–ù–µ–¥–æ—Å–ø–∞–º" and my_tf_count == 0),
            "sort_val": abs(action_diff) if status != "–ù–æ—Ä–º–∞" else 0
        })
        
        table_hybrid.append({
            "–°–ª–æ–≤–æ": lemma, 
            "TF-IDF –¢–û–ü": round(word_idf_map.get(lemma, 0) * (med_total / avg_dl if avg_dl > 0 else 0), 4), 
            "TF-IDF —É –≤–∞—Å": round(weight_hybrid, 4),
            "–°–∞–π—Ç–æ–≤": df, 
            "–ü–µ—Ä–µ—Å–ø–∞–º": max_total
        })

    # --- –≠–¢–ê–ü 5: –†–ê–°–ß–ï–¢ –ò–¢–û–ì–û–í (–®–ò–†–ò–ù–ê) ---
    
    total_width_core_count = len(S_WIDTH_CORE)
    
    def calculate_width_score_rule_90(lemmas_set):
        if total_width_core_count == 0: return 0
        intersection_count = len(lemmas_set.intersection(S_WIDTH_CORE))
        ratio = intersection_count / total_width_core_count
        if ratio >= 0.9: return 100
        else: return int(round((ratio / 0.9) * 100))

    table_rel = []
    
    # –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –≤ —Ç–∞–±–ª–∏—Ü–µ
    for i, item in enumerate(original_results):
        url = item['url']
        pos = item['pos']
        domain = urlparse(url).netloc
        
        # –ù–∞—Ö–æ–¥–∏–º –¥–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å i, —Ç–∞–∫ –∫–∞–∫ comp_bm25_scores –º—ã —Å—Ç—Ä–æ–∏–ª–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É (0..N)
        # –ù–æ original_results –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ N (–µ—Å–ª–∏ —á–∞—Å—Ç—å –Ω–µ —Å–∫–∞—á–∞–ª–∞—Å—å), –Ω—É–∂–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—Ç—å –∞–∫–∫—É—Ä–∞—Ç–Ω–æ
        
        # –ò—â–µ–º —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        parsed_entry = next((d for d in comp_data_full if d.get('url') == url), None)
        
        width_score_val = 0
        depth_score_val_bm25 = 0 
        
        if parsed_entry and parsed_entry.get('body_text'):
            p_lemmas, _ = process_text_detailed(parsed_entry['body_text'], settings)
            p_set = set(p_lemmas)
            
            # –®–∏—Ä–∏–Ω–∞
            width_score_val = calculate_width_score_rule_90(p_set)
            
            # –ì–ª—É–±–∏–Ω–∞ (BM25)
            # –ù–∞–º –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –≤ comp_docs, —á—Ç–æ–±—ã –≤–∑—è—Ç—å –µ–≥–æ –ø–æ—Å—á–∏—Ç–∞–Ω–Ω—ã–π BM25
            # –ò–ª–∏ –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å (–ø—Ä–æ—â–µ –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å, —Ç.–∫. –∏–Ω–¥–µ–∫—Å—ã –º–æ–≥—É—Ç –Ω–µ —Å–æ–≤–ø–∞—Å—Ç—å –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
            c_score_raw = calculate_bm25_for_doc(p_lemmas, len(p_lemmas))
            depth_score_val_bm25 = int(round((c_score_raw / spam_limit) * 100))
                
            width_score_val = min(100, width_score_val)
            # depth_score_val_bm25 –ù–ï –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 100, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å –ø–µ—Ä–µ—Å–ø–∞–º (140 –∏ —Ç.–¥.)
            
        table_rel.append({
            "–î–æ–º–µ–Ω": domain, "–ü–æ–∑–∏—Ü–∏—è": pos,
            "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": width_score_val,
            "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": depth_score_val_bm25
        })
        
    # –í—ã –≤ —Ç–∞–±–ª–∏—Ü–µ
    my_score_w = calculate_width_score_rule_90(my_full_lemmas_set)
    my_score_w = min(100, my_score_w)
    
    if my_data and my_data.get('domain'):
        my_label = f"{my_data['domain']} (–í—ã)"
    else:
        my_label = "–í–∞—à —Å–∞–π—Ç"
        
    table_rel.append({
        "–î–æ–º–µ–Ω": my_label, 
        "–ü–æ–∑–∏—Ü–∏—è": my_serp_pos if my_serp_pos > 0 else len(original_results) + 1,
        "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": my_score_w, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": my_depth_score_final
    })
    
    table_rel_df = pd.DataFrame(table_rel).sort_values(by='–ü–æ–∑–∏—Ü–∏—è', ascending=True).reset_index(drop=True)
        
    return {
        "depth": pd.DataFrame(table_depth), "hybrid": pd.DataFrame(table_hybrid),
        "relevance_top": table_rel_df,
        "my_score": {"width": my_score_w, "depth": my_depth_score_final},
        "missing_semantics_high": missing_semantics_high,
        "missing_semantics_low": missing_semantics_low
    }
# ==========================================
# 5. –§–£–ù–ö–¶–ò–Ø –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø (PAGINATION + EXCEL)
# ==========================================

def render_paginated_table(df, title_text, key_prefix, default_sort_col=None, use_abs_sort_default=False):
    if df.empty:
        st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")
        return

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –∫–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
    col_t1, col_t2 = st.columns([7, 3])
    with col_t1:
        st.markdown(f"### {title_text}")
    
    # 1. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ (–¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
    if f'{key_prefix}_sort_col' not in st.session_state:
        st.session_state[f'{key_prefix}_sort_col'] = default_sort_col if default_sort_col in df.columns else df.columns[0]
    if f'{key_prefix}_sort_order' not in st.session_state:
        st.session_state[f'{key_prefix}_sort_order'] = "–£–±—ã–≤–∞–Ω–∏–µ" 

    # 2. –ü–æ–∏—Å–∫
    search_query = st.text_input(f"üîç –ü–æ–∏—Å–∫ ({title_text})", key=f"{key_prefix}_search")
    if search_query:
        mask = df.astype(str).apply(lambda x: x.str.contains(search_query, case=False, na=False)).any(axis=1)
        df_filtered = df[mask].copy()
    else:
        df_filtered = df.copy()

    if df_filtered.empty:
        st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return

    # 3. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    with st.container():
        st.markdown("<div class='sort-container'>", unsafe_allow_html=True)
        col_s1, col_s2, col_sp = st.columns([2, 2, 4])
        with col_s1:
            sort_col = st.selectbox(
                "üóÇ –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ:", 
                df_filtered.columns, 
                key=f"{key_prefix}_sort_box",
                index=list(df_filtered.columns).index(st.session_state[f'{key_prefix}_sort_col']) if st.session_state[f'{key_prefix}_sort_col'] in df_filtered.columns else 0
            )
            st.session_state[f'{key_prefix}_sort_col'] = sort_col
        with col_s2:
            sort_order = st.radio(
                "–ü–æ—Ä—è–¥–æ–∫:", 
                ["–£–±—ã–≤–∞–Ω–∏–µ", "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ"], 
                horizontal=True,
                key=f"{key_prefix}_order_box",
                index=0 if st.session_state[f'{key_prefix}_sort_order'] == "–£–±—ã–≤–∞–Ω–∏–µ" else 1
            )
            st.session_state[f'{key_prefix}_sort_order'] = sort_order
        st.markdown("</div>", unsafe_allow_html=True)

    ascending = (sort_order == "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ")
    
    # !FIX: –ò–°–ü–†–ê–í–õ–ï–ù–ê –õ–û–ì–ò–ö–ê –°–û–†–¢–ò–†–û–í–ö–ò
    # –¢–µ–ø–µ—Ä—å –ø—Ä–æ–≤–µ—Ä—è–µ–º –¢–ï–ö–£–©–ò–ô –≤—ã–±—Ä–∞–Ω–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü (sort_col), –∞ –Ω–µ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π.
    if sort_col == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è" and "sort_val" in df_filtered.columns:
         df_filtered = df_filtered.sort_values(by="sort_val", ascending=ascending)
    elif "–î–æ–±–∞–≤–∏—Ç—å" in sort_col or "+/-" in sort_col:
        df_filtered['_temp_sort'] = df_filtered[sort_col].abs()
        df_filtered = df_filtered.sort_values(by='_temp_sort', ascending=ascending).drop(columns=['_temp_sort'])
    else:
        df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)

    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
    df_filtered = df_filtered.reset_index(drop=True)
    df_filtered.index = df_filtered.index + 1
    
    # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Excel (–°–ö–ê–ß–ò–í–ê–ï–¢–°–Ø –ü–û–õ–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê)
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        export_df = df_filtered.copy()
        if "is_missing" in export_df.columns: del export_df["is_missing"]
        if "sort_val" in export_df.columns: del export_df["sort_val"]
        export_df.to_excel(writer, index=False, sheet_name='Data')
    excel_data = buffer.getvalue()
    
    with col_t2:
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å Excel (–í—Å–µ –¥–∞–Ω–Ω—ã–µ)",
            data=excel_data,
            file_name=f"{key_prefix}_export.xlsx",
            mime="application/vnd.ms-excel",
            key=f"{key_prefix}_down"
        )

    # 5. –ü–ê–ì–ò–ù–ê–¶–ò–Ø (–û—Ç–æ–±—Ä–∞–∂–∞–µ–º –ø–æ 20 —Å—Ç—Ä–æ–∫)
    ROWS_PER_PAGE = 20
    if f'{key_prefix}_page' not in st.session_state:
        st.session_state[f'{key_prefix}_page'] = 1
        
    total_rows = len(df_filtered)
    total_pages = math.ceil(total_rows / ROWS_PER_PAGE)
    if total_pages == 0: total_pages = 1
    
    current_page = st.session_state[f'{key_prefix}_page']
    if current_page > total_pages: current_page = total_pages
    if current_page < 1: current_page = 1
    st.session_state[f'{key_prefix}_page'] = current_page
    
    start_idx = (current_page - 1) * ROWS_PER_PAGE
    end_idx = start_idx + ROWS_PER_PAGE
    
    df_view = df_filtered.iloc[start_idx:end_idx]

    def highlight_rows(row):
        base_style = 'background-color: #FFFFFF; color: #3D4858; border-bottom: 1px solid #DBEAFE;'
        styles = []
        status = row.get("–°—Ç–∞—Ç—É—Å", "")
        
        for col_name in row.index:
            cell_style = base_style
            if col_name == "–°—Ç–∞—Ç—É—Å":
                if status == "–ù–µ–¥–æ—Å–ø–∞–º":
                    cell_style += "color: #D32F2F; font-weight: bold;" 
                elif status == "–ü–µ—Ä–µ—Å–ø–∞–º":
                    cell_style += "color: #E65100; font-weight: bold;" 
                elif status == "–ù–æ—Ä–º–∞":
                    cell_style += "color: #2E7D32; font-weight: bold;" 
            styles.append(cell_style)
        return styles
    
    cols_to_hide = ["is_missing", "sort_val"]
    
    styled_df = df_view.style.apply(highlight_rows, axis=1)
    
    # –í—ã—Å–æ—Ç–∞ —Ç–∞–±–ª–∏—Ü—ã –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –ø–æ–¥ –∫–æ–ª-–≤–æ —Å—Ç—Ä–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–º–∞–∫—Å 20)
    dynamic_height = (len(df_view) * 35) + 40 
    
    st.dataframe(
        styled_df,
        use_container_width=True,
        height=dynamic_height, 
        column_config={c: None for c in cols_to_hide}
    )
    
    # –ö–Ω–æ–ø–∫–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
    c_spacer, c_btn_prev, c_info, c_btn_next = st.columns([6, 1, 1, 1])
    with c_btn_prev:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_prev", disabled=(current_page <= 1), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] -= 1
            st.rerun()
    with c_info:
        st.markdown(f"<div style='text-align: center; margin-top: 10px; color:{TEXT_COLOR}'><b>{current_page}</b> / {total_pages}</div>", unsafe_allow_html=True)
    with c_btn_next:
        if st.button("‚û°Ô∏è", key=f"{key_prefix}_next", disabled=(current_page >= total_pages), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] += 1
            st.rerun()
    st.markdown("---")

# ==========================================
# 6. –õ–û–ì–ò–ö–ê –î–õ–Ø PERPLEXITY (AI GEN)
# ==========================================

STATIC_DATA_GEN = {
    'IP_PROP4817': "–£—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏",
    'IP_PROP4818': "–û–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –≤ —Ä–µ–≥–∏–æ–Ω—ã —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4819': """<p>–ù–∞–¥–µ–∂–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫–∞–∑–∞ –≤ –ª—é–±—É—é —Ç–æ—á–∫—É —Å—Ç—Ä–∞–Ω—ã: "–°—Ç–∞–ª—å–º–µ—Ç—É—Ä–∞–ª" –æ—Ç–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–≤–∞—Ä 24 —á–∞—Å–∞ –≤ —Å—É—Ç–∫–∏, 7 –¥–Ω–µ–π –≤ –Ω–µ–¥–µ–ª—é. –ë–æ–ª–µ–µ 4 000 –æ—Ç–≥—Ä—É–∑–æ–∫ –≤ –≥–æ–¥. –ü—Ä–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–∏ –∑–∞–∫–∞–∑–∞ –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –≤–∞–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ä—à—Ä—É—Ç.</p>""",
    
    'IP_PROP4820': """<p>–ù–∞—à–∏ –∏–∑–¥–µ–ª–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è—Ö –£—Ä–∞–ª–∞, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞, –ü–æ–≤–æ–ª–∂—å—è, –°–∏–±–∏—Ä–∏. –ü–∞—Ä—Ç–Ω–µ—Ä—ã –ø–æ –ª–æ–≥–∏—Å—Ç–∏–∫–µ –ø—Ä–µ–¥–ª–æ–∂–∞—Ç –¥–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–∫–∞–∑ —Å–∞–º—ã–º —É–¥–æ–±–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º ‚Äì –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–º, –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω—ã–º, –¥–∞–∂–µ –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º. –î–ª—è –≤–∞—Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—É—é —Å—Ö–µ–º—É –ø–æ–¥ —É–¥–æ–±–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è. –ü–æ–≥—Ä—É–∑–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ç–µ—Ö–Ω–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.</p>
<div class="h4">
<h4>–°–∞–º–æ–≤—ã–≤–æ–∑</h4>
</div>
<p>–ï—Å–ª–∏ –æ–±—ã—á–Ω–æ —Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∑–∞–±—Ä–∞—Ç—å —Ç–æ–≤–∞—Ä –∏–ª–∏ –¥–∞–µ—Ç–µ —ç—Ç–æ –ø—Ä–∞–≤–æ —É–ø–æ–ª–Ω–æ–º–æ—á–µ–Ω–Ω—ã–º, –∞–¥—Ä–µ—Å –∏ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã —Å–∫–ª–∞–¥–∞ –≤ —Å–≤–æ–µ–º –≥–æ—Ä–æ–¥–µ —É—Ç–æ—á–Ω—è–π—Ç–µ —É –º–µ–Ω–µ–¥–∂–µ—Ä–∞.</p>
<div class="h4">
<h4>–ì—Ä—É–∑–æ–≤–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏</h4>
</div>
<p>–û—Ç–ø—Ä–∞–≤–∏–º –ø—Ä–æ–∫–∞—Ç –Ω–∞ –≤–∞—à –æ–±—ä–µ–∫—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –∞–≤—Ç–æ–ø–∞—Ä–∫–æ–º. –ü–æ–ª—É—á–µ–Ω–∏–µ –≤ —É–ø–∞–∫–æ–≤–∫–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏, –∞ –∏–º–µ–Ω–Ω–æ –Ω–∞ –¥–µ—Ä–µ–≤—è–Ω–Ω–æ–º –ø–æ–¥–¥–æ–Ω–µ.</p>
<div class="h4">
<h4>–°–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å –¢–ö</h4>
</div>
<p>–î–æ—Å—Ç–∞–≤–∫–∞ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –†–æ—Å—Å–∏–∏ –∏ –°–ù–ì. –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å—Å—è, —Ç–∞–∫ –∫–∞–∫ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç, –æ–¥–Ω–∞–∫–æ, —Å—Ä–∞–≤–Ω–∏–º —Å—Ç–æ–∏–º–æ—Å—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª—É–∂–± –∏ –≤—ã–±–µ—Ä–µ–º –ª—É—á—à—É—é.</p>""",

    'IP_PROP4821': "–û–ø–ª–∞—Ç–∞ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤:",
    'IP_PROP4822': """<p>–ù–∞—à–∞ –∫–æ–º–ø–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞ –ø—Ä–∏–Ω—è—Ç—å –ª—é–±—ã–µ –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–µ –≤–∏–¥—ã –æ–ø–ª–∞—Ç—ã –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏—Ü: –ø–æ —Å—á–µ—Ç—É, –Ω–∞–ª–∏—á–Ω–∞—è –∏ –±–µ–∑–Ω–∞–ª–∏—á–Ω–∞—è, –Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–π –ø–ª–∞—Ç–µ–∂, —Ç–∞–∫–∂–µ –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞ –∏ –æ—Ç—Å—Ä–æ—á–∫–∞ –ø–ª–∞—Ç–µ–∂–∞.</p>""",
    
    'IP_PROP4823': """<div class="h4">
        <h3>–ü—Ä–∏–º–µ—Ä—ã –≤–æ–∑–º–æ–∂–Ω–æ–π –æ–ø–ª–∞—Ç—ã</h3>
</div>
<div class="an-col-12">
        <ul>
                <li style="font-weight: 400;">
                <p>
 <span style="font-weight: 400;">–° –ø–æ–º–æ—â—å—é –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤ —Ü–µ–Ω—Ç—Ä–∞—Ö –ø—Ä–æ–¥–∞–∂</span>
                </p>
 </li>
        </ul>
        <p>
                 –í–∞–∂–Ω–æ! –¶–µ–Ω–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—É–±–ª–∏—á–Ω–æ–π –æ—Ñ–µ—Ä—Ç–æ–π. –ü—Ä–∏—Ö–æ–¥–∏—Ç–µ –≤ –Ω–∞—à –æ—Ñ–∏—Å, —á—Ç–æ–±—ã —É—Ç–æ—á–Ω–∏—Ç—å –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ, –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ—á—Ç–∏ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å, —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –≤–æ–∑–≤—Ä–∞—Ç, —Å—á–µ—Ç, —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ª–æ–≥–∏—Å—Ç–∏–∫—É.
        </p>
        <ul>
                <li style="font-weight: 400;">
                <p>
 <span style="font-weight: 400;">–ù–∞ —Ä–∞—Å—á–µ—Ç–Ω—ã–π —Å—á–µ—Ç</span>
                </p>
 </li>
        </ul>
        <p>
                 –ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É —Å—á–µ—Ç—É –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏ –±–∞–Ω–∫–∞ –∏–ª–∏ –ø—É—Ç–µ–º –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥—Å—Ç–≤ —á–µ—Ä–µ–∑ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç (—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∑–∞—â–∏—â–µ–Ω—ã, —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –æ—Ç–¥–µ–ª–µ–Ω–∏—è). –î–ª—è –ø—Ä–∞–≤–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å —Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –ø–ª–∞—Ç–µ–∂–Ω–æ–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ —Å –æ—Ç–º–µ—Ç–∫–æ–π –±–∞–Ω–∫–∞.
        </p>
        <ul>
                <li style="font-weight: 400;">
                <p>
 <span style="font-weight: 400;">–ù–∞–ª–∏—á–Ω—ã–º–∏ –∏–ª–∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∫–∞—Ä—Ç–æ–π –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏</span>
                </p>
 </li>
        </ul>
        <p>
 <span style="font-weight: 400;">–ü–æ–º–æ–∂–µ–º —Å –æ–ø–ª–∞—Ç–æ–π: –æ–±—ä–µ–º –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ. –ö—Ä—É–ø–Ω—ã–º –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º ‚Äì –¥–µ–Ω—å–≥–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–µ–º–∫–∏ —Ç–æ–≤–∞—Ä–∞.</span>
        </p>
        <p>
                 –ú–µ–Ω–µ–¥–∂–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
        </p>
                <p>
                         –ó–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ —á–µ—Ä–µ–∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç:
                </p>
                <p>
 <a class="btn btn-blue" href="/catalog/">–ö–∞—Ç–∞–ª–æ–≥ (–º–∞–≥–∞–∑–∏–Ω-–º–µ–Ω—é):</a>
                </p>
        </div>
</div>
 <br>""",

    'IP_PROP4824': "–û–ø–∏—Å–∞–Ω–∏–µ, —Å—Ç–∞—Ç—å–∏, –ø–æ–∏—Å–∫, –æ—Ç–∑—ã–≤—ã, –Ω–æ–≤–æ—Å—Ç–∏, –∞–∫—Ü–∏–∏, –∂—É—Ä–Ω–∞–ª, info:",
    'IP_PROP4825': "–ú–æ–∂–µ–º –º–µ—Ç–∞–ª–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –æ—Ü–∏–Ω–∫–æ–≤–∞—Ç—å, –Ω–∏–∫–µ–ª–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–æ–≤–æ–ª–æ—á—å",
    'IP_PROP4826': "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥",
    
    'IP_PROP4834': "–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –±–µ–∑ –ø—Ä–∏–º–µ—Å–µ–π",
    'IP_PROP4835': "–ü–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫",
    'IP_PROP4836': "–ö–∞—á–µ—Å—Ç–≤–æ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä",
    'IP_PROP4837': "–ü–æ—Ä—è–¥–æ–∫ –≤ –ì–û–°–¢–∞—Ö"
}

def get_page_data_for_gen(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
    except Exception as e:
        return None, None, f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}"
    
    if response.status_code != 200:
        return None, None, f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—É—Å–∞: {response.status_code}"

    soup = BeautifulSoup(response.text, 'html.parser')
    
    # –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è
    description_div = soup.find('div', class_='description-container')
    base_text = description_div.get_text(separator="\n", strip=True) if description_div else ""
    
    if not base_text:
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫, –µ—Å–ª–∏ –∫–ª–∞—Å—Å –¥—Ä—É–≥–æ–π
        base_text = soup.body.get_text(separator="\n", strip=True)[:5000]

    # –ü–æ–∏—Å–∫ —Ç–µ–≥–æ–≤
    tags_container = soup.find(class_='popular-tags-inner')
    tags_data = []
    if tags_container:
        links = tags_container.find_all('a')
        for link in links:
            tag_name = link.get_text(strip=True)
            tag_url = link.get('href')
            if tag_url:
                tag_url = urljoin(url, tag_url)
            tags_data.append({'name': tag_name, 'url': tag_url})
    
    return base_text, tags_data, None

def generate_five_blocks(client, base_text, tag_name, seo_words=None):
    if not base_text: return ["Error: No base text"] * 5

    system_instruction = """
    –¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä –∏ —Ñ–∏–ª–æ–ª–æ–≥ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.
    –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –Ω–∞–ø–∏—Å–∞—Ç—å 5 –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ HTML.
    –í–ê–ñ–ù–û: –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π markdown –æ–±–µ—Ä—Ç–∫–∏ (```html). –ü–∏—à–∏ —Å—Ä–∞–∑—É —á–∏—Å—Ç—ã–π –∫–æ–¥.
    
    IMPORTANT: Do not include citations, references, or footnotes like [1], [2], [10] in the text.
    """

    keywords_instruction = ""
    if seo_words and len(seo_words) > 0:
        keywords_str = ", ".join(seo_words)
        keywords_instruction = f"""
        [–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ì–†–ê–ú–ú–ê–¢–ò–ö–ê –ò –°–ö–õ–û–ù–ï–ù–ò–Ø]
        –ù–∏–∂–µ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –≤ –õ–ï–ú–ú–ê–• (–Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ), –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —É–ø–æ—Ç—Ä–µ–±–∏—Ç—å: 
        {keywords_str}
        
        –¢–´ –û–ë–Ø–ó–ê–ù:
        1. –í—Å—Ç–∞–≤–ª—è—è —Å–ª–æ–≤–∞ –∏–∑ —Å–ø–∏—Å–∫–∞, –ò–ó–ú–ï–ù–Ø–¢–¨ –∏—Ö –æ–∫–æ–Ω—á–∞–Ω–∏—è, –ø–∞–¥–µ–∂, —á–∏—Å–ª–æ –∏ —Ä–æ–¥, —á—Ç–æ–±—ã –æ–Ω–∏ –∏–¥–µ–∞–ª—å–Ω–æ —Å–æ–≥–ª–∞—Å–æ–≤—ã–≤–∞–ª–∏—Å—å —Å —Å–æ—Å–µ–¥–Ω–∏–º–∏ —Å–ª–æ–≤–∞–º–∏.
           - –ü–õ–û–•–û: "–ú—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞".
           - –•–û–†–û–®–û: "–ú—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º <b>–±—ã—Å—Ç—Ä—É—é –¥–æ—Å—Ç–∞–≤–∫—É</b>".
           - –ü–õ–û–•–û: "—Ñ–æ—Ä–º–∞—Ç —É–ø–∞–∫–æ–≤–∫–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∞".
           - –•–û–†–û–®–û: "—Ñ–æ—Ä–º–∞—Ç <b>—É–ø–∞–∫–æ–≤–∫–∏</b> –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ <b>—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–≥–æ</b> —Ä–∞–∑–¥–µ–ª–∞".
        2. –ö–∞–∂–¥–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ –∏–∑ —Å–ø–∏—Å–∫–∞ (–≤ –ª—é–±–æ–π —Ñ–æ—Ä–º–µ) –≤—ã–¥–µ–ª—è–π —Ç–µ–≥–æ–º <b>.
        3. –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ 1 —É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞. –ù–µ —Å–ø–∞–º—å.
        4. –ï—Å–ª–∏ —Å–ª–æ–≤ –º–Ω–æ–≥–æ ‚Äî –ø–∏—à–∏ –¥–ª–∏–Ω–Ω—ã–π, —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π —Ç–µ–∫—Å—Ç, –¥–æ–±–∞–≤–ª—è–π –≤–≤–æ–¥–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –ø–æ–ª–µ–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —á—Ç–æ–±—ã —Å–ª–æ–≤–∞ —Å–º–æ—Ç—Ä–µ–ª–∏—Å—å –æ—Ä–≥–∞–Ω–∏—á–Ω–æ, –∞ –Ω–µ —Å–ø–∏—Å–∫–æ–º.
        """

    user_prompt = f"""
    –í–í–û–î–ù–´–ï:
    –¢–æ–≤–∞—Ä (–¢–µ–∫—É—â–∏–π —Ç–µ–≥): "{tag_name}".
    –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: \"\"\"{base_text[:3000]}\"\"\"

    {keywords_instruction}

    –ó–ê–î–ê–ß–ê:
    –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Ä–æ–≤–Ω–æ 5 —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤.

    –°–¢–†–£–ö–¢–£–†–ê –ö–ê–ñ–î–û–ì–û –ë–õ–û–ö–ê:
    1. –ó–∞–≥–æ–ª–æ–≤–æ–∫ (<h2> –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–ª–æ–∫–∞ - –°–¢–†–û–ì–û "{tag_name}", –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π. –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö <h3>).
    2. –ê–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞.
    3. –í–≤–æ–¥–Ω–∞—è —Ñ—Ä–∞–∑–∞ (–∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –¥–≤–æ–µ—Ç–æ—á–∏–µ–º).
    4. –°–ø–∏—Å–æ–∫ <ul> –∏–ª–∏ <ol> (—ç–ª–µ–º–µ–Ω—Ç—ã –∑–∞–∫–∞–Ω—á–∏–≤–∞—é—Ç—Å—è —Ç–æ—á–∫–æ–π —Å –∑–∞–ø—è—Ç–æ–π, –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ—á–∫–æ–π).
    5. –ó–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–π –∞–±–∑–∞—Ü.

    –ì–õ–ê–í–ù–û–ï –ü–†–ê–í–ò–õ–û: –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –∑–≤—É—á–∞—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –¥–ª—è —á–µ–ª–æ–≤–µ–∫–∞. –†–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∑–∞–ø—Ä–µ—â–µ–Ω—ã. –°–∫–ª–æ–Ω—è–π —Å–ª–æ–≤–∞!
    
    NO CITATIONS OR FOOTNOTES LIKE [1].

    –í–´–í–û–î:
    –†–∞–∑–¥–µ–ª–∏ –±–ª–æ–∫–∏ —Å—Ç—Ä–æ–≥–æ —Å—Ç—Ä–æ–∫–æ–π: |||BLOCK_SEP|||
    """

    try:
        response = client.chat.completions.create(
            model="sonar-pro", 
            messages=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7
        )
        content = response.choices[0].message.content
        
        # ------------------------------------
        # REMOVE CITATIONS (Regex cleaning)
        # ------------------------------------
        content = re.sub(r'\[\d+\]', '', content)
        
        # –ß–∏—Å—Ç–∫–∞ –æ—Ç –º–∞—Ä–∫–¥–∞—É–Ω–∞
        content = content.replace("```html", "").replace("```", "")
        
        blocks = content.split("|||BLOCK_SEP|||")
        clean_blocks = [b.strip() for b in blocks if b.strip()]
        
        while len(clean_blocks) < 5:
            clean_blocks.append("")
            
        return clean_blocks[:5]

    except Exception as e:
        return [f"API Error: {str(e)}"] * 5

def generate_html_table(client, user_prompt):
    system_instruction = """
    You are an HTML generator.
    Your task is to generate a semantic HTML table based on the user's request.
    
    IMPORTANT: Do not include citations, references, or footnotes like [1], [2] in the table content.
    
    CRITICAL: You MUST apply specific inline CSS styles to the table elements EXACTLY as follows:
    1. For the <table> tag, use: style="border-collapse: collapse; width: 100%; border: 2px solid black;"
    2. For every <th> tag, use: style="border: 2px solid black; padding: 5px;"
    3. For every <td> tag, use: style="border: 2px solid black; padding: 5px;"
    
    Do not use internal <style> blocks. Use only inline styles.
    Output ONLY the HTML code. Do not wrap it in markdown (```html).
    """
    
    try:
        response = client.chat.completions.create(
            model="sonar-pro", 
            messages=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.5
        )
        content = response.choices[0].message.content
        
        # ------------------------------------
        # REMOVE CITATIONS (Regex cleaning)
        # ------------------------------------
        content = re.sub(r'\[\d+\]', '', content)
        
        # –ß–∏—Å—Ç–∫–∞ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
        content = content.replace("```html", "").replace("```", "").strip()
        return content
    except Exception as e:
        return f"Error: {e}"

# ==========================================
# 7. –ò–ù–¢–ï–†–§–ï–ô–° (TABS)
# ==========================================

# –ò–°–ü–û–õ–¨–ó–£–ï–ú –í–ö–£–õ–ê–î–ö–ò, –ß–¢–û–ë–´ –ù–ï –õ–û–ú–ê–¢–¨ –î–ò–ó–ê–ô–ù –ü–ï–†–í–û–ô –ß–ê–°–¢–ò
tab_seo, tab_ai, tab_tags, tab_tables = st.tabs(["üìä SEO –ê–Ω–∞–ª–∏–∑ (–ì–ê–†)", "ü§ñ AI –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (Perplexity)", "üè∑Ô∏è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–ª–∏—Ç–∫–∏ —Ç–µ–≥–æ–≤", "üß© –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–∞–±–ª–∏—Ü"])

# ------------------------------------------
# –í–∫–ª–¥–∞–∫–∞ 1: –í–ï–°–¨ –°–¢–ê–†–´–ô –ö–û–î (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô –°–¢–†–£–ö–¢–£–†–´)
# ------------------------------------------
with tab_seo:
    col_main, col_sidebar = st.columns([65, 35]) 

    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")

        st.markdown("### URL –∏–ª–∏ –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –í–∞—à–µ–≥–æ —Å–∞–π—Ç–∞")
        my_input_type = st.radio("–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], horizontal=True, label_visibility="collapsed", key="my_page_source_radio")

        my_url = ""
        my_page_content = ""
        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            my_url = st.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã", placeholder="https://site.ru/catalog/tovar", label_visibility="collapsed", key="my_url_input")
        elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            my_page_content = st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏–ª–∏ —Ç–µ–∫—Å—Ç", height=200, label_visibility="collapsed", placeholder="–í—Å—Ç–∞–≤—å—Ç–µ HTML", key="my_content_input")

        st.markdown("### –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å")
        query = st.text_input("–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞", label_visibility="collapsed", key="query_input")

        st.markdown("### –ü–æ–∏—Å–∫ –∏–ª–∏ URL —Å—Ç—Ä–∞–Ω–∏—Ü –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        source_type_new = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤", ["–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)", "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"], horizontal=True, label_visibility="collapsed", key="competitor_source_radio")
        source_type = "API" if "API" in source_type_new else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫" 

        if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
            st.markdown("### –í–≤–µ–¥–∏—Ç–µ —Å–ø–∏—Å–æ–∫ URL")
            st.text_area("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫–∏ –∑–¥–µ—Å—å (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", height=200, key="manual_urls_ui")

        st.markdown("### –†–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º—ã–µ —Å–ø–∏—Å–∫–∏")
        excludes = st.text_area("–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–æ–º–µ–Ω—ã", DEFAULT_EXCLUDE, height=200, key="settings_excludes")
        c_stops = st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞", DEFAULT_STOPS, height=200, key="settings_stops")

        st.markdown("---")
        
        if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", type="primary", use_container_width=True, key="start_analysis_btn"):
            for key in list(st.session_state.keys()):
                if key.endswith('_page'): st.session_state[key] = 1
            st.session_state.start_analysis_flag = True

    with col_sidebar:
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        ua = st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
        search_engine = st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
        region = st.selectbox("–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞", list(REGION_MAP.keys()), key="settings_region")
        device = st.selectbox("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", ["Desktop", "Mobile"], key="settings_device")
        
        top_n = st.selectbox("–ì–ª—É–±–∏–Ω–∞ —Å–±–æ—Ä–∞ (–¢–û–ü)", [10, 20, 30], index=0, key="settings_top_n") 
        
        st.markdown("---")
        st.selectbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ url", ["–í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", "–ì–ª–∞–≤–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", "–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], key="settings_url_type")
        
        col_c1, col_c2 = st.columns(2)
        with col_c1:
            st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ <noindex>", True, key="settings_noindex")
            st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", False, key="settings_alt")
            st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", False, key="settings_numbers")
        with col_c2:
            st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True, key="settings_norm")
            st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã", True, key="settings_agg") 

    # ==========================================
    # –í–´–ü–û–õ–ù–ï–ù–ò–ï (SEO –õ–û–ì–ò–ö–ê)
    # ==========================================
    if st.session_state.get('start_analysis_flag'):
        st.session_state.start_analysis_flag = False

        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ" and not st.session_state.get('my_url_input'):
            st.error("–í–≤–µ–¥–∏—Ç–µ URL!")
            st.stop()
        if my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç" and not st.session_state.get('my_content_input', '').strip():
            st.error("–í–≤–µ–¥–∏—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥!")
            st.stop()
        if source_type == "API" and not st.session_state.get('query_input'):
            st.error("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å!")
            st.stop()
        if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫" and not st.session_state.get("manual_urls_ui", "").strip():
            st.error("–í–≤–µ–¥–∏—Ç–µ —Å–ø–∏—Å–æ–∫ URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤!")
            st.stop()
            
        settings = {
            'noindex': st.session_state.settings_noindex, 
            'alt_title': st.session_state.settings_alt, 
            'numbers': st.session_state.settings_numbers,
            'norm': st.session_state.settings_norm, 
            'ua': st.session_state.settings_ua, 
            'custom_stops': st.session_state.settings_stops.split()
        }
        
        target_urls_raw = []
        my_data = None
        my_domain = ""
        my_serp_pos = 0 
        
        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
                my_url_input = st.session_state.my_url_input
                my_data = parse_page(my_url_input, settings)
            
                if not my_data:
                    st.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –≤–∞—à—É —Å—Ç—Ä–∞–Ω–∏—Ü—É. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL –∏–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ User-Agent.")
                    st.stop()
                my_domain = urlparse(my_url_input).netloc
        elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            my_data = {'url': 'Local', 'domain': 'local', 'body_text': st.session_state.my_content_input, 'anchor_text': ''}
            my_domain = "local" 

        if source_type == "API":
            TARGET_COMPETITORS = st.session_state.settings_top_n
            API_FETCH_DEPTH = 30 
            
            with st.spinner(f"–°–±–æ—Ä –¢–û–ü–∞ (–≥–ª—É–±–∏–Ω–∞ {API_FETCH_DEPTH}) —á–µ—Ä–µ–∑ Arsenkin API..."):
                found_results = get_arsenkin_urls(
                    query=st.session_state.query_input, 
                    engine_type=st.session_state.settings_search_engine,
                    region_name=st.session_state.settings_region,
                    depth_val=API_FETCH_DEPTH
                )
                
            if not found_results:
                st.error("API –Ω–µ –≤–µ—Ä–Ω—É–ª —Å—Å—ã–ª–∫–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ **JSON-–æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞**.")
                st.stop()
                
            excl = [d.strip() for d in st.session_state.settings_excludes.split('\n') if d.strip()]
            if st.session_state.settings_agg: 
                excl.extend(["avito.ru", "ozon.ru", "wildberries.ru", "market.yandex.ru", 
                             "tiu.ru", "youtube.com", "vk.com", "yandex.ru", 
                             "leroymerlin.ru", "petrovich.ru"])
                
            filtered_results_all = []
            for result in found_results:
                url = result['url']
                pos = result['pos']
                domain = urlparse(url).netloc
                
                if my_domain and my_domain == domain:
                    if my_serp_pos == 0 or pos < my_serp_pos:
                        my_serp_pos = pos
                    continue 

                if any(x in domain for x in excl): 
                    continue 

                filtered_results_all.append(result)

            target_urls_raw = filtered_results_all[:TARGET_COMPETITORS]
            
            collected_competitors_count = len(target_urls_raw)
            st.info(f"–ü–æ–ª—É—á–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {len(found_results)}. –í—ã–±—Ä–∞–Ω–æ **{collected_competitors_count}** —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤. –í–∞—à —Å–∞–π—Ç –≤ –¢–û–ü–µ: **{'–î–∞ (–ü–æ–∑. ' + str(my_serp_pos) + ')' if my_serp_pos > 0 else '–ù–µ—Ç'}**.")

        else:
            raw_urls = st.session_state.get("manual_urls_ui", "")
            if raw_urls:
                urls = [u.strip() for u in raw_urls.split('\n') if u.strip()]
                target_urls_raw = [{'url': u, 'pos': i+1} for i, u in enumerate(urls)]
            else:
                target_urls_raw = []
                
            st.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ **{len(target_urls_raw)}** URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤—Ä—É—á–Ω—É—é.")

        if not target_urls_raw and my_input_type != "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã":
            st.error("–ù–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")
            st.stop()
            
        if not my_data and my_input_type != "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã":
            st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞.")
            st.stop()

        comp_data_full = []
        urls_to_fetch = [item['url'] for item in target_urls_raw]
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = {executor.submit(parse_page, u, settings): u for u in urls_to_fetch}
            done = 0
            total = len(urls_to_fetch)
            prog = st.progress(0)
            stat = st.empty()
            
            for f in concurrent.futures.as_completed(futures):
                res = f.result()
                if res: 
                    comp_data_full.append(res)
                done += 1
                prog.progress(done / total)
                stat.text(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: {done}/{total}")
        prog.empty()
        stat.empty()

        if not comp_data_full:
            st.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.")
        
        with st.spinner("–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö..."):
            st.session_state.analysis_results = calculate_metrics(
                comp_data_full, 
                my_data, 
                settings, 
                my_serp_pos, 
                target_urls_raw 
            ) 
            st.session_state.analysis_done = True
            st.rerun()

    # --- –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (–ò–ó SESSION STATE) ---
    if st.session_state.analysis_done and st.session_state.analysis_results:
        results = st.session_state.analysis_results
        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        
        # –ö–ê–†–¢–û–ß–ö–ê –ë–ê–õ–õ–û–í
        st.markdown(f"""
            <div style='background-color: {LIGHT_BG_MAIN}; padding: 15px; border-radius: 8px; border: 1px solid {BORDER_COLOR}; margin-bottom: 20px;'>
                <h4 style='margin:0; color: {PRIMARY_COLOR};'>–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ (–≤ –±–∞–ª–ª–∞—Ö –æ—Ç 0 –¥–æ 100)</h4>
                <p style='margin:5px 0 0 0;'>–®–∏—Ä–∏–Ω–∞ (–æ—Ö–≤–∞—Ç —Å–µ–º–∞–Ω—Ç–∏–∫–∏): <b>{results['my_score']['width']}</b> | –ì–ª—É–±–∏–Ω–∞ (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è): <b>{results['my_score']['depth']}</b></p>
            </div>
        """, unsafe_allow_html=True)

        # --- –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ë–õ–û–ö: –£–ü–£–©–ï–ù–ù–ê–Ø –°–ï–ú–ê–ù–¢–ò–ö–ê (TEXT BLOCK STYLE) ---
        high = results.get('missing_semantics_high', [])
        low = results.get('missing_semantics_low', [])
        
        count_total = len(high) + len(low)
        if count_total > 0:
            with st.expander(f"üß© –£–ø—É—â–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ ({count_total} —Å–ª–æ–≤) ‚Äî –ù–∞–∂–º–∏—Ç–µ, —á—Ç–æ–±—ã —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å", expanded=False):
                
                # 1. –û–°–ù–û–í–ù–´–ï –°–õ–û–í–ê (–í–ê–ñ–ù–´–ï)
                if high:
                    st.markdown("##### ‚≠êÔ∏è –û—Å–Ω–æ–≤–Ω—ã–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞")
                    
                    words_list_h = [item['word'] for item in high]
                    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é
                    text_cloud_h = ", ".join(words_list_h)
                    
                    st.markdown(
                        f"<div style='background-color:#EBF5FF; padding:15px; border-radius:8px; line-height: 1.6; border: 1px solid #BEE3F8; color: #2C5282; font-size: 14px; margin-bottom: 15px;'>"
                        f"{text_cloud_h}"
                        f"</div>", 
                        unsafe_allow_html=True
                    )
                
                # 2. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –°–õ–û–í–ê (–•–í–û–°–¢)
                if low:
                    st.markdown("##### üîπ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤")
                    st.markdown("–°–ª–æ–≤–∞, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Ä–µ–∂–µ, –Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤ –¢–û–ü–µ.")
                    
                    words_list_l = [item['word'] for item in low]
                    text_cloud_l = ", ".join(words_list_l)
                    
                    st.markdown(
                        f"<div style='background-color:#F7FAFC; padding:15px; border-radius:8px; line-height: 1.6; border: 1px solid #E2E8F0; color: #4A5568; font-size: 13px;'>"
                        f"{text_cloud_l}"
                        f"</div>", 
                        unsafe_allow_html=True
                    )
        # ----------------------------------------

        st.markdown(f"""
            <div class="legend-box">
                <span class="text-red">–ö—Ä–∞—Å–Ω—ã–π</span>: —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç —É –≤–∞—Å. <span class="text-bold">–ñ–∏—Ä–Ω—ã–π</span>: —Å–ª–æ–≤–∞, —É—á–∞—Å—Ç–≤—É—é—â–∏–µ –≤ –∞–Ω–∞–ª–∏–∑–µ.<br>
                –ú–∏–Ω–∏–º—É–º: min(—Å—Ä–µ–¥–Ω–µ–µ, –º–µ–¥–∏–∞–Ω–∞). –ü–µ—Ä–µ—Å–ø–∞–º: % –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –º–∞–∫—Å. –¥–∏–∞–ø–∞–∑–æ–Ω–∞. <br>
                ‚ÑπÔ∏è –î–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –≤—Å–µ–≥–æ —Å–ø–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—é –Ω–∞–¥ —Ç–∞–±–ª–∏—Ü–µ–π.
            </div>
        """, unsafe_allow_html=True)

        render_paginated_table(results['depth'], "1. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≥–ª—É–±–∏–Ω–µ", "tbl_depth_1", default_sort_col="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è", use_abs_sort_default=True)
        render_paginated_table(results['hybrid'], "3. –ì–∏–±—Ä–∏–¥–Ω—ã–π –¢–û–ü (TF-IDF)", "tbl_hybrid", default_sort_col="TF-IDF –¢–û–ü", use_abs_sort_default=False)
        render_paginated_table(results['relevance_top'], "4. –¢–û–ü —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–ë–∞–ª–ª—ã 0-100)", "tbl_rel", default_sort_col="–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)", use_abs_sort_default=False)

# ------------------------------------------
# –í–∫–ª–¥–∞–∫–∞ 2: –ù–û–í–´–ô –ú–û–î–£–õ–¨ (PERPLEXITY)
# ------------------------------------------
with tab_ai:
    st.title("AI –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¢–µ–∫—Å—Ç–æ–≤ (Perplexity)")
    st.markdown("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML-–±–ª–æ–∫–æ–≤ –¥–ª—è –ø–æ–¥—Ñ–∏–ª—å—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.")

    with st.container():
        st.markdown("### üîë –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API")
        api_key_input = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à Perplexity API Key (–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å pplx-)", type="password", key="pplx_key_input")
        
        st.markdown("### üì• –í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö")
        target_url_gen = st.text_input("URL –°—Ç—Ä–∞–Ω–∏—Ü—ã (–≥–¥–µ –±—Ä–∞—Ç—å —Ç–µ–≥–∏/—Ç–æ–≤–∞—Ä—ã)", placeholder="https://site.ru/catalog/category/", key="pplx_url_input")
    
    st.markdown("---")

    # --- –õ–û–ì–ò–ö–ê –ì–ï–ù–ï–†–ê–¶–ò–ò (–ü–û –ö–ù–û–ü–ö–ï) ---
    col_btn_start, col_btn_reset = st.columns([2,1])
    
    with col_btn_start:
        start_gen = st.button("üöÄ –ù–∞—á–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é", type="primary", disabled=not api_key_input, key="btn_start_gen")
    
    # –ö–Ω–æ–ø–∫–∞ —Ä—É—á–Ω–æ–≥–æ —Å–±—Ä–æ—Å–∞
    with col_btn_reset:
        if st.button("üîÑ –°–±—Ä–æ—Å–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç", key="btn_reset_gen"):
            st.session_state.ai_generated_df = None
            st.session_state.ai_excel_bytes = None
            st.rerun()

    if start_gen:
        # –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –°–ë–†–û–° –ü–ï–†–ï–î –ù–ê–ß–ê–õ–û–ú –ù–û–í–û–ô –ì–ï–ù–ï–†–ê–¶–ò–ò
        st.session_state.ai_generated_df = None
        st.session_state.ai_excel_bytes = None
        
        if not openai:
            st.error("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ `openai` –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞! `pip install openai`")
            st.stop()
            
        if not target_url_gen:
            st.error("–í–≤–µ–¥–∏—Ç–µ URL!")
            st.stop()
            
        try:
            client = openai.OpenAI(api_key=api_key_input, base_url="https://api.perplexity.ai")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–∞: {e}")
            st.stop()

        with st.status("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã...", expanded=True) as status:
            base_text, tags, error = get_page_data_for_gen(target_url_gen)
            
            if error:
                status.update(label="–û—à–∏–±–∫–∞!", state="error")
                st.error(error)
                st.stop()
                
            if not tags:
                status.update(label="–¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!", state="error")
                st.warning("–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ `popular-tags-inner` –∏–ª–∏ —Å—Å—ã–ª–∫–∏ –≤ –Ω–µ–º.")
                st.stop()
            
            # --- –°–ë–û–† –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í –ò–ó –í–ö–õ–ê–î–ö–ò SEO (–° –§–ò–õ–¨–¢–†–ê–¶–ò–ï–ô) ---
            seo_keywords_list = []
            if st.session_state.analysis_results:
                high_list = st.session_state.analysis_results.get('missing_semantics_high', [])
                if high_list:
                    # 1. –§–∏–ª—å—Ç—Ä—É–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ (whatsapp, zakaz –∏ —Ç.–¥.)
                    clean_candidates = []
                    for item in high_list:
                        word = item['word'].lower()
                        # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –ù–ï –≤ —á–µ—Ä–Ω–æ–º —Å–ø–∏—Å–∫–µ –∏ –¥–ª–∏–Ω–Ω–µ–µ 2 –±—É–∫–≤ (—á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å 'ok', 'pt')
                        if word not in GARBAGE_LATIN_STOPLIST and len(word) > 2:
                            clean_candidates.append(item['word'])
                    
                    # 2. –ë–ï–†–ï–ú –í–°–ï, –ß–¢–û –û–°–¢–ê–õ–û–°–¨ (–ë–ï–ó –õ–ò–ú–ò–¢–ê)
                    seo_keywords_list = clean_candidates
                    
                    st.info(f"–í—ã–±—Ä–∞–Ω–æ {len(seo_keywords_list)} —Å–ª–æ–≤ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è: {', '.join(seo_keywords_list)}")
                else:
                    st.warning("–°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ '–®–∏—Ä–∏–Ω–∞' –ø—É—Å—Ç. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–π–¥–µ—Ç –±–µ–∑ –¥–æ–ø. –∫–ª—é—á–µ–π.")
            
            status.update(label=f"–ù–∞–π–¥–µ–Ω–æ —Ç–µ–≥–æ–≤: {len(tags)}. –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...", state="running")
            
            all_rows = []
            prog_bar = st.progress(0)
            
            for i, tag in enumerate(tags):
                tag_name = tag['name']
                st.write(f"‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞: **{tag_name}** ({i+1}/{len(tags)})")
                
                blocks = generate_five_blocks(client, base_text, tag_name, seo_keywords_list)
                
                row = {
                    'TagName': tag_name,
                    'URL': tag['url'],
                    'IP_PROP4839': blocks[0],
                    'IP_PROP4816': blocks[1],
                    'IP_PROP4838': blocks[2],
                    'IP_PROP4829': blocks[3],
                    'IP_PROP4831': blocks[4],
                    **STATIC_DATA_GEN
                }
                all_rows.append(row)
                prog_bar.progress((i + 1) / len(tags))
                time.sleep(0.5) 
            
            status.update(label="–ì–æ—Ç–æ–≤–æ!", state="complete")
            
            # --- –°–û–•–†–ê–ù–ï–ù–ò–ï –í SESSION STATE ---
            if all_rows:
                df = pd.DataFrame(all_rows)
                cols = [
                    'TagName', 'URL', 
                    'IP_PROP4839', 'IP_PROP4817', 'IP_PROP4818', 'IP_PROP4819', 'IP_PROP4820', 
                    'IP_PROP4821', 'IP_PROP4822', 'IP_PROP4823', 'IP_PROP4824',
                    'IP_PROP4816', 'IP_PROP4825', 'IP_PROP4826', 
                    'IP_PROP4834', 'IP_PROP4835', 'IP_PROP4836', 'IP_PROP4837',
                    'IP_PROP4838', 'IP_PROP4829', 'IP_PROP4831'
                ]
                final_cols = [c for c in cols if c in df.columns]
                df = df[final_cols]
                
                st.session_state.ai_generated_df = df
                
                # –°–æ–∑–¥–∞–µ–º –±—É—Ñ–µ—Ä –±–∞–π—Ç–æ–≤ –æ–¥–∏–Ω —Ä–∞–∑ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                    df.to_excel(writer, index=False, sheet_name='Sheet1')
                st.session_state.ai_excel_bytes = buffer.getvalue()
                
                st.rerun() # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞, —á—Ç–æ–±—ã –æ–±–Ω–æ–≤–∏—Ç—å UI –∏ –ø–æ–∫–∞–∑–∞—Ç—å –∫–Ω–æ–ø–∫—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è

    # --- –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–ê (–í–ù–ï –ë–õ–û–ö–ê –ö–ù–û–ü–ö–ò) ---
    if st.session_state.ai_generated_df is not None:
        st.success("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")
        
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å Excel —Ñ–∞–π–ª",
            data=st.session_state.ai_excel_bytes,
            file_name="seo_texts_result.xlsx",
            mime="application/vnd.ms-excel"
        )
        
        with st.expander("–ü—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫)"):
            st.dataframe(st.session_state.ai_generated_df.head())

# ------------------------------------------
# –í–∫–ª–¥–∞–∫–∞ 3: –ì–ï–ù–ï–†–ê–¢–û–† –ü–õ–ò–¢–ö–ò –¢–ï–ì–û–í (NEW)
# ------------------------------------------
with tab_tags:
    st.title("üè∑Ô∏è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–ª–∏—Ç–∫–∏ —Ç–µ–≥–æ–≤")
    st.markdown("–í—Å—Ç–∞–≤—å—Ç–µ —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏). –°–∫—Ä–∏–ø—Ç –ø–µ—Ä–µ–π–¥–µ—Ç –ø–æ –Ω–∏–º, –∑–∞–±–µ—Ä–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (H1) –∏ —Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç HTML-–∫–æ–¥ –ø–ª–∏—Ç–∫–∏.")
    
    urls_input = st.text_area("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫", height=200, placeholder="https://site.ru/catalog/filter/1/\nhttps://site.ru/catalog/filter/2/", key="tag_urls_input")
    
    if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–ª–∏—Ç–∫—É", type="primary", key="btn_gen_tags"):
        if not urls_input.strip():
            st.error("–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫–∏!")
            st.stop()
            
        urls_list = [u.strip() for u in urls_input.split('\n') if u.strip()]
        
        results_tags = []
        
        # –§—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ H1
        def fetch_h1_title(url):
            try:
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
                r = requests.get(url, headers=headers, timeout=10)
                if r.status_code == 200:
                    r.encoding = 'utf-8' # Force utf-8 usually for Russian sites
                    soup = BeautifulSoup(r.text, 'html.parser')
                    
                    # 1. –ü—Ä–æ–±—É–µ–º H1
                    h1 = soup.find('h1')
                    if h1:
                        return h1.get_text(strip=True)
                    
                    # 2. –ü—Ä–æ–±—É–µ–º Title
                    if soup.title:
                        return soup.title.get_text(strip=True)
                        
                return "–ù–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞"
            except:
                return "–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞"

        # –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π —Å–±–æ—Ä
        with st.status("–°–±–æ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤...", expanded=True) as status:
            progress_bar = st.progress(0)
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                future_to_url = {executor.submit(fetch_h1_title, url): url for url in urls_list}
                
                completed_count = 0
                for future in concurrent.futures.as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        name = future.result()
                        results_tags.append({'url': url, 'name': name})
                    except Exception as exc:
                        results_tags.append({'url': url, 'name': "–û—à–∏–±–∫–∞"})
                    
                    completed_count += 1
                    progress_bar.progress(completed_count / len(urls_list))
            
            status.update(label="–ì–æ—Ç–æ–≤–æ!", state="complete")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML
        if results_tags:
            html_output = '<div class="popular-tags-text">\n<div class="popular-tags-inner-text">\n<div class="tag-items">\n'
            
            for item in results_tags:
                # –í—Å—Ç–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —à–∞–±–ª–æ–Ω
                html_output += f'<a href="{item["url"]}" class="tag-item">{item["name"]}</a>\n'
                
            html_output += '</div>\n</div>\n</div>'
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å–µ—Å—Å–∏—é
            st.session_state.tags_html_result = html_output
            st.rerun() # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É

    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å –≤ —Å–µ—Å—Å–∏–∏
    if st.session_state.tags_html_result:
        st.success("HTML –∫–æ–¥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω:")
        st.code(st.session_state.tags_html_result, language='html')
        if st.button("–°–±—Ä–æ—Å–∏—Ç—å", key="reset_tags"):
            st.session_state.tags_html_result = None
            st.rerun()

# ------------------------------------------
# –í–∫–ª–¥–∞–∫–∞ 4: –ì–ï–ù–ï–†–ê–¢–û–† –¢–ê–ë–õ–ò–¶ (NEW)
# ------------------------------------------
with tab_tables:
    st.title("üß© –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä HTML —Ç–∞–±–ª–∏—Ü")
    st.markdown("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å, –∏ –ò–ò —Å–æ–∑–¥–∞—Å—Ç —Ç–∞–±–ª–∏—Ü—É —Å –∂–µ—Å—Ç–∫–æ –∑–∞–¥–∞–Ω–Ω—ã–º –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ–º (—á–µ—Ä–Ω—ã–µ —Ä–∞–º–∫–∏, –æ—Ç—Å—Ç—É–ø—ã).")
    
    # –ü–æ–≤—Ç–æ—Ä—è–µ–º –≤–≤–æ–¥ –∫–ª—é—á–∞ –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –Ω–µ –±–µ–≥–∞—Ç—å –º–µ–∂–¥—É –≤–∫–ª–∞–¥–∫–∞–º–∏
    pplx_key_table = st.text_input("Perplexity API Key", type="password", key="pplx_key_table")
    
    table_prompt = st.text_area("–û–ø–∏—à–∏—Ç–µ, –∫–∞–∫—É—é —Ç–∞–±–ª–∏—Ü—É –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å", height=150, placeholder="–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –≤–∏–¥–æ–≤ —Ç—Ä—É–± –∏–∑ –ü–í–• —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º")
    
    if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É", type="primary", key="btn_gen_table"):
        if not pplx_key_table:
            st.error("–í–≤–µ–¥–∏—Ç–µ API –∫–ª—é—á!")
            st.stop()
        if not table_prompt:
            st.error("–í–≤–µ–¥–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã!")
            st.stop()
            
        if not openai:
            st.error("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ `openai` –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")
            st.stop()
            
        try:
            client_table = openai.OpenAI(api_key=pplx_key_table, base_url="https://api.perplexity.ai")
            
            with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã..."):
                html_result = generate_html_table(client_table, table_prompt)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å–µ—Å—Å–∏—é
            st.session_state.table_html_result = html_result
            st.rerun()

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ API: {e}")

    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å –≤ —Å–µ—Å—Å–∏–∏
    if st.session_state.table_html_result:
        st.success("–ì–æ—Ç–æ–≤–æ!")
        st.code(st.session_state.table_html_result, language='html')
        
        st.markdown("### –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä (–ø—Ä–∏–º–µ—Ä–Ω—ã–π):")
        st.markdown(st.session_state.table_html_result, unsafe_allow_html=True)
        
        if st.button("–°–±—Ä–æ—Å–∏—Ç—å", key="reset_table"):
            st.session_state.table_html_result = None
            st.rerun()







