import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin, unquote
import inspect
import time
import json
import io
from google import genai
import os
import requests
proxy_url = "http://QYnojH:Uekp4k@196.18.3.35:8000" 

os.environ["http_proxy"] = proxy_url
os.environ["https_proxy"] = proxy_url

try:
    my_ip = requests.get("https://api.ipify.org", timeout=5).text
    st.info(f"üïµÔ∏è –í–ê–® IP –î–õ–Ø –°–ö–†–ò–ü–¢–ê: {my_ip}")
except Exception as e:
    st.error(f"‚ùå –ü—Ä–æ–∫—Å–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {e}")
    
import random
import streamlit.components.v1 as components
import copy
import plotly.graph_objects as go
import pickle
import datetime

# ==========================================
# FIX FOR PYTHON 3.11+
# ==========================================
if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except ImportError:
    morph = None
    USE_NLP = False

try:
    import openai
except ImportError:
    openai = None

try:
    from google import genai
except ImportError:
    genai = None

# ==========================================
# 0. –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ==========================================

def transliterate_text(text):
    mapping = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }
    result = []
    for char in text.lower():
        if char in mapping:
            result.append(mapping[char])
        elif char.isalnum() or char == '-':
            result.append(char)
    return "".join(result)

def force_cyrillic_name_global(slug_text):
    raw = unquote(slug_text).lower()
    raw = raw.replace('.html', '').replace('.php', '')
    if re.search(r'[–∞-—è]', raw):
        return raw.replace('-', ' ').replace('_', ' ').capitalize()

    words = re.split(r'[-_]', raw)
    rus_words = []
    
    exact_map = {
        'nikel': '–Ω–∏–∫–µ–ª—å', 'stal': '—Å—Ç–∞–ª—å', 'med': '–º–µ–¥—å', 'latun': '–ª–∞—Ç—É–Ω—å',
        'bronza': '–±—Ä–æ–Ω–∑–∞', 'svinec': '—Å–≤–∏–Ω–µ—Ü', 'titan': '—Ç–∏—Ç–∞–Ω', 'tsink': '—Ü–∏–Ω–∫',
        'dural': '–¥—é—Ä–∞–ª—å', 'dyural': '–¥—é—Ä–∞–ª—å', 'chugun': '—á—É–≥—É–Ω',
        'alyuminiy': '–∞–ª—é–º–∏–Ω–∏–π', 'al': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è', 'alyuminievaya': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è',
        'nerzhaveyushchiy': '–Ω–µ—Ä–∂–∞–≤–µ—é—â–∏–π', 'nerzhaveyka': '–Ω–µ—Ä–∂–∞–≤–µ–π–∫–∞',
        'profil': '–ø—Ä–æ—Ñ–∏–ª—å', 'shveller': '—à–≤–µ–ª–ª–µ—Ä', 'ugolok': '—É–≥–æ–ª–æ–∫',
        'polosa': '–ø–æ–ª–æ—Å–∞', 'krug': '–∫—Ä—É–≥', 'kvadrat': '–∫–≤–∞–¥—Ä–∞—Ç',
        'list': '–ª–∏—Å—Ç', 'truba': '—Ç—Ä—É–±–∞', 'setka': '—Å–µ—Ç–∫–∞',
        'provoloka': '–ø—Ä–æ–≤–æ–ª–æ–∫–∞', 'armatura': '–∞—Ä–º–∞—Ç—É—Ä–∞', 'balka': '–±–∞–ª–∫–∞',
        'katanka': '–∫–∞—Ç–∞–Ω–∫–∞', 'otvod': '–æ—Ç–≤–æ–¥', 'perehod': '–ø–µ—Ä–µ—Ö–æ–¥',
        'flanec': '—Ñ–ª–∞–Ω–µ—Ü', 'zaglushka': '–∑–∞–≥–ª—É—à–∫–∞', 'metiz': '–º–µ—Ç–∏–∑—ã',
        'profnastil': '–ø—Ä–æ—Ñ–Ω–∞—Å—Ç–∏–ª', 'shtrips': '—à—Ç—Ä–∏–ø—Å', 'lenta': '–ª–µ–Ω—Ç–∞',
        'shina': '—à–∏–Ω–∞', 'prutok': '–ø—Ä—É—Ç–æ–∫', 'shestigrannik': '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫',
        'vtulka': '–≤—Ç—É–ª–∫–∞', 'kabel': '–∫–∞–±–µ–ª—å', 'panel': '–ø–∞–Ω–µ–ª—å',
        'detal': '–¥–µ—Ç–∞–ª—å', 'set': '—Å–µ—Ç—å', 'cep': '—Ü–µ–ø—å', 'svyaz': '—Å–≤—è–∑—å',
        'rezba': '—Ä–µ–∑—å–±–∞', 'gost': '–ì–û–°–¢',
        'polipropilenovye': '–ø–æ–ª–∏–ø—Ä–æ–ø–∏–ª–µ–Ω–æ–≤—ã–µ', 'truby': '—Ç—Ä—É–±—ã',
        'ocinkovannaya': '–æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω–∞—è', 'riflenyy': '—Ä–∏—Ñ–ª–µ–Ω—ã–π'
    }

    for w in words:
        if not w: continue
        if w in exact_map:
            rus_words.append(exact_map[w])
            continue
        
        processed_w = w
        if processed_w.endswith('yy'): processed_w = processed_w[:-2] + '—ã–π'
        elif processed_w.endswith('iy'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('ij'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('yi'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('aya'): processed_w = processed_w[:-3] + '–∞—è'
        elif processed_w.endswith('oye'): processed_w = processed_w[:-3] + '–æ–µ'
        elif processed_w.endswith('ye'): processed_w = processed_w[:-2] + '—ã–µ'

        replacements = [
            ('shch', '—â'), ('sch', '—â'), ('yo', '—ë'), ('zh', '–∂'), ('ch', '—á'), ('sh', '—à'), 
            ('yu', '—é'), ('ya', '—è'), ('kh', '—Ö'), ('ts', '—Ü'), ('ph', '—Ñ'),
            ('a', '–∞'), ('b', '–±'), ('v', '–≤'), ('g', '–≥'), ('d', '–¥'), ('e', '–µ'), 
            ('z', '–∑'), ('i', '–∏'), ('j', '–π'), ('k', '–∫'), ('l', '–ª'), ('m', '–º'), 
            ('n', '–Ω'), ('o', '–æ'), ('p', '–ø'), ('r', '—Ä'), ('s', '—Å'), ('t', '—Ç'), 
            ('u', '—É'), ('f', '—Ñ'), ('h', '—Ö'), ('c', '–∫'), ('w', '–≤'), ('y', '—ã'), ('x', '–∫—Å')
        ]
        
        temp_res = processed_w
        for eng, rus in replacements:
            temp_res = temp_res.replace(eng, rus)
        
        rus_words.append(temp_res)

    draft_phrase = " ".join(rus_words)
    draft_phrase = draft_phrase.replace('–ø—Ä–æ—Ñ–∏–ª', '–ø—Ä–æ—Ñ–∏–ª—å').replace('–ø—Ä–æ—Ñ–∏–ª—å–Ω', '–ø—Ä–æ—Ñ–∏–ª—å–Ω')
    draft_phrase = draft_phrase.replace('–µ–ª–Ω—ã–π', '–µ–ª—å–Ω—ã–π').replace('–∞–ª–Ω—ã–π', '–∞–ª—å–Ω—ã–π')
    draft_phrase = draft_phrase.replace('–µ–ª–Ω–∞—è', '–µ–ª—å–Ω–∞—è').replace('–∞–ª–Ω–∞—è', '–∞–ª—å–Ω–∞—è')
    draft_phrase = draft_phrase.replace('—Å—Ç–∞–ª–Ω', '—Å—Ç–∞–ª—å–Ω').replace('–º–µ–¥—å–Ω', '–º–µ–¥–Ω')
    draft_phrase = draft_phrase.replace('–π–∞', '—è').replace('–π–æ', '—ë')

    return draft_phrase.capitalize()

def get_breadcrumb_only(url, ua_settings="Mozilla/5.0"):
    try:
        session = requests.Session()
        retry = Retry(connect=3, read=3, redirect=3, backoff_factor=0.5)
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        
        headers = {'User-Agent': ua_settings}
        r = session.get(url, headers=headers, timeout=25)
        if r.status_code != 200: 
            return None
        
        soup = BeautifulSoup(r.text, 'html.parser')
        
        breadcrumbs = soup.find(class_=re.compile(r'breadcrumb|breadcrumbs|nav-path|nav-chain|bx-breadcrumb', re.I))
        if not breadcrumbs:
            breadcrumbs = soup.find(id=re.compile(r'breadcrumb|breadcrumbs|nav-path', re.I))

        if breadcrumbs:
            full_text = breadcrumbs.get_text(separator='|||', strip=True)
            parts = [p.strip() for p in full_text.split('|||') if p.strip()]
            clean_parts = [p for p in parts if p not in ['/', '\\', '>', '¬ª', '‚Ä¢', '-', '|']]
            
            if clean_parts:
                last_item = clean_parts[-1]
                if len(last_item) > 2 and last_item.lower() != "–≥–ª–∞–≤–Ω–∞—è":
                    return last_item
    except:
        return None
    return None

def render_clean_block(title, icon, words_list):
    unique_words = sorted(list(set(words_list))) if words_list else []
    count = len(unique_words)
    
    if count > 0:
        content_html = ", ".join(unique_words)
        # –ö–∞—Ä—Ç–æ—á–∫–∞ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è
        html_code = f"""
        <details class="details-card">
            <summary class="card-summary">
                <div>
                    <span class="arrow-icon">‚ñ∂</span>
                    {icon} {title}
                </div>
                <span class="count-tag">{count}</span>
            </summary>
            <div class="card-content">
                {content_html}
            </div>
        </details>
        """
    else:
        # –ï—Å–ª–∏ –ø—É—Å—Ç–æ - –∫–∞—Ä—Ç–æ—á–∫–∞ –Ω–µ–∞–∫—Ç–∏–≤–Ω–∞ (–±–µ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
        html_code = f"""
        <div class="details-card">
            <div class="card-summary" style="cursor: default; color: #9ca3af;">
                <div>{icon} {title}</div>
                <span class="count-tag">0</span>
            </div>
        </div>
        """
    
    st.markdown(html_code, unsafe_allow_html=True)

def render_relevance_chart(df_rel, unique_key="default"):
    if df_rel.empty:
        return

    # 1. –ñ–ï–°–¢–ö–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ > 0
    # –í–∞—à —Å–∞–π—Ç (–ø–æ–∑–∏—Ü–∏—è 0) —É–¥–∞–ª—è–µ—Ç—Å—è –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
    df = df_rel[df_rel['–ü–æ–∑–∏—Ü–∏—è'] > 0].copy()
    
    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø—É—Å—Ç–∞ - –≤—ã—Ö–æ–¥–∏–º
    if df.empty:
        return

    df = df.sort_values(by='–ü–æ–∑–∏—Ü–∏—è')
    x_indices = np.arange(len(df))
    
    tick_links = []
    
    for _, row in df.iterrows():
        # –ß–∏—Å—Ç–∏–º –∏–º—è –¥–æ–º–µ–Ω–∞
        raw_name = row['–î–æ–º–µ–Ω'].replace(' (–í—ã)', '').strip()
        clean_domain = raw_name.replace('www.', '').split('/')[0]
        
        # –§–æ—Ä–º–∞—Ç: "1. site.ru" (–±–µ–∑ #)
        label_text = f"{row['–ü–æ–∑–∏—Ü–∏—è']}. {clean_domain}"
        
        # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å, —Ç–∞–∫ –∫–∞–∫ —à—Ä–∏—Ñ—Ç —Ç–µ–ø–µ—Ä—å –∫—Ä—É–ø–Ω–µ–µ
        if len(label_text) > 25: label_text = label_text[:23] + ".."
        
        url_target = row.get('URL', f"https://{raw_name}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º CSS-–∫–ª–∞—Å—Å .chart-link –≤–º–µ—Å—Ç–æ style="..." –¥–ª—è —Ä–∞–±–æ—Ç—ã hover
        link_html = f"<a href='{url_target}' target='_blank' class='chart-link'>{label_text}</a>"
        tick_links.append(link_html)

    # –ú–µ—Ç—Ä–∏–∫–∏
    df['Total_Rel'] = (df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] + df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)']) / 2
    
    # –¢—Ä–µ–Ω–¥
    z = np.polyfit(x_indices, df['Total_Rel'], 1)
    p = np.poly1d(z)
    df['Trend'] = p(x_indices)

    # 2. –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
    fig = go.Figure()

    # --- –ü–ê–õ–ò–¢–†–ê (Premium) ---
    COLOR_MAIN = '#4F46E5'  # –ò–Ω–¥–∏–≥–æ
    COLOR_WIDTH = '#0EA5E9' # –ì–æ–ª—É–±–æ–π
    COLOR_DEPTH = '#E11D48' # –ú–∞–ª–∏–Ω–æ–≤—ã–π
    COLOR_TREND = '#15803d' # –ó–µ–ª–µ–Ω—ã–π (Forest Green)

    COMMON_CONFIG = dict(
        mode='lines+markers',
        line=dict(width=3, shape='spline'), 
        marker=dict(size=8, line=dict(width=2, color='white'), symbol='circle')
    )

    # 1. –û–ë–©–ê–Ø
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['Total_Rel'],
        name='–û–±—â–∞—è',
        line=dict(color=COLOR_MAIN, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_MAIN, **COMMON_CONFIG['marker']),
        mode='lines+markers'
    ))

    # 2. –®–ò–†–ò–ù–ê
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'],
        name='–®–∏—Ä–∏–Ω–∞',
        line=dict(color=COLOR_WIDTH, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_WIDTH, **COMMON_CONFIG['marker']),
        mode='lines+markers'
    ))

    # 3. –ì–õ–£–ë–ò–ù–ê
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'],
        name='–ì–ª—É–±–∏–Ω–∞',
        line=dict(color=COLOR_DEPTH, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_DEPTH, **COMMON_CONFIG['marker']),
        mode='lines+markers'
    ))

    # 4. –¢–†–ï–ù–î
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['Trend'],
        name='–¢—Ä–µ–Ω–¥',
        line=dict(color=COLOR_TREND, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_TREND, **COMMON_CONFIG['marker']),
        mode='lines+markers',
        opacity=0.8
    ))

# 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Layout (–ö–û–ú–ü–ê–ö–¢–ù–ê–Ø –í–ï–†–°–ò–Ø)
    fig.update_layout(
        template="plotly_white",
        legend=dict(
            orientation="h",
            yanchor="bottom", y=1.02, # –õ–µ–≥–µ–Ω–¥–∞ –ø—Ä—è–º–æ –Ω–∞–¥ –≥—Ä–∞—Ñ–∏–∫–æ–º
            xanchor="center", x=0.5,
            font=dict(size=12, color="#111827", family="Inter, sans-serif")
        ),
        xaxis=dict(
            showgrid=True,
            gridcolor='#F3F4F6',
            linecolor='#E5E7EB',
            tickmode='array',
            tickvals=x_indices,
            ticktext=tick_links, 
            
            tickfont=dict(size=11), # –ß—É—Ç—å –º–µ–Ω—å—à–µ —à—Ä–∏—Ñ—Ç –ø–æ–¥–ø–∏—Å–µ–π
            tickangle=-45, 
            
            fixedrange=True,
            dtick=1, 
            range=[-0.5, len(df) - 0.5], 
            automargin=False 
        ),
        yaxis=dict(
            range=[0, 115], 
            showgrid=True, 
            gridcolor='#F3F4F6', 
            gridwidth=1,
            zeroline=False,
            fixedrange=True
        ),
        # === –í–û–¢ –¢–£–¢ –ú–ï–ù–Ø–ï–ú –†–ê–ó–ú–ï–†–´ ===
        # l/r - –±–æ–∫–∞, t - –≤–µ—Ä—Ö, b - –Ω–∏–∑ (–ø–æ–¥ –ø–æ–¥–ø–∏—Å–∏)
        margin=dict(l=10, r=10, t=30, b=110),
        
        hovermode="x unified",
        
        # –û–±—â–∞—è –≤—ã—Å–æ—Ç–∞ –≥—Ä–∞—Ñ–∏–∫–∞ (–±—ã–ª–æ 550)
        height=400 
    )
    
    # use_container_width=True —Ä–∞—Å—Ç—è–≥–∏–≤–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –Ω–∞ –≤—Å—é —à–∏—Ä–∏–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    st.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False}, key=f"rel_chart_{unique_key}")

def analyze_serp_anomalies(df_rel):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–í–µ—Ä—Å–∏—è v5 - Robust).
    –ü–æ—Ä–æ–≥: 75% –æ—Ç –ª–∏–¥–µ—Ä–∞. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∏–ø–∏–∑–∞—Ü–∏—è.
    """
    if df_rel.empty:
        return [], [], {"type": "none", "msg": ""}

    # –ò—Å–∫–ª—é—á–∞–µ–º "–í–∞—à —Å–∞–π—Ç" –∏–∑ —Ä–∞—Å—á–µ—Ç–æ–≤ —ç—Ç–∞–ª–æ–Ω–∞
    df = df_rel[~df_rel['–î–æ–º–µ–Ω'].str.contains("\(–í—ã\)", na=False)].copy()
    
    if df.empty:
        return [], [], {"type": "none", "msg": ""}

    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –¥–µ–ª–∞–µ–º —á–∏—Å–ª–∞–º–∏ (–∑–∞—â–∏—Ç–∞ –æ—Ç —Å–±–æ–µ–≤)
    df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] = pd.to_numeric(df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'], errors='coerce').fillna(0)
    df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'] = pd.to_numeric(df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'], errors='coerce').fillna(0)

    # –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª
    df['Total'] = (df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] + df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)']) / 2
    
    # 1. –ò–©–ï–ú –õ–ò–î–ï–†–ê
    max_score = df['Total'].max()
    if max_score < 1: max_score = 1 # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
    
    # 2. –ñ–ï–°–¢–ö–ò–ô –ü–û–†–û–ì: 75% –æ—Ç –ª–∏–¥–µ—Ä–∞.
    # –ï—Å–ª–∏ –õ–∏–¥–µ—Ä=100, –ø–æ—Ä–æ–≥=75. –í—Å–µ —á—Ç–æ < 75 - —É–¥–∞–ª—è–µ–º.
    threshold = max(max_score * 0.75, 40) 
    
    anomalies = []
    normal_urls = []
    
    debug_counts = 0
    
    for _, row in df.iterrows():
        # –î–æ—Å—Ç–∞–µ–º —Å—Å—ã–ª–∫—É. –ó–∞—â–∏—Ç–∞ –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤.
        current_url = str(row.get('URL', '')).strip()
        if not current_url or current_url.lower() == 'nan':
             current_url = f"https://{row['–î–æ–º–µ–Ω']}" 

        score = row['Total']
        
        # –ê–ù–ê–õ–ò–ó
        if score < threshold:
            reason = f"–°–∫–æ—Ä {int(score)} < {int(threshold)} (–õ–∏–¥–µ—Ä {int(max_score)})"
            anomalies.append({'url': current_url, 'reason': reason, 'score': score})
            debug_counts += 1
        else:
            normal_urls.append(current_url)

    # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—è–º–∏
    if anomalies:
        st.toast(f"üóëÔ∏è –§–∏–ª—å—Ç—Ä (–õ–∏–¥–µ—Ä {int(max_score)} / –ü–æ—Ä–æ–≥ {int(threshold)}). –ò—Å–∫–ª—é—á–µ–Ω–æ: {len(anomalies)}", icon="‚ö†Ô∏è")
    else:
        # –ï—Å–ª–∏ –Ω–∏–∫–æ–≥–æ –Ω–µ –∏—Å–∫–ª—é—á–∏–ª–∏, –ø–∏—à–µ–º –ø–æ—á–µ–º—É
        st.toast(f"‚úÖ –í—Å–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –æ–∫. (–õ–∏–¥–µ—Ä {int(max_score)} / –ü–æ—Ä–æ–≥ {int(threshold)}). –ú–∏–Ω. –±–∞–ª–ª: {int(df['Total'].min())}", icon="‚ÑπÔ∏è")
    
    # –¢—Ä–µ–Ω–¥
    x = np.arange(len(df)); y = df['Total'].values
    slope = np.polyfit(x, y, 1)[0] if len(x) > 1 else 0
    trend_msg = "üìâ –ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–æ–ø" if slope < -1 else ("üìà –ü–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç—ã–π —Ç–æ–ø" if slope > 1 else "‚û°Ô∏è –†–æ–≤–Ω—ã–π —Ç–æ–ø")

    return normal_urls, anomalies, {"type": "info", "msg": trend_msg}

@st.cache_data
def load_lemmatized_dictionaries():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_path = os.path.join(script_dir, "data")
    
    # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞
    sets = {
        "products": set(),
        "commercial": set(),
        "specs": set(),
        "geo": set(),
        "services": set(),
        "sensitive": set()
    }

    # –ö–∞—Ä—Ç–∞ —Ñ–∞–π–ª–æ–≤
    files_map = {
        "metal_products.json": "products",
        "commercial_triggers.json": "commercial",
        "geo_locations.json": "geo",
        "services_triggers.json": "services",
        "tech_specs.json": "specs",
        "SENSITIVE_STOPLIST.json": "sensitive"
    }

    for filename, set_key in files_map.items():
        full_path = os.path.join(base_path, filename)
        if not os.path.exists(full_path):
            continue
        
        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                data = json.load(f) 
                
                words_bucket = []
                if isinstance(data, dict):
                    for cat_list in data.values():
                        words_bucket.extend(cat_list)
                elif isinstance(data, list):
                    words_bucket = data
                
                for phrase in words_bucket:
                    w_clean = str(phrase).lower().strip().replace('—ë', '–µ')
                    if not w_clean: continue
                    sets[set_key].add(w_clean)
                    if morph:
                        normal_form = morph.parse(w_clean)[0].normal_form.replace('—ë', '–µ')
                        sets[set_key].add(normal_form)
                    if ' ' in w_clean:
                        parts = w_clean.split()
                        for p in parts:
                            sets[set_key].add(p)
                            if morph: 
                                sets[set_key].add(morph.parse(p)[0].normal_form.replace('—ë', '–µ'))
        except: pass

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 6 –Ω–∞–±–æ—Ä–æ–≤
    return sets["products"], sets["commercial"], sets["specs"], sets["geo"], sets["services"], sets["sensitive"]

def classify_semantics_with_api(words_list, yandex_key):
    # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º 6 —Å–ª–æ–≤–∞—Ä–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Ä–Ω—É–ª–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
    PRODUCTS_SET, COMM_SET, SPECS_SET, GEO_SET, SERVICES_SET, SENS_SET = load_lemmatized_dictionaries()
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ —Ñ–∞–π–ª–∞ –∏ –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –≤ –∫–æ–¥–µ
    FULL_SENSITIVE = SENS_SET.union(SENSITIVE_STOPLIST)

    if 'debug_geo_count' not in st.session_state:
        st.session_state.debug_geo_count = len(GEO_SET)
    
    st.sidebar.info(f"–°–ª–æ–≤–∞—Ä–∏ (–∏–∑ —Ñ–∞–π–ª–æ–≤):\nüì¶ –¢–æ–≤–∞—Ä—ã: {len(PRODUCTS_SET)}\nüí∞ –ö–æ–º–º–µ—Ä—Ü–∏—è: {len(COMM_SET)}\nüõ†Ô∏è –£—Å–ª—É–≥–∏: {len(SERVICES_SET)}\nüåç –ì–æ—Ä–æ–¥–∞: {len(GEO_SET)}")

    dim_pattern = re.compile(r'\d+(?:[\.\,]\d+)?\s?[—Öx\*√ó]\s?\d+', re.IGNORECASE)
    grade_pattern = re.compile(r'^([–∞-—èa-z]{1,4}\-?\d+[–∞-—èa-z0-9]*)$', re.IGNORECASE)
    
    categories = {'products': set(), 'services': set(), 'commercial': set(), 
                  'dimensions': set(), 'geo': set(), 'general': set(), 'sensitive': set()}
    
    for word in words_list:
        word_lower = word.lower()
        
        # 1. –°–¢–û–ü-–°–õ–û–í–ê
        is_sensitive = False
        if word_lower in FULL_SENSITIVE: is_sensitive = True
        else:
            for stop_w in FULL_SENSITIVE:
                if len(stop_w) > 3 and stop_w in word_lower: is_sensitive = True; break
        if is_sensitive: categories['sensitive'].add(word_lower); continue
        
        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
        lemma = word_lower
        if morph:
            p = morph.parse(word_lower)[0]
            lemma = p.normal_form

        # 2. –†–ê–ó–ú–ï–†–´ / –ì–û–°–¢
        if word_lower in SPECS_SET or lemma in SPECS_SET:
            categories['dimensions'].add(word_lower); continue
        if dim_pattern.search(word_lower) or grade_pattern.match(word_lower) or word_lower.isdigit():
            categories['dimensions'].add(word_lower); continue

        # 3. –¢–û–í–ê–†–´ (–£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞)
        if word_lower in PRODUCTS_SET or lemma in PRODUCTS_SET:
            categories['products'].add(word_lower); continue
        
        is_product_root = False
        for prod in PRODUCTS_SET:
            check_root = prod[:-1] if len(prod) > 4 else prod
            if len(check_root) > 3 and check_root in word_lower:
                categories['products'].add(word_lower)
                is_product_root = True
                break
        if is_product_root: continue

        # 4. –ì–ï–û
        if lemma in GEO_SET or word_lower in GEO_SET:
            categories['geo'].add(word_lower); continue
        
        # 5. –£–°–õ–£–ì–ò
        if lemma in SERVICES_SET or word_lower in SERVICES_SET:
             categories['services'].add(word_lower); continue
        if lemma.endswith('–æ–±—Ä–∞–±–æ—Ç–∫–∞') or lemma.endswith('–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ') or lemma == "—Ä–µ–∑–∫–∞":
            categories['services'].add(word_lower); continue

        # 6. –ö–û–ú–ú–ï–†–¶–ò–Ø
        if lemma in COMM_SET or word_lower in COMM_SET:
            categories['commercial'].add(word_lower); continue
            
        # 7. –û–ë–©–ò–ï
        categories['general'].add(word_lower)

    return {k: sorted(list(v)) for k, v in categories.items()}

# ==========================================
# STATE INIT
# ==========================================
if 'sidebar_gen_df' not in st.session_state: st.session_state.sidebar_gen_df = None
if 'sidebar_excel_bytes' not in st.session_state: st.session_state.sidebar_excel_bytes = None
if 'analysis_results' not in st.session_state: st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state: st.session_state.analysis_done = False
if 'ai_generated_df' not in st.session_state: st.session_state.ai_generated_df = None
if 'ai_excel_bytes' not in st.session_state: st.session_state.ai_excel_bytes = None
if 'tags_html_result' not in st.session_state: st.session_state.tags_html_result = None
if 'table_html_result' not in st.session_state: st.session_state.table_html_result = None
if 'tags_generated_df' not in st.session_state: st.session_state.tags_generated_df = None
if 'tags_excel_data' not in st.session_state: st.session_state.tags_excel_data = None

# Current lists
if 'categorized_products' not in st.session_state: st.session_state.categorized_products = []
if 'categorized_services' not in st.session_state: st.session_state.categorized_services = []
if 'categorized_commercial' not in st.session_state: st.session_state.categorized_commercial = []
if 'categorized_dimensions' not in st.session_state: st.session_state.categorized_dimensions = []
if 'categorized_geo' not in st.session_state: st.session_state.categorized_geo = []
if 'categorized_general' not in st.session_state: st.session_state.categorized_general = []
if 'categorized_sensitive' not in st.session_state: st.session_state.categorized_sensitive = []

# Original lists (for restoration)
if 'orig_products' not in st.session_state: st.session_state.orig_products = []
if 'orig_services' not in st.session_state: st.session_state.orig_services = []
if 'orig_commercial' not in st.session_state: st.session_state.orig_commercial = []
if 'orig_dimensions' not in st.session_state: st.session_state.orig_dimensions = []
if 'orig_geo' not in st.session_state: st.session_state.orig_geo = []
if 'orig_general' not in st.session_state: st.session_state.orig_general = []

if 'auto_tags_words' not in st.session_state: st.session_state.auto_tags_words = []
if 'auto_promo_words' not in st.session_state: st.session_state.auto_promo_words = []
if 'persistent_urls' not in st.session_state: st.session_state['persistent_urls'] = ""

st.set_page_config(layout="wide", page_title="GAR PRO v2.6 (Mass Promo)", page_icon="üìä")

GARBAGE_LATIN_STOPLIST = {
    'whatsapp', 'viber', 'telegram', 'skype', 'vk', 'instagram', 'facebook', 'youtube', 'twitter',
    'cookie', 'cookies', 'policy', 'privacy', 'agreement', 'terms',
    'click', 'submit', 'send', 'zakaz', 'basket', 'cart', 'order', 'call', 'back', 'callback',
    'login', 'logout', 'sign', 'register', 'auth', 'account', 'profile',
    'search', 'menu', 'nav', 'navigation', 'footer', 'header', 'sidebar',
    'img', 'jpg', 'png', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'svg',
    'ok', 'error', 'undefined', 'null', 'true', 'false', 'var', 'let', 'const', 'function', 'return',
    'ru', 'en', 'com', 'net', 'org', 'biz', 'shop', 'store',
    'phone', 'email', 'tel', 'fax', 'mob', 'address', 'copyright', 'all', 'rights', 'reserved',
    'div', 'span', 'class', 'id', 'style', 'script', 'body', 'html', 'head', 'meta', 'link'
}

SENSITIVE_STOPLIST_RAW = {
    "—É–∫—Ä–∞–∏–Ω–∞", "ukraine", "ua", "–≤—Å—É", "–∑—Å—É", "–∞—Ç–æ",
    "–∫–∏–µ–≤", "–ª—å–≤–æ–≤", "—Ö–∞—Ä—å–∫–æ–≤", "–æ–¥–µ—Å—Å–∞", "–¥–Ω–µ–ø—Ä", "–º–∞—Ä–∏—É–ø–æ–ª—å",
    "–¥–æ–Ω–µ—Ü–∫", "–ª—É–≥–∞–Ω—Å–∫", "–¥–Ω—Ä", "–ª–Ω—Ä", "–¥–æ–Ω–±–∞—Å—Å", 
    "–º–µ–ª–∏—Ç–æ–ø–æ–ª—å", "–±–µ—Ä–¥—è–Ω—Å–∫", "–±–∞—Ö–º—É—Ç", "–∑–∞–ø–æ—Ä–æ–∂—å–µ", "—Ö–µ—Ä—Å–æ–Ω",
    "–∫—Ä—ã–º", "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å", "—Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å"
}
SENSITIVE_STOPLIST = {w.lower() for w in SENSITIVE_STOPLIST_RAW}

def check_password():
    if st.session_state.get("authenticated"):
        return True
    st.markdown("""<style>.main { display: flex; flex-direction: column; justify-content: center; align-items: center; } .auth-logo-box { text-align: center; margin-bottom: 1rem; padding-top: 0; }</style>""", unsafe_allow_html=True)
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="auth-logo-box"><h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3></div>', unsafe_allow_html=True)
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "ZVC01w4_pIquj0bMiaAu":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

if "arsenkin_token" in st.session_state:
    ARSENKIN_TOKEN = st.session_state.arsenkin_token
else:
    try: ARSENKIN_TOKEN = st.secrets["api"]["arsenkin_token"]
    except (FileNotFoundError, KeyError): ARSENKIN_TOKEN = None

if "yandex_dict_key" in st.session_state:
    YANDEX_DICT_KEY = st.session_state.yandex_dict_key
else:
    try: YANDEX_DICT_KEY = st.secrets["api"]["yandex_dict_key"]
    except (FileNotFoundError, KeyError): YANDEX_DICT_KEY = None

REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956},
    "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931},
    "–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É": {"ya": 39, "go": 1012028},
    "–£—Ñ–∞": {"ya": 172, "go": 1012091},
    "–ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫": {"ya": 62, "go": 1012001},
    "–í–æ—Ä–æ–Ω–µ–∂": {"ya": 193, "go": 1012134},
    "–ü–µ—Ä–º—å": {"ya": 50, "go": 1012015},
    "–í–æ–ª–≥–æ–≥—Ä–∞–¥": {"ya": 38, "go": 1012131},
    "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–°–∞—Ä–∞—Ç–æ–≤": {"ya": 194, "go": 1012046},
    "–¢—é–º–µ–Ω—å": {"ya": 283, "go": 1012089},
    "–¢–æ–ª—å—è—Ç—Ç–∏": {"ya": 240, "go": 1012080},
    "–ò–∂–µ–≤—Å–∫": {"ya": 44, "go": 1011979},
    "–ë–∞—Ä–Ω–∞—É–ª": {"ya": 197, "go": 1011855},
    "–ò—Ä–∫—É—Ç—Å–∫": {"ya": 63, "go": 1011977},
    "–£–ª—å—è–Ω–æ–≤—Å–∫": {"ya": 195, "go": 1012092},
    "–•–∞–±–∞—Ä–æ–≤—Å–∫": {"ya": 76, "go": 1011973},
    "–í–ª–∞–¥–∏–≤–æ—Å—Ç–æ–∫": {"ya": 75, "go": 1012129},
    "–Ø—Ä–æ—Å–ª–∞–≤–ª—å": {"ya": 16, "go": 1012140},
    "–ú–∞—Ö–∞—á–∫–∞–ª–∞": {"ya": 28, "go": 1011993},
    "–¢–æ–º—Å–∫": {"ya": 67, "go": 1012082},
    "–û—Ä–µ–Ω–±—É—Ä–≥": {"ya": 48, "go": 1012009},
    "–ö–µ–º–µ—Ä–æ–≤–æ": {"ya": 64, "go": 1011985},
    "–ù–æ–≤–æ–∫—É–∑–Ω–µ—Ü–∫": {"ya": 237, "go": 1011987},
    "–†—è–∑–∞–Ω—å": {"ya": 11, "go": 1012033},
    "–ù–∞–±–µ—Ä–µ–∂–Ω—ã–µ –ß–µ–ª–Ω—ã": {"ya": 234, "go": 1011905},
    "–ü–µ–Ω–∑–∞": {"ya": 49, "go": 1012013},
    "–õ–∏–ø–µ—Ü–∫": {"ya": 9, "go": 1011991},
    "–¢—É–ª–∞": {"ya": 15, "go": 1012085},
    "–ö–∏—Ä–æ–≤": {"ya": 46, "go": 1011989},
    "–ß–µ–±–æ–∫—Å–∞—Ä—ã": {"ya": 45, "go": 1011880},
    "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥": {"ya": 22, "go": 1011981},
    "–ö—É—Ä—Å–∫": {"ya": 8, "go": 1011988},
    "–£–ª–∞–Ω-–£–¥—ç": {"ya": 68, "go": 1012090},
    "–°—Ç–∞–≤—Ä–æ–ø–æ–ª—å": {"ya": 36, "go": 1012070},
    "–°–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å": {"ya": 959, "go": 1012048},
    "–°–æ—á–∏": {"ya": 239, "go": 1012053},
    "–†–æ—Å—Å–∏—è": {"ya": 225, "go": 2643},
    "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493},
    "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601},
    "–ê—Å—Ç–∞–Ω–∞ (KZ)": {"ya": 163, "go": 1014620}
}

DEFAULT_EXCLUDE_DOMAINS = {
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "aliexpress.ru", 
    "ebay.com", "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", 
    "pandao.ru", "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", 
    "banki.ru", "regmarkets.ru", "zoon.ru", "pulscen.ru", "prodoctorov.ru", 
    "blizko.ru", "domclick.ru", "satom.ru", "quto.ru", "edadeal.ru", 
    "cataloxy.ru", "irr.ru", "onliner.by", "shop.by", "deal.by", "yell.ru", 
    "profi.ru", "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by", 
    "market.yandex.ru", "youtube.com", "www.youtube.com", "gosuslugi.ru", 
    "www.gosuslugi.ru", "dzen.ru", "2gis.by", "wildberries.ru", "rutube.ru", 
    "vk.com", "facebook.com", "chipdip.ru"
    }

DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n—Å—Ç—Ä\n—É–ª\n—à—Ç\n—Å–º\n–º–º\n–º–ª\n–∫–≥\n–∫–≤\n–º¬≤\n—Å–º¬≤\n–º2\n—Å–º2"

PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE"

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif; color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important; }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important; color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] div[role="gridcell"] {{
            background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; border-bottom: 1px solid {ROW_BORDER_COLOR} !important;
        }}
        .legend-box {{ padding: 10px; background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px; }}
        .text-red {{ color: #D32F2F; font-weight: bold; }}
        .text-green {{ color: #2E7D32; font-weight: bold; }}
        .text-bold {{ font-weight: 600; }}
        .sort-container {{ background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 8px; margin-bottom: 10px; border: 1px solid {BORDER_COLOR}; }}
        
        .stApp > header {{ background-color: transparent !important; }}
        .stTextInput input:disabled, .stTextArea textarea:disabled, .stSelectbox div[aria-disabled="true"] {{
            opacity: 1 !important; background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; cursor: text !important; -webkit-text-fill-color: {TEXT_COLOR} !important; border-color: {BORDER_COLOR} !important;
        }}
        .stButton button:disabled {{ opacity: 1 !important; background-color: {PRIMARY_COLOR} !important; color: white !important; cursor: progress !important; }}
        div[data-testid="stAppViewContainer"] {{ filter: none !important; opacity: 1 !important; transition: none !important; }}
        /* –°—Ç–∏–ª–∏ –¥–ª—è —Å—Å—ã–ª–æ–∫ –≤–Ω—É—Ç—Ä–∏ –≥—Ä–∞—Ñ–∏–∫–∞ Plotly */
        .chart-link {{
            color: #277EFF !important;
            font-weight: 600 !important;
            text-decoration: none !important;
            border-bottom: 4px solid #CBD5E1 !important; 
            display: inline-block !important;
            transition: border-color 0.2s ease !important;
        }}
        .chart-link:hover {{
            border-bottom-color: #277EFF !important;
            cursor: pointer !important;
        }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# PARSING & METRICS
# ==========================================
# ... (–û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ —Ñ—É–Ω–∫—Ü–∏–π –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)

def get_yandex_dict_info(text, api_key):
    if not api_key: return {'lemma': text, 'pos': 'unknown'}
    url = "https://dictionary.yandex.net/api/v1/dicservice.json/lookup"
    params = {'key': api_key, 'lang': 'ru-ru', 'text': text, 'ui': 'ru'}
    try:
        r = requests.get(url, params=params, timeout=2)
        if r.status_code == 200:
            data = r.json()
            if data.get('def'):
                first_def = data['def'][0]
                return {'lemma': first_def.get('text', text), 'pos': first_def.get('pos', 'unknown')}
    except: pass
    return {'lemma': text, 'pos': 'unknown'}

def get_arsenkin_urls(query, engine_type, region_name, api_token, depth_val=10):
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check"
    url_get = "https://arsenkin.ru/api/tools/get"
    headers = {"Authorization": f"Bearer {api_token}", "Content-type": "application/json"}
    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []
    if "–Ø–Ω–¥–µ–∫—Å" in engine_type: se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type: se_params.append({"type": 11, "region": reg_ids['go']})

    payload = {"tools_name": "check-top", "data": {"queries": [query], "is_snippet": False, "noreask": True, "se": se_params, "depth": depth_val}}
    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        resp_json = r.json()
        if "error" in resp_json or "task_id" not in resp_json: st.error(f"‚ùå –û—à–∏–±–∫–∞ API: {resp_json}"); return []
        task_id = resp_json["task_id"]
        st.toast(f"–ó–∞–¥–∞—á–∞ ID {task_id} –∑–∞–ø—É—â–µ–Ω–∞")
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}"); return []

    status = "process"
    attempts = 0
    # Timeout increased to 10 minutes (120 * 5s)
    while status == "process" and attempts < 120:
        time.sleep(5); attempts += 1
        try:
            r_check = requests.post(url_check, headers=headers, json={"task_id": task_id})
            res_check_data = r_check.json()
            if res_check_data.get("status") == "finish": status = "done"; break
        except: pass

    if status != "done": st.error(f"‚è≥ –¢–∞–π–º-–∞—É—Ç API"); return []

    try:
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id}, timeout=30)
        res_data = r_final.json()
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}"); return []

    results_list = []
    try:
        collect = res_data.get('result', {}).get('result', {}).get('collect')
        if not collect: return []
        final_url_list = []
        if isinstance(collect, list) and len(collect) > 0 and isinstance(collect[0], list): final_url_list = collect[0][0]
        else:
             unique_urls = set()
             for engine_data in collect:
                 if isinstance(engine_data, dict):
                     for _, serps in engine_data.items():
                         for item in serps:
                             if item.get('url') and item.get('url') not in unique_urls:
                                 results_list.append({'url': item['url'], 'pos': item['pos']})
                                 unique_urls.add(item['url'])
             return results_list

        if final_url_list:
            for index, url in enumerate(final_url_list): results_list.append({'url': url, 'pos': index + 1})
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}"); return []
    return results_list

def process_text_detailed(text, settings, n_gram=1):
    text = text.lower().replace('—ë', '–µ')
    words = re.findall(r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+', text)
    stops = set(w.lower().replace('—ë', '–µ') for w in settings['custom_stops'])
    lemmas = []
    forms_map = defaultdict(set)
    for w in words:
        if len(w) < 2: continue
        if not settings['numbers'] and w.isdigit(): continue
        if w in stops: continue
        lemma = w
        if USE_NLP and n_gram == 1:
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            lemma = p.normal_form.replace('—ë', '–µ')
        lemmas.append(lemma)
        forms_map[lemma].add(w)
    return lemmas, forms_map

def parse_page(url, settings, query_context=""):
    import streamlit as st
    try:
        from curl_cffi import requests as cffi_requests
        headers = {
            'User-Agent': settings['ua'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        }
        r = cffi_requests.get(url, headers=headers, timeout=20, impersonate="chrome110")
        if r.status_code == 403: raise Exception("CURL_CFFI –ø–æ–ª—É—á–∏–ª 403 Forbidden")
        if r.status_code != 200: return None
        content = r.content
        encoding = r.encoding if r.encoding else 'utf-8'
    except Exception:
        try:
            import requests
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            session = requests.Session()
            headers = {'User-Agent': settings['ua']}
            r = session.get(url, headers=headers, timeout=20, verify=False)
            if r.status_code != 200: return None
            content = r.content
            encoding = r.apparent_encoding
        except Exception: return None

    try:
        # 1. –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Soup (–ü–æ–ª–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞)
        soup = BeautifulSoup(content, 'html.parser', from_encoding=encoding)
        
        # === –ù–û–í–û–ï: –°–æ–±–∏—Ä–∞–µ–º Title –∏ Description –æ—Ç–¥–µ–ª—å–Ω–æ ===
        page_title = soup.title.string.strip() if soup.title and soup.title.string else ""
        
        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})
        page_desc = meta_desc_tag['content'].strip() if meta_desc_tag and meta_desc_tag.get('content') else ""
        # ====================================================

        # === –õ–û–ì–ò–ö–ê –¢–ê–ë–õ–ò–¶–´ 2 (–ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ URL/–°—Å—ã–ª–∫–µ) ===
        product_titles = []
        search_roots = set()
        if query_context:
            clean_q = query_context.lower().replace('–∫—É–ø–∏—Ç—å', '').replace('—Ü–µ–Ω–∞', '').replace(' –≤ ', ' ')
            words = re.findall(r'[–∞-—èa-z]+', clean_q)
            for w in words:
                if len(w) > 3: search_roots.add(w[:-1])
                else: search_roots.add(w)
        
        parsed_current = urlparse(url)
        current_path_clean = parsed_current.path.rstrip('/')
        seen_titles = set()
        
        for a in soup.find_all('a', href=True):
            txt = a.get_text(strip=True)
            raw_href = a['href']
            if len(txt) < 5 or len(txt) > 300: continue
            if raw_href.startswith('#') or raw_href.startswith('javascript'): continue
            
            abs_href = urljoin(url, raw_href)
            parsed_href = urlparse(abs_href)
            href_path_clean = parsed_href.path.rstrip('/')
            
            is_child_path = href_path_clean.startswith(current_path_clean)
            is_deeper = len(href_path_clean) > len(current_path_clean)
            is_not_query_param_only = (href_path_clean != current_path_clean)

            if is_child_path and is_deeper and is_not_query_param_only:
                txt_lower = txt.lower()
                href_lower = abs_href.lower()
                has_keywords = False
                if search_roots:
                    for root in search_roots:
                        if root in txt_lower or root in href_lower:
                            has_keywords = True; break
                else:
                    if re.search(r'\d', txt): has_keywords = True

                is_buy_button = txt_lower in {'–∫—É–ø–∏—Ç—å', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ', '–≤ –∫–æ—Ä–∑–∏–Ω—É', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Ü–µ–Ω–∞'}
                if has_keywords and not is_buy_button:
                    if txt not in seen_titles:
                        product_titles.append(txt)
                        seen_titles.add(txt)
        # ========================================================
        
        h1_tag = soup.find('h1')
        h1_text = h1_tag.get_text(strip=True) if h1_tag else ""

        # 2. –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –¢–∞–±–ª–∏—Ü—ã 2 (–£–¥–∞–ª—è–µ–º –±–ª–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤)
        soup_no_grid = BeautifulSoup(content, 'html.parser', from_encoding=encoding)
        grid_div = soup_no_grid.find('div', class_='an-container-fluid an-container-xl')
        if grid_div: grid_div.decompose()
        
        # === [–í–ê–ñ–ù–û] –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ö–û–ù–¢–ï–ù–¢–ê –ü–û –ì–ê–õ–û–ß–ö–ê–ú ===
        tags_to_remove = []
        if settings['noindex']: tags_to_remove.append('noindex')
        
        for s in [soup, soup_no_grid]:
            for c in s.find_all(string=lambda text: isinstance(text, Comment)): c.extract()
            if tags_to_remove:
                for t in s.find_all(tags_to_remove): t.decompose()
            for script in s(["script", "style", "svg", "path", "noscript"]): script.decompose()

        # –¢–µ–∫—Å—Ç —Å—Å—ã–ª–æ–∫ (–∞–Ω–∫–æ—Ä—ã)
        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)
        
        # –°–±–æ—Ä –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ì–û —Ç–µ–∫—Å—Ç–∞ (Description, Alt, Title)
        extra_text = []
        # Description –¥–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Ç–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–∂–µ
        if page_desc: extra_text.append(page_desc)

        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])

        # –°–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç
        body_text_raw = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text = re.sub(r'\s+', ' ', body_text_raw).strip()

        body_text_no_grid_raw = soup_no_grid.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text_no_grid = re.sub(r'\s+', ' ', body_text_no_grid_raw).strip()

        if not body_text: return None
            
        return {
            'url': url, 
            'domain': urlparse(url).netloc, 
            'body_text': body_text, 
            'body_text_no_grid': body_text_no_grid,
            'anchor_text': anchor_text,
            'h1': h1_text,
            'product_titles': product_titles,
            # !!! –ù–û–í–´–ï –ü–û–õ–Ø –î–õ–Ø DASHBOARD !!!
            'meta_title': page_title,
            'meta_desc': page_desc
        }
    except Exception:
        return None

def analyze_meta_gaps(comp_data_full, my_data, settings):
    """
    –£–ú–ù–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† META-–¢–ï–ì–û–í v2.1
    1. –£—á–∏—Ç—ã–≤–∞–µ—Ç –≤–µ—Å –ø–æ–∑–∏—Ü–∏–∏ (—Å–ª–æ–≤–∞ —Ç–æ–ø–æ–≤ –≤–∞–∂–Ω–µ–µ).
    2. –ü–æ—Ä–æ–≥ –≤—Ö–æ–∂–¥–µ–Ω–∏—è: –°–¢–†–û–ì–û 50% (—Å–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —É –ø–æ–ª–æ–≤–∏–Ω—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤).
    3. –§–∏–ª—å—Ç—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–≥–∏ –∏ —Å–æ—é–∑—ã.
    """
    if not comp_data_full: return None
    
    # === 1. –ù–ê–°–¢–†–û–ô–ö–ò –ê–õ–ì–û–†–ò–¢–ú–ê ===
    TOTAL_COMPS = len(comp_data_full)
    
    # !!! –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–¢–†–û–ì–û 50% !!!
    MIN_OCCURRENCE_PCT = 0.4 
    
    # –ú–∏–Ω–∏–º—É–º 2 —Å–∞–π—Ç–∞, –¥–∞–∂–µ –µ—Å–ª–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤—Å–µ–≥–æ 3
    MIN_COUNT = max(2, int(TOTAL_COMPS * MIN_OCCURRENCE_PCT))

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (–ß–∏—Å—Ç–∫–∞ –º—É—Å–æ—Ä–∞)
    def fast_tokenize(text):
        if not text: return set()
        
        # –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
        stop_garbage = {
            '–≤', '–Ω–∞', '–∏', '—Å', '—Å–æ', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '—É', 
            '–æ', '–æ–±', '–∑–∞', '–Ω–∞–¥', '–ø–æ–¥', '–ø—Ä–∏', '–ø—Ä–æ', '–±–µ–∑', '—á–µ—Ä–µ–∑', '–º–µ–∂–¥—É',
            '–∞', '–Ω–æ', '–∏–ª–∏', '–¥–∞', '–∫–∞–∫', '—á—Ç–æ', '—á—Ç–æ–±—ã', '–µ—Å–ª–∏', '—Ç–æ', '–ª–∏', '–±—ã', '–∂–µ', 
            '–≥', '–æ–±–ª', '—Ä', '—Ä—É–±', '–º–º', '—Å–º', '–º', '–∫–≥', '—Ç', '—à—Ç', '–¥–Ω',
            '–≤–µ—Å—å', '–≤—Å–µ', '–≤—Å—ë', '—Å–≤–æ–π', '–≤–∞—à', '–Ω–∞—à', '–º—ã', '–≤—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏',
            '–∫—É–ø–∏—Ç—å', '—Ü–µ–Ω–∞', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø—Ä–æ–¥–∞–∂–∞', '–Ω–µ–¥–æ—Ä–æ–≥–æ', 
            '–º–æ—Å–∫–≤–∞', '—Å–ø–±' 
        }
        # –£–±–∏—Ä–∞–µ–º –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —à—Ç–∞–º–ø—ã –∏–∑ —Å—Ç–æ–ø-–ª–∏—Å—Ç–∞, —á—Ç–æ–±—ã –æ–Ω–∏ –ø–æ–ø–∞–¥–∞–ª–∏ –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if '–∫—É–ø–∏—Ç—å' in stop_garbage: stop_garbage.remove('–∫—É–ø–∏—Ç—å') 
        if '—Ü–µ–Ω–∞' in stop_garbage: stop_garbage.remove('—Ü–µ–Ω–∞')
        
        if settings.get('custom_stops'):
            stop_garbage.update(set(settings['custom_stops']))

        lemmas = set()
        words = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9]+', text.lower())
        
        for w in words:
            if len(w) < 2: continue 
            if w in stop_garbage: continue
            
            # NLP –§–∏–ª—å—Ç—Ä
            if morph:
                try:
                    p = morph.parse(w)[0]
                    # –ò—Å–∫–ª—é—á–∞–µ–º –ü—Ä–µ–¥–ª–æ–≥–∏, –°–æ—é–∑—ã, –ß–∞—Å—Ç–∏—Ü—ã, –ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è, –ú–µ–∂–¥–æ–º–µ—Ç–∏—è
                    if p.tag.POS in {'PREP', 'CONJ', 'PRCL', 'NPRO', 'INTJ'}:
                        continue
                    if p.normal_form in stop_garbage:
                        continue
                    lemmas.add(p.normal_form)
                except: 
                    lemmas.add(w)
            else:
                lemmas.add(w)
        return lemmas

    # === 2. –°–ë–û–† –î–ê–ù–ù–´–• –° –í–ï–°–ê–ú–ò ===
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞: word -> {'count': 0, 'score': 0.0}
    stats_map = {
        'title': defaultdict(lambda: {'count': 0, 'score': 0.0}),
        'desc': defaultdict(lambda: {'count': 0, 'score': 0.0}),
        'h1': defaultdict(lambda: {'count': 0, 'score': 0.0})
    }
    
    detailed_rows = []

    for i, item in enumerate(comp_data_full):
        # –í–µ—Å –ø–æ–∑–∏—Ü–∏–∏: 1-–µ –º–µ—Å—Ç–æ = –≤–µ—Å–æ–º–µ–µ, —á–µ–º 10-–µ
        rank_weight = 1.0 + ( (TOTAL_COMPS - i) / TOTAL_COMPS ) * 1.5
        
        t_tok = fast_tokenize(item.get('meta_title', ''))
        d_tok = fast_tokenize(item.get('meta_desc', ''))
        h_tok = fast_tokenize(item.get('h1', ''))
        
        for w in t_tok:
            stats_map['title'][w]['count'] += 1
            stats_map['title'][w]['score'] += rank_weight
            
        for w in d_tok:
            stats_map['desc'][w]['count'] += 1
            stats_map['desc'][w]['score'] += rank_weight
            
        for w in h_tok:
            stats_map['h1'][w]['count'] += 1
            stats_map['h1'][w]['score'] += rank_weight

        detailed_rows.append({
            'URL': item['url'],
            'Title': item.get('meta_title', ''),
            'Description': item.get('meta_desc', ''),
            'H1': item.get('h1', '')
        })

    # === 3. –ê–ù–ê–õ–ò–ó –†–ê–ó–†–´–í–û–í (GAPS) ===
    
    my_tokens = {
        'title': fast_tokenize(my_data.get('meta_title', '')),
        'desc': fast_tokenize(my_data.get('meta_desc', '')),
        'h1': fast_tokenize(my_data.get('h1', ''))
    }

    def process_category(cat_key):
        data = stats_map[cat_key]
        important_words = []
        
        for word, metrics in data.items():
            # 1. –û—Ç—Å–µ–∫–∞–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ä–µ–∂–µ, —á–µ–º —É 50% –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
            if metrics['count'] < MIN_COUNT:
                continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–æ –∏ –µ–≥–æ "–≤–∞–∂–Ω–æ—Å—Ç—å" (Score)
            important_words.append((word, metrics['score']))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (Score)
        important_words.sort(key=lambda x: x[1], reverse=True)
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —è–¥—Ä–æ (–¢–æ–ø-15 —Å–ª–æ–≤, –ø—Ä–æ—à–µ–¥—à–∏—Ö —Ñ–∏–ª—å—Ç—Ä 50%)
        core_semantics = [x[0] for x in important_words[:15]]
        
        if not core_semantics:
            return 100, [] 
            
        matches = 0
        missing = []
        
        for w in core_semantics:
            if w in my_tokens[cat_key]:
                matches += 1
            else:
                missing.append(w)
        
        if len(core_semantics) > 0:
            score = int((matches / len(core_semantics)) * 100)
        else:
            score = 100
            
        return score, missing

    s_t, m_t = process_category('title')
    s_d, m_d = process_category('desc')
    s_h, m_h = process_category('h1')

    return {
        'scores': {'title': s_t, 'desc': s_d, 'h1': s_h},
        'missing': {'title': m_t, 'desc': m_d, 'h1': m_h},
        'detailed': detailed_rows,
        'my_data': {
            'Title': my_data.get('meta_title', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'),
            'Description': my_data.get('meta_desc', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'),
            'H1': my_data.get('h1', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω')
        }
    }
        
def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    # –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –æ–∫—Ä—É–≥–ª–µ–Ω–∏—è
    def math_round(number):
        return int(number + 0.5)

    all_forms_map = defaultdict(set)
    global_forms_counter = defaultdict(Counter) 
    
    # 1. –û–ë–†–ê–ë–û–¢–ö–ê –ú–û–ï–ì–û –°–ê–ô–¢–ê (–¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ª–∏—á–Ω—ã—Ö –±–∞–ª–ª–æ–≤)
    if not my_data or not my_data.get('body_text'): 
        my_lemmas, my_forms, my_anchors, my_len = [], {}, [], 0
        my_clean_domain = "local"
    else:
        my_lemmas, my_forms = process_text_detailed(my_data['body_text'], settings)
        my_anchors, _ = process_text_detailed(my_data['anchor_text'], settings)
        my_len = len(my_lemmas)
        for k, v in my_forms.items(): all_forms_map[k].update(v)
        my_clean_domain = my_data['domain'].lower().replace('www.', '').split(':')[0]

    # 2. –û–ë–†–ê–ë–û–¢–ö–ê –ö–û–ù–ö–£–†–ï–ù–¢–û–í (–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑—ã –¥–ª—è –ú–µ–¥–∏–∞–Ω—ã)
    comp_docs = []
    for p in comp_data_full:
        if not p.get('body_text'): continue
        
        # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –£–ë–†–ê–ù–ê –ü–†–û–í–ï–†–ö–ê "–ï–°–õ–ò –≠–¢–û –ú–û–ô –°–ê–ô–¢ - –ü–†–û–ü–£–°–¢–ò–¢–¨" ---
        # –¢–µ–ø–µ—Ä—å, –µ—Å–ª–∏ –≤–∞—à —Å–∞–π—Ç –µ—Å—Ç—å –≤ –≤—ã–¥–∞—á–µ (–¢–æ–ø-10/20), –æ–Ω –ø–æ–ø–∞–¥–µ—Ç –≤ comp_docs 
        # –∏ –±—É–¥–µ—Ç –≤–ª–∏—è—Ç—å –Ω–∞ –º–µ–¥–∏–∞–Ω—É, –∫–∞–∫ –≤—ã –∏ –ø—Ä–æ—Å–∏–ª–∏.
        
        body, c_forms = process_text_detailed(p['body_text'], settings)
        
        # –°–±–æ—Ä —Ä–µ–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        raw_words_for_stats = re.findall(r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+', p['body_text'].lower())
        for rw in raw_words_for_stats:
            if len(rw) < 2: continue
            if morph:
                parsed = morph.parse(rw)[0]
                if 'PREP' not in parsed.tag and 'CONJ' not in parsed.tag:
                    rw_lemma = parsed.normal_form.replace('—ë', '–µ')
                    global_forms_counter[rw_lemma][rw] += 1
        
        anchor, _ = process_text_detailed(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor, 'url': p['url'], 'domain': p['domain']})
        for k, v in c_forms.items(): all_forms_map[k].update(v)

    if not comp_docs:
        return { "depth": pd.DataFrame(), "hybrid": pd.DataFrame(), "relevance_top": pd.DataFrame(), "my_score": {"width": 0, "depth": 0}, "missing_semantics_high": [], "missing_semantics_low": [] }

    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–æ—Ä–ø—É—Å–∞
    c_lens = [len(d['body']) for d in comp_docs]
    avg_dl = np.mean(c_lens) if c_lens else 1
    N = len(comp_docs)
    
    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
    
    word_counts_per_doc = [Counter(d['body']) for d in comp_docs]

    # IDF
    word_idf_map = {}
    for lemma in vocab:
        df = doc_freqs[lemma]
        if df == 0: continue
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        word_idf_map[lemma] = max(idf, 0.01)

    # =========================================================================
    # 3. –†–ê–°–ß–ï–¢ –ú–ï–î–ò–ê–ù
    # =========================================================================
    
    table_depth = []
    table_hybrid = []
    missing_semantics_high = []
    missing_semantics_low = []
    
    words_with_median_gt_0 = set() 
    my_found_words_from_median = set() 
    
    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        
        # 1. –°–û–ë–ò–†–ê–ï–ú –°–´–†–´–ï –î–ê–ù–ù–´–ï
        raw_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        sorted_raw = sorted(raw_counts)
        
        if sorted_raw:
            rec_median_absolute = math_round(np.median(sorted_raw))
            obs_min = math_round(sorted_raw[0])
            obs_max = math_round(sorted_raw[-1])
        else:
            rec_median_absolute = 0; obs_min = 0; obs_max = 0

        # 2. –°–ß–ò–¢–ê–ï–ú –¶–ï–õ–ï–í–£–Æ –ú–ï–î–ò–ê–ù–£
        if settings['norm'] and my_len > 0:
            norm_counts = []
            for i in range(N):
                raw_cnt = raw_counts[i]
                comp_len = c_lens[i]
                if comp_len > 0:
                    normalized_val = raw_cnt * (my_len / comp_len)
                    norm_counts.append(normalized_val)
                else:
                    norm_counts.append(0)
            
            if norm_counts:
                rec_median_target = math_round(np.median(sorted(norm_counts)))
            else:
                rec_median_target = 0
        else:
            rec_median_target = rec_median_absolute

        # --- –ú–û–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ---
        my_tf_count = my_lemmas.count(lemma)
        
        if obs_max == 0 and my_tf_count == 0: continue

        # --- –®–ò–†–ò–ù–ê ---
        if rec_median_target >= 1:
            words_with_median_gt_0.add(lemma)
            if my_tf_count > 0:
                my_found_words_from_median.add(lemma)

        # --- –£–ü–£–©–ï–ù–ù–ê–Ø –°–ï–ú–ê–ù–¢–ò–ö–ê ---
        display_word = lemma
        if global_forms_counter[lemma]:
            display_word = global_forms_counter[lemma].most_common(1)[0][0]

        if my_tf_count == 0:
            weight = word_idf_map.get(lemma, 0) * (rec_median_target if rec_median_target > 0 else 1)
            item = {'word': display_word, 'weight': weight}
            if rec_median_target >= 1: missing_semantics_high.append(item)
            else: missing_semantics_low.append(item)

        # --- –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò ---
        diff = rec_median_target - my_tf_count
        
        if diff == 0: status = "–ù–æ—Ä–º–∞"; action_text = "‚úÖ"; sort_val = 0
        elif diff > 0: status = "–ù–µ–¥–æ—Å–ø–∞–º"; action_text = f"+{diff}"; sort_val = diff
        else: status = "–ü–µ—Ä–µ—Å–ø–∞–º"; action_text = f"{diff}"; sort_val = abs(diff)

        forms_str = ", ".join(sorted(list(all_forms_map.get(lemma, set())))) if all_forms_map.get(lemma) else lemma

        table_depth.append({
            "–°–ª–æ–≤–æ": display_word,
            "–°–ª–æ–≤–æ—Ñ–æ—Ä–º—ã": forms_str,
            "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_tf_count,
            "–ú–µ–¥–∏–∞–Ω–∞": rec_median_absolute, 
            "–ú–∏–Ω–∏–º—É–º (–∫–æ–Ω–∫—É—Ä.)": obs_min,
            "–ú–∞–∫—Å–∏–º—É–º (–∫–æ–Ω–∫—É—Ä.)": obs_max,
            "–°—Ç–∞—Ç—É—Å": status,
            "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": action_text,
            "is_missing": (my_tf_count == 0),
            "sort_val": sort_val
        })
        
        table_hybrid.append({
            "–°–ª–æ–≤–æ": display_word,
            "TF-IDF –¢–û–ü": round(word_idf_map.get(lemma, 0) * (rec_median_absolute / avg_dl if avg_dl > 0 else 0), 4),
            "TF-IDF —É –≤–∞—Å": round(word_idf_map.get(lemma, 0) * (my_tf_count / my_len if my_len > 0 else 0), 4),
            "–°–∞–π—Ç–æ–≤": doc_freqs[lemma],
            "–ü–µ—Ä–µ—Å–ø–∞–º": obs_max
        })

    # =========================================================================
    # 4. –§–ò–ù–ê–õ–ò–ó–ê–¶–ò–Ø (–ë–ê–õ–õ–´)
    # =========================================================================
    
    total_needed = len(words_with_median_gt_0)
    total_found = len(my_found_words_from_median)
    
    if total_needed > 0:
        ratio = total_found / total_needed
        my_width_score_final = int(min(100, ratio * 120))
    else:
        my_width_score_final = 0

    S_WIDTH_CORE = words_with_median_gt_0 
    
    def calculate_raw_power(doc_tokens, doc_len):
        if avg_dl == 0 or doc_len == 0: return 0
        score = 0
        counts = Counter(doc_tokens)
        k1 = 1.2; b = 0.75
        for word in S_WIDTH_CORE:
            if word not in counts: continue
            tf = counts[word]
            idf = word_idf_map.get(word, 0)
            term_weight = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avg_dl)))
            score += term_weight
        return score

    comp_raw_scores = []
    competitor_scores_map = {}
    
    for i, doc in enumerate(comp_docs):
        c_found = len(set(doc['body']).intersection(S_WIDTH_CORE))
        if total_needed > 0:
            c_width_val = int(min(100, (c_found / total_needed) * 120))
        else:
            c_width_val = 0
            
        raw_val = calculate_raw_power(doc['body'], c_lens[i])
        comp_raw_scores.append(raw_val)
        competitor_scores_map[doc['url']] = {'width_final': c_width_val, 'raw_depth': raw_val}

    if comp_raw_scores:
        median_raw = np.median(comp_raw_scores)
        ref_val = median_raw if median_raw > 0.1 else 1.0
    else:
        ref_val = 1.0
    
    k_norm = 80.0 / ref_val
    my_raw_bm25 = calculate_raw_power(my_lemmas, my_len)
    my_depth_score_final = int(round(min(100, my_raw_bm25 * k_norm)))

    # –î–æ—Å—á–∏—Ç—ã–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É –≤—Å–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º
    for url, data in competitor_scores_map.items():
        data['depth_final'] = int(round(min(100, data['raw_depth'] * k_norm)))

    # –°–±–æ—Ä–∫–∞ —Ç–∞–±–ª–∏—Ü—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    missing_semantics_high.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low = missing_semantics_low[:500]

    table_rel = []
    my_site_found_in_selection = False
    
    # 1. –ü—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ –ò–°–•–û–î–ù–û–ú–£ —Å–ø–∏—Å–∫—É –∏–∑ –≤—ã–¥–∞—á–∏ (Top N)
    for item in original_results:
        url = item['url']
        if url not in competitor_scores_map: continue
        
        row_domain = urlparse(url).netloc.lower().replace('www.', '')
        is_my_site = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞—à –ª–∏ —ç—Ç–æ —Å–∞–π—Ç
        if my_clean_domain and my_clean_domain != "local" and my_clean_domain in row_domain:
            is_my_site = True
            my_site_found_in_selection = True
            display_name = f"{urlparse(url).netloc} (–í—ã)"
        else:
            display_name = urlparse(url).netloc
            
        scores = competitor_scores_map[url]
        table_rel.append({ 
            "–î–æ–º–µ–Ω": display_name, 
            "URL": url,
            "–ü–æ–∑–∏—Ü–∏—è": item['pos'], 
            "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": scores['width_final'], 
            "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": scores['depth_final'] 
        })
        
    # 2. –ï—Å–ª–∏ –Ω–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ –ù–ï –±—ã–ª–æ –≤ –¢–æ–ø N, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤—Ä—É—á–Ω—É—é –≤ –∫–æ–Ω–µ—Ü
    if not my_site_found_in_selection:
        pos_to_show = my_serp_pos if my_serp_pos > 0 else 0
        my_label = f"{my_data['domain']} (–í—ã)" if (my_data and my_data.get('domain')) else "–í–∞—à —Å–∞–π—Ç"
        my_full_url = my_data['url'] if (my_data and 'url' in my_data) else "#"
        table_rel.append({ 
            "–î–æ–º–µ–Ω": my_label, 
            "URL": my_full_url, 
            "–ü–æ–∑–∏—Ü–∏—è": pos_to_show, 
            "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": my_width_score_final, 
            "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": my_depth_score_final 
        })

    # –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π
    df_rel_for_analysis = pd.DataFrame(table_rel)
    good_urls, bad_urls_dicts, trend_info = analyze_serp_anomalies(df_rel_for_analysis)
    
    st.session_state['detected_anomalies'] = bad_urls_dicts
    st.session_state['serp_trend_info'] = trend_info
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º "—á–∏—Å—Ç—ã–µ" —É—Ä–ª—ã (–¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞)

    return { 
        "depth": pd.DataFrame(table_depth), 
        "hybrid": pd.DataFrame(table_hybrid), 
        "relevance_top": pd.DataFrame(table_rel).sort_values(by='–ü–æ–∑–∏—Ü–∏—è', ascending=True).reset_index(drop=True), 
        "my_score": {"width": my_width_score_final, "depth": my_depth_score_final}, 
        "missing_semantics_high": missing_semantics_high, 
        "missing_semantics_low": missing_semantics_low 
    }
    
def get_hybrid_word_type(word, main_marker_root, specs_dict=None):
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä 3.1 (–§–∏–∫—Å –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤).
    """
    w = word.lower()
    specs_dict = specs_dict or set()
    
    # 1. –ú–ê–†–ö–ï–†
    if w == main_marker_root: return "1. üíé –ú–∞—Ä–∫–µ—Ä (–¢–æ–≤–∞—Ä)"
    if morph:
        norm = morph.parse(w)[0].normal_form
        if norm == main_marker_root: return "1. üíé –ú–∞—Ä–∫–µ—Ä (–¢–æ–≤–∞—Ä)"

    # 2. –°–¢–ê–ù–î–ê–†–¢–´
    if re.search(r'(gost|din|iso|en|tu|astm|aisi|–≥–æ—Å—Ç|–æ—Å—Ç|—Ç—É|–¥–∏–Ω)', w):
        return "6. üìú –°—Ç–∞–Ω–¥–∞—Ä—Ç"

    # 3. –†–ê–ó–ú–ï–†–´ / –¢–ï–•. –ü–ê–†–ê–ú–ï–¢–†–´
    # –ê. –ì–æ–ª—ã–µ —Ü–∏—Ñ—Ä—ã (10, 50.5)
    if re.fullmatch(r'\d+([.,]\d+)?', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    # –ë. –†–∞–∑–º–µ—Ä—ã —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ (10—Ö20, 10*20, 10-20, 10/20) <--- –î–û–ë–ê–í–ò–õ –¢–ò–†–ï –ò –°–õ–ï–®
    if re.search(r'^\d+[x—Ö*\-/]\d+', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    # –í. –ï–¥–∏–Ω–∏—Ü—ã (–º–º, –∫–≥)
    if re.search(r'\d+(–º–º|mm|–º|m|kg|–∫–≥|bar|–±–∞—Ä|–∞—Ç–º)$', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    # –ì. –ü—Ä–µ—Ñ–∏–∫—Å—ã (–î—É, –†—É, SDR)
    if re.match(r'^(d|dn|pn|sn|sdr|–¥—É|—Ä—É|√∏)\d+', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"

    # 4. –ú–ê–†–ö–ò / –°–ü–õ–ê–í–´
    if w in specs_dict: return "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –º–∞—Ä–æ–∫ (–ë—É–∫–≤—ã+–¶–∏—Ñ—Ä—ã)
    if re.search(r'\d', w): return "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"

    # 5. –õ–ê–¢–ò–ù–ò–¶–ê (–ë—Ä–µ–Ω–¥—ã)
    if re.search(r'^[a-z\-]+$', w): return "7. üî† –õ–∞—Ç–∏–Ω–∏—Ü–∞/–ë—Ä–µ–Ω–¥"

    # 6. –¢–ï–ö–°–¢
    if morph:
        p = morph.parse(w)[0]
        tag = p.tag
        if {'PREP'} in tag or {'CONJ'} in tag: return "SKIP"
        if {'ADJF'} in tag or {'PRTF'} in tag or {'ADJS'} in tag: return "2. üé® –°–≤–æ–π—Å—Ç–≤–∞"
        if {'NOUN'} in tag: return "4. üîó –î–æ–ø–æ–ª–Ω–µ–Ω–∏—è"

    if w.endswith(('–∏–π', '—ã–π', '–∞—è', '–æ–µ', '—ã–µ', '–∞—è')): return "2. üé® –°–≤–æ–π—Å—Ç–≤–∞"
    return "4. üîó –î–æ–ø–æ–ª–Ω–µ–Ω–∏—è"
    
def calculate_naming_metrics(comp_data_full, my_data, settings):
    """
    –¢–∞–±–ª–∏—Ü–∞ 2. –ë–µ–∑ "–æ–±—Ä–µ–∑–∞–Ω–∏—è" —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤.
    """
    # –ü–æ–¥–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è
    SPECS_SET = st.session_state.get('categorized_dimensions', set())
    if not SPECS_SET: _, _, SPECS_SET, _, _, _ = load_lemmatized_dictionaries()

    # 1. –ú–æ–π —Å–∞–π—Ç
    my_tokens = []
    if my_data and my_data.get('body_text_no_grid'):
        # –°–≤–æ—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –î—É50
        raw_w = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', my_data['body_text_no_grid'].lower())
        for w in raw_w:
            # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–ª–æ–≤–∞
            if not re.search(r'\d', w) and morph:
                my_tokens.append(morph.parse(w)[0].normal_form)
            else:
                my_tokens.append(w)

    # 2. –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã
    all_words_flat = []
    site_vocab_map = []
    
    for p in comp_data_full:
        titles = p.get('product_titles', [])
        valid_titles = [t for t in titles if 5 < len(t) < 150]
        
        if not valid_titles:
            site_vocab_map.append(set())
            continue
            
        curr_site_tokens = set()
        for t in valid_titles:
            words = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower())
            for w in words:
                if len(w) < 2: continue
                
                # –õ–û–ì–ò–ö–ê –°–û–•–†–ê–ù–ï–ù–ò–Ø –§–û–†–ú–´:
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Ü–∏—Ñ—Ä–∞ -> —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å (d50 -> d50)
                if re.search(r'\d', w):
                    token = w
                elif re.search(r'^[a-z]+$', w): # –õ–∞—Ç–∏–Ω–∏—Ü–∞ -> –∫–∞–∫ –µ—Å—Ç—å
                    token = w
                elif morph: # –†—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ -> –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º (—Å—Ç–∞–ª—å–Ω–∞—è -> —Å—Ç–∞–ª—å–Ω–æ–π)
                    token = morph.parse(w)[0].normal_form
                else:
                    token = w
                
                all_words_flat.append(token)
                curr_site_tokens.add(token)
                
        site_vocab_map.append(curr_site_tokens)

    if not all_words_flat: return pd.DataFrame()
    N_sites = len(site_vocab_map)

    # 3. –ú–∞—Ä–∫–µ—Ä (–°–∞–º–æ–µ —á–∞—Å—Ç–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–ª–æ–≤–æ)
    counts = Counter([w for w in all_words_flat if not re.search(r'\d', w)])
    main_marker_root = ""
    # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
    for w, c in counts.most_common(10):
        if morph and 'NOUN' in morph.parse(w)[0].tag: main_marker_root = w; break
    if not main_marker_root and counts: main_marker_root = counts.most_common(1)[0][0]

    # 4. –°–±–æ—Ä —Ç–∞–±–ª–∏—Ü—ã
    vocab = sorted(list(set(all_words_flat)))
    table_rows = []
    
    for token in vocab:
        if token in GARBAGE_LATIN_STOPLIST: continue
        
        # –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å
        sites_with_word = sum(1 for s_set in site_vocab_map if token in s_set)
        freq_percent = int((sites_with_word / N_sites) * 100)
        
        # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø
        cat = get_hybrid_word_type(token, main_marker_root, SPECS_SET)
        
        if cat == "SKIP": continue
        
        # –§–∏–ª—å—Ç—Ä—ã –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        # –ú–∞—Ä–∫–∏ –∏ –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç 5%
        is_spec = "–ú–∞—Ä–∫–∞" in cat or "–°—Ç–∞–Ω–¥–∞—Ä—Ç" in cat
        if is_spec and freq_percent < 5: continue
        
        # –û–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞ –æ—Ç 15%
        if not is_spec and "–†–∞–∑–º–µ—Ä—ã" not in cat and freq_percent < 15: continue
        
        # –†–∞–∑–º–µ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ —Ä–µ–∞–ª—å–Ω–æ —á–∞—Å—Ç—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ö–æ–¥–æ–≤–æ–π –¥–∏–∞–º–µ—Ç—Ä)
        # –ò–Ω–∞—á–µ —Ç–∞–±–ª–∏—Ü–∞ –±—É–¥–µ—Ç –∑–∞–±–∏—Ç–∞ —Ü–∏—Ñ—Ä–∞–º–∏ 10, 11, 12...
        if "–†–∞–∑–º–µ—Ä—ã" in cat and freq_percent < 15: continue

        rec_median = 1 if freq_percent > 30 else 0
        my_tf = my_tokens.count(token)
        diff = rec_median - my_tf
        action_text = f"+{diff}" if diff > 0 else ("‚úÖ" if diff == 0 else f"{diff}")
        
        table_rows.append({
            "–¢–∏–ø —Ö–∞—Ä-–∫–∏": cat[3:],
            "–°–ª–æ–≤–æ": token, # –í—ã–≤–æ–¥–∏–º —Ç–æ–∫–µ–Ω –∫–∞–∫ –µ—Å—Ç—å (—Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏ –±—É–∫–≤–∞–º–∏)
            "–ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å (%)": f"{freq_percent}%",
            "–£ –í–∞—Å": my_tf,
            "–ú–µ–¥–∏–∞–Ω–∞": rec_median,
            "–î–æ–±–∞–≤–∏—Ç—å": action_text,
            "raw_freq": freq_percent,
            "cat_sort": int(cat[0])
        })
        
    df = pd.DataFrame(table_rows)
    if not df.empty:
        df = df.sort_values(by=["cat_sort", "raw_freq"], ascending=[True, False])
        
    return df

def analyze_ideal_name(comp_data_full):
    """
    –°—Ç—Ä–æ–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å —É—á–µ—Ç–æ–º –ú–∞—Ä–æ–∫ –∏ –ì–û–°–¢–æ–≤.
    """
    # –ü–æ–¥–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
    SPECS_SET = st.session_state.get('categorized_dimensions', set())
    if not SPECS_SET: _, _, SPECS_SET, _, _, _ = load_lemmatized_dictionaries()

    titles = []
    for d in comp_data_full:
        ts = d.get('product_titles', [])
        titles.extend([t for t in ts if 5 < len(t) < 150])
    
    if not titles: return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö", []

    # –ú–∞—Ä–∫–µ—Ä
    all_w = []
    for t in titles: all_w.extend(re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower()))
    c = Counter(all_w)
    main_marker_root = ""
    for w, _ in c.most_common(5):
        if not re.search(r'\d', w):
             if morph and 'NOUN' in morph.parse(w)[0].tag: main_marker_root = w; break
             elif not morph: main_marker_root = w; break
    if not main_marker_root and c: main_marker_root = c.most_common(1)[0][0]

    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    structure_counter = Counter()
    vocab_by_type = defaultdict(Counter)
    
    sample = titles[:500]
    
    for t in sample:
        words = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower())
        pattern = []
        
        for w in words:
            if len(w) < 2: continue
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º —Å–ª–æ–≤–∞—Ä—è
            cat_full = get_hybrid_word_type(w, main_marker_root, SPECS_SET)
            if cat_full == "SKIP": continue
            
            # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∏–º—è —Ç–∏–ø–∞ ("–°–≤–æ–π—Å—Ç–≤–∞", "–ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤", "–°—Ç–∞–Ω–¥–∞—Ä—Ç")
            # "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤" -> "–ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"
            try:
                cat_short = cat_full.split('.', 1)[1].strip().split(' ', 1)[1] # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –∏–∫–æ–Ω–∫–∏
            except:
                cat_short = cat_full # Fallback
            
            vocab_by_type[cat_short][w] += 1
            
            if not pattern or pattern[-1] != cat_short:
                pattern.append(cat_short)
        
        if pattern:
            structure_str = " + ".join(pattern)
            structure_counter[structure_str] += 1
            
    # –°–±–æ—Ä–∫–∞
    if not structure_counter: return "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞", []
    
    best_struct_str, _ = structure_counter.most_common(1)[0]
    best_struct_list = best_struct_str.split(" + ")
    
    final_parts = []
    used_words = set()
    
    for block in best_struct_list:
        # –î–ª—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç–∞–≤–∏–º –∑–∞–≥–ª—É—à–∫—É
        if "–†–∞–∑–º–µ—Ä—ã" in block or "–°—Ç–∞–Ω–¥–∞—Ä—Ç" in block or "–ú–∞—Ä–∫–∞" in block:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –æ–Ω –æ—á–µ–Ω—å –ø–æ–ø—É–ª—è—Ä–µ–Ω
            top_cand = vocab_by_type[block].most_common(1)
            if top_cand and top_cand[0][1] > (len(sample) * 0.3): # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —É 30%
                 final_parts.append(top_cand[0][0])
            else:
                 final_parts.append(f"[{block.upper()}]")
            continue
            
        # –î–ª—è —Å–ª–æ–≤ (–ú–∞—Ä–∫–µ—Ä, –°–≤–æ–π—Å—Ç–≤–∞) –±–µ—Ä–µ–º –¢–û–ü-1
        candidates = vocab_by_type[block].most_common(3)
        for w, cnt in candidates:
            if w not in used_words:
                if "–ú–∞—Ä–∫–µ—Ä" in block: w = w.capitalize()
                final_parts.append(w)
                used_words.add(w)
                break
                
    ideal_name = " ".join(final_parts)
    
    # –û—Ç—á–µ—Ç
    report = []
    report.append(f"**–°—Ö–µ–º–∞:** {best_struct_str}")
    report.append("")
    report.append("**–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**")
    for block in best_struct_list:
        if "–†–∞–∑–º–µ—Ä—ã" in block: continue
        top = [f"{w}" for w, c in vocab_by_type[block].most_common(3)]
        report.append(f"- **{block}**: {', '.join(top)}")
            
    return ideal_name, report

def render_paginated_table(df, title_text, key_prefix, default_sort_col=None, use_abs_sort_default=False, default_sort_order="–£–±—ã–≤–∞–Ω–∏–µ", show_controls=True):
    if df.empty: st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö."); return
    col_t1, col_t2 = st.columns([7, 3])
    with col_t1: st.markdown(f"### {title_text}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ñ–æ–ª—Ç–æ–≤ –≤ Session State
    if f'{key_prefix}_sort_col' not in st.session_state: 
        st.session_state[f'{key_prefix}_sort_col'] = default_sort_col if (default_sort_col and default_sort_col in df.columns) else df.columns[0]
    
    if f'{key_prefix}_sort_order' not in st.session_state: 
        st.session_state[f'{key_prefix}_sort_order'] = default_sort_order

    search_query = st.text_input(f"üîç –ü–æ–∏—Å–∫ ({title_text})", key=f"{key_prefix}_search")
    if search_query:
        mask = df.astype(str).apply(lambda x: x.str.contains(search_query, case=False, na=False)).any(axis=1)
        df_filtered = df[mask].copy()
    else: df_filtered = df.copy()

    if df_filtered.empty: st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."); return

    # === –õ–û–ì–ò–ö–ê –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø –ö–û–ù–¢–†–û–õ–û–í ===
    if show_controls:
        with st.container():
            st.markdown("<div class='sort-container'>", unsafe_allow_html=True)
            col_s1, col_s2, col_sp = st.columns([2, 2, 4])
            with col_s1:
                current_sort = st.session_state[f'{key_prefix}_sort_col']
                if current_sort not in df_filtered.columns: current_sort = df_filtered.columns[0]
                sort_col = st.selectbox("üóÇ –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ:", df_filtered.columns, key=f"{key_prefix}_sort_box", index=list(df_filtered.columns).index(current_sort))
                st.session_state[f'{key_prefix}_sort_col'] = sort_col
            with col_s2:
                def_index = 0 if st.session_state[f'{key_prefix}_sort_order'] == "–£–±—ã–≤–∞–Ω–∏–µ" else 1
                sort_order = st.radio("–ü–æ—Ä—è–¥–æ–∫:", ["–£–±—ã–≤–∞–Ω–∏–µ", "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ"], horizontal=True, key=f"{key_prefix}_order_box", index=def_index)
                st.session_state[f'{key_prefix}_sort_order'] = sort_order
            st.markdown("</div>", unsafe_allow_html=True)
    else:
        # –ï—Å–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª—ã —Å–∫—Ä—ã—Ç—ã, –±–µ—Ä–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ session_state (–¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ)
        sort_col = st.session_state[f'{key_prefix}_sort_col']
        sort_order = st.session_state[f'{key_prefix}_sort_order']

    ascending = (sort_order == "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ")
    
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    if use_abs_sort_default and sort_col == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è" and "sort_val" in df_filtered.columns: 
        df_filtered = df_filtered.sort_values(by="sort_val", ascending=ascending)
    elif ("–î–æ–±–∞–≤–∏—Ç—å" in sort_col or "+/-" in sort_col) and df_filtered[sort_col].dtype == object:
        try:
            df_filtered['_temp_sort'] = df_filtered[sort_col].astype(str).str.replace(r'[^\d]', '', regex=True)
            df_filtered['_temp_sort'] = pd.to_numeric(df_filtered['_temp_sort'], errors='coerce').fillna(0)
            df_filtered = df_filtered.sort_values(by='_temp_sort', ascending=ascending).drop(columns=['_temp_sort'])
        except: df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)
    else: 
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–ª–æ–Ω–∫–∏ (–Ω–∞ —Å–ª—É—á–∞–π —Å–º–µ–Ω—ã –¥–∞–Ω–Ω—ã—Ö)
        if sort_col in df_filtered.columns:
            df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)

    df_filtered = df_filtered.reset_index(drop=True); df_filtered.index = df_filtered.index + 1
    
    # –≠–∫—Å–ø–æ—Ä—Ç
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        export_df = df_filtered.copy()
        if "is_missing" in export_df.columns: del export_df["is_missing"]
        if "sort_val" in export_df.columns: del export_df["sort_val"]
        export_df.to_excel(writer, index=False, sheet_name='Data')
    excel_data = buffer.getvalue()
    with col_t2: st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel", data=excel_data, file_name=f"{key_prefix}_export.xlsx", mime="application/vnd.ms-excel", key=f"{key_prefix}_down")

    # –ü–∞–≥–∏–Ω–∞—Ü–∏—è
    ROWS_PER_PAGE = 20
    if f'{key_prefix}_page' not in st.session_state: st.session_state[f'{key_prefix}_page'] = 1
    total_rows = len(df_filtered); total_pages = math.ceil(total_rows / ROWS_PER_PAGE)
    if total_pages == 0: total_pages = 1
    current_page = st.session_state[f'{key_prefix}_page']
    if current_page > total_pages: current_page = total_pages
    if current_page < 1: current_page = 1
    st.session_state[f'{key_prefix}_page'] = current_page
    start_idx = (current_page - 1) * ROWS_PER_PAGE
    end_idx = start_idx + ROWS_PER_PAGE
    df_view = df_filtered.iloc[start_idx:end_idx]

    def highlight_rows(row):
        base_style = 'background-color: #FFFFFF; color: #3D4858; border-bottom: 1px solid #DBEAFE;'
        styles = []
        status = row.get("–°—Ç–∞—Ç—É—Å", "")
        for col_name in row.index:
            cell_style = base_style
            if col_name == "–°—Ç–∞—Ç—É—Å":
                if status == "–ù–µ–¥–æ—Å–ø–∞–º": cell_style += "color: #D32F2F; font-weight: bold;"
                elif status == "–ü–µ—Ä–µ—Å–ø–∞–º": cell_style += "color: #E65100; font-weight: bold;"
                elif status == "–ù–æ—Ä–º–∞": cell_style += "color: #2E7D32; font-weight: bold;"
            styles.append(cell_style)
        return styles

    cols_to_hide = [c for c in ["is_missing", "sort_val"] if c in df_view.columns]
    try: styled_df = df_view.style.apply(highlight_rows, axis=1)
    except: styled_df = df_view
    st.dataframe(styled_df, use_container_width=True, height=(len(df_view) * 35) + 40, column_config={c: None for c in cols_to_hide})
    
    c_spacer, c_btn_prev, c_info, c_btn_next = st.columns([6, 1, 1, 1])
    with c_btn_prev:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_prev", disabled=(current_page <= 1), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] -= 1
            st.rerun()
    with c_info: st.markdown(f"<div style='text-align: center; margin-top: 10px;'><b>{current_page}</b> / {total_pages}</div>", unsafe_allow_html=True)
    with c_btn_next:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_next", disabled=(current_page >= total_pages), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] += 1
            st.rerun()
    st.markdown("---")
# ==========================================
# PERPLEXITY GEN
# ==========================================
STATIC_DATA_GEN = {
    'IP_PROP4817': "–£—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏",
    'IP_PROP4818': "–û–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –≤ —Ä–µ–≥–∏–æ–Ω—ã —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4819': """<p>–ù–∞–¥–µ–∂–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫–∞–∑–∞ –≤ –ª—é–±—É—é —Ç–æ—á–∫—É —Å—Ç—Ä–∞–Ω—ã: "–°—Ç–∞–ª—å–º–µ—Ç—É—Ä–∞–ª" –æ—Ç–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–≤–∞—Ä 24 —á–∞—Å–∞ –≤ —Å—É—Ç–∫–∏, 7 –¥–Ω–µ–π –≤ –Ω–µ–¥–µ–ª—é. –ë–æ–ª–µ–µ 4 000 –æ—Ç–≥—Ä—É–∑–æ–∫ –≤ –≥–æ–¥. –ü—Ä–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–∏ –∑–∞–∫–∞–∑–∞ –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –≤–∞–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ä—à—Ä—É—Ç.</p>""",
    'IP_PROP4820': """<p>–ù–∞—à–∏ –∏–∑–¥–µ–ª–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è—Ö –£—Ä–∞–ª–∞, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞, –ü–æ–≤–æ–ª–∂—å—è, –°–∏–±–∏—Ä–∏. –ü–∞—Ä—Ç–Ω–µ—Ä—ã –ø–æ –ª–æ–≥–∏—Å—Ç–∏–∫–µ –ø—Ä–µ–¥–ª–æ–∂–∞—Ç –¥–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–∫–∞–∑ —Å–∞–º—ã–º —É–¥–æ–±–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º ‚Äì –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–º, –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω—ã–º, –¥–∞–∂–µ –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º. –î–ª—è –≤–∞—Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—É—é —Å—Ö–µ–º—É –ø–æ–¥ —É–¥–æ–±–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è. –ü–æ–≥—Ä—É–∑–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ç–µ—Ö–Ω–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.</p><div class="h4"><h4>–°–∞–º–æ–≤—ã–≤–æ–∑</h4></div><p>–ï—Å–ª–∏ –æ–±—ã—á–Ω–æ —Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∑–∞–±—Ä–∞—Ç—å —Ç–æ–≤–∞—Ä –∏–ª–∏ –¥–∞–µ—Ç–µ —ç—Ç–æ –ø—Ä–∞–≤–æ —É–ø–æ–ª–Ω–æ–º–æ—á–µ–Ω–Ω—ã–º, –∞–¥—Ä–µ—Å –∏ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã —Å–∫–ª–∞–¥–∞ –≤ —Å–≤–æ–µ–º –≥–æ—Ä–æ–¥–µ —É—Ç–æ—á–Ω—è–π—Ç–µ —É –º–µ–Ω–µ–¥–∂–µ—Ä–∞.</p><div class="h4"><h4>–ì—Ä—É–∑–æ–≤–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏</h4></div><p>–û—Ç–ø—Ä–∞–≤–∏–º –ø—Ä–æ–∫–∞—Ç –Ω–∞ –≤–∞—à –æ–±—ä–µ–∫—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –∞–≤—Ç–æ–ø–∞—Ä–∫–æ–º. –ü–æ–ª—É—á–µ–Ω–∏–µ –≤ —É–ø–∞–∫–æ–≤–∫–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏, –∞ –∏–º–µ–Ω–Ω–æ –Ω–∞ –¥–µ—Ä–µ–≤—è–Ω–Ω–æ–º –ø–æ–¥–¥–æ–Ω–µ.</p><div class="h4"><h4>–°–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å –¢–ö</h4></div><p>–î–æ—Å—Ç–∞–≤–∫–∞ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –†–æ—Å—Å–∏–∏ –∏ –°–ù–ì. –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å—Å—è, —Ç–∞–∫ –∫–∞–∫ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç, –æ–¥–Ω–∞–∫–æ, —Å—Ä–∞–≤–Ω–∏–º —Å—Ç–æ–∏–º–æ—Å—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª—É–∂–± –∏ –≤—ã–±–µ—Ä–µ–º –ª—É—á—à—É—é.</p>""",
    'IP_PROP4821': "–û–ø–ª–∞—Ç–∞ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤:",
    'IP_PROP4822': """<p>–ù–∞—à–∞ –∫–æ–º–ø–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞ –ø—Ä–∏–Ω—è—Ç—å –ª—é–±—ã–µ –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–µ –≤–∏–¥—ã –æ–ø–ª–∞—Ç—ã –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏—Ü: –ø–æ —Å—á–µ—Ç—É, –Ω–∞–ª–∏—á–Ω–∞—è –∏ –±–µ–∑–Ω–∞–ª–∏—á–Ω–∞—è, –Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–π –ø–ª–∞—Ç–µ–∂, —Ç–∞–∫–∂–µ –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞ –∏ –æ—Ç—Å—Ä–æ—á–∫–∞ –ø–ª–∞—Ç–µ–∂–∞.</p>""",
    'IP_PROP4823': """<div class="h4"><h3>–ü—Ä–∏–º–µ—Ä—ã –≤–æ–∑–º–æ–∂–Ω–æ–π –æ–ø–ª–∞—Ç—ã</h3></div><div class="an-col-12"><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–° –ø–æ–º–æ—â—å—é –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤ —Ü–µ–Ω—Ç—Ä–∞—Ö –ø—Ä–æ–¥–∞–∂</span></p></li></ul><p>–í–∞–∂–Ω–æ! –¶–µ–Ω–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—É–±–ª–∏—á–Ω–æ–π –æ—Ñ–µ—Ä—Ç–æ–π. –ü—Ä–∏—Ö–æ–¥–∏—Ç–µ –≤ –Ω–∞—à –æ—Ñ–∏—Å, —á—Ç–æ–±—ã —É—Ç–æ—á–Ω–∏—Ç—å –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ, –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ—á—Ç–∏ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å, —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –≤–æ–∑–≤—Ä–∞—Ç, —Å—á–µ—Ç, —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ª–æ–≥–∏—Å—Ç–∏–∫—É.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞ —Ä–∞—Å—á–µ—Ç–Ω—ã–π —Å—á–µ—Ç</span></p></li></ul><p>–ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É —Å—á–µ—Ç—É –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏ –±–∞–Ω–∫–∞ –∏–ª–∏ –ø—É—Ç–µ–º –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥—Å—Ç–≤ —á–µ—Ä–µ–∑ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç (—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∑–∞—â–∏—â–µ–Ω—ã, —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –æ—Ç–¥–µ–ª–µ–Ω–∏—è). –î–ª—è –ø—Ä–∞–≤–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å —Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –ø–ª–∞—Ç–µ–∂–Ω–æ–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ —Å –æ—Ç–º–µ—Ç–∫–æ–π –±–∞–Ω–∫–∞.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞–ª–∏—á–Ω—ã–º–∏ –∏–ª–∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∫–∞—Ä—Ç–æ–π –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏</span></p></li></ul><p><span style="font-weight: 400;">–ü–æ–º–æ–∂–µ–º —Å –æ–ø–ª–∞—Ç–æ–π: –æ–±—ä–µ–º –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ. –ö—Ä—É–ø–Ω—ã–º –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º ‚Äì –¥–µ–Ω—å–≥–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–µ–º–∫–∏ —Ç–æ–≤–∞—Ä–∞.</span></p><p>–ú–µ–Ω–µ–¥–∂–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.</p><p>–ó–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ —á–µ—Ä–µ–∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç:</p><p><a class="btn btn-blue" href="/catalog/">–ö–∞—Ç–∞–ª–æ–≥ (–º–∞–≥–∞–∑–∏–Ω-–º–µ–Ω—é):</a></p></div></div><br>""",
    'IP_PROP4824': "–û–ø–∏—Å–∞–Ω–∏–µ, —Å—Ç–∞—Ç—å–∏, –ø–æ–∏—Å–∫, –æ—Ç–∑—ã–≤—ã, –Ω–æ–≤–æ—Å—Ç–∏, –∞–∫—Ü–∏–∏, –∂—É—Ä–Ω–∞–ª, info:",
    'IP_PROP4825': "–ú–æ–∂–µ–º –º–µ—Ç–∞–ª–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –æ—Ü–∏–Ω–∫–æ–≤–∞—Ç—å, –Ω–∏–∫–µ–ª–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–æ–≤–æ–ª–æ—á—å",
    'IP_PROP4826': "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥",
    'IP_PROP4834': "–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –±–µ–∑ –ø—Ä–∏–º–µ—Å–µ–π",
    'IP_PROP4835': "–ü–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫",
    'IP_PROP4836': "–ö–∞—á–µ—Å—Ç–≤–æ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä",
    'IP_PROP4837': "–ü–æ—Ä—è–¥–æ–∫ –≤ –ì–û–°–¢–∞—Ö"
}

def get_page_data_for_gen(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.encoding = 'utf-8'
    except Exception as e: return None, None, None, f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}"
    
    if response.status_code != 200: return None, None, None, f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—É—Å–∞: {response.status_code}"
    
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # 1. –ó–ê–ì–û–õ–û–í–û–ö: –ò—â–µ–º —Å—Ç—Ä–æ–≥–æ H2 (–∫–∞–∫ –≤—ã –ø—Ä–æ—Å–∏–ª–∏)
    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–π —á–∞—Å—Ç–∏ (—á–∞—Å—Ç–æ –±—ã–≤–∞–µ—Ç h2 –≤ –º–µ–Ω—é, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–º –Ω–µ –Ω—É–∂–µ–Ω)
    # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–ª–∞—Å—Å description-container, –∏—â–µ–º –≤–Ω—É—Ç—Ä–∏ –Ω–µ–≥–æ, –µ—Å–ª–∏ –Ω–µ—Ç - –ø–µ—Ä–≤—ã–π H2 –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    description_div = soup.find('div', class_='description-container')
    
    target_h2 = None
    if description_div:
        target_h2 = description_div.find('h2')
    
    if not target_h2:
        target_h2 = soup.find('h2')
        
    page_header = target_h2.get_text(strip=True) if target_h2 else "–û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞" # –î–µ—Ñ–æ–ª—Ç, –µ—Å–ª–∏ H2 –Ω–µ—Ç —Å–æ–≤—Å–µ–º

    # 2. –§–∞–∫—Ç—É—Ä–∞ (—Ç–µ–∫—Å—Ç)
    base_text = description_div.get_text(separator="\n", strip=True) if description_div else soup.body.get_text(separator="\n", strip=True)[:5000]
    
    # 3. –¢–µ–≥–∏
    tags_container = soup.find(class_='popular-tags-inner')
    tags_data = []
    if tags_container:
        links = tags_container.find_all('a')
        for link in links:
            tag_url = urljoin(url, link.get('href')) if link.get('href') else None
            if tag_url: tags_data.append({'name': link.get_text(strip=True), 'url': tag_url})
            
    return base_text, tags_data, page_header, None

def generate_ai_content_blocks(api_key, base_text, tag_name, forced_header, num_blocks=5, seo_words=None):
    if not base_text: return ["Error: No base text"] * num_blocks
    
    from openai import OpenAI
    client = OpenAI(api_key=api_key, base_url="https://litellm.tokengate.ru/v1")
    
    seo_words = seo_words or []
    seo_instruction_block = ""
    
    if seo_words:
        seo_list_str = ", ".join(seo_words)
        seo_instruction_block = f"""
--- –í–ê–ñ–ù–ê–Ø –ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û SEO-–°–õ–û–í–ê–ú ---
–¢–µ–±–µ –Ω—É–∂–Ω–æ –≤–Ω–µ–¥—Ä–∏—Ç—å –≤ —Ç–µ–∫—Å—Ç —Å–ª–µ–¥—É—é—â–∏–µ —Å–ª–æ–≤–∞ –≤ –ª—é–±–æ–π –ø–æ–¥—Ö–æ–¥—è—â–µ–π –ø–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ª–µ–º–º–µ: {{{seo_list_str}}}

–ü–†–ê–í–ò–õ–ê –í–ù–ï–î–†–ï–ù–ò–Ø –ò –í–´–î–ï–õ–ï–ù–ò–Ø:
1. –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï: –†–∞—Å–∫–∏–¥–∞–π —Å–ª–æ–≤–∞ –ø–æ –≤—Å–µ–º {num_blocks} –±–ª–æ–∫–∞–º.
2. –°–¢–†–û–ì–ò–ô –ó–ê–ü–†–ï–¢: –ò—Å–ø–æ–ª—å–∑—É–π —Ç–µ–≥ <b> –¢–û–õ–¨–ö–û –¥–ª—è —ç—Ç–∏—Ö SEO-—Å–ª–æ–≤. –ù–µ –≤—ã–¥–µ–ª—è–π –∂–∏—Ä–Ω—ã–º –Ω–∏—á–µ–≥–æ –¥—Ä—É–≥–æ–≥–æ.
3. –ï–°–¢–ï–°–¢–í–ï–ù–ù–û–°–¢–¨: –ú–µ–Ω—è–π —Å–ª–æ–≤–æ—Ñ–æ—Ä–º—ã –ø–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç. –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∏ –ª–æ–≥–∏—á–Ω—ã–º, –Ω–µ –ø–∏—à–∏ —á—É—à—å.
-------------------------------------------
"""

    # 2. –ü–†–û–ú–¢–´ (–¢–ï–ö–°–¢ –û–°–¢–ê–í–õ–ï–ù –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô)
    system_instruction = (
        "–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä –∏ –≤–µ—Ä—Å—Ç–∞–ª—å—â–∏–∫. "
        "–¢–≤–æ—è —Ü–µ–ª—å ‚Äî –ø–∏—Å–∞—Ç—å –≥–ª—É–±–æ–∫–∏–π, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –ø–æ–ª–µ–∑–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–æ–≤, –Ω–∞—Å—ã—â–µ–Ω–Ω—ã–π —Ñ–∞–∫—Ç–∞–º–∏ –∏ —Ü–∏—Ñ—Ä–∞–º–∏. "
        "–¢—ã –≤—ã–¥–∞–µ—à—å –¢–û–õ–¨–ö–û HTML-–∫–æ–¥. "
        "–°—Ç–∏–ª—å: –î–µ–ª–æ–≤–æ–π, —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π, –Ω–æ \"—á–µ–ª–æ–≤–µ—á–Ω—ã–π\" –∏ –ø–æ–Ω—è—Ç–Ω—ã–π. –ò–∑–±–µ–≥–∞–π –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º–æ–≤ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. "
        "–§–∞–∫—Ç—ã –∏ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞: –í—Å–µ —Å—É–∂–¥–µ–Ω–∏—è –ø–æ–¥–∫—Ä–µ–ø–ª—è–π –∏–∑–º–µ—Ä–∏–º—ã–º–∏ —Ñ–∞–∫—Ç–∞–º–∏, —Ü–∏—Ñ—Ä–∞–º–∏, —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –ì–û–°–¢—ã, –º–∞—Ä–∫–∏ —Å—Ç–∞–ª–∏ –∏ –¥—Ä—É–≥–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤—ã. "
        "–ò—Å–ø–æ–ª—å–∑—É–π –ø–æ–∏—Å–∫–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π. "
        "–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å: –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–¥–∞–≤–∞—Ç—å. –ì–æ–≤–æ—Ä–∏ –æ—Ç –ª–∏—Ü–∞ –∫–æ–º–ø–∞–Ω–∏–∏-–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è/–ø–æ—Å—Ç–∞–≤—â–∏–∫–∞. "
        "–í–º–µ—Å—Ç–æ \"–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫\" –∏—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—â–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –∏ —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É. "
        "–§–æ—Ä–º—É–ª–∞ –ì–ª–∞–≤—Ä–µ–¥–∞ –¥–ª—è B2B: –í —Ç–µ–∫—Å—Ç–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã: —á—Ç–æ —ç—Ç–æ? –∫–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç? –∫–æ–º—É –ø–æ–¥–æ–π–¥–µ—Ç? "
        "–∫–∞–∫–∏–µ –µ—Å—Ç—å —Ä–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç–∏? –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å–∫—Ä–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞—Ö –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞, —Å–∫–ª–∞–¥—Å–∫–∏—Ö –∑–∞–ø–∞—Å–∞—Ö –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è –ø–æ–¥ –∑–∞–∫–∞–∑. "
        "–°–¢–†–û–ì–ò–ï –ó–ê–ü–†–ï–¢–´: "
        "1. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –£–∫—Ä–∞–∏–Ω—ã, —É–∫—Ä–∞–∏–Ω—Å–∫–∏—Ö –≥–æ—Ä–æ–¥–æ–≤ (–ö–∏–µ–≤, –õ—å–≤–æ–≤ –∏ –¥—Ä.), –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–º—ã, –≤–∞–ª—é—Ç—É –≥—Ä–∏–≤–Ω—É. –ö–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–æ–≥–æ –¥–ª—è –†–§. "
        "2. –ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∏ –≤ —Ç–µ–∫—Å—Ç–µ, –Ω–∏ –≤ —Å–ø–∏—Å–∫–∞—Ö. –ß–∏—Å—Ç–∏ —Ç–µ–∫—Å—Ç –æ—Ç –Ω–∏—Ö –ø–æ–ª–Ω–æ—Å—Ç—å—é. "
        "3. –ò–º–µ–Ω–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ, –Ω–∞–∑–≤–∞–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤ –ø–∏—à–∏ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã. –ú–∞—Ä–∫–∏ –ø–∏—à–∏ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –º–∞—Ä–æ—á–Ω–∏–∫–∞–º–∏. –ì–û–°–¢ –≤—Å–µ–≥–¥–∞ –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏."
    )

    user_prompt = f"""
    –ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:
    –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: "{tag_name}"
    –ë–∞–∑–æ–≤—ã–π —Ç–µ–∫—Å—Ç (—Ñ–∞–∫—Ç—É—Ä–∞): \"\"\"{base_text[:3500]}\"\"\"
    
    {seo_instruction_block}
    
    –ó–ê–î–ê–ß–ê:
    –ù–∞–ø–∏—à–∏ {num_blocks} HTML-–±–ª–æ–∫–æ–≤, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º: |||BLOCK_SEP|||
    
    –û–ë–©–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
    1. –û–ë–™–ï–ú: –ö–∞–∂–¥—ã–π –±–ª–æ–∫ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∞–∫—Å–∏–º—É–º 800 —Å–∏–º–≤–æ–ª–æ–≤. –†–∞—Å–∫—Ä—ã–≤–∞–π —Ç–µ–º—É –ø–æ–¥—Ä–æ–±–Ω–æ.
    2. –ß–ò–°–¢–û–¢–ê: –ò—Å–∫–ª—é—á–∏ –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.
    3. –ü–û–õ–¨–ó–ê: –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –≥—Ä–∞–º–æ—Ç–Ω—ã–º –∏ –ø–æ–ª–µ–∑–Ω—ã–º –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –ø–æ –∑–∞–∫—É–ø–∫–∞–º. –ò–∑–±–µ–≥–∞–π "–≤–æ–¥—ã".
    
    –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –°–¢–†–£–ö–¢–£–†–ï –ö–ê–ñ–î–û–ì–û –ë–õ–û–ö–ê:
    –ö–∞–∂–¥—ã–π –∏–∑ {num_blocks} –±–ª–æ–∫–æ–≤ –¥–æ–ª–∂–µ–Ω —Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –ø–æ—Ä—è–¥–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤:
    1. –ó–∞–≥–æ–ª–æ–≤–æ–∫ (<h2> —Ç–æ–ª—å–∫–æ –¥–ª—è 1-–≥–æ –±–ª–æ–∫–∞, <h3> –¥–ª—è –±–ª–æ–∫–æ–≤ 2-5).
    2. –ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞ (<p>) - —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π.
    3. –í–≤–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –ø–æ–¥–≤–æ–¥—è—â–µ–µ –∫ —Å–ø–∏—Å–∫—É (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:", "–°—Ñ–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è:").
    4. –ú–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ (<ul> c <li>).
    5. –í—Ç–æ—Ä–æ–π (–∑–∞–≤–µ—Ä—à–∞—é—â–∏–π) –∞–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞ (<p>) - —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π.
    
    –¢–ï–ú–´ –ë–õ–û–ö–û–í:
    --- –ë–õ–û–ö 1 (–í–≤–æ–¥–Ω—ã–π) ---
    - –ó–∞–≥–æ–ª–æ–≤–æ–∫: <h2>{forced_header}</h2>
    - –û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞, –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ, –∫–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏.
    
    --- –ë–õ–û–ö–ò 2, 3, 4, 5 (–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏) ---
    - –ó–∞–≥–æ–ª–æ–≤–∫–∏: <h3> (–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ, –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ, –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏, –°–æ—Ä—Ç–∞–º–µ–Ω—Ç –∏ —Ç.–¥.).
    - –ò—Å–ø–æ–ª—å–∑—É–π —Ñ–∞–∫—Ç—É—Ä—É –∏–∑ "–ë–∞–∑–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞".
    
    –§–ò–ù–ê–õ–¨–ù–´–ï –£–°–õ–û–í–ò–Ø:
    - –ù–∏–∫–∞–∫–∏—Ö –≤–≤–æ–¥–Ω—ã—Ö —Å–ª–æ–≤ —Ç–∏–ø–∞ "–í–æ—Ç –≤–∞—à –∫–æ–¥".
    - –ù–∏–∫–∞–∫–æ–≥–æ Markdown (```).
    - –¢–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π HTML, —Ä–∞–∑–±–∏—Ç—ã–π —á–µ—Ä–µ–∑ |||BLOCK_SEP|||.
    """
    
    try:
        response = client.chat.completions.create(
            model="google/gemini-2.5-pro",
            messages=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7 
        )
        content = response.choices[0].message.content
        
        # === –ñ–ï–°–¢–ö–ê–Ø –ß–ò–°–¢–ö–ê –û–¢ –ú–£–°–û–†–ê –ò –ñ–ò–†–ù–û–ì–û ===
        content = re.sub(r'^```[a-zA-Z]*\s*', '', content.strip())
        content = re.sub(r'\s*```$', '', content.strip())
        content = content.strip().lstrip('`.').strip()
        # –£–¥–∞–ª—è–µ–º –∂–∏—Ä–Ω–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω–æ –≤—Å–µ –∂–µ –ø—Ä–æ—Å–æ—á–∏–ª–æ—Å—å
        content = re.sub(r'</?(b|strong)>', '', content, flags=re.IGNORECASE)
        
        blocks = [b.strip() for b in content.split("|||BLOCK_SEP|||") if b.strip()]
        
        cleaned_blocks = []
        for b in blocks:
            cb = re.sub(r'^```[a-zA-Z]*', '', b).strip().lstrip('`.').strip()
            if cb: cleaned_blocks.append(cb)
            
        while len(cleaned_blocks) < num_blocks: cleaned_blocks.append("")
        return cleaned_blocks[:num_blocks]
    except Exception as e:
        return [f"API Error: {str(e)}"] * num_blocks

# ==========================================
# 7. UI TABS RESTRUCTURED
# ==========================================
tab_seo_main, tab_wholesale_main, tab_projects = st.tabs(["üìä SEO –ê–Ω–∞–ª–∏–∑", "üè≠ –û–ø—Ç–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä", "üìÅ –ü—Ä–æ–µ–∫—Ç—ã"])

# ------------------------------------------
# TAB 1: SEO ANALYSIS (KEPT AS IS)
# ------------------------------------------
with tab_seo_main:
    col_main, col_sidebar = st.columns([65, 35])
    
    # === –õ–ï–í–ê–Ø –ö–û–õ–û–ù–ö–ê (–û–°–ù–û–í–ù–ê–Ø) ===
    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
        
        # –°–±—Ä–æ—Å –∫—ç—à–∞ –¥–ª—è —Å–ª–æ–≤–∞—Ä–µ–π
        if st.button("üßπ –û–±–Ω–æ–≤–∏—Ç—å —Å–ª–æ–≤–∞—Ä–∏ (–ö—ç—à)", key="clear_cache_btn"):
            st.cache_data.clear()
            st.rerun()

        my_input_type = st.radio("–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], horizontal=True, label_visibility="collapsed", key="my_page_source_radio")
        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            st.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã", placeholder="https://site.ru/catalog/tovar", label_visibility="collapsed", key="my_url_input")
        elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏–ª–∏ —Ç–µ–∫—Å—Ç", height=200, label_visibility="collapsed", placeholder="–í—Å—Ç–∞–≤—å—Ç–µ HTML", key="my_content_input")

        st.markdown("### –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å")
        st.text_input("–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞", label_visibility="collapsed", key="query_input")
        
        st.markdown("### –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        
        # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ-–ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è ---
        if st.session_state.get('force_radio_switch'):
            st.session_state["competitor_source_radio"] = "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"
            st.session_state['force_radio_switch'] = False
        # -----------------------------------------------

        source_type_new = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫", ["–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)", "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"], horizontal=True, label_visibility="collapsed", key="competitor_source_radio")
        source_type = "API" if "API" in source_type_new else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"
        
        if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
            # --- –í–°–¢–ê–í–ò–¢–¨ –≠–¢–û–¢ –ë–õ–û–ö –¢–£–¢ ---
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç —Ñ–∏–ª—å—Ç—Ä–∞
            if 'temp_update_urls' in st.session_state:
                st.session_state['persistent_urls'] = st.session_state['temp_update_urls']
                del st.session_state['temp_update_urls']

            # –ö–Ω–æ–ø–∫–∞ —Å–±—Ä–æ—Å–∞
            if st.session_state.get('analysis_done'):
                col_reset, _ = st.columns([1, 4])
                with col_reset:
                    if st.button("üîÑ –ù–æ–≤—ã–π –ø–æ–∏—Å–∫ (–°–±—Ä–æ—Å)", type="secondary"):
                        keys_to_clear = ['analysis_done', 'analysis_results', 'persistent_urls', 'excluded_urls_auto', 'detected_anomalies']
                        for k in keys_to_clear:
                            if k in st.session_state: del st.session_state[k]
                        st.rerun()

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–µ—Å–ª–∏ –Ω–µ—Ç)
            if 'persistent_urls' not in st.session_state:
                st.session_state['persistent_urls'] = ""

            has_exclusions = st.session_state.get('excluded_urls_auto') and len(st.session_state.get('excluded_urls_auto')) > 5
            
            if has_exclusions:
                c_url_1, c_url_2 = st.columns(2)
                with c_url_1:
                    # –ü–†–û–°–¢–û –í–ò–î–ñ–ï–¢. –ë–µ–∑ value=..., —Ç–∞–∫ –∫–∞–∫ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º key.
                    # –ó–Ω–∞—á–µ–Ω–∏–µ —Å–∞–º–æ –ø–æ–¥—Ç—è–Ω–µ—Ç—Å—è –∏–∑ st.session_state['persistent_urls']
                    st.text_area(
                        "‚úÖ –ê–∫—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã (–î–ª—è –∞–Ω–∞–ª–∏–∑–∞)", 
                        height=200, 
                        key="persistent_urls" 
                    )
                with c_url_2:
                    st.text_area(
                        "üö´ –ê–≤—Ç–æ-–∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ", 
                        height=200, 
                        value=st.session_state.get('excluded_urls_auto', ""),
                        disabled=True # –°–¥–µ–ª–∞–ª –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–º, —á—Ç–æ–±—ã –Ω–µ –ø—É—Ç–∞—Ç—å
                    )
            else:
                st.text_area(
                    "–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", 
                    height=200, 
                    key="persistent_urls"
                )

        # –ì–†–ê–§–ò–ö
        if st.session_state.get('analysis_done') and st.session_state.get('analysis_results'):
            results = st.session_state.analysis_results
            if 'relevance_top' in results and not results['relevance_top'].empty:
                st.markdown("<br>", unsafe_allow_html=True)
                with st.expander("üìä –ì—Ä–∞—Ñ–∏–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–ù–∞–∂–º–∏—Ç–µ, —á—Ç–æ–±—ã —Ä–∞—Å–∫—Ä—ã—Ç—å)", expanded=False):
                    st.info(
                        """
                        **–õ–æ–≥–∏–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞**  
                        –ì—Ä–∞—Ñ–∏–∫ —Å—Ç—Ä–æ–∏—Ç—Å—è –ø–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –∏–∑ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è **–∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –ª–∏–Ω–∏–∏ —Ç—Ä–µ–Ω–¥–∞** –∏ —Ç–æ—á–Ω–æ–≥–æ –≤—ã—è–≤–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–µ –æ—Ç–ª–∏—á–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –æ—Ç —Å–ª—É—á–∞–π–Ω—ã—Ö "–≤—ã–±—Ä–æ—Å–æ–≤" –≤ –≤—ã–¥–∞—á–µ.
                        
                        *–í–∞–∂–Ω–æ: –í—Å–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∏–∂–µ (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ, —Ç–∞–±–ª–∏—Ü—ã, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞) —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è —Å—Ç—Ä–æ–≥–æ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É –≤–∞–º–∏ –ª–∏–º–∏—Ç—É (–¢–æ–ø-10/20) –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–∂–µ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –≤—ã –≤—ã–±—Ä–∞–ª–∏ "–ê–≤—Ç–æ-—Ñ–∏–ª—å—Ç—Ä —Å–ª–∞–±—ã—Ö —Å–∞–π—Ç–æ–≤. –í —Å–ª—É—á–∞–µ –µ—Å–ª–∏ "–ê–≤—Ç–æ-—Ñ–∏–ª—å—Ç—Ä" –Ω–µ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω, –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–ª–∏—è—é—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –≤—Ä—á–Ω—É—é.*
                        """, 
                        icon="‚ÑπÔ∏è"
                    )
                    graph_data = st.session_state.get('full_graph_data', results['relevance_top'])
                    render_relevance_chart(graph_data, unique_key="main")
                st.markdown("<br>", unsafe_allow_html=True)

        # --- –ö–ù–û–ü–ö–ê –ó–ê–ü–£–°–ö–ê ---
        def run_analysis_callback():
            saved_filter_state = st.session_state.get('settings_auto_filter', True)
            keys_to_clear = [
                'analysis_results', 'analysis_done', 'naming_table_df',
                'ideal_h1_result', 'gen_result_df', 'unified_excel_data',
                'detected_anomalies', 'serp_trend_info',
                'excluded_urls_auto'
            ]
            for k in keys_to_clear:
                if k in st.session_state: del st.session_state[k]
            st.session_state.settings_auto_filter = saved_filter_state
            for k in list(st.session_state.keys()):
                if k.endswith('_page'): st.session_state[k] = 1
            st.session_state.start_analysis_flag = True

        st.markdown("<br>", unsafe_allow_html=True) # –û—Ç—Å—Ç—É–ø –ø–µ—Ä–µ–¥ –∫–Ω–æ–ø–∫–æ–π
        st.button(
            "–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", 
            type="primary", 
            use_container_width=True, 
            key="start_analysis_btn",
            on_click=run_analysis_callback 
        )

    # === –ü–†–ê–í–ê–Ø –ö–û–õ–û–ù–ö–ê (–°–ê–ô–î–ë–ê–†) ===
    with col_sidebar:
        if not ARSENKIN_TOKEN:
             new_arsenkin = st.text_input("Arsenkin Token", type="password", key="input_arsenkin")
             if new_arsenkin: st.session_state.arsenkin_token = new_arsenkin; ARSENKIN_TOKEN = new_arsenkin 
        if not YANDEX_DICT_KEY:
             new_yandex = st.text_input("Yandex Dict Key", type="password", key="input_yandex")
             if new_yandex: st.session_state.yandex_dict_key = new_yandex; YANDEX_DICT_KEY = new_yandex
        
        st.markdown("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
        st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
        st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
        st.selectbox("–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞", list(REGION_MAP.keys()), key="settings_region")
        st.selectbox("–ö–æ–ª-–≤–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", [10, 20], index=0, key="settings_top_n")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–µ–∫–±–æ–∫—Å–æ–≤
        if "settings_noindex" not in st.session_state: st.session_state.settings_noindex = True
        if "settings_alt" not in st.session_state: st.session_state.settings_alt = False
        if "settings_numbers" not in st.session_state: st.session_state.settings_numbers = False
        if "settings_norm" not in st.session_state: st.session_state.settings_norm = True
        if "settings_auto_filter" not in st.session_state: st.session_state.settings_auto_filter = True

        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å <noindex>", key="settings_noindex")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", key="settings_alt")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", key="settings_numbers")
        st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", key="settings_norm")
        st.checkbox("–ê–≤—Ç–æ-—Ñ–∏–ª—å—Ç—Ä —Å–ª–∞–±—ã—Ö —Å–∞–π—Ç–æ–≤", key="settings_auto_filter", help="–°–∞–π—Ç—ã —Å –Ω–∏–∑–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã –≤ —Å–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö.")
        
        # === [–ò–ó–ú–ï–ù–ï–ù–ò–ï] –°–ü–ò–°–ö–ò –ü–ï–†–ï–ù–ï–°–ï–ù–´ –°–Æ–î–ê ===
        st.markdown("---")
        st.markdown("üõë **–ò—Å–∫–ª—é—á–µ–Ω–∏—è**")
        
        if "settings_excludes" not in st.session_state: st.session_state.settings_excludes = DEFAULT_EXCLUDE
        if "settings_stops" not in st.session_state: st.session_state.settings_stops = DEFAULT_STOPS

        st.text_area("–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–æ–º–µ–Ω—ã", height=100, key="settings_excludes", help="–î–æ–º–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–∞—Ä—Å–µ—Ä –ø—Ä–æ–ø—É—Å—Ç–∏—Ç —Å—Ä–∞–∑—É.")
        st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞", height=100, key="settings_stops", help="–°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–ø–∞–¥—É—Ç –≤ –∞–Ω–∞–ª–∏–∑.")
# ==========================================
    # –ë–õ–û–ö 1: –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
    # ==========================================
    if st.session_state.analysis_done and st.session_state.analysis_results:
        results = st.session_state.analysis_results
        
        d_score = results['my_score']['depth']
        w_score = results['my_score']['width']
        
        # –¶–≤–µ—Ç–∞ –±–∞–ª–ª–æ–≤
        w_color = "#2E7D32" if w_score >= 80 else ("#E65100" if w_score >= 50 else "#D32F2F")
        
        if 75 <= d_score <= 88:
            d_color = "#2E7D32"; d_status = "–ò–î–ï–ê–õ (–¢–æ–ø)"
        elif 88 < d_score <= 100:
            d_color = "#D32F2F"; d_status = "–ü–ï–†–ï–°–ü–ê–ú (–†–∏—Å–∫)"
        elif 55 <= d_score < 75:
            d_color = "#F9A825"; d_status = "–°—Ä–µ–¥–Ω—è—è"
        else:
            d_color = "#D32F2F"; d_status = "–ù–∏–∑–∫–∞—è"

        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        
        # –°—Ç–∏–ª–∏
        st.markdown("""
        <style>
            details > summary { list-style: none; }
            details > summary::-webkit-details-marker { display: none; }
            .details-card { background-color: #f8f9fa; border: 1px solid #e9ecef; border-radius: 8px; margin-bottom: 10px; }
            .card-summary { padding: 12px 15px; cursor: pointer; font-weight: 700; display: flex; justify-content: space-between; }
            .count-tag { background: #e5e7eb; padding: 2px 8px; border-radius: 10px; font-size: 12px; }
            .flat-card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; height: 340px; display: flex; flex-direction: column; }
            .flat-header { height: 50px; padding: 0 20px; font-weight: 700; border-bottom: 1px solid #f3f4f6; display: flex; align-items: center; justify-content: space-between; }
            .flat-content { flex-grow: 1; padding: 15px 20px; overflow-y: auto; font-size: 13px; line-height: 1.4; }
            .flat-footer { height: 150px; padding: 12px 20px; border-top: 1px solid #f3f4f6; background: #fafafa; }
            .flat-len-badge { padding: 2px 8px; border-radius: 4px; font-weight: 700; font-size: 10px; }
            .flat-miss-tag { border: 1px solid #fecaca; color: #991b1b; padding: 2px 6px; font-size: 11px; border-radius: 4px; margin: 2px; display: inline-block; }
        </style>
        """, unsafe_allow_html=True)

        # –í—ã–≤–æ–¥ –±–∞–ª–ª–æ–≤
        st.markdown(f"""
        <div style='display: flex; gap: 20px; flex-wrap: wrap; margin-bottom: 20px;'>
            <div style='flex: 1; background:{LIGHT_BG_MAIN}; padding:15px; border-radius:8px; border-left: 5px solid {w_color};'>
                <div style='font-size: 12px; color: #666;'>–®–ò–†–ò–ù–ê (–û—Ö–≤–∞—Ç —Ç–µ–º)</div>
                <div style='font-size: 24px; font-weight: bold; color: {w_color};'>{w_score}/100</div>
            </div>
            <div style='flex: 1; background:{LIGHT_BG_MAIN}; padding:15px; border-radius:8px; border-left: 5px solid {d_color};'>
                <div style='font-size: 12px; color: #666;'>–ì–õ–£–ë–ò–ù–ê (–¶–µ–ª—å: ~80)</div>
                <div style='font-size: 24px; font-weight: bold; color: {d_color};'>{d_score}/100 <span style='font-size:14px; font-weight:normal;'>({d_status})</span></div>
            </div>
        </div>
        """, unsafe_allow_html=True)

        # --- –†–ê–°–ß–ï–¢ META (–ß—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å –∏—Ö –ø–µ—Ä–≤—ã–º–∏) ---
        my_data_saved = st.session_state.get('saved_my_data')
        meta_res = None
        
        if 'raw_comp_data' in st.session_state and my_data_saved:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            s_meta = {
                'noindex': True, 'alt_title': False, 'numbers': False, 'norm': True, 
                'ua': "Mozilla/5.0", 'custom_stops': st.session_state.get('settings_stops', "").split()
            }
            meta_res = analyze_meta_gaps(st.session_state['raw_comp_data'], my_data_saved, s_meta)

        # --- –í–´–í–û–î META DASHBOARD (–ö–ê–†–¢–û–ß–ö–ò) ---
        if meta_res:
            st.markdown("### üß¨ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ Title, Description –∏ H1")
            
            # –•–µ–ª–ø–µ—Ä—ã –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏
            def check_len_status(text, type_key):
                length = len(text) if text else 0
                limits = {'Title': (30, 70), 'Description': (150, 250), 'H1': (20, 60)}
                mn, mx = limits.get(type_key, (0,0))
                if mn <= length <= mx: return length, "–•–û–†–û–®–û", "#059669", "#ECFDF5"
                return length, "–ü–õ–û–•–û", "#DC2626", "#FEF2F2"

            def render_flat_card(col, label, type_key, icon, txt, score, missing):
                length, status, col_txt, col_bg = check_len_status(txt, type_key)
                rel_col = "#10B981" if score >= 90 else ("#F59E0B" if score >= 50 else "#EF4444")
                
                miss_html = ""
                if missing:
                    tags = "".join([f'<span class="flat-miss-tag">{w}</span>' for w in missing[:10]])
                    miss_html = f"<div style='margin-top:5px;'>{tags}</div>"
                else:
                    miss_html = "<div style='color:#059669; font-weight:bold; margin-top:10px;'>‚úî –í—Å—ë –æ—Ç–ª–∏—á–Ω–æ</div>"

                html = f"""
                <div class="flat-card">
                    <div class="flat-header">
                        <div>{icon} {label}</div>
                        <span class="flat-len-badge" style="background:{col_bg}; color:{col_txt}">{length} –∑–Ω.</span>
                    </div>
                    <div class="flat-content">{txt if txt else '<span style="color:#ccc">–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö</span>'}</div>
                    <div class="flat-footer">
                        <div style="display:flex; justify-content:space-between; font-weight:bold; font-size:11px; color:#9ca3af;">
                            <span>–†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–¨</span> 
                            <span style="color:{rel_col}">{score}%</span>
                        </div>
                        <div style="width:100%; height:6px; background:#e5e7eb; border-radius:3px; margin-top:5px; overflow:hidden;">
                            <div style="width:{score}%; height:100%; background:{rel_col};"></div>
                        </div>
                        {miss_html}
                    </div>
                </div>
                """
                col.markdown(html, unsafe_allow_html=True)

            c1, c2, c3 = st.columns(3)
            m_s = meta_res['scores']; m_m = meta_res['missing']; m_d = meta_res['my_data']
            
            render_flat_card(c1, "Title", "Title", "üìë", m_d['Title'], m_s['title'], m_m['title'])
            render_flat_card(c2, "Description", "Description", "üìù", m_d['Description'], m_s['desc'], m_m['desc'])
            render_flat_card(c3, "H1 –ó–∞–≥–æ–ª–æ–≤–æ–∫", "H1", "#Ô∏è‚É£", m_d['H1'], m_s['h1'], m_m['h1'])
            
            st.markdown("<br>", unsafe_allow_html=True)

# 1. –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ï –Ø–î–†–û
        with st.expander("üõí –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ", expanded=True):
            if not st.session_state.get('orig_products') and not st.session_state.get('categorized_general'):
                st.info("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑.")
            else:
                # --- –§–£–ù–ö–¶–ò–Ø –ü–ï–†–ï–°–ß–ï–¢–ê (CALLBACK) ---
                def sync_semantics_with_stoplist():
                    # 1. –°—á–∏—Ç—ã–≤–∞–µ–º, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Å—Ç–∞–≤–∏–ª/–Ω–∞–ø–∏—Å–∞–ª –≤ –ø–æ–ª–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
                    raw_input = st.session_state.get('sensitive_words_input_final', "")
                    # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç (–º–Ω–æ–∂–µ—Å—Ç–≤–æ) –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞, –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
                    current_stop_set = set(w.strip().lower() for w in raw_input.split('\n') if w.strip())

                    # 2. –ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã–µ —Å–ø–∏—Å–∫–∏ –∏–∑ –ú–∞—Å—Ç–µ—Ä-—Å–ø–∏—Å–∫–æ–≤ (orig_...)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º: –µ—Å–ª–∏ —Å–ª–æ–≤–∞ –Ω–µ—Ç –≤ —Å—Ç–æ–ø-–ª–∏—Å—Ç–µ ‚Äî –æ–Ω–æ –∏–¥–µ—Ç –≤ —Ä–∞–±–æ—Ç—É
                    st.session_state.categorized_products = [w for w in st.session_state.orig_products if w.lower() not in current_stop_set]
                    st.session_state.categorized_services = [w for w in st.session_state.orig_services if w.lower() not in current_stop_set]
                    st.session_state.categorized_commercial = [w for w in st.session_state.orig_commercial if w.lower() not in current_stop_set]
                    st.session_state.categorized_geo = [w for w in st.session_state.orig_geo if w.lower() not in current_stop_set]
                    st.session_state.categorized_dimensions = [w for w in st.session_state.orig_dimensions if w.lower() not in current_stop_set]
                    st.session_state.categorized_general = [w for w in st.session_state.orig_general if w.lower() not in current_stop_set]

                    # 3. –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º (—á—Ç–æ–±—ã –º—É—Å–æ—Ä –Ω–µ –ø–æ–ø–∞–ª –≤ —Ç–µ–≥–∏)
                    all_active_products = st.session_state.categorized_products
                    if len(all_active_products) < 20:
                        st.session_state.auto_tags_words = all_active_products
                        st.session_state.auto_promo_words = []
                    else:
                        mid = math.ceil(len(all_active_products) / 2)
                        st.session_state.auto_tags_words = all_active_products[:mid]
                        st.session_state.auto_promo_words = all_active_products[mid:]
                    
                    st.toast("–°–ø–∏—Å–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã!", icon="‚úÖ")

                # --- –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –ö–ê–†–¢–û–ß–ï–ö ---
                c1, c2, c3 = st.columns(3)
                with c1: render_clean_block("–¢–æ–≤–∞—Ä—ã", "üß±", st.session_state.categorized_products)
                with c2: render_clean_block("–ì–µ–æ", "üåç", st.session_state.categorized_geo)
                with c3: render_clean_block("–ö–æ–º–º–µ—Ä—Ü–∏—è", "üí∞", st.session_state.categorized_commercial)
                
                c4, c5, c6 = st.columns(3)
                with c4: render_clean_block("–£—Å–ª—É–≥–∏", "üõ†Ô∏è", st.session_state.categorized_services)
                with c5: render_clean_block("–†–∞–∑–º–µ—Ä—ã/–ì–û–°–¢", "üìè", st.session_state.categorized_dimensions)
                with c6: render_clean_block("–û–±—â–∏–µ", "üìÇ", st.session_state.categorized_general)

                # --- –ë–õ–û–ö –°–¢–û–ü-–°–õ–û–í (–†–ï–î–ê–ö–¢–ò–†–£–ï–ú–´–ô) ---
                st.markdown("<br>", unsafe_allow_html=True)
                st.markdown("#### üõë –°—Ç–æ–ø-–ª–∏—Å—Ç")
                st.caption("–°—é–¥–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ø–∞–ª–∏ —Å–ª–æ–≤–∞ –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Å–ø–∏—Å–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤, –æ–Ω–∏ –Ω–µ –±—É–¥—É—Ç –≤–ª–∏—è—Ç—å –Ω–∞ —Ä–∞—Å—á–µ—Ç—ã –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ. –î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ –∏–ª–∏ —É–¥–∞–ª–∏—Ç–µ –ª–∏—à–Ω–∏–µ.")

                col_text, col_btn = st.columns([4, 1])
                
                with col_text:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º key, —á—Ç–æ–±—ã –∑–Ω–∞—á–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω—è–ª–æ—Å—å –≤ session_state
                    st.text_area(
                        "–°–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π",
                        height=150,
                        key="sensitive_words_input_final", 
                        label_visibility="collapsed"
                    )
                
                with col_btn:
                    st.write("") # –û—Ç—Å—Ç—É–ø
                    st.button(
                        "üîÑ –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∏ –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å", 
                        type="primary", 
                        use_container_width=True,
                        on_click=sync_semantics_with_stoplist
                    )
                    st.info("–£–¥–∞–ª–∏—Ç–µ —Å–ª–æ–≤–æ –∏–∑ —Å–ø–∏—Å–∫–∞ —Å–ª–µ–≤–∞, —á—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å –µ–≥–æ –≤ –≥—Ä—É–ø–ø—ã –≤—ã—à–µ.")

        # 2. –¢–ê–ë–õ–ò–¶–ê –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò
        with st.expander("üèÜ 4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–¢–∞–±–ª–∏—Ü–∞)", expanded=True):
            render_paginated_table(results['relevance_top'], "4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", "tbl_rel", 
                                   default_sort_col="–ü–æ–∑–∏—Ü–∏—è", default_sort_order="–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ", show_controls=False)

        st.markdown("<br>", unsafe_allow_html=True)
        st.caption("üëá –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")

        # 3. –ù–ê–ô–ú–ò–ù–ì
        with st.expander("üè∑Ô∏è –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ç–æ–≤–∞—Ä–æ–≤", expanded=False):
            if 'naming_table_df' in st.session_state and not st.session_state.naming_table_df.empty:
                st.dataframe(st.session_state.naming_table_df, use_container_width=True, hide_index=True)
            else:
                st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")

        # 4. –î–ï–¢–ê–õ–ò META (–¢–ê–ë–õ–ò–¶–ê) - –í–û–¢ –¢–£–¢ –ë–´–õ–ê –û–®–ò–ë–ö–ê
        with st.expander("üïµÔ∏è –ú–µ—Ç–∞-–¥–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤", expanded=False):
            # –í—Å—Ç–∞–≤–ª—è–µ–º –∑–∞—â–∏—Ç—É: –µ—Å–ª–∏ meta_res –Ω–µ—Ç, –Ω–µ —Å—Ç—Ä–æ–∏–º —Ç–∞–±–ª–∏—Ü—É
            if meta_res and 'detailed' in meta_res:
                df_meta_table = pd.DataFrame(meta_res['detailed'])
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É "–í–∞—à —Å–∞–π—Ç"
                my_row = pd.DataFrame([{
                    'URL': '–í–ê–® –°–ê–ô–¢', 
                    'Title': meta_res['my_data']['Title'], 
                    'Description': meta_res['my_data']['Description'], 
                    'H1': meta_res['my_data']['H1']
                }])
                df_meta_table = pd.concat([my_row, df_meta_table], ignore_index=True)
                
                st.dataframe(
                    df_meta_table, 
                    use_container_width=True, 
                    column_config={
                        "URL": st.column_config.LinkColumn("–°—Å—ã–ª–∫–∞"),
                        "Title": st.column_config.TextColumn("Title", width="medium"),
                        "Description": st.column_config.TextColumn("Description", width="large"),
                        "H1": st.column_config.TextColumn("H1", width="small"),
                    }
                )
            else:
                st.warning("–î–∞–Ω–Ω—ã–µ –ø–æ –º–µ—Ç–∞-—Ç–µ–≥–∞–º –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã (–≤–æ–∑–º–æ–∂–Ω–æ, –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ).")

        # 5. –£–ü–£–©–ï–ù–ù–ê–Ø –°–ï–ú–ê–ù–¢–ò–ö–ê
        high = results.get('missing_semantics_high', [])
        low = results.get('missing_semantics_low', [])
        
        if high or low:
            # –°—á–∏—Ç–∞–µ–º –æ–±—â—É—é —Å—É–º–º—É
            total_missing = len(high) + len(low)
            
            with st.expander(f"üß© –£–ø—É—â–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ ({total_missing})", expanded=False):
                # 1. –í–ê–ñ–ù–´–ï (–ú–µ–¥–∏–∞–Ω–∞ >= 1) - –°–∏–Ω—è—è –ø–ª–∞—à–∫–∞
                if high: 
                    words_high = ", ".join([x['word'] for x in high])
                    st.markdown(f"""
                    <div style='background:#EBF5FF; padding:12px; border-radius:8px; border:1px solid #BFDBFE; color:#1E40AF; margin-bottom:10px;'>
                        <div style='font-weight:bold; margin-bottom:4px;'>üî• –í–∞–∂–Ω—ã–µ (–ï—Å—Ç—å —É –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤):</div>
                        <div style='font-size:14px; line-height:1.5;'>{words_high}</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                # 2. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï (–ú–µ–¥–∏–∞–Ω–∞ < 1) - –°–µ—Ä–∞—è –ø–ª–∞—à–∫–∞
                if low: 
                    words_low = ", ".join([x['word'] for x in low])
                    st.markdown(f"""
                    <div style='background:#F8FAFC; padding:12px; border-radius:8px; border:1px solid #E2E8F0; color:#475569;'>
                        <div style='font-weight:bold; margin-bottom:4px;'>üî∏ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ (–í—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ä–µ–∂–µ):</div>
                        <div style='font-size:13px; line-height:1.5;'>{words_low}</div>
                    </div>
                    """, unsafe_allow_html=True)

# 6. –ì–õ–£–ë–ò–ù–ê (–ó–ê–ö–†–´–¢–û)
        with st.expander("üìâ 1. –ì–ª—É–±–∏–Ω–∞ (–î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞)", expanded=False):
            render_paginated_table(
                results['depth'], 
                "–ì–ª—É–±–∏–Ω–∞", 
                "tbl_depth_1", 
                default_sort_col="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è", 
                use_abs_sort_default=True
            )

        # 7. TF-IDF (–ó–ê–ö–†–´–¢–û)
        with st.expander("üßÆ 3. TF-IDF –ê–Ω–∞–ª–∏–∑", expanded=False):
            render_paginated_table(
                results['hybrid'], 
                "3. TF-IDF", 
                "tbl_hybrid", 
                default_sort_col="TF-IDF –¢–û–ü", 
                show_controls=False 
            )
# ==========================================
    # –ë–õ–û–ö 2: –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ï –ò –†–ê–°–ß–ï–¢
    # ==========================================
    if st.session_state.get('start_analysis_flag'):
        st.session_state.start_analysis_flag = False
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        settings = {
            'noindex': st.session_state.settings_noindex, 
            'alt_title': st.session_state.settings_alt, 
            'numbers': st.session_state.settings_numbers, 
            'norm': st.session_state.settings_norm, 
            'ua': st.session_state.settings_ua, 
            'custom_stops': st.session_state.settings_stops.split()
        }
        
        my_data, my_domain, my_serp_pos = None, "", 0
        current_input_type = st.session_state.get("my_page_source_radio")
        
        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –í–ê–®–ï–ô —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if current_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
                my_data = parse_page(st.session_state.my_url_input, settings, st.session_state.query_input)
                if not my_data: st.error("–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."); st.stop()
                my_domain = urlparse(st.session_state.my_url_input).netloc
        elif current_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            my_data = {'url': 'Local', 'domain': 'local', 'body_text': st.session_state.my_content_input, 'anchor_text': ''}

        st.session_state['saved_my_data'] = my_data 
            
        # 2. –°–±–æ—Ä –ö–ê–ù–î–ò–î–ê–¢–û–í
        candidates_pool = []
        current_source_val = st.session_state.get("competitor_source_radio")
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–µ—Ä–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (10 –∏–ª–∏ 20) –¥–ª—è –§–ò–ù–ê–õ–ê
        user_target_top_n = st.session_state.settings_top_n
        # –ê —Å–∫–∞—á–∏–≤–∞–µ–º –≤—Å–µ–≥–¥–∞ –ú–ê–ö–°–ò–ú–£–ú (30), —á—Ç–æ–±—ã –±—ã–ª–æ –∏–∑ —á–µ–≥–æ –≤—ã–±–∏—Ä–∞—Ç—å
        download_limit = 30 
        
        if "API" in current_source_val:
            if not ARSENKIN_TOKEN: st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç API —Ç–æ–∫–µ–Ω Arsenkin."); st.stop()
            with st.spinner(f"API Arsenkin (–ó–∞–ø—Ä–æ—Å –¢–æ–ø-30)..."):
                raw_top = get_arsenkin_urls(st.session_state.query_input, st.session_state.settings_search_engine, st.session_state.settings_region, ARSENKIN_TOKEN, depth_val=30)
                
                if not raw_top: st.stop()
                
                excl = [d.strip() for d in st.session_state.settings_excludes.split('\n') if d.strip()]
                agg_list = [
                    "avito", "ozon", "wildberries", "market.yandex", "tiu", "youtube", "vk.com", "yandex",
                    "leroymerlin", "petrovich", "satom", "pulscen", "blizko", "deal.by", "satu.kz", "prom.ua",
                    "wikipedia", "dzen", "rutube", "kino", "otzovik", "irecommend", "profi.ru", "zoon", "2gis",
                    "megamarket.ru", "lamoda.ru", "utkonos.ru", "vprok.ru", "allbiz.ru", "all-companies.ru",
                    "orgpage.ru", "list-org.com", "rusprofile.ru", "e-katalog.ru", "kufar.by", "wildberries.kz",
                    "ozon.kz", "kaspi.kz", "pulscen.kz", "allbiz.kz", "wildberries.uz", "olx.uz", "pulscen.uz",
                    "allbiz.uz", "wildberries.kg", "pulscen.kg", "allbiz.kg", "all.biz", "b2b-center.ru"
                ]
                excl.extend(agg_list)
                for res in raw_top:
                    dom = urlparse(res['url']).netloc.lower()
                    if my_domain and (my_domain in dom or dom in my_domain):
                        if my_serp_pos == 0 or res['pos'] < my_serp_pos: 
                            my_serp_pos = res['pos']
                    is_garbage = False
                    for x in excl:
                        if x.lower() in dom:
                            is_garbage = True
                            break
                    if is_garbage: continue
                    candidates_pool.append(res)
        else:
            raw_input_urls = st.session_state.get("persistent_urls", "")
            candidates_pool = [{'url': u.strip(), 'pos': i+1} for i, u in enumerate(raw_input_urls.split('\n')) if u.strip()]

        if not candidates_pool: st.error("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤."); st.stop()
        
        # 3. –°–ö–ê–ß–ò–í–ê–ù–ò–ï (–í—Å–µ—Ö 30)
        comp_data_valid = []
        with st.status(f"üïµÔ∏è –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (–í—Å–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidates_pool)})...", expanded=True) as status:
            with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
                futures = {
                    executor.submit(parse_page, item['url'], settings, st.session_state.query_input): item 
                    for item in candidates_pool
                }
                done_count = 0
                for f in concurrent.futures.as_completed(futures):
                    original_item = futures[f]
                    try:
                        res = f.result()
                        if res:
                            res['pos'] = original_item['pos']
                            comp_data_valid.append(res)
                    except: pass
                    done_count += 1
                    status.update(label=f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {done_count}/{len(candidates_pool)} | –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ: {len(comp_data_valid)}")

            comp_data_valid.sort(key=lambda x: x['pos'])
            # –°–Ω–∞—á–∞–ª–∞ –±–µ—Ä–µ–º –í–°–ï–•, –∫—Ç–æ —Å–∫–∞—á–∞–ª—Å—è (–¥–æ 30), –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
            data_for_graph = comp_data_valid[:download_limit]
            targets_for_graph = [{'url': d['url'], 'pos': d['pos']} for d in data_for_graph]

        # 5. –†–ê–°–ß–ï–¢ –ú–ï–¢–†–ò–ö (–î–í–û–ô–ù–û–ô –ü–†–û–ì–û–ù)
        with st.spinner("–ê–Ω–∞–ª–∏–∑ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è..."):
            
            # --- –≠–¢–ê–ü 1: –ß–µ—Ä–Ω–æ–≤–æ–π –ø—Ä–æ–≥–æ–Ω (–ø–æ –≤—Å–µ–º 30 —Å–∞–π—Ç–∞–º) ---
            # –≠—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ –∏ –Ω–∞–π—Ç–∏ –∞–Ω–æ–º–∞–ª–∏–∏
            results_full = calculate_metrics(data_for_graph, my_data, settings, my_serp_pos, targets_for_graph)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ü–û–õ–ù–´–ï –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ (—á—Ç–æ–±—ã –Ω–∞ –Ω–µ–º –±—ã–ª–∏ –≤—Å–µ)
            st.session_state['full_graph_data'] = results_full['relevance_top']
            
            # –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π –ø–æ –ø–æ–ª–Ω–æ–º—É —Å–ø–∏—Å–∫—É
            df_rel_check = results_full['relevance_top']
            good_urls, bad_urls_dicts, trend = analyze_serp_anomalies(df_rel_check)
            st.session_state['serp_trend_info'] = trend
            
            # --- –≠–¢–ê–ü 2: –û—Ç–±–æ—Ä —á–∏—Å—Ç–æ–≤—ã—Ö (–¢–æ–ø-10/20 –±–µ–∑ –º—É—Å–æ—Ä–∞) ---
            
# 1. –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–µ—Ö —Å–∞–π—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –≤ —Å–ø–∏—Å–∫–µ –ø–ª–æ—Ö–∏—Ö
            bad_urls_set = set(item['url'] for item in bad_urls_dicts)
            
            # === –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –§–ò–õ–¨–¢–†–ê–¶–ò–ò ===
            # –ï—Å–ª–∏ —ç—Ç–æ API - –º—ã —Ñ–∏–ª—å—Ç—Ä—É–µ–º –∏ —Ä–µ–∂–µ–º —Ç–æ–ø.
            # –ï—Å–ª–∏ —ç—Ç–æ –†–£–ß–ù–û–ô —Ä–µ–∂–∏–º - –º—ã –ù–ï —Ñ–∏–ª—å—Ç—Ä—É–µ–º (–¥–æ–≤–µ—Ä—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é).
            if "API" in current_source_val:
                clean_data_pool = [d for d in data_for_graph if d['url'] not in bad_urls_set]
                final_clean_data = clean_data_pool[:user_target_top_n]
            else:
                # –í —Ä—É—á–Ω–æ–º —Ä–µ–∂–∏–º–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï–• —Å–∫–∞—á–∞–Ω–Ω—ã—Ö, –Ω–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º "—Å–ª–∞–±—ã—Ö"
                final_clean_data = data_for_graph 
            
            # <--- –í–ê–ñ–ù–û: –°—Ç—Ä–æ–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–¥–µ—Ç –°–¢–†–û–ì–û –ü–û–°–õ–ï –±–ª–æ–∫–∞ if/else --->
            st.session_state['raw_comp_data'] = final_clean_data
            # ------------------------------------------------------------------

            final_clean_targets = [{'url': d['url'], 'pos': d['pos']} for d in final_clean_data]
            
            # 3. –§–ò–ù–ê–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ (–¢–æ–ª—å–∫–æ –ø–æ —ç–ª–∏—Ç–µ)
            results_final = calculate_metrics(final_clean_data, my_data, settings, my_serp_pos, final_clean_targets)
            
# 3. –§–ò–ù–ê–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ (–¢–æ–ª—å–∫–æ –ø–æ —ç–ª–∏—Ç–µ)
            results_final = calculate_metrics(final_clean_data, my_data, settings, my_serp_pos, final_clean_targets)
            st.session_state.analysis_results = results_final
            
            # --- –û—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ (–Ω–µ–π–º–∏–Ω–≥, —Å–µ–º–∞–Ω—Ç–∏–∫–∞) ---
            naming_df = calculate_naming_metrics(final_clean_data, my_data, settings)
            st.session_state.naming_table_df = naming_df 
            st.session_state.ideal_h1_result = analyze_ideal_name(final_clean_data)
            st.session_state.analysis_done = True
            
            # ==========================================
            # üî• –ë–õ–û–ö: –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –°–ï–ú–ê–ù–¢–ò–ö–ò (–°–¢–†–û–ì–û –ó–î–ï–°–¨)
            # ==========================================
            words_to_check = [x['word'] for x in results_final.get('missing_semantics_high', [])]
            
            # –ï—Å–ª–∏ "–≤–∞–∂–Ω—ã—Ö" —Å–ª–æ–≤ –º–∞–ª–æ, –±–µ—Ä–µ–º –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ, —á—Ç–æ–±—ã –∑–∞–ø–æ–ª–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã
            if len(words_to_check) < 5:
                words_to_check.extend([x['word'] for x in results_final.get('missing_semantics_low', [])[:20]])

            if not words_to_check:
                st.session_state.categorized_products = []; st.session_state.categorized_services = []
                st.session_state.categorized_commercial = []; st.session_state.categorized_dimensions = []
            else:
                with st.spinner("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏..."):
                    categorized = classify_semantics_with_api(words_to_check, YANDEX_DICT_KEY)
                
                st.session_state.categorized_products = categorized['products']
                st.session_state.categorized_services = categorized['services']
                st.session_state.categorized_commercial = categorized['commercial']
                st.session_state.categorized_geo = categorized['geo']
                st.session_state.categorized_dimensions = categorized['dimensions']
                st.session_state.categorized_general = categorized['general']
                st.session_state.categorized_sensitive = categorized['sensitive']

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—ã
                st.session_state.orig_products = categorized['products'] + categorized['sensitive']
                st.session_state.orig_services = categorized['services'] + categorized['sensitive']
                st.session_state.orig_commercial = categorized['commercial'] + categorized['sensitive']
                st.session_state.orig_geo = categorized['geo'] + categorized['sensitive']
                st.session_state.orig_dimensions = categorized['dimensions'] + categorized['sensitive']
                st.session_state.orig_general = categorized['general'] + categorized['sensitive']
                
                st.session_state['sensitive_words_input_final'] = "\n".join(categorized['sensitive'])

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
            all_found_products = st.session_state.categorized_products
            count_prods = len(all_found_products)
            if count_prods < 20:
                st.session_state.auto_tags_words = all_found_products
                st.session_state.auto_promo_words = []
            else:
                half_count = int(math.ceil(count_prods / 2))
                st.session_state.auto_tags_words = all_found_products[:half_count]
                st.session_state.auto_promo_words = all_found_products[half_count:]
            
            st.session_state['tags_products_edit_final'] = "\n".join(st.session_state.auto_tags_words)
            st.session_state['promo_keywords_area_final'] = "\n".join(st.session_state.auto_promo_words)
            # ==========================================
            # –ö–û–ù–ï–¶ –ë–õ–û–ö–ê –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò
            # ==========================================
            
            
            # === –£–ú–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø (Smart Filter Logic) ===
            
            # 1. –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–Ω–æ–º–∞–ª–∏–π
            if "API" in current_source_val and 'full_graph_data' in st.session_state:
                df_rel_check = st.session_state['full_graph_data']
            else:
                df_rel_check = st.session_state.analysis_results['relevance_top']
            
            # 2. –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π
            good_urls, bad_urls_dicts, trend = analyze_serp_anomalies(df_rel_check)
            st.session_state['serp_trend_info'] = trend
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞
            is_filter_enabled = st.session_state.get("settings_auto_filter", True)
            
            def get_strict_key(u):
                if not u: return ""
                return str(u).lower().strip().replace("https://", "").replace("http://", "").replace("www.", "").rstrip('/')

            final_clean_text = ""
            
            # --- –õ–û–ì–ò–ö–ê –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø ---
            if is_filter_enabled and bad_urls_dicts:
                # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–ª–æ—Ö–∏—Ö
                st.session_state['detected_anomalies'] = bad_urls_dicts
                
                blacklist_keys = set()
                excluded_display_list = []
                for item in bad_urls_dicts:
                    raw_u = item.get('url', '')
                    if raw_u:
                        blacklist_keys.add(get_strict_key(raw_u))
                        excluded_display_list.append(str(raw_u).strip())
                
                st.session_state['excluded_urls_auto'] = "\n".join(excluded_display_list)
                
                # 2. –°–æ–±–∏—Ä–∞–µ–º —Ö–æ—Ä–æ—à–∏—Ö
                clean_active_list = []
                seen_keys = set()
                for u in good_urls:
                    key = get_strict_key(u)
                    if key and key not in blacklist_keys and key not in seen_keys:
                        clean_active_list.append(str(u).strip())
                        seen_keys.add(key)
                
                final_clean_text = "\n".join(clean_active_list)
                st.toast(f"–§–∏–ª—å—Ç—Ä —Å—Ä–∞–±–æ—Ç–∞–ª. –ò—Å–∫–ª—é—á–µ–Ω–æ: {len(blacklist_keys)}", icon="‚úÇÔ∏è")
            
            else:
                # –§–∏–ª—å—Ç—Ä –≤—ã–∫–ª—é—á–µ–Ω –∏–ª–∏ –ø–ª–æ—Ö–∏—Ö –Ω–µ—Ç - –±–µ—Ä–µ–º –≤—Å—ë
                clean_all = []
                seen_all = set()
                combined_pool = good_urls + [x['url'] for x in (bad_urls_dicts or [])]
                for u in combined_pool:
                    key = get_strict_key(u)
                    if key and key not in seen_all:
                        clean_all.append(str(u).strip())
                        seen_all.add(key)
                
                final_clean_text = "\n".join(clean_all)
                # –ß–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—ã–µ –æ—à–∏–±–∫–∏
                st.session_state.pop('excluded_urls_auto', None)
                st.session_state.pop('detected_anomalies', None)

            # === –§–ò–ù–ê–õ–¨–ù–ê–Ø –ó–ê–ü–ò–°–¨ –ò –ü–ï–†–ï–ó–ê–ì–†–£–ó–ö–ê ===
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –í–†–ï–ú–ï–ù–ù–£–Æ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
            st.session_state['temp_update_urls'] = final_clean_text
            
            # –°—Ç–∞–≤–∏–º —Ñ–ª–∞–≥ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–∞–¥–∏–æ-–∫–Ω–æ–ø–∫–∏
            st.session_state['force_radio_switch'] = True
            
            # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É, —á—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –°–í–ï–†–•–£
            st.rerun()

# ------------------------------------------
# TAB 2: WHOLESALE GENERATOR (COMBINED)
# ------------------------------------------
with tab_wholesale_main:
    st.header("üè≠ –ï–¥–∏–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    
    # ==========================================
    # 0. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• (–ò–ó –¢–ï–ö–£–©–ï–ì–û –°–û–°–¢–û–Ø–ù–ò–Ø)
    # ==========================================
    cat_products = st.session_state.get('categorized_products', [])
    cat_services = st.session_state.get('categorized_services', [])
    
    # 1. –î–ª—è –¢–µ–≥–æ–≤ –∏ –ü—Ä–æ–º–æ (–°–∞–π–¥–±–∞—Ä –∏—Å–∫–ª—é—á–µ–Ω)
    structure_keywords = cat_products + cat_services
    count_struct = len(structure_keywords)

    if 'auto_tags_words' in st.session_state and st.session_state.auto_tags_words:
         tags_list_source = st.session_state.auto_tags_words
         promo_list_source = st.session_state.auto_promo_words
    else:
         if count_struct > 0:
            if count_struct < 10:
                tags_list_source = structure_keywords
                promo_list_source = []
            else:
                # –î–µ–ª–∏–º –≤—Å–µ–≥–¥–∞ –ø–æ–ø–æ–ª–∞–º (–¢–µ–≥–∏ / –ü—Ä–æ–º–æ), –°–∞–π–¥–±–∞—Ä –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
                mid = math.ceil(count_struct / 2)
                tags_list_source = structure_keywords[:mid]
                promo_list_source = structure_keywords[mid:]
         else:
             tags_list_source = []
             promo_list_source = []
    
    # –°–∞–π–¥–±–∞—Ä –≤—Å–µ–≥–¥–∞ –ø—É—Å—Ç–æ–π
    sidebar_default_text = ""

    tags_default_text = ", ".join(tags_list_source)
    promo_default_text = ", ".join(promo_list_source)

    # 2. –î–ª—è –¢–∞–±–ª–∏—Ü (–†–∞–∑–º–µ—Ä—ã/–ì–û–°–¢)
    cat_dimensions = st.session_state.get('categorized_dimensions', [])
    tech_context_default = ", ".join(cat_dimensions) if cat_dimensions else ""

    # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ö–æ–º–º–µ—Ä—Ü–∏–∏/–û–±—â–∏—Ö –∏ –ì–ï–û
    cat_commercial = st.session_state.get('categorized_commercial', [])
    cat_general = st.session_state.get('categorized_general', [])
    cat_geo = st.session_state.get('categorized_geo', [])
    
    # –ò–°–ö–õ–Æ–ß–ê–ï–ú –ì–ï–û –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    text_context_list_raw = cat_commercial + cat_general
    text_context_default = ", ".join(text_context_list_raw)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ñ–æ–ª—Ç –¥–ª—è –ì–ï–û –±–ª–æ–∫–∞
    geo_context_default = ", ".join(cat_geo)

    # --- –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ê–ö–¢–ò–í–ù–û–°–¢–ò –ú–û–î–£–õ–ï–ô ---
    auto_check_text = bool(text_context_list_raw)
    auto_check_tags = bool(tags_list_source)
    auto_check_tables = bool(cat_dimensions)
    auto_check_promo = bool(promo_list_source)
    auto_check_geo = bool(cat_geo)

    # ==========================================
    # 1. –í–í–û–î–ù–´–ï –î–ê–ù–ù–´–ï
    # ==========================================
    with st.container(border=True):
        st.subheader("1. –ò—Å—Ç–æ—á–Ω–∏–∫ –∏ –î–æ—Å—Ç—É–ø—ã")
        
        col_source, col_key = st.columns([3, 1])
        
        use_manual_html = st.checkbox("üìù –í—Å—Ç–∞–≤–∏—Ç—å HTML –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", key="cb_manual_html_mode", value=False)
        
        with col_source:
            if use_manual_html:
                manual_html_source = st.text_area(
                    "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (HTML)", 
                    height=200, 
                    placeholder="<html>...</html>", 
                    help="–°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Å—é–¥–∞ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."
                )
                main_category_url = None
            else:
                main_category_url = st.text_input(
                    "URL –ö–∞—Ç–µ–≥–æ—Ä–∏–∏", 
                    placeholder="https://site.ru/catalog/...", 
                    help="–°–∫—Ä–∏–ø—Ç —Å–æ–±–µ—Ä–µ—Ç —Ç–æ–≤–∞—Ä—ã —Å —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
                )
                manual_html_source = None

        with col_key:
            try:
                key_from_secrets = st.secrets["GEMINI_KEY"]
            except (FileNotFoundError, KeyError):
                key_from_secrets = ""

            default_key = st.session_state.get('gemini_key_cache', key_from_secrets)
            gemini_api_key = st.text_input("Google Gemini API Key", value=default_key, type="password")

    # ==========================================
    # 2. –í–´–ë–û–† –ú–û–î–£–õ–ï–ô
    # ==========================================
    st.subheader("2. –ö–∞–∫–∏–µ –±–ª–æ–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º?")
    st.info("‚ÑπÔ∏è **–ê–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞:** –ì–∞–ª–æ—á–∫–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ç–∞–º, –≥–¥–µ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—à–ª–∏—Å—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–ª–æ–≤–∞.")
    col_ch1, col_ch2, col_ch3, col_ch4, col_ch5, col_ch6 = st.columns(6)
    
    with col_ch1: use_text = st.checkbox("ü§ñ AI –¢–µ–∫—Å—Ç—ã", value=auto_check_text)
    with col_ch2: use_tags = st.checkbox("üè∑Ô∏è –¢–µ–≥–∏", value=auto_check_tags)
    with col_ch3: use_tables = st.checkbox("üß© –¢–∞–±–ª–∏—Ü—ã", value=auto_check_tables)
    with col_ch4: use_promo = st.checkbox("üî• –ü—Ä–æ–º–æ", value=auto_check_promo)
    
    # –û–¢–ö–õ–Æ–ß–ê–ï–ú –°–ê–ô–î–ë–ê–† –ó–î–ï–°–¨
    with col_ch5: use_sidebar = st.checkbox("üìë –°–∞–π–¥–±–∞—Ä (–û—Ç–∫–ª)", value=False, disabled=True, key="sidebar_disabled_ui")
    
    with col_ch6: use_geo = st.checkbox("üåç –ì–µ–æ-–±–ª–æ–∫", value=auto_check_geo)

    # ==========================================
    # 3. –ù–ê–°–¢–†–û–ô–ö–ò –ú–û–î–£–õ–ï–ô
    # ==========================================
    global_tags_list = []
    global_promo_list = []
    global_sidebar_list = []
    global_geo_list = []
    tags_file_content = ""
    table_prompts = []
    df_db_promo = None
    promo_title = "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º"
    sidebar_content = ""
    text_context_final_list = []
    tech_context_final_str = ""
    
    # –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–ª–æ–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞ (–ø–æ –¥–µ—Ñ–æ–ª—Ç—É 5)
    num_text_blocks_val = 5 

    if any([use_text, use_tags, use_tables, use_promo, use_sidebar, use_geo]):
        st.subheader("3. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥—É–ª–µ–π")

        # --- AI TEXT ---
        if use_text:
            with st.container(border=True):
                st.markdown("#### ü§ñ 1. AI –¢–µ–∫—Å—Ç—ã")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–±–æ—Ä –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–ª–æ–∫–æ–≤
                col_txt1, col_txt2 = st.columns([1, 4])
                with col_txt1:
                    num_text_blocks_val = st.selectbox("–ö–æ–ª-–≤–æ –±–ª–æ–∫–æ–≤", [1, 2, 3, 4, 5], index=4, key="sb_num_blocks")
                
                with col_txt2:
                    ai_words_input = st.text_area(
                        "–°–ª–æ–≤–∞ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è (–ö–æ–º–º–µ—Ä—Ü–∏—è + –û–±—â–∏–µ)", 
                        value=text_context_default, 
                        height=100, 
                        key="ai_text_context_editable",
                        help="–≠—Ç–∏ —Å–ª–æ–≤–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –ø–æ—Å—Ç–∞—Ä–∞–µ—Ç—Å—è –≤–Ω–µ–¥—Ä–∏—Ç—å –≤ —Ç–µ–∫—Å—Ç."
                    )
                
                text_context_final_list = [x.strip() for x in re.split(r'[,\n]+', ai_words_input) if x.strip()]

        # --- TAGS ---
        if use_tags:
            with st.container(border=True):
                st.markdown("#### üè∑Ô∏è 2. –¢–µ–≥–∏")
                kws_input_tags = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=tags_default_text, 
                    height=100, 
                    key="kws_tags_auto"
                )
                global_tags_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_tags) if x.strip()]
                if not global_tags_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                col_t1, col_t2 = st.columns([1, 2])
                with col_t1: u_manual = st.checkbox("–°–≤–æ—è –±–∞–∑–∞ —Å—Å—ã–ª–æ–∫ (.txt)", key="cb_tags_vert")
                with col_t2:
                    default_tags_path = "data/links_base.txt"
                    if not u_manual and os.path.exists(default_tags_path):
                        st.success(f"‚úÖ –ë–∞–∑–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (`links_base.txt`)")
                        with open(default_tags_path, "r", encoding="utf-8") as f: tags_file_content = f.read()
                    elif u_manual:
                        up_t = st.file_uploader("–§–∞–π–ª .txt", type=["txt"], key="up_tags_vert", label_visibility="collapsed")
                        if up_t: tags_file_content = up_t.getvalue().decode("utf-8")
                    else: st.error("‚ùå –§–∞–π–ª –±–∞–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω!")

# --- –§–£–ù–ö–¶–ò–Ø –ì–õ–£–ë–û–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê –ö–û–ù–¢–ï–ö–°–¢–ê –î–õ–Ø –¢–ê–ë–õ–ò–¶ ---
        def generate_context_aware_headers(count, query, dimensions_list, general_list):
            """
            –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ò –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (—Ä–∞–∑–º–µ—Ä—ã, –æ–±—â–∏–µ), 
            —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —Ç–∏–ø—ã —Ç–∞–±–ª–∏—Ü –Ω—É–∂–Ω—ã.
            """
            query_lower = query.lower()
            
            # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–∫–∏ —Å–ª–æ–≤ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            dims_str = " ".join(dimensions_list).lower()
            gen_str = " ".join(general_list).lower()
            full_context = f"{dims_str} {gen_str} {query_lower}"
            
            # --- 1. –î–ï–¢–ï–ö–¢–û–†–´ –°–ò–ì–ù–ê–õ–û–í (–ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Å–µ–º–∞–Ω—Ç–∏–∫–µ) ---
            
            # –ü—Ä–∏–∑–Ω–∞–∫ —Ä–∞–∑–º–µ—Ä–æ–≤: –µ—Å—Ç—å —Ü–∏—Ñ—Ä—ã —Å '—Ö' (10—Ö20), —Å–ª–æ–≤–∞ –º–º, –∫–≥, —Ç–æ–Ω–Ω–∞, —Ä–∞–∑–º–µ—Ä
            has_sizes_signal = (
                len(dimensions_list) > 0 or 
                bool(re.search(r'\d+[x—Ö*]\d+', full_context)) or 
                any(x in full_context for x in ['—Ä–∞–∑–º–µ—Ä', '–≥–∞–±–∞—Ä–∏—Ç', '—Ç–æ–ª—â–∏–Ω', '–¥–∏–∞–º–µ—Ç—Ä', '—Ä–∞—Å–∫—Ä–æ–π', '–≤–µ—Å', '–º–∞—Å—Å'])
            )
            
            # –ü—Ä–∏–∑–Ω–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤: –ì–û–°–¢, –û–°–¢, –¢–£, DIN, AISI
            has_gost_signal = any(x in full_context for x in ['–≥–æ—Å—Ç', 'din', 'aisi', 'astm', '—Ç—É ', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç'])
            
            # –ü—Ä–∏–∑–Ω–∞–∫ –º–∞—Ä–æ–∫/–º–∞—Ç–µ—Ä–∏–∞–ª–∞: —Å—Ç–∞–ª—å, —Å–ø–ª–∞–≤, –º–∞—Ä–∫–∞, —Å—Ç.3, 09–≥2—Å
            has_grade_signal = any(x in full_context for x in ['–º–∞—Ä–∫', '—Å–ø–ª–∞–≤', '—Å—Ç–∞–ª—å', '—Å—Ç.', '–º–∞—Ç–µ—Ä–∏–∞–ª', '—Ö–∏–º–∏—á', '—Å–æ—Å—Ç–∞–≤'])
            
            # –ü—Ä–∏–∑–Ω–∞–∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è: –¥–ª—è —á–µ–≥–æ, —Å—Ñ–µ—Ä–∞, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            has_usage_signal = any(x in full_context for x in ['–ø—Ä–∏–º–µ–Ω–µ–Ω', '—Å—Ñ–µ—Ä', '–Ω–∞–∑–Ω–∞—á–µ–Ω', '–∏—Å–ø–æ–ª—å–∑'])

            # --- 2. –°–ë–û–†–ö–ê –û–ß–ï–†–ï–î–ò (PRIORITY QUEUE) ---
            # –ú—ã —Ä–∞—Å—Å—Ç–∞–≤–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—ã –≤ –ø–æ—Ä—è–¥–∫–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
            priority_stack = []
            
            # –ï—Å–ª–∏ —ç—Ç–æ –º–µ—Ç–∞–ª–ª–æ–ø—Ä–æ–∫–∞—Ç (–µ—Å—Ç—å –º–∞—Ä–∫–∏/—Å–ø–ª–∞–≤—ã), –æ–±—ã—á–Ω–æ —Å–Ω–∞—á–∞–ª–∞ —Å—Ç–∞–≤—è—Ç –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–ª–∏ –ú–∞—Ä–∫–∏
            if has_grade_signal:
                priority_stack.append("–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã")
                
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã - —ç—Ç–æ —Å—É–ø–µ—Ä –≤–∞–∂–Ω–æ, —Å—Ç–∞–≤–∏–º –≤ –Ω–∞—á–∞–ª–æ
            if has_sizes_signal:
                # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –ú–∞—Ä–∫–∏, —Ç–æ –†–∞–∑–º–µ—Ä—ã –≤—Ç–æ—Ä—ã–º–∏. –ï—Å–ª–∏ –Ω–µ—Ç - –ø–µ—Ä–≤—ã–º–∏.
                priority_stack.append("–¢–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤")
                
            # –ï—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ì–û–°–¢–æ–≤
            if has_gost_signal:
                priority_stack.append("–ì–û–°–¢—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã")
                
            # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—Ä–æ —Ö–∏–º —Å–æ—Å—Ç–∞–≤ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ)
            if "—Ö–∏–º" in full_context and "—Å–æ—Å—Ç–∞–≤" in full_context:
                 # –í—Å—Ç–∞–≤–ª—è–µ–º "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤" –≤–º–µ—Å—Ç–æ "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã" –∏–ª–∏ —Ä—è–¥–æ–º
                 if "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã" in priority_stack:
                     idx = priority_stack.index("–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã")
                     priority_stack.insert(idx+1, "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤")
                 else:
                     priority_stack.append("–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤")

            # --- 3. –ó–ê–ü–û–õ–ù–ï–ù–ò–ï –ü–£–°–¢–û–¢ (DEFAULTS) ---
            # –ï—Å–ª–∏ –º—ã –≤—ã–±—Ä–∞–ª–∏ 5 —Ç–∞–±–ª–∏—Ü, –∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞—à–ª–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ 2, –Ω—É–∂–Ω–æ –¥–æ–±–∏—Ç—å –æ—Å—Ç–∞–ª—å–Ω–æ–µ
            defaults = [
                "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞
                "–°–≤–æ–π—Å—Ç–≤–∞",
                "–°—Ñ–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è",
                "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–¥–µ–ª–∏—è",
                "–ê–Ω–∞–ª–æ–≥–∏"
            ]
            
            final_headers = []
            # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ, —á—Ç–æ –Ω–∞—à–ª–∏ —É–º–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
            for p in priority_stack:
                if p not in final_headers: final_headers.append(p)
            
            # –î–æ–±–∏–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏
            for d in defaults:
                if d not in final_headers: final_headers.append(d)
                
            # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –≤—Å—ë —Ä–∞–≤–Ω–æ –º–∞–ª–æ (—Ä–µ–¥–∫–∏–π —Å–ª—É—á–∞–π)
            while len(final_headers) < count:
                final_headers.append("–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏")
                
            return final_headers[:count]

        # --- –ë–õ–û–ö –ò–ù–¢–ï–†–§–ï–ô–°–ê TABLES ---
        if use_tables:
            with st.container(border=True):
                st.markdown("#### üß© 3. –¢–∞–±–ª–∏—Ü—ã")
                
                # –î–ê–ù–ù–´–ï –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê (–ë–µ—Ä–µ–º –∏–∑ session_state, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞)
                raw_query = st.session_state.get('query_input', '')
                found_dims = st.session_state.get('categorized_dimensions', []) # –°–ª–æ–≤–∞—Ä—å —Ä–∞–∑–º–µ—Ä–æ–≤
                found_general = st.session_state.get('categorized_general', []) # –°–ª–æ–≤–∞—Ä—å –æ–±—â–∏—Ö —Å–ª–æ–≤
                
                col_ctx, col_cnt = st.columns([3, 1]) 
                
                with col_ctx:
                    tech_context_final_str = st.text_area(
                        "–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ç–∞–±–ª–∏—Ü (–ú–∞—Ä–∫–∏, –ì–û–°–¢, –†–∞–∑–º–µ—Ä—ã)", 
                        value=tech_context_default, # –ó–¥–µ—Å—å –ª–µ–∂–∞—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
                        height=68, 
                        key="table_context_editable",
                        help="–≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–º–æ–≥—É—Ç AI —Å–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É."
                    )
                
                with col_cnt:
                    cnt_options = [1, 2, 3, 4, 5]
                    cnt = st.selectbox("–ö–æ–ª-–≤–æ —Ç–∞–±–ª–∏—Ü", cnt_options, index=1, key="num_tbl_vert_select")

                # --- –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê ---
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢–û–ì–û, –ß–¢–û –ù–ê–®–õ–ò –í –°–ï–ú–ê–ù–¢–ò–ö–ï
                smart_headers_list = generate_context_aware_headers(cnt, raw_query, found_dims, found_general)

                table_presets = [
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–°–≤–æ–π—Å—Ç–≤–∞", "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–¥–µ–ª–∏—è",
                    "–û–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ", "–¢–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤", "–°–æ—Ä—Ç–∞–º–µ–Ω—Ç",
                    "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤", "–§–∏–∑–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞", "–ú–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞",
                    "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã", "–°–æ—Å—Ç–∞–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞", "–ì–û–°–¢—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã",
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –ì–û–°–¢", "–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ",
                    "–°—Ñ–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", "–£—Å–ª–æ–≤–∏—è —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏", "–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è",
                    "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", "–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏", "–ê–Ω–∞–ª–æ–≥–∏",
                    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π", "–†–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç–∏"
                ]
                
                table_prompts = []
                st.write("") 
                
                cols = st.columns(cnt)
                
                for i, col in enumerate(cols):
                    with col:
                        st.caption(f"**–¢–∞–±–ª–∏—Ü–∞ {i+1}**")
                        
                        # –ê–≤—Ç–æ-–≤—ã–±–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
                        suggested_topic = smart_headers_list[i]
                        
                        try: default_idx = table_presets.index(suggested_topic)
                        except: default_idx = 0
                        
                        is_manual = st.checkbox("–°–≤–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫", key=f"cb_tbl_manual_{i}")
                        
                        if is_manual:
                            selected_topic = st.text_input(
                                f"–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª. {i+1}", value="", 
                                key=f"tbl_topic_custom_{i}", label_visibility="collapsed"
                            )
                            if not selected_topic.strip(): selected_topic = "–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏" 
                        else:
                            selected_topic = st.selectbox(
                                f"–¢–µ–º–∞ —Ç–∞–±–ª. {i+1}", 
                                table_presets, 
                                index=default_idx, # <--- –£–ú–ù–´–ô –ò–ù–î–ï–ö–°
                                key=f"tbl_topic_select_{i}",
                                label_visibility="collapsed"
                            )
                        
                        table_prompts.append(selected_topic)

# --- PROMO (–° –ê–ù–ê–õ–ò–ó–û–ú –ö–û–ú–ú–ï–†–ß–ï–°–ö–ò–• –§–ê–ö–¢–û–†–û–í) ---
        if use_promo:
            with st.container(border=True):
                st.markdown("#### üî• 4. –ü—Ä–æ–º–æ-–±–ª–æ–∫")
                
                kws_input_promo = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=promo_default_text, 
                    height=100, 
                    key="kws_promo_auto"
                )
                global_promo_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_promo) if x.strip()]
                if not global_promo_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                
                col_p1, col_p2 = st.columns([1, 2])
                with col_p1:
                    promo_presets = [
                        "–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ", "–ü–æ—Ö–æ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã", "–í–∞—Å –º–æ–∂–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å",
                        "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º", "–î—Ä—É–≥–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", "–í–∞–º –º–æ–∂–µ—Ç –ø—Ä–∏–≥–æ–¥–∏—Ç—å—Å—è",
                        "–¢–∞–∫–∂–µ –≤ —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ", "–° —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º –ø–æ–∫—É–ø–∞—é—Ç", "–ß–∞—Å—Ç–æ –ø–æ–∫—É–ø–∞—é—Ç –≤–º–µ—Å—Ç–µ",
                        "–°–æ–ø—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–≤–∞—Ä—ã", "–•–∏—Ç—ã –ø—Ä–æ–¥–∞–∂", "–í—ã–±–æ—Ä –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π",
                        "–õ–∏–¥–µ—Ä—ã —Å–ø—Ä–æ—Å–∞", "–ü–æ–ø—É–ª—è—Ä–Ω–æ–µ —Å–µ–π—á–∞—Å", "–¢–æ–ø —Ç–æ–≤–∞—Ä–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏",
                        "–õ—É—á—à–∞—è —Ü–µ–Ω–∞", "–°–ø–µ—Ü–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", "–£—Å–ø–µ–π—Ç–µ –∑–∞–∫–∞–∑–∞—Ç—å",
                        "–ù–µ –∑–∞–±—É–¥—å—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å", "–í—ã –Ω–µ–¥–∞–≤–Ω–æ —Å–º–æ—Ç—Ä–µ–ª–∏"
                    ]

                    # --- –õ–û–ì–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê –ö–û–ú–ú–ï–†–¶–ò–ò ---
                    # –ë–µ—Ä–µ–º –∑–∞–ø—Ä–æ—Å + —Å–ø–∏—Å–æ–∫ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ (Tab 1)
                    raw_query = st.session_state.get('query_input', '').lower()
                    comm_words = st.session_state.get('categorized_commercial', [])
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                    comm_context = f"{raw_query} {' '.join(comm_words)}".lower()
                    
                    target_header = "–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ" # –î–µ—Ñ–æ–ª—Ç (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π)

                    # 1. –Ø–≤–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è (–µ—Å—Ç—å —Å–ª–æ–≤–∞ '—Ü–µ–Ω–∞', '–∫—É–ø–∏—Ç—å' –≤ —Å–µ–º–∞–Ω—Ç–∏–∫–µ –∏–ª–∏ –∑–∞–ø—Ä–æ—Å–µ)
                    is_commercial = any(x in comm_context for x in ["–∫—É–ø–∏—Ç—å", "—Ü–µ–Ω–∞", "–∑–∞–∫–∞–∑", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–ø—Ä–∞–π—Å", "–º–∞–≥–∞–∑–∏–Ω", "–∫–æ—Ä–∑–∏–Ω–∞"])
                    
                    # 2. –ê–∫—Ü–∏–æ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
                    is_promo = any(x in comm_context for x in ["–∞–∫—Ü–∏—è", "—Å–∫–∏–¥–∫", "—Ä–∞—Å–ø—Ä–æ–¥–∞–∂", "–≤—ã–≥–æ–¥–Ω"])
                    
                    # 3. –†–µ–π—Ç–∏–Ω–≥–æ–≤—ã–µ —Å–ª–æ–≤–∞
                    is_top = any(x in comm_context for x in ["—Ç–æ–ø", "–ª—É—á—à", "—Ä–µ–π—Ç–∏–Ω–≥", "–ø–æ–ø—É–ª—è—Ä–Ω"])

                    if is_promo:
                        target_header = "–°–ø–µ—Ü–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"
                    elif is_top:
                        target_header = "–õ–∏–¥–µ—Ä—ã —Å–ø—Ä–æ—Å–∞"
                    elif is_commercial:
                        # –ï—Å–ª–∏ —ç—Ç–æ —è–≤–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è, –ª—É—á—à–µ "–ü–æ–∫—É–ø–∞—é—Ç –≤–º–µ—Å—Ç–µ" –∏–ª–∏ "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º"
                        target_header = "–° —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º –ø–æ–∫—É–ø–∞—é—Ç"
                    
                    try: promo_smart_idx = promo_presets.index(target_header)
                    except: promo_smart_idx = 0

                    use_custom_header = st.checkbox("–í–≤–µ—Å—Ç–∏ —Å–≤–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫", key="cb_custom_header")
                    
                    if use_custom_header:
                        promo_title = st.text_input("–í–∞—à –∑–∞–≥–æ–ª–æ–≤–æ–∫", placeholder="–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ", key="pr_tit_vert")
                    else:
                        promo_title = st.selectbox(
                            "–í–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–≥–æ–ª–æ–≤–∫–∞", 
                            promo_presets, 
                            index=promo_smart_idx, # <--- –£–ú–ù–´–ô –í–´–ë–û–†
                            key="promo_header_select"
                        )

                    st.markdown("<br>", unsafe_allow_html=True)
                    u_img_man = st.checkbox("–°–≤–æ—è –±–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫", key="cb_img_vert")

                with col_p2:
                    default_img_db = "data/images_db.xlsx"
                    if not u_img_man and os.path.exists(default_img_db):
                        st.success("‚úÖ –ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ (`images_db.xlsx`)")
                        try: df_db_promo = pd.read_excel(default_img_db)
                        except: pass
                    elif u_img_man:
                        up_i = st.file_uploader("–§–∞–π–ª .xlsx", type=['xlsx'], key="up_img_vert", label_visibility="collapsed")
                        if up_i: df_db_promo = pd.read_excel(up_i)
                    else: st.error("‚ùå –ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")

        # --- SIDEBAR ---
        if use_sidebar:
            with st.container(border=True):
                st.markdown("#### üìë 5. –°–∞–π–¥–±–∞—Ä")
                kws_input_sidebar = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏", 
                    value=sidebar_default_text, 
                    height=100, 
                    key="kws_sidebar_auto"
                )
                global_sidebar_list = [x.strip() for x in kws_input_sidebar.split('\n') if x.strip()]
                if not global_sidebar_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                col_s1, col_s2 = st.columns([1, 2])
                with col_s1: u_sb_man = st.checkbox("–°–≤–æ–π —Ñ–∞–π–ª –º–µ–Ω—é (.txt)", key="cb_sb_vert")
                with col_s2:
                    def_menu = "data/menu_structure.txt"
                    if not u_sb_man and os.path.exists(def_menu):
                        st.success("‚úÖ –ú–µ–Ω—é —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (`menu_structure.txt`)")
                        with open(def_menu, "r", encoding="utf-8") as f: sidebar_content = f.read()
                    elif u_sb_man:
                        up_s = st.file_uploader("–§–∞–π–ª .txt", type=['txt'], key="up_sb_vert", label_visibility="collapsed")
                        if up_s: sidebar_content = up_s.getvalue().decode("utf-8")
                    else: st.error("‚ùå –§–∞–π–ª –º–µ–Ω—é –Ω–µ –Ω–∞–π–¥–µ–Ω!")

        # --- GEO BLOCK ---
        if use_geo:
            with st.container(border=True):
                st.markdown("#### üåç 6. –ì–µ–æ-–±–ª–æ–∫")
                kws_input_geo = st.text_area(
                    "–°–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤/—Ä–µ–≥–∏–æ–Ω–æ–≤ (–∏–∑ –≤–∫–ª–∞–¥–∫–∏ –ê–Ω–∞–ª–∏–∑) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=geo_context_default, 
                    height=100, 
                    key="kws_geo_auto"
                )
                global_geo_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_geo) if x.strip()]
                
                if not global_geo_list:
                    st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤ –ø—É—Å—Ç!")
                else:
                    st.info(f"–ë—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ç–µ–∫—Å—Ç –¥–æ—Å—Ç–∞–≤–∫–∏ –¥–ª—è –ø–æ–ª—è IP_PROP4819 —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º —ç—Ç–∏—Ö –≥–æ—Ä–æ–¥–æ–≤.")

    st.markdown("---")
    
# ==========================================
    # 4. –ó–ê–ü–£–°–ö –ì–ï–ù–ï–†–ê–¶–ò–ò (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê)
    # ==========================================
    
    ready_to_go = True
    if use_manual_html:
        if not manual_html_source: ready_to_go = False
    else:
        if not main_category_url: ready_to_go = False

    if (use_text or use_tables or use_geo) and not gemini_api_key: ready_to_go = False
    if use_promo and df_db_promo is None: ready_to_go = False

# ==========================================
    # üÜò –ë–õ–û–ö –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò (Gemini 2.0 Flash)
    # ==========================================
    st.markdown("---")
    with st.expander("üõ†Ô∏è –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê API (–ï—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∏)", expanded=True):
        if st.button("üì° –ü–†–û–í–ï–†–ò–¢–¨ GEMINI 2.0"):
            if not gemini_api_key:
                st.error("‚ùå –ö–ª—é—á API –Ω–µ –≤–≤–µ–¥–µ–Ω!")
            else:
                try:
                    from openai import OpenAI
                    client = OpenAI(api_key=gemini_api_key, base_url="https://litellm.tokengate.ru/v1")
                    response = client.chat.completions.create(
                        model="google/gemini-2.5-pro",
                        messages=[{"role": "user", "content": "Say OK"}]
                    )
                    st.success(f"‚úÖ –£–°–ü–ï–•! –û—Ç–≤–µ—Ç: {response.choices[0].message.content}")
                except Exception as e:
                    st.error(f"‚ùå –û–®–ò–ë–ö–ê: {str(e)}")
                    if "404" in str(e):
                        st.info("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω—É–∂–Ω–∞ –¥—Ä—É–≥–∞—è):")
                        try:
                            models = [m.name for m in genai.list_models()]
                            st.code("\n".join(models))
                        except: pass

    st.markdown("---")

# ==========================================
    # 4. –ó–ê–ü–£–°–ö –ì–ï–ù–ï–†–ê–¶–ò–ò (–ü–ê–ö–ï–¢–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê + LOGS)
    # ==========================================

    # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –æ–Ω–∞ –±—ã–ª–∞ —Ç–æ–ª—å–∫–æ –≤—ã—à–µ)
    if use_manual_html:
        if not manual_html_source: ready_to_go = False
    else:
        if not main_category_url: ready_to_go = False
    if (use_text or use_tables or use_geo) and not gemini_api_key: ready_to_go = False
    if use_promo and df_db_promo is None: ready_to_go = False

# ==========================================
    # 4. –£–ú–ù–´–ô –ó–ê–ü–£–°–ö (–ê–í–¢–û-–¶–ï–ü–û–ß–ö–ê) - –í–ï–†–°–ò–Ø –° –ó–ê–©–ò–¢–û–ô –û–¢ –î–£–ë–õ–ï–ô
    # ==========================================
    st.markdown("### üöÄ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø—É—Å–∫–æ–º (–ê–≤—Ç–æ-—Ü–µ–ø–æ—á–∫–∞)")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    if 'auto_run_active' not in st.session_state: st.session_state.auto_run_active = False
    if 'auto_current_index' not in st.session_state: st.session_state.auto_current_index = 0

    # –ö–û–õ–û–ù–ö–ò –£–ü–†–ê–í–õ–ï–ù–ò–Ø
    col_batch1, col_batch2, col_batch3 = st.columns([1, 1, 2])
    
    with col_batch1:
        # –ï—Å–ª–∏ –∞–≤—Ç–æ-—Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–µ–Ω - –ø–æ–ª–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å
        if st.session_state.auto_run_active:
            start_val = st.session_state.auto_current_index
            st.text_input("–¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ä—Ç:", value=str(start_val), disabled=True)
            start_index = start_val
        else:
            # –ï—Å–ª–∏ —Å—Ç–æ–∏–º - –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å —Ä—É–∫–∞–º–∏
            start_index = st.number_input("–ù–∞—á–∞—Ç—å —Å —Ç–æ–≤–∞—Ä–∞ ‚Ññ (—Å 0)", min_value=0, value=0, step=1)

    with col_batch2:
        safe_batch_size = st.number_input("–†–∞–∑–º–µ—Ä –ø–∞—á–∫–∏ (—à—Ç)", min_value=1, value=5, help="–õ—É—á—à–µ 3-5 —à—Ç.")
        
    with col_batch3:
        st.write("")
        st.write("")
        enable_auto_chain = st.checkbox("üîÑ –í–∫–ª—é—á–∏—Ç—å –ê–í–¢–û-–¶–ï–ü–û–ß–ö–£", value=True, help="–°–∫—Ä–∏–ø—Ç –±—É–¥–µ—Ç —Å–∞–º –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å.")

    # === –ö–ù–û–ü–ö–ò –°–¢–ê–†–¢ / –°–¢–û–ü ===
    c_start, c_stop = st.columns([2, 1])
    with c_start:
        # –ö–Ω–æ–ø–∫–∞ –°–¢–ê–†–¢
        # –ï—Å–ª–∏ –∞–≤—Ç–æ-—Ä–µ–∂–∏–º —É–∂–µ –∏–¥–µ—Ç, –∫–Ω–æ–ø–∫–∞ –≤—ã–≥–ª—è–¥–∏—Ç –Ω–∞–∂–∞—Ç–æ–π (–Ω–æ –Ω–µ–∞–∫—Ç–∏–≤–Ω–∞ –¥–ª—è –∫–ª–∏–∫–∞, —á—Ç–æ–±—ã –Ω–µ –¥–≤–æ–∏–ª–æ—Å—å)
        btn_label = "üöÄ –ó–ê–ü–£–°–¢–ò–¢–¨ –ü–†–û–¶–ï–°–°"
        start_clicked = st.button(btn_label, type="primary", disabled=(not ready_to_go or st.session_state.auto_run_active), use_container_width=True)
    
    with c_stop:
        # –ö–Ω–æ–ø–∫–∞ –°–¢–û–ü
        if st.button("üõë –°–¢–û–ü", type="secondary", use_container_width=True, help="–ù–∞–∂–º–∏—Ç–µ, —á—Ç–æ–±—ã –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ü–µ–ø–æ—á–∫—É –ø–æ—Å–ª–µ —Ç–µ–∫—É—â–µ–π –ø–∞—á–∫–∏."):
            st.session_state.auto_run_active = False
            st.warning("‚õî –ö–æ–º–∞–Ω–¥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–∏–Ω—è—Ç–∞. –°–∫—Ä–∏–ø—Ç –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ø–æ—Å–ª–µ —Ç–µ–∫—É—â–µ–π –ø–∞—á–∫–∏.")
            # –ú—ã –Ω–µ –¥–µ–ª–∞–µ–º st.stop() —Ç—É—Ç, —á—Ç–æ–±—ã –¥–∞—Ç—å —Å–∫—Ä–∏–ø—Ç—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ—Ç—Ä–∏—Å–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å, 
            # –ø—Ä–æ—Å—Ç–æ —Å–Ω–∏–º–∞–µ–º —Ñ–ª–∞–≥ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.

    # –õ–û–ì–ò–ö–ê –ó–ê–ü–£–°–ö–ê
    # –ú—ã –∑–∞–ø—É—Å–∫–∞–µ–º—Å—è, –µ—Å–ª–∏ –Ω–∞–∂–∞–ª–∏ –°—Ç–∞—Ä—Ç –ò–õ–ò –µ—Å–ª–∏ —Ñ–ª–∞–≥ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —É–∂–µ True (–ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏)
    should_run = start_clicked or st.session_state.auto_run_active

    if should_run:
        # –í–∫–ª—é—á–∞–µ–º —Ñ–ª–∞–≥, –µ—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π –∫–ª–∏–∫
        if not st.session_state.auto_run_active:
             st.session_state.auto_run_active = True
             st.session_state.auto_current_index = start_index

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã (–µ—Å–ª–∏ –Ω–µ—Ç)
        if 'gen_result_df' not in st.session_state or st.session_state.gen_result_df is None:
             st.session_state.gen_result_df = pd.DataFrame(columns=[
                'Page URL', 'Product Name', 'IP_PROP4839', 'IP_PROP4817', 'IP_PROP4818', 
                'IP_PROP4819', 'IP_PROP4820', 'IP_PROP4821', 'IP_PROP4822', 'IP_PROP4823', 
                'IP_PROP4824', 'IP_PROP4816', 'IP_PROP4825', 'IP_PROP4826', 'IP_PROP4834', 
                'IP_PROP4835', 'IP_PROP4836', 'IP_PROP4837', 'IP_PROP4838', 'IP_PROP4829', 'IP_PROP4831'
            ])

        EXCEL_COLUMN_ORDER = st.session_state.gen_result_df.columns.tolist()
        TEXT_CONTAINERS = ['IP_PROP4839', 'IP_PROP4816', 'IP_PROP4838', 'IP_PROP4829', 'IP_PROP4831']

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        raw_txt_val = st.session_state.get("ai_text_context_editable", "")
        if not raw_txt_val: raw_txt_val = text_context_default
        actual_text_list = [x.strip() for x in re.split(r'[,\n]+', raw_txt_val) if x.strip()]

        raw_geo_val = st.session_state.get("kws_geo_auto", "")
        if not raw_geo_val: raw_geo_val = geo_context_default
        actual_geo_list = [x.strip() for x in re.split(r'[,\n]+', raw_geo_val) if x.strip()]

        user_num_blocks = st.session_state.get("sb_num_blocks", 5)

        # –ü–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã
        live_download_placeholder = st.empty()
        live_table_placeholder = st.empty()
        log_container = st.status(f"üöÄ –†–∞–±–æ—Ç–∞–µ–º... (–ù–∞—á–∞–ª–∏ —Å {start_index})", expanded=True)

        # API Client
        client = None
        if (use_text or use_tables or use_geo) and gemini_api_key:
            try:
                from openai import OpenAI
                client = OpenAI(api_key=gemini_api_key, base_url="https://litellm.tokengate.ru/v1")
            except Exception as e:
                log_container.error(f"–û—à–∏–±–∫–∞ API: {e}")
                st.session_state.auto_run_active = False
                st.stop()
        
        # --- –ü–û–î–ì–û–¢–û–í–ö–ê –¢–ï–ì–û–í ---
        all_tags_links = []
        if use_tags:
            if tags_file_content: all_tags_links = [l.strip() for l in io.StringIO(tags_file_content).readlines() if l.strip()]
            elif os.path.exists("data/links_base.txt"):
                with open("data/links_base.txt", "r", encoding="utf-8") as f: all_tags_links = [l.strip() for l in f.readlines() if l.strip()]

        tags_data_prepared = [] 
        if use_tags:
            for kw in global_tags_list:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                matches = [u for u in all_tags_links if tr in u.lower()]
                if matches: tags_data_prepared.append((kw, matches)) 
                else:
                    if kw not in actual_text_list: actual_text_list.append(kw)

        p_img_map = {}
        if use_promo and df_db_promo is not None:
            for _, row in df_db_promo.iterrows():
                u = str(row.iloc[0]).strip(); img = str(row.iloc[1]).strip()
                if u and u != 'nan' and img and img != 'nan': p_img_map[u.rstrip('/')] = img
        # -----------------------------------------------

        # –§—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        def resolve_real_names(urls_list, status_msg=""):
            if not urls_list: return {}
            results_map = {}
            if status_msg: log_container.write(status_msg)
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                future_to_url = {executor.submit(get_breadcrumb_only, u, st.session_state.settings_ua): u for u in urls_list}
                for future in concurrent.futures.as_completed(future_to_url):
                    url_key = future_to_url[future]
                    try:
                        extracted_name = future.result()
                        if extracted_name: results_map[url_key] = extracted_name
                    except: pass
            return results_map

        # –°–ë–û–† –°–¢–†–ê–ù–ò–¶
        log_container.write("üì• –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü...")
        target_pages = []
        try:
            if use_manual_html:
                soup_main = BeautifulSoup(manual_html_source, 'html.parser')
            else:
                session = requests.Session()
                r = session.get(main_category_url, timeout=30, verify=False)
                if r.status_code == 200: soup_main = BeautifulSoup(r.text, 'html.parser')
                else: st.stop()
            
            if soup_main:
                tags_container = soup_main.find(class_='popular-tags-inner')
                if tags_container:
                    for link in tags_container.find_all('a'):
                        target_pages.append({'url': urljoin(main_category_url or "http://localhost", link.get('href')), 'name': link.get_text(strip=True)})
                if not target_pages:
                    h1_found = soup_main.find('h1')
                    target_pages.append({'url': main_category_url or "local", 'name': h1_found.get_text(strip=True) if h1_found else "–¢–æ–≤–∞—Ä"})
        except Exception as e:
            log_container.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü: {e}")
            st.session_state.auto_run_active = False
            st.stop()

        # === –†–ê–°–ß–ï–¢ –ü–ê–ß–ö–ò ===
        total_found = len(target_pages)
        if start_index >= total_found:
             st.session_state.auto_run_active = False
             st.success("üéâ –í—Å–µ —Ç–æ–≤–∞—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
             st.stop()

        end_index = min(start_index + safe_batch_size, total_found)
        target_pages_batch = target_pages[start_index:end_index]
        
        log_container.write(f"üìä –ü–ê–ß–ö–ê: {start_index+1} ‚Äî {end_index} –∏–∑ {total_found}")

        # === –¶–ò–ö–õ –ì–ï–ù–ï–†–ê–¶–ò–ò ===
        for i, page in enumerate(target_pages_batch):
            
            # [–í–ê–ñ–ù–û] –ó–ê–©–ò–¢–ê –û–¢ –î–£–ë–õ–ï–ô
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —ç—Ç–æ—Ç URL –≤ –Ω–∞—à–µ–π —Ç–∞–±–ª–∏—Ü–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            current_urls_in_df = st.session_state.gen_result_df['Page URL'].values
            if page['url'] in current_urls_in_df:
                log_container.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –¥—É–±–ª—è: {page['name']} (–£–∂–µ –µ—Å—Ç—å –≤ —Ç–∞–±–ª–∏—Ü–µ)")
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Ç–µ—Ä–∞—Ü–∏—é, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É

            current_num = start_index + i + 1
            log_container.write(f"‚ñ∂Ô∏è **[{current_num}/{total_found}] {page['name']}**")
            
            try:
                base_text_raw, _, real_header_h2, _ = get_page_data_for_gen(page['url'])
                header_for_ai = real_header_h2 if real_header_h2 else page['name']
                row_data = {col: "" for col in EXCEL_COLUMN_ORDER}
                row_data['Page URL'] = page['url']; row_data['Product Name'] = header_for_ai
                for k, v in STATIC_DATA_GEN.items():
                    if k in row_data: row_data[k] = v
                
                injections = []

                # –¢–ï–ì–ò
                if use_tags and tags_data_prepared:
                    tags_pool = tags_data_prepared
                    if len(tags_pool) > 15: tags_pool = random.sample(tags_pool, 15)
                    selected_urls_map = {} 
                    for kw, links in tags_pool:
                        valid = [u for u in links if u.rstrip('/') != page['url'].rstrip('/')]
                        if valid:
                             sel = random.choice(valid); selected_urls_map[sel] = kw
                    urls_to_fetch = list(selected_urls_map.keys())
                    real_names_map = resolve_real_names(urls_to_fetch)
                    html_t = []
                    for u in urls_to_fetch:
                        display_name = real_names_map.get(u, selected_urls_map[u])
                        if display_name == selected_urls_map[u]: display_name = display_name.capitalize()
                        html_t.append(f'<a href="{u}" class="tag-item">{display_name}</a>')
                    if html_t:
                        tags_block = f'''<div class="popular-tags-text"><div class="popular-tags-inner-text"><div class="tag-items">{"\n".join(html_t)}</div></div></div>'''
                        injections.append(tags_block)

                # –¢–ê–ë–õ–ò–¶–´
                if use_tables and client:
                    for t_topic in table_prompts:
                        ctx = f"–î–∞–Ω–Ω—ã–µ: {tech_context_final_str}"
                        prompt_tbl = f"Create HTML <table> for '{header_for_ai}'. Topic: {t_topic}. Context: {ctx}. No markdown."
                        try:
                            resp = client.chat.completions.create(model="google/gemini-2.5-pro", messages=[{"role": "user", "content": prompt_tbl}], temperature=0)
                            raw_table = resp.choices[0].message.content.replace("```html", "").replace("```", "").strip()
                            injections.append(raw_table)
                        except: pass
                
                # –ü–†–û–ú–û
                if use_promo and p_img_map:
                    p_cands = [u for u in p_img_map.keys() if u.rstrip('/') != page['url'].rstrip('/')]
                    if p_cands:
                        sel_p = random.sample(p_cands, min(8, max(3, len(p_cands))))
                        promo_names_map = resolve_real_names(sel_p)
                        gallery_items = []
                        for u in sel_p:
                            nm = promo_names_map.get(u, force_cyrillic_name_global(u.split("/")[-1]))
                            img_src = p_img_map[u]
                            item_html = f'''<div class="gallery-item"><h3><a href="{u}" target="_blank">{nm}</a></h3><figure><a href="{u}" target="_blank"><picture><img src="{img_src}" loading="lazy"></picture></a></figure></div>'''
                            gallery_items.append(item_html)

                        p_html = f'''
<style>.outer-full-width-section {{ padding: 25px 0; width: 100%; }}.gallery-content-wrapper {{ max-width: 1400px; margin: 0 auto; padding: 25px 15px; box-sizing: border-box; border-radius: 10px; overflow: hidden; background-color: #F6F7FC; }}h3.gallery-title {{ color: #3D4858; font-size: 1.8em; font-weight: normal; padding: 0; margin-top: 0; margin-bottom: 15px; text-align: left; }}.five-col-gallery {{ display: flex; justify-content: flex-start; align-items: flex-start; gap: 20px; margin-bottom: 0; padding: 0; list-style: none; flex-wrap: nowrap !important; overflow-x: auto !important; padding-bottom: 15px; }}.gallery-item {{ flex: 0 0 260px !important; box-sizing: border-box; text-align: center; scroll-snap-align: start; }}.gallery-item h3 {{ font-size: 1.1em; margin-bottom: 8px; font-weight: normal; text-align: center; line-height: 1.1em; display: block; min-height: 40px; }}.gallery-item h3 a {{ text-decoration: none; color: #333; display: block; height: 100%; display: flex; align-items: center; justify-content: center; transition: color 0.2s ease; }}.gallery-item h3 a:hover {{ color: #007bff; }}.gallery-item figure {{ width: 100%; margin: 0; float: none !important; height: 260px; overflow: hidden; margin-bottom: 5px; border-radius: 8px; }}.gallery-item figure a {{ display: block; height: 100%; text-decoration: none; }}.gallery-item img {{ width: 100%; height: 100%; display: block; margin: 0 auto; object-fit: cover; transition: transform 0.3s ease; border-radius: 8px; }}.gallery-item figure a:hover img {{ transform: scale(1.05); }}</style>
<div class="outer-full-width-section"><div class="gallery-content-wrapper"><h3 class="gallery-title">{promo_title}</h3><div class="five-col-gallery">{"".join(gallery_items)}</div></div></div>'''
                        injections.append(p_html)

                # –¢–ï–ö–°–¢
                blocks = [""] * 5
                if use_text and client:
                    log_container.write(f"   ‚Ü≥ ü§ñ –ü–∏—à–µ–º —Ç–µ–∫—Å—Ç...")
                    blocks_raw = generate_ai_content_blocks(gemini_api_key, base_text_raw or "", page['name'], header_for_ai, user_num_blocks, actual_text_list)
                    cleaned_blocks = [b.replace("```html", "").replace("```", "").strip() for b in blocks_raw]
                    for i_b in range(len(cleaned_blocks)):
                        if i_b < 5: blocks[i_b] = cleaned_blocks[i_b]

                # –°–õ–ò–Ø–ù–ò–ï
                effective_blocks_count = max(1, user_num_blocks)
                for i_inj, inj in enumerate(injections):
                    target_idx = i_inj % effective_blocks_count
                    blocks[target_idx] = blocks[target_idx] + "\n\n" + inj

                # –ì–ï–û
                if use_geo and client:
                    log_container.write(f"   ‚Ü≥ üåç –ü–∏—à–µ–º –¥–æ—Å—Ç–∞–≤–∫—É...")
                    try:
                         cities = ", ".join(random.sample(actual_geo_list, min(15, len(actual_geo_list))))
                         prompt_geo = f"Write ONE HTML paragraph about delivery to {cities}. No markdown."
                         resp = client.chat.completions.create(model="google/gemini-2.5-pro", messages=[{"role": "user", "content": prompt_geo}], temperature=0.1)
                         row_data['IP_PROP4819'] = resp.choices[0].message.content.replace("```html", "").replace("```", "").strip()
                    except: pass

                for i_c, c_name in enumerate(TEXT_CONTAINERS):
                    row_data[c_name] = blocks[i_c]

                # === –°–û–•–†–ê–ù–ï–ù–ò–ï ===
                new_row_df = pd.DataFrame([row_data])
                st.session_state.gen_result_df = pd.concat([st.session_state.gen_result_df, new_row_df], ignore_index=True)
                
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                    st.session_state.gen_result_df.to_excel(writer, index=False)
                st.session_state.unified_excel_data = buffer.getvalue()
                
                try: st.session_state.gen_result_df.to_excel("backup_auto.xlsx", index=False)
                except: pass

                live_table_placeholder.dataframe(st.session_state.gen_result_df.tail(3), use_container_width=True)
                
                with live_download_placeholder.container():
                    st.download_button(
                        label=f"üíæ –°–ö–ê–ß–ê–¢–¨ –¢–ï–ö–£–©–ò–ô –†–ï–ó–£–õ–¨–¢–ê–¢ ({len(st.session_state.gen_result_df)} —Å—Ç—Ä.)",
                        data=st.session_state.unified_excel_data,
                        file_name=f"FULL_RESULT_{int(time.time())}.xlsx",
                        mime="application/vnd.ms-excel",
                        key=f"dl_live_{int(time.time())}_{i}"
                    )

            except Exception as e:
                log_container.error(f"–°–±–æ–π: {e}")

        log_container.update(label=f"‚úÖ –ü–∞—á–∫–∞ {start_index}-{end_index} –≥–æ—Ç–æ–≤–∞!", state="complete", expanded=False)
        
        # === –õ–û–ì–ò–ö–ê –ê–í–¢–û-–ü–ï–†–ï–ó–ê–ü–£–°–ö–ê ===
        if enable_auto_chain:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞–∂–∞–ª–∏ –ª–∏ –°–¢–û–ü –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ
            if not st.session_state.auto_run_active:
                st.warning("‚õî –¶–µ–ø–æ—á–∫–∞ –±—ã–ª–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –≤—Ä—É—á–Ω—É—é.")
            else:
                next_start = end_index
                if next_start < total_found:
                    st.session_state.auto_current_index = next_start
                    st.info(f"‚è≥ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ 1 —Å–µ–∫... –°–ª–µ–¥—É—é—â–∞—è –ø–∞—á–∫–∞ —Å {next_start}.")
                    time.sleep(1)
                    st.rerun() 
                else:
                    st.session_state.auto_run_active = False
                    st.balloons()
                    st.success("üèÅ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–û–õ–ù–û–°–¢–¨–Æ –ó–ê–í–ï–†–®–ï–ù–ê!")

    # –ö–ù–û–ü–ö–ê –°–ö–ê–ß–ò–í–ê–ù–ò–Ø (–ü–û–Ø–í–õ–Ø–ï–¢–°–Ø –°–†–ê–ó–£ –ü–û–°–õ–ï –ü–ï–†–í–û–ô –°–¢–†–û–ö–ò)
    if st.session_state.get('unified_excel_data') is not None:
        count = len(st.session_state.gen_result_df)
        st.success(f"–ì–æ—Ç–æ–≤–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {count}")
        st.download_button(
            label=f"üì• –°–ö–ê–ß–ê–¢–¨ –†–ï–ó–£–õ–¨–¢–ê–¢ ({count} —Å—Ç—Ä.)",
            data=st.session_state.unified_excel_data,
            file_name=f"wholesale_result_{int(time.time())}.xlsx",
            mime="application/vnd.ms-excel",
            key="btn_dl_fixed"
        )

    # ==========================================
    # 5. –ë–õ–û–ö –ü–†–ï–î–ü–†–û–°–ú–û–¢–†–ê
    # ==========================================
    if 'gen_result_df' in st.session_state and st.session_state.gen_result_df is not None and not st.session_state.gen_result_df.empty:
        st.markdown("---")
        st.header("üëÄ –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º")
        
        st.markdown("""
        <style>
            .preview-box {
                border: 1px solid #e2e8f0;
                background-color: #ffffff;
                padding: 20px;
                border-radius: 8px;
                max-height: 600px;
                overflow-y: auto;
                box-shadow: inset 0 2px 4px 0 rgba(0, 0, 0, 0.06);
            }
        </style>
        """, unsafe_allow_html=True)

        df_p = st.session_state.gen_result_df
        
        if 'Product Name' in df_p.columns:
            sel_p = st.selectbox("–°—Ç—Ä–∞–Ω–∏—Ü–∞:", df_p['Product Name'].tolist(), key="ws_prev_sel")
            row_p = df_p[df_p['Product Name'] == sel_p].iloc[0]
            
            # –§–∏–ª—å—Ç—Ä —Ç–∞–±–æ–≤
            relevant_cols = []
            if use_text or use_sidebar or use_tags or use_tables or use_promo:
                relevant_cols.extend(['IP_PROP4839', 'IP_PROP4816', 'IP_PROP4838', 'IP_PROP4829', 'IP_PROP4831'])
            if use_geo:
                relevant_cols.append('IP_PROP4819')

            active_tabs = [c for c in relevant_cols if str(row_p.get(c, "")).strip() != ""]
            
            if active_tabs:
                tabs = st.tabs(active_tabs)
                for i, col in enumerate(active_tabs):
                    with tabs[i]:
                        content_to_show = str(row_p[col])
                        st.markdown(f"<div class='preview-box'>{content_to_show}</div>", unsafe_allow_html=True)
            else:
                st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
# ==========================================
# TAB 3: PROJECT MANAGER (SAVE/LOAD)
# ==========================================
with tab_projects:
    st.header("üìÅ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞–º–∏")
    st.markdown("–ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –≤ —Ñ–∞–π–ª –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–∞–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–µ–∫—Ç.")

    col_save, col_load = st.columns(2)

    # --- –§–£–ù–ö–¶–ò–Ø –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–Ø (CALLBACK) ---
    def restore_state_callback(data_to_restore):
        """
        –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –î–û –ø–µ—Ä–µ—Ä–∏—Å–æ–≤–∫–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.
        –ü–æ—ç—Ç–æ–º—É –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å session_state.
        """
        try:
            state_dict = data_to_restore["state"]
            restored_count = 0
            
            # 1. –û–±–Ω–æ–≤–ª—è–µ–º session_state
            for k, v in state_dict.items():
                st.session_state[k] = v
                restored_count += 1
            
            # 2. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–ª–∞–≥–∏
            st.session_state['analysis_done'] = True
            
            # 3. –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ (–ø–æ—è–≤–∏—Ç—Å—è –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏)
            st.toast(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {restored_count} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤!", icon="üéâ")
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –≤–Ω—É—Ç—Ä–∏ callback: {e}")

    # --- –ë–õ–û–ö –°–û–•–†–ê–ù–ï–ù–ò–Ø ---
    with col_save:
        with st.container(border=True):
            st.subheader("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–æ–µ–∫—Ç")
            
            if not st.session_state.get('analysis_done'):
                st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ–¥–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ (–í–∫–ª–∞–¥–∫–∞ SEO), —á—Ç–æ–±—ã –±—ã–ª–æ —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å.")
            else:
                st.info("–ë—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã, —Å–ø–∏—Å–∫–∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∏, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, —Å—Å—ã–ª–∫–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.")
                
                timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M")
                query_slug = transliterate_text(st.session_state.get('query_input', 'project'))[:20]
                default_filename = f"GAR_PRO_{query_slug}_{timestamp}.pkl"
                
                project_snapshot = {
                    "meta": {
                        "version": "2.6",
                        "date": str(datetime.datetime.now())
                    },
                    "state": {}
                }
                
                # –ö–ª—é—á–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                keys_to_save = [
                    'analysis_results', 'analysis_done', 'naming_table_df', 'ideal_h1_result',
                    'detected_anomalies', 'serp_trend_info', 'full_graph_data',
                    'categorized_products', 'categorized_services', 'categorized_commercial',
                    'categorized_dimensions', 'categorized_geo', 'categorized_general', 'categorized_sensitive',
                    'orig_products', 'orig_services', 'orig_commercial', 
                    'orig_dimensions', 'orig_geo', 'orig_general',
                    'sensitive_words_input_final', 'auto_tags_words', 'auto_promo_words',
                    'my_url_input', 'query_input', 'my_content_input', 'my_page_source_radio',
                    'competitor_source_radio', 'persistent_urls', 'excluded_urls_auto',
                    'settings_excludes', 'settings_stops', 'arsenkin_token', 'yandex_dict_key',
                    'settings_ua', 'settings_search_engine', 'settings_region', 'settings_top_n',
                    'settings_noindex', 'settings_alt', 'settings_numbers', 'settings_norm',
                    'gen_result_df', 'unified_excel_data'
                ]
                
                for k in keys_to_save:
                    if k in st.session_state:
                        project_snapshot["state"][k] = st.session_state[k]

                try:
                    pickle_data = pickle.dumps(project_snapshot)
                    st.download_button(
                        label="üì• –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª –ø—Ä–æ–µ–∫—Ç–∞ (.pkl)",
                        data=pickle_data,
                        file_name=default_filename,
                        mime="application/octet-stream",
                        type="primary",
                        use_container_width=True
                    )
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ø–∞–∫–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")

    # --- –ë–õ–û–ö –ó–ê–ì–†–£–ó–ö–ò ---
    with col_load:
        with st.container(border=True):
            st.subheader("üìÇ –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–µ–∫—Ç")
            
            uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª .pkl", type=["pkl"], key="project_loader")
            
            if uploaded_file is not None:
                try:
                    loaded_data = pickle.load(uploaded_file)
                    
                    if isinstance(loaded_data, dict) and "state" in loaded_data:
                        date_str = loaded_data['meta'].get('date', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        st.success(f"–ü—Ä–æ–µ–∫—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω! (–î–∞—Ç–∞: {date_str})")
                        
                        # –ò–°–ü–û–õ–¨–ó–£–ï–ú ON_CLICK –ò ARGS
                        # –≠—Ç–æ –≥–ª–∞–≤–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: —Ñ—É–Ω–∫—Ü–∏—è restore_state_callback –≤—ã–∑–æ–≤–µ—Ç—Å—è –î–û —Ç–æ–≥–æ,
                        # –∫–∞–∫ Streamlit –Ω–∞—á–Ω–µ—Ç –æ—Ç—Ä–∏—Å–æ–≤—ã–≤–∞—Ç—å –≤–∏–¥–∂–µ—Ç—ã –∑–∞–Ω–æ–≤–æ.
                        st.button(
                            "üöÄ –í–û–°–°–¢–ê–ù–û–í–ò–¢–¨ –°–û–°–¢–û–Ø–ù–ò–ï", 
                            type="primary", 
                            use_container_width=True,
                            on_click=restore_state_callback,
                            args=(loaded_data,)
                        )
                    else:
                        st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –ø—Ä–æ–µ–∫—Ç–∞.")
                except Exception as e:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")


























