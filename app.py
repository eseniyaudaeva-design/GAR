import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin, unquote
import inspect
import time
import json
import io
import os
import random
import streamlit.components.v1 as components

# ==========================================
# 0. –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò (–¢—Ä–∞–Ω—Å–ª–∏—Ç –∏ –°–ø–µ–ª–ª–µ—Ä)
# ==========================================

def transliterate_text(text):
    """
    –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ -> –õ–∞—Ç–∏–Ω–∏—Ü–∞ (–¥–ª—è –ø–æ–∏—Å–∫–∞)
    """
    mapping = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }
    result = []
    for char in text.lower():
        if char in mapping:
            result.append(mapping[char])
        elif char.isalnum() or char == '-':
            result.append(char)
    return "".join(result)

def spell_check_yandex(text):
    """
    –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å.–°–ø–µ–ª–ª–µ—Ä
    """
    if not text: return ""
    url = "https://speller.yandex.net/services/spellservice.json/checkText"
    params = {"text": text, "lang": "ru", "options": 518}
    try:
        r = requests.get(url, params=params, timeout=1.5)
        if r.status_code == 200:
            data = r.json()
            fixed_text = text
            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏, –∏–¥—è —Å –∫–æ–Ω—Ü–∞, —á—Ç–æ–±—ã –Ω–µ —Å–±–∏—Ç—å –∏–Ω–¥–µ–∫—Å—ã (—Ö–æ—Ç—è —Ç—É—Ç –∑–∞–º–µ–Ω–∞ –ø–æ —Å–ª–æ–≤—É)
            # –î–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Å—Ç–æ –∑–∞–º–µ–Ω—è–µ–º —Å–ª–æ–≤–∞
            for error in data:
                if error.get('s'):
                    fixed_text = fixed_text.replace(error['word'], error['s'][0])
            return fixed_text
    except:
        pass
    return text

def reverse_transliterate_smart(text):
    """
    –õ–∞—Ç–∏–Ω–∏—Ü–∞ (–∏–∑ URL) -> –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ + –°–ø–µ–ª–ª–µ—Ä
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç 'alyuminievaya-truba' –≤ '–ê–ª—é–º–∏–Ω–∏–µ–≤–∞—è —Ç—Ä—É–±–∞'
    """
    # 1. –ë–∞–∑–æ–≤–∞—è —á–∏—Å—Ç–∫–∞
    text = text.lower().strip()
    text = re.sub(r'\.html|\.php|\.htm', '', text)
    text = text.replace('_', ' ').replace('/', '')
    # –î–µ—Ñ–∏—Å—ã –º–µ–Ω—è–µ–º –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
    text = text.replace('-', ' ')

    # 2. –û–±—Ä–∞—Ç–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ (–°–ª–æ–∂–Ω—ã–µ —Å–æ—á–µ—Ç–∞–Ω–∏—è —Å–Ω–∞—á–∞–ª–∞)
    replacements = [
        ('shch', '—â'), ('sch', '—â'), ('sh', '—à'), ('ch', '—á'), ('zh', '–∂'),
        ('yu', '—é'), ('ya', '—è'), ('yo', '—ë'), ('ts', '—Ü'), ('kh', '—Ö'),
        ('iy', '–∏–π'), ('yy', '—ã–π'), ('ij', '–∏–π') # –æ–∫–æ–Ω—á–∞–Ω–∏—è
    ]
    for eng, rus in replacements:
        text = text.replace(eng, rus)

    # 3. –ü—Ä–æ—Å—Ç—ã–µ –±—É–∫–≤—ã
    mapping = {
        'a': '–∞', 'b': '–±', 'c': '–∫', 'd': '–¥', 'e': '–µ', 'f': '—Ñ', 'g': '–≥',
        'h': '—Ö', 'i': '–∏', 'j': '–π', 'k': '–∫', 'l': '–ª', 'm': '–º', 'n': '–Ω',
        'o': '–æ', 'p': '–ø', 'r': '—Ä', 's': '—Å', 't': '—Ç', 'u': '—É',
        'v': '–≤', 'w': '–≤', 'x': '–∫—Å', 'z': '–∑', 'y': '–∏' 
    }
    
    chars = []
    for c in text:
        chars.append(mapping.get(c, c)) # –ï—Å–ª–∏ –Ω–µ—Ç –≤ –º–∞–ø–ø–∏–Ω–≥–µ (—Ü–∏—Ñ—Ä—ã), –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
    raw_rus = "".join(chars)

    # 4. –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ —Å–ø–µ–ª–ª–µ—Ä –¥–ª—è —Ñ–∏–∫—Å–∞ –æ–∫–æ–Ω—á–∞–Ω–∏–π –∏ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏
    final_text = spell_check_yandex(raw_rus)
    
    return final_text.capitalize()


# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ openai
try:
    import openai
except ImportError:
    openai = None

# ==========================================
# 0.5 –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–û–°–¢–û–Ø–ù–ò–Ø (SESSION STATE)
# ==========================================
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False

if 'ai_generated_df' not in st.session_state:
    st.session_state.ai_generated_df = None
if 'ai_excel_bytes' not in st.session_state:
    st.session_state.ai_excel_bytes = None

if 'tags_html_result' not in st.session_state:
    st.session_state.tags_html_result = None
if 'table_html_result' not in st.session_state:
    st.session_state.table_html_result = None

if 'categorized_products' not in st.session_state:
    st.session_state.categorized_products = []
if 'categorized_services' not in st.session_state:
    st.session_state.categorized_services = []
if 'categorized_commercial' not in st.session_state:
    st.session_state.categorized_commercial = []
if 'categorized_dimensions' not in st.session_state:
    st.session_state.categorized_dimensions = []

if 'persistent_urls' not in st.session_state:
    st.session_state['persistent_urls'] = ""

if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´ –ò –°–ü–ò–°–ö–ò
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO v2.5 (Smart Promo)", page_icon="üìä")

GARBAGE_LATIN_STOPLIST = {
    'whatsapp', 'viber', 'telegram', 'skype', 'vk', 'instagram', 'facebook', 'youtube', 'twitter',
    'cookie', 'cookies', 'policy', 'privacy', 'agreement', 'terms',
    'click', 'submit', 'send', 'zakaz', 'basket', 'cart', 'order', 'call', 'back', 'callback',
    'login', 'logout', 'sign', 'register', 'auth', 'account', 'profile',
    'search', 'menu', 'nav', 'navigation', 'footer', 'header', 'sidebar',
    'img', 'jpg', 'png', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'svg',
    'ok', 'error', 'undefined', 'null', 'true', 'false', 'var', 'let', 'const', 'function', 'return',
    'ru', 'en', 'com', 'net', 'org', 'biz', 'shop', 'store',
    'phone', 'email', 'tel', 'fax', 'mob', 'address', 'copyright', 'all', 'rights', 'reserved',
    'div', 'span', 'class', 'id', 'style', 'script', 'body', 'html', 'head', 'meta', 'link'
}

# ==========================================
# 2. –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø
# ==========================================
def check_password():
    if st.session_state.get("authenticated"):
        return True

    st.markdown("""
        <style>
        .main { display: flex; flex-direction: column; justify-content: center; align-items: center; }
        .auth-logo-box { text-align: center; margin-bottom: 1rem; padding-top: 0; }
        </style>
    """, unsafe_allow_html=True)

    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="auth-logo-box"><h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3></div>', unsafe_allow_html=True)
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "jfV6Xel-Q7vp-_s2UYPO":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

# ==========================================
# 3. –ü–û–õ–£–ß–ï–ù–ò–ï API –ö–õ–Æ–ß–ï–ô
# ==========================================
if "arsenkin_token" in st.session_state:
    ARSENKIN_TOKEN = st.session_state.arsenkin_token
else:
    try: ARSENKIN_TOKEN = st.secrets["api"]["arsenkin_token"]
    except (FileNotFoundError, KeyError): ARSENKIN_TOKEN = None

if "yandex_dict_key" in st.session_state:
    YANDEX_DICT_KEY = st.session_state.yandex_dict_key
else:
    try: YANDEX_DICT_KEY = st.secrets["api"]["yandex_dict_key"]
    except (FileNotFoundError, KeyError): YANDEX_DICT_KEY = None


REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956},
    "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931},
    "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–ö–∏–µ–≤ (UA)": {"ya": 143, "go": 1012852},
    "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493},
    "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601}
}

DEFAULT_EXCLUDE_DOMAINS = [
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "ebay.com",
    "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", "pandao.ru",
    "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", "banki.ru",
    "regmarkets.ru", "zoon.ru", "pulscen.ru", "prodoctorov.ru", "blizko.ru",
    "domclick.ru", "satom.ru", "quto.ru", "edadeal.ru", "cataloxy.ru",
    "irr.ru", "onliner.by", "shop.by", "deal.by", "yell.ru",
    "profi.ru", "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by",
    "market.yandex.ru", "youtube.com", "gosuslugi.ru", "dzen.ru",
    "2gis.by", "wildberries.ru", "rutube.ru", "vk.com", "facebook.com"
]
DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n–∫—É–ø–∏—Ç—å\n—Ü–µ–Ω–∞\n—à—Ç\n—Å–º\n–º–º\n–∫–≥\n–∫–≤\n–º2\n—Å—Ç—Ä\n—É–ª"

# –¶–≤–µ—Ç–∞ –∏ —Å—Ç–∏–ª–∏
PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE"

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif; color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important; }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important; color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] div[role="gridcell"] {{
            background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; border-bottom: 1px solid {ROW_BORDER_COLOR} !important;
        }}
        .legend-box {{ padding: 10px; background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px; }}
        .text-red {{ color: #D32F2F; font-weight: bold; }}
        .text-green {{ color: #2E7D32; font-weight: bold; }}
        .text-bold {{ font-weight: 600; }}
        .sort-container {{ background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 8px; margin-bottom: 10px; border: 1px solid {BORDER_COLOR}; }}
        
        .stApp > header {{ background-color: transparent !important; }}
        .stTextInput input:disabled, .stTextArea textarea:disabled, .stSelectbox div[aria-disabled="true"] {{
            opacity: 1 !important; background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; cursor: text !important; -webkit-text-fill-color: {TEXT_COLOR} !important; border-color: {BORDER_COLOR} !important;
        }}
        .stButton button:disabled {{ opacity: 1 !important; background-color: {PRIMARY_COLOR} !important; color: white !important; cursor: progress !important; }}
        div[data-testid="stAppViewContainer"] {{ filter: none !important; opacity: 1 !important; transition: none !important; }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 4. –õ–û–ì–ò–ö–ê (–ë–≠–ö–ï–ù–î)
# ==========================================

try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except Exception as e:
    morph = None
    USE_NLP = False
    st.sidebar.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ NLP: {e}")

def get_yandex_dict_info(text, api_key):
    if not api_key: return {'lemma': text, 'pos': 'unknown'}
    url = "https://dictionary.yandex.net/api/v1/dicservice.json/lookup"
    params = {'key': api_key, 'lang': 'ru-ru', 'text': text, 'ui': 'ru'}
    try:
        r = requests.get(url, params=params, timeout=2)
        if r.status_code == 200:
            data = r.json()
            if data.get('def'):
                first_def = data['def'][0]
                return {'lemma': first_def.get('text', text), 'pos': first_def.get('pos', 'unknown')}
    except: pass
    return {'lemma': text, 'pos': 'unknown'}

def classify_semantics_with_api(words_list, yandex_key):
    dim_pattern = re.compile(r'\d+(?:[\.\,]\d+)?\s?[—Öx\*√ó]\s?\d+', re.IGNORECASE)
    grade_pattern = re.compile(r'^([–∞-—èa-z]{1,4}\-?\d+[–∞-—èa-z0-9]*)$', re.IGNORECASE)
    gost_pattern = re.compile(r'(–≥–æ—Å—Ç|din|—Ç—É|iso|—Å—Ç|—Å–ø)\s?\d+', re.IGNORECASE)
    SITE_UI_GARBAGE = {'–º–µ–Ω—é', '–ø–æ–∏—Å–∫', '–≥–ª–∞–≤–Ω–∞—è', '–∫–∞—Ä—Ç–∞', '—Å–∞–π—Ç', '–ª–∏—á–Ω—ã–π', '–∫–∞–±–∏–Ω–µ—Ç', '–≤—Ö–æ–¥', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è', '–∫–æ—Ä–∑–∏–Ω–∞', '–∏–∑–±—Ä–∞–Ω–Ω–æ–µ', '—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ', '–ø—Ä–æ—Ñ–∏–ª—å', '—Ç–µ–ª–µ—Ñ–æ–Ω', '–∞–¥—Ä–µ—Å', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', 'email', '–∑–≤–æ–Ω–æ–∫', 'callback', '–æ—Ç–∑—ã–≤', '–æ—Ç–∑—ã–≤—ã', '–≤–æ–ø—Ä–æ—Å', '–æ—Ç–≤–µ—Ç', '–º–µ–Ω–µ–¥–∂–µ—Ä', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è', '–ø–æ–ª–∏—Ç–∏–∫–∞', '–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ', '–æ—Ñ–µ—Ä—Ç–∞', 'cookie', '—Å–æ–≥–ª–∞—à–∞—Ç—å—Å—è', '—Å–æ–≥–ª–∞—Å–∏–µ', '–ø—Ä–∏–Ω–∏–º–∞—Ç—å', '–æ—Ç–ø—Ä–∞–≤–∏—Ç—å', '–æ—à–∏–±–∫–∞', '—É—Å–ø–µ—à–Ω–æ', '–∫–Ω–æ–ø–∫–∞', '—Ñ–æ—Ä–º–∞', '–ø–æ–ª–µ', '–æ–±–∑–æ—Ä', '–Ω–æ–≤–æ—Å—Ç–∏', '—Å—Ç–∞—Ç—å–∏', '—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞', '–æ–ø–∏—Å–∞–Ω–∏–µ', '–ø–∞—Ä–∞–º–µ—Ç—Ä', '—Å–≤–æ–π—Å—Ç–≤–æ', '–∞—Ä—Ç–∏–∫—É–ª', '–∫–æ–¥', '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä', '—Ñ–∏–ª—å—Ç—Ä', '—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞', '–ø–æ–∫–∞–∑–∞—Ç—å', '—Å–±—Ä–æ—Å–∏—Ç—å', '–∏–º—è', '—Ñ–∞–º–∏–ª–∏—è', '—Å–æ–æ–±—â–µ–Ω–∏–µ', '—Ñ–∞–π–ª', '–¥–æ–∫—É–º–µ–Ω—Ç', '—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç', '–∫–∞—Ç–µ–≥–æ—Ä–∏—è', '—Ä–∞–∑–¥–µ–ª', '—Å–ø–∏—Å–æ–∫', '–≤–∏–¥', '—Ç–∏–ø', '–∫–ª–∞—Å—Å', '—Å–µ—Ä–∏—è', '—Ä–µ–π—Ç–∏–Ω–≥', '–Ω–∞–ª–∏—á–∏–µ', '—Å–∫–ª–∞–¥', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', '–±—Ä–µ–Ω–¥', '–º–∞—Ä–∫–∞', '–≤–µ—Å', '–¥–ª–∏–Ω–∞', '—à–∏—Ä–∏–Ω–∞', '–≤—ã—Å–æ—Ç–∞', '—Ç–æ–ª—â–∏–Ω–∞', '–¥–∏–∞–º–µ—Ç—Ä', '—Ä–∞–∑–º–µ—Ä', '–æ–±—ä–µ–º', '–º–∞—Å—Å–∞', '—Ç–æ–Ω–Ω–∞', '–º–µ—Ç—Ä', '—à—Ç', '–∫–≥', '—É–ø–∞–∫–æ–≤–∫–∞', '—Ü–µ–Ω–∞', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '–º–∞–≥–∞–∑–∏–Ω', '–∫–∞—Ç–∞–ª–æ–≥', '–≥–æ–¥'}
    COMMERCIAL_WORDS = {'–∫—É–ø–∏—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Ü–µ–Ω–∞', '—Ü–µ–Ω—ã', '–ø—Ä–∞–π—Å', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø—Ä–æ–¥–∞–∂–∞', '–Ω–µ–¥–æ—Ä–æ–≥–æ', '–¥–µ—à–µ–≤–æ', '–¥–æ—Ä–æ–≥–æ', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞', '–æ–ø—Ç–æ–º', '—Ä–æ–∑–Ω–∏—Ü–∞', '—Ä—É–±', '—Ä—É–±–ª–µ–π', '—É–µ', '–∑–∞–∫–∞–∑', '–æ–ø–ª–∞—Ç–∞', '–ø–ª–∞—Ç–µ–∂', '—Ä–∞—Å—Å—Ä–æ—á–∫–∞', '–∫—Ä–µ–¥–∏—Ç', '–ª–∏–∑–∏–Ω–≥', '–¥–æ—Å—Ç–∞–≤–∫–∞', '—Å–∞–º–æ–≤—ã–≤–æ–∑', '–æ—Ç–≥—Ä—É–∑–∫–∞', '–ø–æ—Å—Ç–∞–≤–∫–∞', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '–ª–æ–≥–∏—Å—Ç–∏–∫–∞', '–≥–∞—Ä–∞–Ω—Ç–∏—è', '–≤–æ–∑–≤—Ä–∞—Ç', '–æ–±–º–µ–Ω', '–≤—ã–≥–æ–¥–Ω—ã–π', '–Ω–∏–∑–∫–∏–π', '–≤—ã—Å–æ–∫–∏–π', '–ª—É—á—à–∏–π', '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π', '–Ω–∞–¥–µ–∂–Ω—ã–π', '–±–æ–ª—å—à–æ–π', '–º–∞–ª—ã–π', '—É–¥–æ–±–Ω—ã–π', '–±—ã—Å—Ç—Ä—ã–π', '–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π', '—Ö–æ—Ä–æ—à–∏–π', '–¥–æ—Å—Ç—É–ø–Ω—ã–π', '–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π', '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π', '–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π', '—É–Ω–∏–∫–∞–ª—å–Ω—ã–π', '—à–∏—Ä–æ–∫–∏–π', '–æ–≥—Ä–æ–º–Ω—ã–π', '—Ä–∞–∑–ª–∏—á–Ω—ã–π'}
    GEO_ROOTS = ['–º–æ—Å–∫–≤', '–ø–∏—Ç–µ—Ä', '—Å–ø–±', '–µ–∫–±', '–µ–∫–∞—Ç–µ—Ä–∏–Ω', '—Ä–æ—Å—Å–∏', '—Ä—Ñ', '–≥–æ—Ä–æ–¥', '–æ–±–ª–∞—Å—Ç', '–Ω–æ–≤–≥–æ—Ä–æ–¥', '–∫–∞–∑–∞–Ω', '–∫–∏–µ–≤', '–º–∏–Ω—Å–∫', '–∞–ª–º–∞—Ç—ã', '—Å–∞–º–∞—Ä–∞', '–æ–º—Å–∫', '—á–µ–ª—è–±–∏–Ω', '—Ä–æ—Å—Ç–æ–≤', '—É—Ñ–∞', '–≤–æ–ª–≥–æ–≥—Ä–∞–¥', '–ø–µ—Ä–º', '–∫—Ä–∞—Å–Ω–æ—è—Ä', '–≤–æ—Ä–æ–Ω–µ–∂', '—Å–∞—Ä–∞—Ç–æ–≤', '–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä', '—Ç—é–º–µ–Ω', '–∏–∂–µ–≤—Å–∫', '—Ç–æ–ª—å—è—Ç—Ç–∏', '–±–∞—Ä–Ω–∞—É–ª', '–∏—Ä–∫—É—Ç—Å–∫', '—É–ª—å—è–Ω–æ–≤—Å–∫', '—Ö–∞–±–∞—Ä–æ–≤—Å–∫']
    SERVICE_KEYWORDS = {'—Ä–µ–∑–∫–∞', '–≥–∏–±–∫–∞', '—Å–≤–∞—Ä–∫–∞', '–æ—Ü–∏–Ω–∫–æ–≤–∫–∞', '—Ä—É–±–∫–∞', '–º–æ–Ω—Ç–∞–∂', '—É–∫–ª–∞–¥–∫–∞', '–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∏–∑–æ–ª—è—Ü–∏—è', '—Å–≤–µ—Ä–ª–µ–Ω–∏–µ', '–≥—Ä—É–Ω—Ç–æ–≤–∫–∞', '–ø–æ–∫—Ä–∞—Å–∫–∞', '—É—Å–ª—É–≥–∞', '–º–µ—Ç–∞–ª–ª–æ–æ–±—Ä–∞–±–æ—Ç–∫–∞', '–æ–±—Ä–∞–±–æ—Ç–∫–∞', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '—Ä–µ–º–æ–Ω—Ç', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ', '–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ'}

    categories = {'products': set(), 'services': set(), 'commercial': set(), 'dimensions': set()}
    api_candidates = []

    for word in words_list:
        word_lower = word.lower()
        if dim_pattern.search(word_lower) or grade_pattern.match(word_lower) or gost_pattern.search(word_lower) or word_lower.isdigit():
            categories['dimensions'].add(word_lower); continue
        
        lemma = morph.parse(word_lower)[0].normal_form if morph else word_lower
        if lemma in SITE_UI_GARBAGE or lemma in COMMERCIAL_WORDS: categories['commercial'].add(lemma); continue
        if any(root in lemma for root in GEO_ROOTS): categories['commercial'].add(lemma); continue
        if lemma in SERVICE_KEYWORDS or lemma.endswith('–æ–±—Ä–∞–±–æ—Ç–∫–∞'): categories['services'].add(lemma); continue
        api_candidates.append(word_lower)

    yandex_results = {} 
    if api_candidates and yandex_key:
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            future_to_word = {executor.submit(get_yandex_dict_info, w, yandex_key): w for w in api_candidates}
            for future in concurrent.futures.as_completed(future_to_word):
                orig_word = future_to_word[future]
                try: yandex_results[orig_word] = future.result()
                except: yandex_results[orig_word] = {'lemma': orig_word, 'pos': 'unknown'}
    else:
        for w in api_candidates: yandex_results[w] = {'lemma': w, 'pos': 'unknown'}

    for word in api_candidates:
        info = yandex_results.get(word, {'lemma': word, 'pos': 'unknown'})
        lemma = info['lemma']
        pos = info['pos']
        
        if lemma in SITE_UI_GARBAGE or lemma in COMMERCIAL_WORDS: categories['commercial'].add(lemma); continue
        is_service = False
        if lemma.endswith('–Ω–∏–µ') or lemma.endswith('–µ–Ω–∏–µ') or lemma.endswith('–æ–±—Ä–∞–±–æ—Ç–∫–∞') or lemma in SERVICE_KEYWORDS: is_service = True
        
        if is_service: categories['services'].add(lemma); continue

        if pos == 'noun' or pos == 'adjective' or pos == 'participle' or pos == 'unknown':
            if len(lemma) > 2: categories['products'].add(lemma)
        else: categories['commercial'].add(lemma)

    return {k: sorted(list(v)) for k, v in categories.items()}

def get_arsenkin_urls(query, engine_type, region_name, api_token, depth_val=10):
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check"
    url_get = "https://arsenkin.ru/api/tools/get"
    headers = {"Authorization": f"Bearer {api_token}", "Content-type": "application/json"}
    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []
    if "–Ø–Ω–¥–µ–∫—Å" in engine_type: se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type: se_params.append({"type": 11, "region": reg_ids['go']})

    payload = {"tools_name": "check-top", "data": {"queries": [query], "is_snippet": False, "noreask": True, "se": se_params, "depth": depth_val}}
    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        resp_json = r.json()
        if "error" in resp_json or "task_id" not in resp_json: st.error(f"‚ùå –û—à–∏–±–∫–∞ API: {resp_json}"); return []
        task_id = resp_json["task_id"]
        st.toast(f"–ó–∞–¥–∞—á–∞ ID {task_id} –∑–∞–ø—É—â–µ–Ω–∞")
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}"); return []

    status = "process"
    attempts = 0
    while status == "process" and attempts < 40:
        time.sleep(5); attempts += 1
        try:
            r_check = requests.post(url_check, headers=headers, json={"task_id": task_id})
            res_check_data = r_check.json()
            if res_check_data.get("status") == "finish": status = "done"; break
        except: pass

    if status != "done": st.error(f"‚è≥ –¢–∞–π–º-–∞—É—Ç API"); return []

    try:
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id}, timeout=30)
        res_data = r_final.json()
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}"); return []

    results_list = []
    try:
        collect = res_data.get('result', {}).get('result', {}).get('collect')
        if not collect: return []
        final_url_list = []
        if isinstance(collect, list) and len(collect) > 0 and isinstance(collect[0], list): final_url_list = collect[0][0]
        else:
             unique_urls = set()
             for engine_data in collect:
                 if isinstance(engine_data, dict):
                     for _, serps in engine_data.items():
                         for item in serps:
                             if item.get('url') and item.get('url') not in unique_urls:
                                 results_list.append({'url': item['url'], 'pos': item['pos']})
                                 unique_urls.add(item['url'])
             return results_list

        if final_url_list:
            for index, url in enumerate(final_url_list): results_list.append({'url': url, 'pos': index + 1})
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}"); return []
    return results_list

def process_text_detailed(text, settings, n_gram=1):
    text = text.lower().replace('—ë', '–µ')
    words = re.findall(r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+', text)
    stops = set(w.lower().replace('—ë', '–µ') for w in settings['custom_stops'])
    lemmas = []
    forms_map = defaultdict(set)
    for w in words:
        if len(w) < 2: continue
        if not settings['numbers'] and w.isdigit(): continue
        if w in stops: continue
        lemma = w
        if USE_NLP and n_gram == 1:
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            lemma = p.normal_form.replace('—ë', '–µ')
        lemmas.append(lemma)
        forms_map[lemma].add(w)
    return lemmas, forms_map

def parse_page(url, settings):
    headers = {'User-Agent': settings['ua']}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code != 200: return None
        soup = BeautifulSoup(r.text, 'html.parser')
        tags_to_remove = []
        if settings['noindex']: tags_to_remove.append('noindex')
        for c in soup.find_all(string=lambda text: isinstance(text, Comment)): c.extract()
        if tags_to_remove:
            for t in soup.find_all(tags_to_remove): t.decompose()
        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)
        extra_text = []
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'): extra_text.append(meta_desc['content'])
        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])
        body_text_raw = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text = re.sub(r'\s+', ' ', body_text_raw).strip()
        if not body_text: return None
        return {'url': url, 'domain': urlparse(url).netloc, 'body_text': body_text, 'anchor_text': anchor_text}
    except: return None

def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    all_forms_map = defaultdict(set)
    if not my_data or not my_data.get('body_text'): my_lemmas, my_forms, my_anchors, my_len = [], {}, [], 0
    else:
        my_lemmas, my_forms = process_text_detailed(my_data['body_text'], settings)
        my_anchors, _ = process_text_detailed(my_data['anchor_text'], settings)
        my_len = len(my_lemmas)
        for k, v in my_forms.items(): all_forms_map[k].update(v)

    comp_data_parsed = [d for d in comp_data_full if d.get('body_text')]
    comp_docs = []
    for p in comp_data_parsed:
        body, c_forms = process_text_detailed(p['body_text'], settings)
        anchor, _ = process_text_detailed(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor, 'url': p['url'], 'domain': p['domain']})
        for k, v in c_forms.items(): all_forms_map[k].update(v)

    if not comp_docs:
        return { "depth": pd.DataFrame(), "hybrid": pd.DataFrame(), "relevance_top": pd.DataFrame(), "my_score": {"width": 0, "depth": 0}, "missing_semantics_high": [], "missing_semantics_low": [] }

    c_lens = [len(d['body']) for d in comp_docs]
    avg_dl = np.mean(c_lens) if c_lens else 1
    median_len = np.median(c_lens) if c_lens else 0
    norm_k_recs = (my_len / median_len) if (median_len > 0 and my_len > 0 and settings['norm']) else 1.0

    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    N = len(comp_docs)
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
    word_counts_per_doc = [Counter(d['body']) for d in comp_docs]

    word_idf_map = {}
    for lemma in vocab:
        df = doc_freqs[lemma]
        if df == 0: continue
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        word_idf_map[lemma] = max(idf, 0.01)

    S_WIDTH_CORE = set()
    missing_semantics_high = []
    missing_semantics_low = []
    my_full_lemmas_set = set(my_lemmas) | set(my_anchors)
    lsi_candidates_weighted = []

    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        c_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        med_val = np.median(c_counts)
        percent = int((doc_freqs[lemma] / N) * 100)
        weight_simple = word_idf_map.get(lemma, 0) * med_val
        if med_val > 0: lsi_candidates_weighted.append((lemma, weight_simple))
        is_width_word = False
        if med_val >= 1: S_WIDTH_CORE.add(lemma); is_width_word = True

        if lemma not in my_full_lemmas_set:
            if len(lemma) < 2 or lemma.isdigit(): continue
            item = {'word': lemma, 'percent': percent, 'weight': weight_simple}
            if is_width_word: missing_semantics_high.append(item)
            elif percent >= 30: missing_semantics_low.append(item)

    missing_semantics_high.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low.sort(key=lambda x: x['percent'], reverse=True)
    lsi_candidates_weighted.sort(key=lambda x: x[1], reverse=True)
    S_DEPTH_TOP70 = set([x[0] for x in lsi_candidates_weighted[:70]])
    total_width_core_count = len(S_WIDTH_CORE)

    def calculate_bm25_okapi(doc_tokens, doc_len):
        if avg_dl == 0 or doc_len == 0: return 0
        score = 0
        counts = Counter(doc_tokens)
        k1 = 1.2; b = 0.75
        target_words = S_WIDTH_CORE if S_WIDTH_CORE else S_DEPTH_TOP70
        for word in target_words:
            if word not in counts: continue
            tf = counts[word]
            idf = word_idf_map.get(word, 0)
            score += idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avg_dl)))
        return score

    def calculate_width_score_val(lemmas_set):
        if total_width_core_count == 0: return 0
        ratio = len(lemmas_set.intersection(S_WIDTH_CORE)) / total_width_core_count
        return 100 if ratio >= 0.9 else int(round((ratio / 0.9) * 100))

    competitor_scores_map = {}
    comp_bm25_list = []
    for i, doc in enumerate(comp_docs):
        raw_bm25 = calculate_bm25_okapi(doc['body'], c_lens[i])
        comp_bm25_list.append(raw_bm25)
        width_val = calculate_width_score_val(set(doc['body']))
        competitor_scores_map[doc['url']] = {'width_final': min(100, width_val), 'bm25_val': raw_bm25}

    median_bm25_top = np.median(comp_bm25_list) if comp_bm25_list else 0
    spam_limit = median_bm25_top * 1.25 if median_bm25_top > 0 else 1

    for url, scores in competitor_scores_map.items():
        depth_val = int(round((scores['bm25_val'] / spam_limit) * 100))
        scores['depth_final'] = min(100, depth_val)

    my_bm25 = calculate_bm25_okapi(my_lemmas, my_len)
    my_depth_score_final = min(100, int(round((my_bm25 / spam_limit) * 100)))
    my_width_score_final = min(100, calculate_width_score_val(my_full_lemmas_set))

    table_depth, table_hybrid = [], []
    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        df = doc_freqs[lemma]
        if df < 2 and lemma not in my_lemmas: continue
        my_tf_count = my_lemmas.count(lemma)
        forms_str = ", ".join(sorted(list(all_forms_map.get(lemma, set())))) if all_forms_map.get(lemma) else lemma
        c_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        med_total = np.median(c_counts); max_total = np.max(c_counts)
        base_min = min(np.mean(c_counts), med_total)
        rec_min = int(math.ceil(base_min * norm_k_recs))
        rec_max = int(round(max_total * norm_k_recs))
        if rec_max < rec_min: rec_max = rec_min
        rec_median = med_total * norm_k_recs
        
        status = "–ù–æ—Ä–º–∞"; action_diff = 0; action_text = "‚úÖ"
        if my_tf_count < rec_min:
            status = "–ù–µ–¥–æ—Å–ø–∞–º"; action_diff = int(round(rec_min - my_tf_count))
            if action_diff == 0: action_diff = 1
            action_text = f"+{action_diff}"
        elif my_tf_count > rec_max:
            status = "–ü–µ—Ä–µ—Å–ø–∞–º"; action_diff = int(round(my_tf_count - rec_max))
            if action_diff == 0: action_diff = 1
            action_text = f"-{action_diff}"

        depth_percent = int(round((my_tf_count / rec_median) * 100)) if rec_median > 0.1 else (0 if my_tf_count == 0 else 100)
        weight_hybrid = word_idf_map.get(lemma, 0) * (my_tf_count / my_len if my_len > 0 else 0)
        table_depth.append({
            "–°–ª–æ–≤–æ": lemma, "–°–ª–æ–≤–æ—Ñ–æ—Ä–º—ã": forms_str, "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_tf_count,
            "–ú–µ–¥–∏–∞–Ω–∞": round(med_total, 1), "–ú–∏–Ω–∏–º—É–º (—Ä–µ–∫)": rec_min, "–ú–∞–∫—Å–∏–º—É–º (—Ä–µ–∫)": rec_max,
            "–ì–ª—É–±–∏–Ω–∞ %": min(100, depth_percent), "–°—Ç–∞—Ç—É—Å": status, "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": action_text,
            "is_missing": (status == "–ù–µ–¥–æ—Å–ø–∞–º" and my_tf_count == 0), "sort_val": abs(action_diff) if status != "–ù–æ—Ä–º–∞" else 0
        })
        table_hybrid.append({
            "–°–ª–æ–≤–æ": lemma, "TF-IDF –¢–û–ü": round(word_idf_map.get(lemma, 0) * (med_total / avg_dl if avg_dl > 0 else 0), 4),
            "TF-IDF —É –≤–∞—Å": round(weight_hybrid, 4), "–°–∞–π—Ç–æ–≤": df, "–ü–µ—Ä–µ—Å–ø–∞–º": max_total
        })

    table_rel = []
    for item in original_results:
        url = item['url']
        scores = competitor_scores_map.get(url, {'width_final':0, 'depth_final':0})
        table_rel.append({ "–î–æ–º–µ–Ω": urlparse(url).netloc, "–ü–æ–∑–∏—Ü–∏—è": item['pos'],
