import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
from collections import Counter
import math
import inspect
import concurrent.futures
from urllib.parse import urlparse

# ==========================================
# 1. –°–¢–ò–õ–¨ (–ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –ë–ï–õ–´–ô + –ö–û–ú–ü–ê–ö–¢–ù–´–ô)
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO", page_icon="üíé")

st.markdown("""
    <style>
        /* –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –°–í–ï–¢–õ–ê–Ø –¢–ï–ú–ê –î–õ–Ø –í–°–ï–• –≠–õ–ï–ú–ï–ù–¢–û–í */
        :root {
            --primary-color: #2563EB;
            --background-color: #F0F4F8;
            --secondary-background-color: #FFFFFF;
            --text-color: #1E293B;
            --font: 'Inter', sans-serif;
        }
        
        /* –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å–±—Ä–æ—Å */
        [data-testid="stAppViewContainer"], [data-testid="stHeader"] {
            background-color: #F0F4F8 !important;
        }
        
        /* –£–±–∏—Ä–∞–µ–º —á–µ—Ä–Ω–æ—Ç—É –≤ –≤—ã–ø–∞–¥–∞—é—â–∏—Ö —Å–ø–∏—Å–∫–∞—Ö –∏ —Ö–µ–¥–µ—Ä–∞—Ö */
        div[data-baseweb="select"] > div, div[data-baseweb="popover"], ul[data-baseweb="menu"] {
            background-color: #FFFFFF !important;
            color: #1E293B !important;
        }
        li[data-baseweb="option"] {
            color: #1E293B !important;
        }
        div[data-testid="stExpander"] {
            background-color: #FFFFFF !important;
            color: #1E293B !important;
            border: 1px solid #E2E8F0;
        }
        .streamlit-expanderHeader {
            background-color: #FFFFFF !important;
            color: #1E293B !important;
        }
        
        /* –¢–µ–∫—Å—Ç */
        h1, h2, h3, h4, label, span, p {
            color: #1E293B !important;
        }
        
        /* –ü–æ–ª—è –≤–≤–æ–¥–∞ (Input/Textarea) */
        .stTextInput input, .stTextArea textarea {
            background-color: #FFFFFF !important;
            color: #1E293B !important;
            border: 1px solid #CBD5E1 !important;
        }
        
        /* –£–º–µ–Ω—å—à–∞–µ–º —à–∏—Ä–∏–Ω—É –∫–æ–ª–æ–Ω–æ–∫ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö */
        [data-testid="column"] {
            padding: 0 10px;
        }
        
        /* –ö–Ω–æ–ø–∫–∞ */
        div.stButton > button {
            background-color: #2563EB !important;
            color: white !important;
            border: none;
            padding: 10px;
            font-weight: 600;
        }
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 2. –ë–≠–ö–ï–ù–î
# ==========================================

# --- –ü–∞—Ç—á NLP ---
try:
    if not hasattr(inspect, 'getargspec'):
        def getargspec(func):
            spec = inspect.getfullargspec(func)
            return spec.args, spec.varargs, spec.varkw, spec.defaults
        inspect.getargspec = getargspec
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except:
    morph = None
    USE_NLP = False

# --- –ü–æ–∏—Å–∫ ---
try:
    from googlesearch import search
    USE_SEARCH = True
except:
    USE_SEARCH = False

DEFAULT_EXCLUDE = """yandex.ru
avito.ru
ozon.ru
wildberries.ru
youtube.com
dzen.ru
hh.ru
t.me
tiu.ru"""

DEFAULT_STOPS = """—Ä—É–±–ª–µ–π
—Ä—É–±
–∫—É–ø–∏—Ç—å
—Ü–µ–Ω–∞
—à—Ç
—Å–º
–º–º
–∫–≥
–∫–≤
–º2
—Å—Ç—Ä
—É–ª"""

def process_text(text, settings, n_gram=1):
    pattern = r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+' if settings['numbers'] else r'[–∞-—è–ê-–Ø—ë–Åa-zA-Z]+'
    words = re.findall(pattern, text.lower())
    stops = set(w.lower() for w in settings['custom_stops'])
    clean_words = []
    
    for w in words:
        if len(w) < 2 or w in stops: continue
        lemma = w
        if USE_NLP and n_gram == 1: 
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            lemma = p.normal_form
        clean_words.append(lemma)
    
    if n_gram > 1:
        ngrams = []
        for i in range(len(clean_words) - n_gram + 1):
            phrase = " ".join(clean_words[i:i+n_gram])
            ngrams.append(phrase)
        return ngrams
    return clean_words

def parse_page(url, settings):
    headers = {'User-Agent': settings['ua']}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code != 200: return None
        soup = BeautifulSoup(r.text, 'html.parser')
        
        title = soup.title.string.strip() if soup.title and soup.title.string else ""
        desc = ""
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc: desc = meta_desc.get("content", "").strip()
        h1 = soup.find("h1").get_text(strip=True) if soup.find("h1") else ""
        
        if settings['noindex']:
            for t in soup.find_all(['noindex', 'script', 'style', 'head', 'footer', 'nav']): t.decompose()
        else:
            for t in soup(['script', 'style', 'head']): t.decompose()
            
        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)
        
        extra_text = []
        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])
        body_text = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        
        return {
            'url': url, 'domain': urlparse(url).netloc, 'title': title, 'desc': desc, 'h1': h1,
            'body_text': body_text, 'anchor_text': anchor_text
        }
    except: return None

def calculate_metrics(comp_data, my_data, settings):
    my_lemmas = process_text(my_data['body_text'], settings)
    my_anchors = process_text(my_data['anchor_text'], settings)
    
    comp_docs = []
    for p in comp_data:
        body = process_text(p['body_text'], settings)
        anchor = process_text(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor})
        
    avg_len = np.mean([len(d['body']) for d in comp_docs])
    my_len = len(my_lemmas)
    norm_k = (my_len / avg_len) if (settings['norm'] and avg_len > 0) else 1.0
    
    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    
    N = len(comp_docs)
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
        
    k1, b = 1.2, 0.75
    table_depth, table_hybrid = [], []
    
    for word in vocab:
        df = doc_freqs[word]
        if df < 2 and word not in my_lemmas: continue 
        
        my_tf = my_lemmas.count(word)
        my_anch_tf = my_anchors.count(word)
        
        c_body_tfs = [d['body'].count(word) for d in comp_docs]
        c_anch_tfs = [d['anchor'].count(word) for d in comp_docs]
        
        med_tf = np.median(c_body_tfs)
        med_anch = np.median(c_anch_tfs)
        max_tf = np.max(c_body_tfs)
        mean_tf = np.mean(c_body_tfs)
        
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        
        bm25_scores = []
        for i, d in enumerate(comp_docs):
            tf = c_body_tfs[i]
            dl = len(d['body'])
            score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (dl / avg_len)))
            bm25_scores.append(score)
        bm25_top = np.median(bm25_scores)
        bm25_my = idf * (my_tf * (k1 + 1)) / (my_tf + k1 * (1 - b + b * (my_len / avg_len)))
        
        target_body = int(med_tf * 1.3 * norm_k)
        diff_body = target_body - my_tf
        target_anch = int(med_anch * norm_k)
        diff_anch = target_anch - my_anch_tf
        
        if med_tf > 0.5 or my_tf > 0:
            table_depth.append({
                "–°–ª–æ–≤–æ": word, "–°–ª–æ–≤–æ—Ñ–æ—Ä–º—ã": word, "–ü–æ–≤—Ç–æ—Ä—ã —É –≤–∞—Å": my_tf, 
                "–ú–∏–Ω–∏–º—É–º": np.min(c_body_tfs), "–ú–∞–∫—Å–∏–º—É–º": int(max_tf * norm_k),
                "–û–±—â–µ–µ –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": diff_body,
                "–¢–µ–≥ A —É –≤–∞—Å": my_anch_tf, "–¢–µ–≥ A —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏": target_anch,
                "–¢–µ–≥ A –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": diff_anch,
                "–¢–µ–∫—Å—Ç —É –≤–∞—Å": my_tf, "–¢–µ–∫—Å—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏": target_body, "–¢–µ–∫—Å—Ç –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": diff_body,
                "–ü–µ—Ä–µ—Å–ø–∞–º": int(max_tf * norm_k), "–ü–µ—Ä–µ—Å–ø–∞–º*IDF": round(max_tf * norm_k * idf, 1),
                "diff_abs": abs(diff_body)
            })
            table_hybrid.append({
                "–°–ª–æ–≤–æ": word, "TF-IDF –¢–û–ü": round(med_tf * idf, 2), "TF-IDF –≤–∞—à —Å–∞–π—Ç": round(my_tf * idf, 2),
                "BM25 –¢–û–ü": round(bm25_top, 2), "BM25 –≤–∞—à —Å–∞–π—Ç": round(bm25_my, 2), "IDF": round(idf, 2),
                "–ö–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤": df, "–ú–µ–¥–∏–∞–Ω–∞": round(med_tf, 1), "–ü–µ—Ä–µ—Å–ø–∞–º": max_tf,
                "–°—Ä–µ–¥–Ω–µ–µ –ø–æ –¢–û–ü—É": round(mean_tf, 1), "–í–∞—à —Å–∞–π—Ç": my_tf,
                "<a> –ø–æ –¢–û–ü—É": round(med_anch, 1), "<a> –≤–∞—à —Å–∞–π—Ç": my_anch_tf
            })

    my_bi = process_text(my_data['body_text'], settings, 2)
    comp_bi = [process_text(p['body_text'], settings, 2) for p in comp_data]
    all_bi = set(my_bi)
    for c in comp_bi: all_bi.update(c)
    bi_freqs = Counter()
    for c in comp_bi:
        for b_ in set(c): bi_freqs[b_] += 1
    table_ngrams = []
    for bg in all_bi:
        df = bi_freqs[bg]
        if df < 2 and bg not in my_bi: continue
        my_c = my_bi.count(bg)
        comp_c = [c.count(bg) for c in comp_bi]
        med_c = np.median(comp_c)
        if med_c > 0 or my_c > 0:
            table_ngrams.append({
                "N-–≥—Ä–∞–º–º–∞": bg, "–ö–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤": df, "–ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ": med_c,
                "–°—Ä–µ–¥–Ω–µ–µ": round(np.mean(comp_c), 1), "–ù–∞ –Ω–∞—à–µ–º —Å–∞–π—Ç–µ": my_c,
                "TF-IDF": round(my_c * math.log(N/df if df>0 else 1), 3)
            })

    table_rel = []
    for i, p in enumerate(comp_data):
        p_lemmas = process_text(p['body_text'], settings)
        w = len(set(p_lemmas).intersection(vocab))
        table_rel.append({
            "–î–æ–º–µ–Ω": p['domain'], "–ü–æ–∑–∏—Ü–∏—è": i+1, "URL": p['url'],
            "–®–∏—Ä–∏–Ω–∞": w, "–ì–ª—É–±–∏–Ω–∞": len(p_lemmas)
        })
        
    return {
        "depth": pd.DataFrame(table_depth), "hybrid": pd.DataFrame(table_hybrid),
        "ngrams": pd.DataFrame(table_ngrams), "relevance_top": pd.DataFrame(table_rel),
        "my_score": {"width": len(set(my_lemmas).intersection(vocab)), "depth": len(my_lemmas)}
    }

# ==========================================
# 3. –ò–ù–¢–ï–†–§–ï–ô–°
# ==========================================

st.markdown("<h2 style='text-align: center; color: #1D4ED8;'>SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä</h2>", unsafe_allow_html=True)

# 1. –í–í–û–î
st.markdown("##### üìù –í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö")
c1, c2 = st.columns(2)
with c1:
    my_url = st.text_input("–í–∞—à URL", placeholder="https://site.ru/page")
with c2:
    query = st.text_input("–ó–∞–ø—Ä–æ—Å", placeholder="–∫—É–ø–∏—Ç—å –æ–∫–Ω–∞")

# 2. –ö–û–ù–ö–£–†–ï–ù–¢–´
st.markdown("##### üïµÔ∏è –ò—Å—Ç–æ—á–Ω–∏–∫")
source_type = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫:", ["Google (–ê–≤—Ç–æ)", "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"], horizontal=True, label_visibility="collapsed")

if source_type == "Google (–ê–≤—Ç–æ)":
    cl1, cl2 = st.columns([1, 2]) # –°–¥–µ–ª–∞–ª –∫–æ–ª–æ–Ω–∫—É –¥–ª—è –¥–æ–º–µ–Ω–æ–≤ —à–∏—Ä–µ, —á—Ç–æ–±—ã –Ω–µ –æ–±—Ä–µ–∑–∞–ª–æ—Å—å
    with cl1:
        top_n = st.selectbox("–ì–ª—É–±–∏–Ω–∞ –¢–û–ü–∞", [5, 10, 20], index=1)
    with cl2:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º text_area —Å –º–∞–ª–æ–π –≤—ã—Å–æ—Ç–æ–π, —á—Ç–æ–±—ã –ø–∏—Å–∞—Ç—å –≤ —Å—Ç–æ–ª–±–∏–∫, –Ω–æ –∫–æ–º–ø–∞–∫—Ç–Ω–æ
        excludes = st.text_area("–ò—Å–∫–ª—é—á–∏—Ç—å –¥–æ–º–µ–Ω—ã (–∫–∞–∂–¥—ã–π —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", DEFAULT_EXCLUDE, height=100)
else:
    manual_urls = st.text_area("URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (—Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", height=150)

# 3. –ù–ê–°–¢–†–û–ô–ö–ò
st.markdown("##### ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
with st.expander("–û—Ç–∫—Ä—ã—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã", expanded=False):
    col_set1, col_set2 = st.columns([1, 1]) # –î–≤–µ —Ä–∞–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    
    with col_set1:
        ua = st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"])
        # –°–¥–µ–ª–∞–ª –ø–æ–ª–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ —É–∂–µ (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –∫–æ–ª–æ–Ω–∫–∏) –∏ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–º
        c_stops = st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞ (—Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", DEFAULT_STOPS, height=150)
        
    with col_set2:
        st.write("") # –û—Ç—Å—Ç—É–ø
        s_noindex = st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å noindex", True)
        s_alt = st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", False)
        s_num = st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", False)
        s_norm = st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True)
        s_agg = st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã", True)

st.markdown("<br>", unsafe_allow_html=True)

if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó üöÄ"):
    if not my_url:
        st.error("–í–≤–µ–¥–∏—Ç–µ URL!")
        st.stop()
        
    settings = {
        'noindex': s_noindex, 'alt_title': s_alt, 'numbers': s_num,
        'norm': s_norm, 'ua': ua, 'custom_stops': c_stops.split()
    }
    
    target_urls = []
    if source_type == "Google (–ê–≤—Ç–æ)":
        if not query:
            st.error("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å!")
            st.stop()
        try:
            excl = excludes.split()
            if s_agg: excl.extend(["avito", "ozon", "wildberries", "market", "tiu"])
            with st.spinner("–°–±–æ—Ä –¢–û–ü–∞..."):
                found = search(query, num_results=top_n*2, lang="ru")
                cnt = 0
                for u in found:
                    if my_url in u: continue
                    if any(x in u for x in excl): continue
                    target_urls.append(u)
                    cnt += 1
                    if cnt >= top_n: break
        except:
            st.error("–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞.")
            st.stop()
    else:
        target_urls = [u.strip() for u in manual_urls.split('\n') if u.strip()]
        
    if not target_urls:
        st.error("–ù–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.")
        st.stop()
        
    prog = st.progress(0)
    status = st.empty()
    status.text("–°–∫–∞—á–∏–≤–∞–µ–º –≤–∞—à —Å–∞–π—Ç...")
    my_data = parse_page(my_url, settings)
    
    if not my_data:
        st.error("–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–∞–π—Ç—É.")
        st.stop()
        
    comp_data = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(parse_page, u, settings): u for u in target_urls}
        done = 0
        for f in concurrent.futures.as_completed(futures):
            res = f.result()
            if res: comp_data.append(res)
            done += 1
            prog.progress(done / len(target_urls))
            
    prog.empty()
    status.empty()
    
    if len(comp_data) < 2:
        st.error("–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö.")
        st.stop()
        
    results = calculate_metrics(comp_data, my_data, settings)
    st.success("–ì–æ—Ç–æ–≤–æ!")
    
    m1, m2, m3 = st.columns(3)
    m1.metric("–®–∏—Ä–∏–Ω–∞", results['my_score']['width'])
    m2.metric("–ì–ª—É–±–∏–Ω–∞", results['my_score']['depth'])
    m3.metric("–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤", len(comp_data))
    
    st.markdown("### 1. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
    df_d = results['depth']
    if not df_d.empty:
        df_d = df_d.sort_values(by="diff_abs", ascending=False)
        def color(v):
            if isinstance(v, (int, float)):
                if v > 0: return 'background-color: #DCFCE7; color: #14532D'
                if v < 0: return 'background-color: #FEE2E2; color: #7F1D1D'
            return ''
        st.dataframe(
            df_d.style.map(color, subset=['–û–±—â–µ–µ –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å', '–¢–µ–≥ A –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å', '–¢–µ–∫—Å—Ç –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å']),
            column_config={"diff_abs": None}, use_container_width=True, height=600
        )
        st.download_button("–°–∫–∞—á–∞—Ç—å CSV", df_d.to_csv().encode('utf-8'), "depth.csv")
    
    with st.expander("2. –ì–∏–±—Ä–∏–¥–Ω—ã–π –¢–û–ü"):
        st.dataframe(results['hybrid'].sort_values(by="TF-IDF –¢–û–ü", ascending=False), use_container_width=True)
        
    with st.expander("3. N-–≥—Ä–∞–º–º—ã"):
        st.dataframe(results['ngrams'].sort_values(by="TF-IDF", ascending=False), use_container_width=True)
        
    with st.expander("4. –¢–û–ü —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"):
        st.dataframe(results['relevance_top'], use_container_width=True)
