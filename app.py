import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin
import inspect
import time
import json
import io
import os
import random

def transliterate_text(text):
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç '–®–≤–µ–ª–ª–µ—Ä' –≤ 'shveller', '–ê–Ω–æ–¥' –≤ 'anod'.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–∞ –≤ URL.
    """
    mapping = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }
    result = []
    for char in text.lower():
        if char in mapping:
            result.append(mapping[char])
        elif char.isalnum() or char == '-':
            result.append(char)
    return "".join(result)

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ openai
try:
    import openai
except ImportError:
    openai = None

# ==========================================
# 0. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–û–°–¢–û–Ø–ù–ò–Ø (SESSION STATE)
# ==========================================
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False

# –°–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
if 'ai_generated_df' not in st.session_state:
    st.session_state.ai_generated_df = None
if 'ai_excel_bytes' not in st.session_state:
    st.session_state.ai_excel_bytes = None

# –°–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –¢–µ–≥–æ–≤ –∏ –¢–∞–±–ª–∏—Ü
if 'tags_html_result' not in st.session_state:
    st.session_state.tags_html_result = None
if 'table_html_result' not in st.session_state:
    st.session_state.table_html_result = None

# --- –ù–û–í–´–ï –°–û–°–¢–û–Ø–ù–ò–Ø –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò ---
if 'categorized_products' not in st.session_state:
    st.session_state.categorized_products = []
if 'categorized_services' not in st.session_state:
    st.session_state.categorized_services = []
if 'categorized_commercial' not in st.session_state:
    st.session_state.categorized_commercial = []
if 'categorized_dimensions' not in st.session_state:
    st.session_state.categorized_dimensions = []

# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫
if 'persistent_urls' not in st.session_state:
    st.session_state['persistent_urls'] = ""

if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´ –ò –°–ü–ò–°–ö–ò
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO v2.3 (Secure)", page_icon="üìä")

GARBAGE_LATIN_STOPLIST = {
    'whatsapp', 'viber', 'telegram', 'skype', 'vk', 'instagram', 'facebook', 'youtube', 'twitter',
    'cookie', 'cookies', 'policy', 'privacy', 'agreement', 'terms',
    'click', 'submit', 'send', 'zakaz', 'basket', 'cart', 'order', 'call', 'back', 'callback',
    'login', 'logout', 'sign', 'register', 'auth', 'account', 'profile',
    'search', 'menu', 'nav', 'navigation', 'footer', 'header', 'sidebar',
    'img', 'jpg', 'png', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'svg',
    'ok', 'error', 'undefined', 'null', 'true', 'false', 'var', 'let', 'const', 'function', 'return',
    'ru', 'en', 'com', 'net', 'org', 'biz', 'shop', 'store',
    'phone', 'email', 'tel', 'fax', 'mob', 'address', 'copyright', 'all', 'rights', 'reserved',
    'div', 'span', 'class', 'id', 'style', 'script', 'body', 'html', 'head', 'meta', 'link'
}

# ==========================================
# 2. –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø
# ==========================================
def check_password():
    if st.session_state.get("authenticated"):
        return True

    st.markdown("""
        <style>
        .main { display: flex; flex-direction: column; justify-content: center; align-items: center; }
        .auth-logo-box { text-align: center; margin-bottom: 1rem; padding-top: 0; }
        </style>
    """, unsafe_allow_html=True)

    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="auth-logo-box"><h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3></div>', unsafe_allow_html=True)
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä–æ–ª—å –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
            if password == "jfV6Xel-Q7vp-_s2UYPO":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

# ==========================================
# 3. –ü–û–õ–£–ß–ï–ù–ò–ï API –ö–õ–Æ–ß–ï–ô (–ë–ï–ó–û–ü–ê–°–ù–û)
# ==========================================
# –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å Arsenkin Token
if "arsenkin_token" in st.session_state:
    ARSENKIN_TOKEN = st.session_state.arsenkin_token
else:
    try:
        ARSENKIN_TOKEN = st.secrets["api"]["arsenkin_token"]
    except (FileNotFoundError, KeyError):
        ARSENKIN_TOKEN = None

# –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å Yandex Key
if "yandex_dict_key" in st.session_state:
    YANDEX_DICT_KEY = st.session_state.yandex_dict_key
else:
    try:
        YANDEX_DICT_KEY = st.secrets["api"]["yandex_dict_key"]
    except (FileNotFoundError, KeyError):
        YANDEX_DICT_KEY = None


REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956},
    "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931},
    "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–ö–∏–µ–≤ (UA)": {"ya": 143, "go": 1012852},
    "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493},
    "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601}
}

DEFAULT_EXCLUDE_DOMAINS = [
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "ebay.com",
    "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", "pandao.ru",
    "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", "banki.ru",
    "regmarkets.ru", "zoon.ru", "pulscen.ru", "prodoctorov.ru", "blizko.ru",
    "domclick.ru", "satom.ru", "quto.ru", "edadeal.ru", "cataloxy.ru",
    "irr.ru", "onliner.by", "shop.by", "deal.by", "yell.ru",
    "profi.ru", "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by",
    "market.yandex.ru", "youtube.com", "gosuslugi.ru", "dzen.ru",
    "2gis.by", "wildberries.ru", "rutube.ru", "vk.com", "facebook.com"
]
DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n–∫—É–ø–∏—Ç—å\n—Ü–µ–Ω–∞\n—à—Ç\n—Å–º\n–º–º\n–∫–≥\n–∫–≤\n–º2\n—Å—Ç—Ä\n—É–ª"

# –¶–≤–µ—Ç–∞ –∏ —Å—Ç–∏–ª–∏
PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE"

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif; color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important; }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important; color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] div[role="gridcell"] {{
            background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; border-bottom: 1px solid {ROW_BORDER_COLOR} !important;
        }}
        .legend-box {{ padding: 10px; background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px; }}
        .text-red {{ color: #D32F2F; font-weight: bold; }}
        .text-green {{ color: #2E7D32; font-weight: bold; }}
        .text-bold {{ font-weight: 600; }}
        .sort-container {{ background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 8px; margin-bottom: 10px; border: 1px solid {BORDER_COLOR}; }}
        
        .stApp > header {{ background-color: transparent !important; }}
        .stTextInput input:disabled, .stTextArea textarea:disabled, .stSelectbox div[aria-disabled="true"] {{
            opacity: 1 !important; background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; cursor: text !important; -webkit-text-fill-color: {TEXT_COLOR} !important; border-color: {BORDER_COLOR} !important;
        }}
        .stButton button:disabled {{ opacity: 1 !important; background-color: {PRIMARY_COLOR} !important; color: white !important; cursor: progress !important; }}
        div[data-testid="stAppViewContainer"] {{ filter: none !important; opacity: 1 !important; transition: none !important; }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 4. –õ–û–ì–ò–ö–ê (–ë–≠–ö–ï–ù–î)
# ==========================================

try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except Exception as e:
    morph = None
    USE_NLP = False
    st.sidebar.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ NLP: {e}")

# --- –§–£–ù–ö–¶–ò–Ø –ó–ê–ü–†–û–°–ê –ö –Ø–ù–î–ï–ö–° –°–õ–û–í–ê–†–Æ ---
def get_yandex_dict_info(text, api_key):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É (–ª–µ–º–º—É) –∏ —á–∞—Å—Ç—å —Ä–µ—á–∏ (pos) —á–µ—Ä–µ–∑ API –Ø–Ω–¥–µ–∫—Å–∞.
    """
    if not api_key:
        return {'lemma': text, 'pos': 'unknown'}
        
    url = "https://dictionary.yandex.net/api/v1/dicservice.json/lookup"
    params = {
        'key': api_key,
        'lang': 'ru-ru', 
        'text': text,
        'ui': 'ru'
    }
    try:
        r = requests.get(url, params=params, timeout=2)
        if r.status_code == 200:
            data = r.json()
            if data.get('def'):
                first_def = data['def'][0]
                lemma = first_def.get('text', text)
                pos = first_def.get('pos', 'unknown')
                return {'lemma': lemma, 'pos': pos}
    except:
        pass
    return {'lemma': text, 'pos': 'unknown'}

# --- –§–£–ù–ö–¶–ò–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –° API –Ø–ù–î–ï–ö–°–ê ---
def classify_semantics_with_api(words_list, yandex_key):
    # --- 1. –ë–ê–ó–û–í–´–ï –°–ü–ò–°–ö–ò (–¢–û–õ–¨–ö–û –ú–£–°–û–† –ò –ö–û–ú–ú–ï–†–¶–ò–Ø) ---
    # –ú—ã —É–±–∏—Ä–∞–µ–º "–ë–µ–ª—ã–µ —Å–ø–∏—Å–∫–∏" —Ç–æ–≤–∞—Ä–æ–≤. –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ —Ç–æ—á–Ω–æ –ù–ï —Ç–æ–≤–∞—Ä.
    
    # –†–µ–≥—É–ª—è—Ä–∫–∏ –¥–ª—è —Ä–∞–∑–º–µ—Ä–æ–≤/–º–∞—Ä–æ–∫
    dim_pattern = re.compile(r'\d+(?:[\.\,]\d+)?\s?[—Öx\*√ó]\s?\d+', re.IGNORECASE)
    grade_pattern = re.compile(r'^([–∞-—èa-z]{1,4}\-?\d+[–∞-—èa-z0-9]*)$', re.IGNORECASE)
    gost_pattern = re.compile(r'(–≥–æ—Å—Ç|din|—Ç—É|iso|—Å—Ç|—Å–ø)\s?\d+', re.IGNORECASE)

    # UI –º—É—Å–æ—Ä (–º–µ–Ω—é, –∫–æ—Ä–∑–∏–Ω–∞ –∏ —Ç.–¥.)
    SITE_UI_GARBAGE = {
        '–º–µ–Ω—é', '–ø–æ–∏—Å–∫', '–≥–ª–∞–≤–Ω–∞—è', '–∫–∞—Ä—Ç–∞', '—Å–∞–π—Ç', '–∫–∞–±–∏–Ω–µ—Ç', '–≤—Ö–æ–¥', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è', 
        '–∫–æ—Ä–∑–∏–Ω–∞', '–∏–∑–±—Ä–∞–Ω–Ω–æ–µ', '–ø—Ä–æ—Ñ–∏–ª—å', '—Ç–µ–ª–µ—Ñ–æ–Ω', '–∞–¥—Ä–µ—Å', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', 'email', 
        '–∑–≤–æ–Ω–æ–∫', 'callback', '–æ—Ç–∑—ã–≤', '–æ—Ç–∑—ã–≤—ã', '–≤–æ–ø—Ä–æ—Å', '–æ—Ç–≤–µ—Ç', '–º–µ–Ω–µ–¥–∂–µ—Ä', 
        '–ø–æ–ª–∏—Ç–∏–∫–∞', '–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ', '–æ—Ñ–µ—Ä—Ç–∞', 'cookie', 
        '–æ—Ç–ø—Ä–∞–≤–∏—Ç—å', '–æ—à–∏–±–∫–∞', '–∫–Ω–æ–ø–∫–∞', '—Ñ–æ—Ä–º–∞', '–ø–æ–ª–µ', '–æ–±–∑–æ—Ä', '–Ω–æ–≤–æ—Å—Ç–∏', '—Å—Ç–∞—Ç—å–∏',
        '—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞', '–æ–ø–∏—Å–∞–Ω–∏–µ', '–ø–∞—Ä–∞–º–µ—Ç—Ä', '—Å–≤–æ–π—Å—Ç–≤–æ', '–∞—Ä—Ç–∏–∫—É–ª', '–∫–æ–¥',
        '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä', '—Ñ–∏–ª—å—Ç—Ä', '—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞', '–ø–æ–∫–∞–∑–∞—Ç—å', '—Å–±—Ä–æ—Å–∏—Ç—å', '–∫–∞—Ç–µ–≥–æ—Ä–∏—è', 
        '—Ä–∞–∑–¥–µ–ª', '—Å–ø–∏—Å–æ–∫', '–≤–∏–¥', '—Ç–∏–ø', '–∫–ª–∞—Å—Å', '—Å–µ—Ä–∏—è', '—Ä–µ–π—Ç–∏–Ω–≥', '–Ω–∞–ª–∏—á–∏–µ', 
        '—Å–∫–ª–∞–¥', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', '–±—Ä–µ–Ω–¥', '–º–∞—Ä–∫–∞', '–≤–µ—Å', '–¥–ª–∏–Ω–∞', '—à–∏—Ä–∏–Ω–∞', 
        '–≤—ã—Å–æ—Ç–∞', '—Ç–æ–ª—â–∏–Ω–∞', '–¥–∏–∞–º–µ—Ç—Ä', '—Ä–∞–∑–º–µ—Ä', '–æ–±—ä–µ–º', '–º–∞—Å—Å–∞', '—Ç–æ–Ω–Ω–∞', '–º–µ—Ç—Ä', 
        '—à—Ç', '–∫–≥', '—É–ø–∞–∫–æ–≤–∫–∞', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '–º–∞–≥–∞–∑–∏–Ω', '–∫–∞—Ç–∞–ª–æ–≥', '–≥–æ–¥', '–¥–µ–Ω—å', '—á–∞—Å'
    }

    # –Ø–≤–Ω—ã–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —Å–ª–æ–≤–∞ (–∫–æ—Ç–æ—Ä—ã–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è —Å—á–∏—Ç–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º–∏)
    COMMERCIAL_STOP_WORDS = {
        '–∫—É–ø–∏—Ç—å', '—Ü–µ–Ω–∞', '—Ü–µ–Ω—ã', '–ø—Ä–∞–π—Å', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø—Ä–æ–¥–∞–∂–∞', '–Ω–µ–¥–æ—Ä–æ–≥–æ', 
        '–¥–µ—à–µ–≤–æ', '–¥–æ—Ä–æ–≥–æ', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞', '–æ–ø—Ç–æ–º', '—Ä–æ–∑–Ω–∏—Ü–∞', 
        '—Ä—É–±', '—Ä—É–±–ª–µ–π', '–∑–∞–∫–∞–∑', '–æ–ø–ª–∞—Ç–∞', '–ø–ª–∞—Ç–µ–∂', '—Ä–∞—Å—Å—Ä–æ—á–∫–∞', '–∫—Ä–µ–¥–∏—Ç', 
        '–ª–∏–∑–∏–Ω–≥', '–¥–æ—Å—Ç–∞–≤–∫–∞', '—Å–∞–º–æ–≤—ã–≤–æ–∑', '–æ—Ç–≥—Ä—É–∑–∫–∞', '–ø–æ—Å—Ç–∞–≤–∫–∞', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç', 
        '–ª–æ–≥–∏—Å—Ç–∏–∫–∞', '–≥–∞—Ä–∞–Ω—Ç–∏—è', '–≤–æ–∑–≤—Ä–∞—Ç', '–æ–±–º–µ–Ω', '—Å–Ω–∞–±–∂–µ–Ω–∏–µ', '–≤—ã–≥–æ–¥–Ω—ã–π', 
        '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π', '–Ω–∞–¥–µ–∂–Ω—ã–π', '–±–æ–ª—å—à–æ–π', '–º–∞–ª—ã–π', '—É–¥–æ–±–Ω—ã–π', '–±—ã—Å—Ç—Ä—ã–π', 
        '–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π', '—Ö–æ—Ä–æ—à–∏–π', '–¥–æ—Å—Ç—É–ø–Ω—ã–π', '–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π', 
        '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π', '–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π', '—É–Ω–∏–∫–∞–ª—å–Ω—ã–π', '—à–∏—Ä–æ–∫–∏–π', '–ø–∞—Ä—Ç–Ω–µ—Ä', 
        '–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ', '—Å–≤—è–∑—å', '–∑–≤–æ–Ω–∏—Ç—å'
    }

    SERVICE_KEYWORDS = {
        '—Ä–µ–∑–∫–∞', '–≥–∏–±–∫–∞', '—Å–≤–∞—Ä–∫–∞', '–æ—Ü–∏–Ω–∫–æ–≤–∫–∞', '—Ä—É–±–∫–∞', '–º–æ–Ω—Ç–∞–∂', '—É–∫–ª–∞–¥–∫–∞', 
        '–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∏–∑–æ–ª—è—Ü–∏—è', '—Å–≤–µ—Ä–ª–µ–Ω–∏–µ', '–≥—Ä—É–Ω—Ç–æ–≤–∫–∞', '–ø–æ–∫—Ä–∞—Å–∫–∞', 
        '–º–µ—Ç–∞–ª–ª–æ–æ–±—Ä–∞–±–æ—Ç–∫–∞', '–æ–±—Ä–∞–±–æ—Ç–∫–∞', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '—Ä–µ–º–æ–Ω—Ç', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ', 
        '–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ', '—Ä–∞–∑–º–æ—Ç–∫–∞', '–ø—Ä–æ—Ç—è–∂–∫–∞', '—Ü–∏–Ω–∫–æ–≤–∞–Ω–∏–µ', '–ø–æ–∫—Ä—ã—Ç–∏–µ'
    }

    categories = {
        'products': set(),
        'services': set(),
        'commercial': set(),
        'dimensions': set()
    }

    # –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —á–µ—Ä–µ–∑ API (–µ—Å–ª–∏ Pymorphy –Ω–µ —Å–ø—Ä–∞–≤–∏–ª—Å—è)
    # –ù–æ –≤ 95% —Å–ª—É—á–∞–µ–≤ Pymorphy —Å–ø—Ä–∞–≤–∏—Ç—Å—è —Å–∞–º.
    
    for word in words_list:
        word_lower = word.lower()

        # 1. –†–∞–∑–º–µ—Ä—ã –∏ —Ü–∏—Ñ—Ä—ã
        if dim_pattern.search(word_lower) or grade_pattern.match(word_lower) or gost_pattern.search(word_lower) or word_lower.isdigit():
            categories['dimensions'].add(word_lower)
            continue

        # 2. –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (PyMorphy2)
        # –≠—Ç–æ –∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç. –ú—ã —Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ç–µ–≥–∏.
        if morph:
            p = morph.parse(word_lower)[0]
            lemma = p.normal_form
            tags = p.tag
        else:
            # –ï—Å–ª–∏ NLP –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è - —Ñ–æ–ª–ª–±—ç–∫ –Ω–∞ –ø—Ä–æ—Å—Ç—É—é –ª–æ–≥–∏–∫—É
            lemma = word_lower
            tags = set()

        # --- –§–ò–õ–¨–¢–†–´ –ú–£–°–û–†–ê ---
        
        # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –≤ —Å—Ç–æ–ø-–ª–∏—Å—Ç–∞—Ö
        if lemma in SITE_UI_GARBAGE or lemma in COMMERCIAL_STOP_WORDS:
            categories['commercial'].add(lemma)
            continue
        
        # –ï—Å–ª–∏ —ç—Ç–æ –ì–µ–æ–≥—Ä–∞—Ñ–∏—è (Geox), –ò–º—è (Name), –§–∞–º–∏–ª–∏—è (Surn)
        if 'Geox' in tags or 'Name' in tags or 'Surn' in tags or 'Patr' in tags:
            categories['commercial'].add(lemma) # –ì–æ—Ä–æ–¥–∞ –∏ –∏–º–µ–Ω–∞ - –≤ –∫–æ–º–º–µ—Ä—Ü–∏—é/–æ–±—â–µ–µ
            continue

        # –ï—Å–ª–∏ —ç—Ç–æ –ì–ª–∞–≥–æ–ª (INFN - –∏–Ω—Ñ–∏–Ω–∏—Ç–∏–≤, VERB - –≥–ª–∞–≥–æ–ª, GRND - –¥–µ–µ–ø—Ä–∏—á–∞—Å—Ç–∏–µ)
        # "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è", "–ó–≤–æ–Ω–∏—Ç—å", "–ù–∞—Ö–æ–¥–∏—Ç—å—Å—è"
        if 'INFN' in tags or 'VERB' in tags or 'GRND' in tags or 'PRTF' in tags:
            categories['commercial'].add(lemma)
            continue
            
        # –ï—Å–ª–∏ —ç—Ç–æ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ, –ø—Ä–µ–¥–ª–æ–≥, —Å–æ—é–∑ (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
        if 'PREP' in tags or 'CONJ' in tags or 'PRCL' in tags or 'NPRO' in tags:
            categories['commercial'].add(lemma)
            continue

        # --- –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –£–°–õ–£–ì ---
        
        # 1. –ü–æ —Å–ª–æ–≤–∞—Ä—é
        if lemma in SERVICE_KEYWORDS or lemma.endswith('–æ–±—Ä–∞–±–æ—Ç–∫–∞'):
            categories['services'].add(lemma)
            continue
            
        # 2. –ü–æ —Å—É—Ñ—Ñ–∏–∫—Å–∞–º (–≠–≤—Ä–∏—Å—Ç–∏–∫–∞)
        # –°–ª–æ–≤–∞ –Ω–∞ -–Ω–∏–µ (–∫—Ä–æ–º–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è/–∫—Ä–µ–ø–ª–µ–Ω–∏—è) –∏ -–∫–∞ (–≥–∏–±–∫–∞, —Ä–µ–∑–∫–∞) —á–∞—Å—Ç–æ —É—Å–ª—É–≥–∏
        if lemma.endswith('–Ω–∏–µ') and lemma not in ['–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ', '–∫—Ä–µ–ø–ª–µ–Ω–∏–µ', '—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ', '–ø—Ä–∏—Å–ø–æ—Å–æ–±–ª–µ–Ω–∏–µ']:
             # –ß–∞—Å—Ç–æ —ç—Ç–æ –æ—Ç–≥–ª–∞–≥–æ–ª—å–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ -> –£—Å–ª—É–≥–∏
             categories['services'].add(lemma)
             continue
             
        if lemma.endswith('–∫–∞') and ('NOUN' in tags) and lemma not in ['–±–∞–ª–∫–∞', '–ø—Ä–æ–≤–æ–ª–æ–∫–∞', '—Å–µ—Ç–∫–∞', '—Ç—Ä—É–±–∫–∞', '–≥–∞–π–∫–∞', '—à–∞–π–±–∞', '–ø–æ–∫–æ–≤–∫–∞', '—É–ø–∞–∫–æ–≤–∫–∞', '—Ä–µ–π–∫–∞']:
            # –†–∏—Å–∫–æ–≤–∞–Ω–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞, –Ω–æ –¥–ª—è —Ä–µ–∑–∫–∏/–≥–∏–±–∫–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç. 
            # –ï—Å–ª–∏ —Å–æ–º–Ω–µ–≤–∞–µ–º—Å—è - –ª—É—á—à–µ –ø—É—Å—Ç—å —É–ø–∞–¥–µ—Ç –≤ —É—Å–ª—É–≥–∏, —á–µ–º –≤ —Ç–æ–≤–∞—Ä—ã.
            # –ù–æ —Å–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –¥–ª—è –º–µ—Ç–∞–ª–ª–∞ –≤–∞–∂–µ–Ω (–±–∞–ª–∫–∞, —Å–µ—Ç–∫–∞).
            if lemma in ['—Ä–µ–∑–∫–∞', '–≥–∏–±–∫–∞', '—Ä—É–±–∫–∞', '–∫–æ–≤–∫–∞', '—Å–≤–∞—Ä–∫–∞', '–¥–æ—Å—Ç–∞–≤–∫–∞', '–Ω–∞—Ä–µ–∑–∫–∞', '—É–∫–ª–∞–¥–∫–∞', '–ø–æ–∫—Ä–∞—Å–∫–∞']:
                 categories['services'].add(lemma)
                 continue

        # --- –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–û–í–ê–†–û–í (–û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê) ---
        
        # –ï—Å–ª–∏ –º—ã –¥–æ—à–ª–∏ –¥–æ —Å—é–¥–∞:
        # 1. –≠—Ç–æ –Ω–µ –º—É—Å–æ—Ä
        # 2. –≠—Ç–æ –Ω–µ –≥–æ—Ä–æ–¥
        # 3. –≠—Ç–æ –Ω–µ –≥–ª–∞–≥–æ–ª
        # 4. –≠—Ç–æ –Ω–µ —è–≤–Ω–∞—è —É—Å–ª—É–≥–∞
        
        # –ï—Å–ª–∏ —ç—Ç–æ –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ (NOUN) -> –°–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –¢–û–í–ê–†
        if 'NOUN' in tags:
            if len(lemma) > 2:
                categories['products'].add(lemma)
            continue
            
        # –ï—Å–ª–∏ —ç—Ç–æ –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ (ADJF) -> –ú–∞—Ç–µ—Ä–∏–∞–ª (–ú–µ–¥–Ω—ã–π, –°—Ç–∞–ª—å–Ω–æ–π) -> –¢–û–í–ê–†
        # –û–±—ã—á–Ω—ã–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ (—Ö–æ—Ä–æ—à–∏–π, –±—ã—Å—Ç—Ä—ã–π) –æ—Ç—Å–µ—è–ª–∏—Å—å –≤ COMMERCIAL_STOP_WORDS
        if 'ADJF' in tags:
             if len(lemma) > 2:
                categories['products'].add(lemma)
             continue
             
        # –ï—Å–ª–∏ —ç—Ç–æ –õ–∞—Ç–∏–Ω–∏—Ü–∞ (–∫–æ—Ç–æ—Ä–∞—è –Ω–µ –ø–æ–ø–∞–ª–∞ –≤ —Ä–∞–∑–º–µ—Ä—ã), —Å—á–∏—Ç–∞–µ–º —Ç–æ–≤–∞—Ä–æ–º (–±—Ä–µ–Ω–¥—ã, –º–∞—Ä–∫–∏)
        if re.search(r'[a-zA-Z]', lemma):
            categories['products'].add(lemma)
            continue

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ (Unknown), –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫–∏–¥–∞–µ–º –≤ —Ç–æ–≤–∞—Ä—ã, –µ—Å–ª–∏ —Å–ª–æ–≤–æ –¥–ª–∏–Ω–Ω–æ–µ.
        # –õ—É—á—à–µ –ø–æ–ª—É—á–∏—Ç—å "—Å—Ç—Ä–∞–Ω–Ω—ã–π —Ç–æ–≤–∞—Ä", —á–µ–º –ø–æ—Ç–µ—Ä—è—Ç—å "—Ä–µ–¥–∫–∏–π –º–µ—Ç–∞–ª–ª".
        if len(lemma) > 3:
            categories['products'].add(lemma)
        else:
            categories['commercial'].add(lemma)

    return {k: sorted(list(v)) for k, v in categories.items()}

# --- –§–£–ù–ö–¶–ò–Ø API ARSENKIN ---
def get_arsenkin_urls(query, engine_type, region_name, api_token, depth_val=10):
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check"
    url_get = "https://arsenkin.ru/api/tools/get"

    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-type": "application/json"
    }

    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []

    if "–Ø–Ω–¥–µ–∫—Å" in engine_type:
        se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type:
        se_params.append({"type": 11, "region": reg_ids['go']})

    payload = {
        "tools_name": "check-top",
        "data": {
            "queries": [query],
            "is_snippet": False,
            "noreask": True,
            "se": se_params,
            "depth": depth_val
        }
    }

    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        resp_json = r.json()
        if "error" in resp_json or "task_id" not in resp_json:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ API (—Å—Ç–∞—Ä—Ç): {resp_json}")
            return []
        task_id = resp_json["task_id"]
        st.toast(f"–ó–∞–¥–∞—á–∞ ID {task_id} –∑–∞–ø—É—â–µ–Ω–∞")
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–µ –∑–∞–¥–∞—á–∏: {e}")
        return []

    status = "process"
    attempts = 0
    max_attempts = 40
    progress_info = st.empty()
    bar = st.progress(0)
    res_check_data = {}

    while status == "process" and attempts < max_attempts:
        time.sleep(5)
        attempts += 1
        bar.progress(attempts / max_attempts)
        progress_info.text(f"–û–∂–∏–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ API... ({attempts*5} —Å–µ–∫)")
        try:
            r_check = requests.post(url_check, headers=headers, json={"task_id": task_id})
            res_check_data = r_check.json()
            if res_check_data.get("status") == "finish":
                status = "done"
                break
            if str(res_check_data.get("code")) == "429":
                continue
        except Exception:
            pass

    bar.empty()
    progress_info.empty()

    if status != "done":
        st.error(f"‚è≥ –í—Ä–µ–º—è –≤—ã—à–ª–æ. –°—Ç–∞—Ç—É—Å: {res_check_data.get('status', 'Unknown')}")
        return []

    res_data = {}
    try:
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id}, timeout=30)
        res_data = r_final.json()
        if res_data.get("code") != "TASK_RESULT":
            st.error(f"‚ùå –û—à–∏–±–∫–∞: API –Ω–µ –≤–µ—Ä–Ω—É–ª —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.")
            return []
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
        return []

    results_list = []
    try:
        if 'result' in res_data and 'result' in res_data['result'] and 'collect' in res_data['result']['result']:
            collect = res_data['result']['result']['collect']
        else:
            unique_urls = set()
            if 'result' in res_data and isinstance(res_data['result'], list):
                return res_data['result']
            return []

        final_url_list = []
        if collect and isinstance(collect, list) and len(collect) > 0 and \
           collect[0] and isinstance(collect[0], list) and len(collect[0]) > 0 and \
           collect[0][0] and isinstance(collect[0][0], list):
            final_url_list = collect[0][0]
        else:
            unique_urls = set()
            for engine_data in collect:
                if isinstance(engine_data, dict):
                    for engine_id, serps in engine_data.items():
                        if isinstance(serps, list):
                            for item in serps:
                                url = item.get('url')
                                pos = item.get('pos')
                                if url and pos:
                                    if url not in unique_urls:
                                        results_list.append({'url': url, 'pos': pos})
                                        unique_urls.add(url)
            return results_list

        if final_url_list:
            for index, url in enumerate(final_url_list):
                results_list.append({'url': url, 'pos': index + 1})
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        return []
    return results_list

def process_text_detailed(text, settings, n_gram=1):
    text = text.lower().replace('—ë', '–µ')
    pattern = r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+'
    words = re.findall(pattern, text)
    stops = set(w.lower().replace('—ë', '–µ') for w in settings['custom_stops'])
    lemmas = []
    forms_map = defaultdict(set)

    for w in words:
        if len(w) < 2: continue
        if not settings['numbers'] and w.isdigit(): continue
        if w in stops: continue

        lemma = w
        if USE_NLP and n_gram == 1:
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            lemma = p.normal_form.replace('—ë', '–µ')

        lemmas.append(lemma)
        forms_map[lemma].add(w)

    if n_gram > 1:
        ngrams = []
        for i in range(len(lemmas) - n_gram + 1):
            phrase = " ".join(lemmas[i:i+n_gram])
            ngrams.append(phrase)
        return ngrams, {}

    return lemmas, forms_map

def parse_page(url, settings):
    headers = {'User-Agent': settings['ua']}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code != 200: return None
        soup = BeautifulSoup(r.text, 'html.parser')

        tags_to_remove = []
        if settings['noindex']: tags_to_remove.append('noindex')
        for c in soup.find_all(string=lambda text: isinstance(text, Comment)): c.extract()
        if tags_to_remove:
            for t in soup.find_all(tags_to_remove): t.decompose()

        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)

        extra_text = []
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'): extra_text.append(meta_desc['content'])
        meta_kw = soup.find('meta', attrs={'name': 'keywords'})
        if meta_kw and meta_kw.get('content'): extra_text.append(meta_kw['content'])

        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])

        body_text_raw = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text = re.sub(r'\s+', ' ', body_text_raw).strip()

        if not body_text: return None
        return {'url': url, 'domain': urlparse(url).netloc, 'body_text': body_text, 'anchor_text': anchor_text}
    except:
        return None

def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    all_forms_map = defaultdict(set)

    if not my_data or not my_data.get('body_text'):
        my_lemmas, my_forms, my_anchors, my_len = [], {}, [], 0
    else:
        my_lemmas, my_forms = process_text_detailed(my_data['body_text'], settings)
        my_anchors, _ = process_text_detailed(my_data['anchor_text'], settings)
        my_len = len(my_lemmas)
        for k, v in my_forms.items(): all_forms_map[k].update(v)

    comp_data_parsed = [d for d in comp_data_full if d.get('body_text')]
    comp_docs = []
    for p in comp_data_parsed:
        body, c_forms = process_text_detailed(p['body_text'], settings)
        anchor, _ = process_text_detailed(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor, 'url': p['url'], 'domain': p['domain']})
        for k, v in c_forms.items(): all_forms_map[k].update(v)

    if not comp_docs:
        return { "depth": pd.DataFrame(), "hybrid": pd.DataFrame(), "relevance_top": pd.DataFrame(),
            "my_score": {"width": 0, "depth": 0}, "missing_semantics_high": [], "missing_semantics_low": [] }

    c_lens = [len(d['body']) for d in comp_docs]
    avg_dl = np.mean(c_lens) if c_lens else 1
    if avg_dl == 0: avg_dl = 1
    median_len = np.median(c_lens) if c_lens else 0
    norm_k_recs = (my_len / median_len) if (median_len > 0 and my_len > 0 and settings['norm']) else 1.0

    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    N = len(comp_docs)

    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
    word_counts_per_doc = []
    for d in comp_docs: word_counts_per_doc.append(Counter(d['body']))

    word_idf_map = {}
    for lemma in vocab:
        df = doc_freqs[lemma]
        if df == 0: continue
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        word_idf_map[lemma] = max(idf, 0.01)

    S_WIDTH_CORE = set()
    missing_semantics_high = []
    missing_semantics_low = []
    my_full_lemmas_set = set(my_lemmas) | set(my_anchors)
    lsi_candidates_weighted = []

    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        c_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        med_val = np.median(c_counts)
        percent = int((doc_freqs[lemma] / N) * 100)
        weight_simple = word_idf_map.get(lemma, 0) * med_val
        if med_val > 0: lsi_candidates_weighted.append((lemma, weight_simple))
        is_width_word = False
        if med_val >= 1:
            S_WIDTH_CORE.add(lemma)
            is_width_word = True

        if lemma not in my_full_lemmas_set:
            if len(lemma) < 2 or lemma.isdigit(): continue
            item = {'word': lemma, 'percent': percent, 'weight': weight_simple}
            if is_width_word: missing_semantics_high.append(item)
            elif percent >= 30: missing_semantics_low.append(item)

    missing_semantics_high.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low.sort(key=lambda x: x['percent'], reverse=True)
    lsi_candidates_weighted.sort(key=lambda x: x[1], reverse=True)
    S_DEPTH_TOP70 = set([x[0] for x in lsi_candidates_weighted[:70]])
    total_width_core_count = len(S_WIDTH_CORE)

    def calculate_bm25_okapi(doc_tokens, doc_len):
        if avg_dl == 0 or doc_len == 0: return 0
        score = 0
        counts = Counter(doc_tokens)
        k1 = 1.2
        b = 0.75
        target_words = S_WIDTH_CORE if S_WIDTH_CORE else S_DEPTH_TOP70
        for word in target_words:
            if word not in counts: continue
            tf = counts[word]
            idf = word_idf_map.get(word, 0)
            numerator = tf * (k1 + 1)
            denominator = tf + k1 * (1 - b + b * (doc_len / avg_dl))
            score += idf * (numerator / denominator)
        return score

    def calculate_width_score_val(lemmas_set):
        if total_width_core_count == 0: return 0
        intersection_count = len(lemmas_set.intersection(S_WIDTH_CORE))
        ratio = intersection_count / total_width_core_count
        if ratio >= 0.9: return 100
        else: return int(round((ratio / 0.9) * 100))

    competitor_scores_map = {}
    comp_bm25_list = []
    for i, doc in enumerate(comp_docs):
        raw_bm25 = calculate_bm25_okapi(doc['body'], c_lens[i])
        comp_bm25_list.append(raw_bm25)
        width_val = calculate_width_score_val(set(doc['body']))
        competitor_scores_map[doc['url']] = {'width_final': min(100, width_val), 'bm25_val': raw_bm25}

    median_bm25_top = np.median(comp_bm25_list) if comp_bm25_list else 0
    spam_limit = median_bm25_top * 1.25 if median_bm25_top > 0 else 1

    for url, scores in competitor_scores_map.items():
        depth_val = int(round((scores['bm25_val'] / spam_limit) * 100))
        scores['depth_final'] = min(100, depth_val)

    my_bm25 = calculate_bm25_okapi(my_lemmas, my_len)
    my_depth_score_final = min(100, int(round((my_bm25 / spam_limit) * 100)))
    my_width_score_final = min(100, calculate_width_score_val(my_full_lemmas_set))

    table_depth, table_hybrid = [], []
    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        df = doc_freqs[lemma]
        if df < 2 and lemma not in my_lemmas: continue
        my_tf_count = my_lemmas.count(lemma)
        forms_set = all_forms_map.get(lemma, set())
        forms_str = ", ".join(sorted(list(forms_set))) if forms_set else lemma
        c_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        med_total = np.median(c_counts)
        max_total = np.max(c_counts)

        base_min = min(np.mean(c_counts), med_total)
        rec_min = int(math.ceil(base_min * norm_k_recs))
        rec_max = int(round(max_total * norm_k_recs))
        if rec_max < rec_min: rec_max = rec_min
        rec_median = med_total * norm_k_recs

        status = "–ù–æ—Ä–º–∞"
        action_diff = 0
        action_text = "‚úÖ"
        if my_tf_count < rec_min:
            status = "–ù–µ–¥–æ—Å–ø–∞–º"
            action_diff = int(round(rec_min - my_tf_count))
            if action_diff == 0: action_diff = 1
            action_text = f"+{action_diff}"
        elif my_tf_count > rec_max:
            status = "–ü–µ—Ä–µ—Å–ø–∞–º"
            action_diff = int(round(my_tf_count - rec_max))
            if action_diff == 0: action_diff = 1
            action_text = f"-{action_diff}"

        depth_percent = 0
        if rec_median > 0.1: depth_percent = int(round((my_tf_count / rec_median) * 100))
        else: depth_percent = 0 if my_tf_count == 0 else 100

        weight_hybrid = word_idf_map.get(lemma, 0) * (my_tf_count / my_len if my_len > 0 else 0)
        table_depth.append({
            "–°–ª–æ–≤–æ": lemma, "–°–ª–æ–≤–æ—Ñ–æ—Ä–º—ã": forms_str, "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_tf_count,
            "–ú–µ–¥–∏–∞–Ω–∞": round(med_total, 1), "–ú–∏–Ω–∏–º—É–º (—Ä–µ–∫)": rec_min, "–ú–∞–∫—Å–∏–º—É–º (—Ä–µ–∫)": rec_max,
            "–ì–ª—É–±–∏–Ω–∞ %": min(100, depth_percent), "–°—Ç–∞—Ç—É—Å": status, "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": action_text,
            "is_missing": (status == "–ù–µ–¥–æ—Å–ø–∞–º" and my_tf_count == 0),
            "sort_val": abs(action_diff) if status != "–ù–æ—Ä–º–∞" else 0
        })
        table_hybrid.append({
            "–°–ª–æ–≤–æ": lemma, "TF-IDF –¢–û–ü": round(word_idf_map.get(lemma, 0) * (med_total / avg_dl if avg_dl > 0 else 0), 4),
            "TF-IDF —É –≤–∞—Å": round(weight_hybrid, 4), "–°–∞–π—Ç–æ–≤": df, "–ü–µ—Ä–µ—Å–ø–∞–º": max_total
        })

    table_rel = []
    for item in original_results:
        url = item['url']
        scores = competitor_scores_map.get(url, {'width_final':0, 'depth_final':0})
        table_rel.append({
            "–î–æ–º–µ–Ω": urlparse(url).netloc, "–ü–æ–∑–∏—Ü–∏—è": item['pos'],
            "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": scores['width_final'], "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": scores['depth_final']
        })

    my_label = f"{my_data['domain']} (–í—ã)" if (my_data and my_data.get('domain')) else "–í–∞—à —Å–∞–π—Ç"
    table_rel.append({ "–î–æ–º–µ–Ω": my_label, "–ü–æ–∑–∏—Ü–∏—è": my_serp_pos if my_serp_pos > 0 else len(original_results) + 1,
        "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": my_width_score_final, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": my_depth_score_final })

    return {
        "depth": pd.DataFrame(table_depth), "hybrid": pd.DataFrame(table_hybrid),
        "relevance_top": pd.DataFrame(table_rel).sort_values(by='–ü–æ–∑–∏—Ü–∏—è', ascending=True).reset_index(drop=True),
        "my_score": {"width": my_width_score_final, "depth": my_depth_score_final},
        "missing_semantics_high": missing_semantics_high, "missing_semantics_low": missing_semantics_low
    }

# ==========================================
# 5. –§–£–ù–ö–¶–ò–Ø –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø (PAGINATION)
# ==========================================
def render_paginated_table(df, title_text, key_prefix, default_sort_col=None, use_abs_sort_default=False):
    if df.empty:
        st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")
        return

    col_t1, col_t2 = st.columns([7, 3])
    with col_t1: st.markdown(f"### {title_text}")

    if f'{key_prefix}_sort_col' not in st.session_state:
        st.session_state[f'{key_prefix}_sort_col'] = default_sort_col if (default_sort_col and default_sort_col in df.columns) else df.columns[0]
    if f'{key_prefix}_sort_order' not in st.session_state:
        st.session_state[f'{key_prefix}_sort_order'] = "–£–±—ã–≤–∞–Ω–∏–µ"

    search_query = st.text_input(f"üîç –ü–æ–∏—Å–∫ ({title_text})", key=f"{key_prefix}_search")
    if search_query:
        mask = df.astype(str).apply(lambda x: x.str.contains(search_query, case=False, na=False)).any(axis=1)
        df_filtered = df[mask].copy()
    else:
        df_filtered = df.copy()

    if df_filtered.empty:
        st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return

    with st.container():
        st.markdown("<div class='sort-container'>", unsafe_allow_html=True)
        col_s1, col_s2, col_sp = st.columns([2, 2, 4])
        with col_s1:
            current_sort = st.session_state[f'{key_prefix}_sort_col']
            if current_sort not in df_filtered.columns: current_sort = df_filtered.columns[0]
            sort_col = st.selectbox("üóÇ –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ:", df_filtered.columns, key=f"{key_prefix}_sort_box", index=list(df_filtered.columns).index(current_sort))
            st.session_state[f'{key_prefix}_sort_col'] = sort_col
        with col_s2:
            sort_order = st.radio("–ü–æ—Ä—è–¥–æ–∫:", ["–£–±—ã–≤–∞–Ω–∏–µ", "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ"], horizontal=True, key=f"{key_prefix}_order_box", index=0 if st.session_state[f'{key_prefix}_sort_order'] == "–£–±—ã–≤–∞–Ω–∏–µ" else 1)
            st.session_state[f'{key_prefix}_sort_order'] = sort_order
        st.markdown("</div>", unsafe_allow_html=True)

    ascending = (sort_order == "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ")
    if use_abs_sort_default and sort_col == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è" and "sort_val" in df_filtered.columns:
         df_filtered = df_filtered.sort_values(by="sort_val", ascending=ascending)
    elif ("–î–æ–±–∞–≤–∏—Ç—å" in sort_col or "+/-" in sort_col) and df_filtered[sort_col].dtype == object:
        try:
            df_filtered['_temp_sort'] = df_filtered[sort_col].astype(str).str.replace(r'[^\d]', '', regex=True)
            df_filtered['_temp_sort'] = pd.to_numeric(df_filtered['_temp_sort'], errors='coerce').fillna(0)
            df_filtered = df_filtered.sort_values(by='_temp_sort', ascending=ascending).drop(columns=['_temp_sort'])
        except:
             df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)
    else:
        df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)

    df_filtered = df_filtered.reset_index(drop=True)
    df_filtered.index = df_filtered.index + 1

    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        export_df = df_filtered.copy()
        if "is_missing" in export_df.columns: del export_df["is_missing"]
        if "sort_val" in export_df.columns: del export_df["sort_val"]
        export_df.to_excel(writer, index=False, sheet_name='Data')
    excel_data = buffer.getvalue()

    with col_t2:
        st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel", data=excel_data, file_name=f"{key_prefix}_export.xlsx", mime="application/vnd.ms-excel", key=f"{key_prefix}_down")

    ROWS_PER_PAGE = 20
    if f'{key_prefix}_page' not in st.session_state: st.session_state[f'{key_prefix}_page'] = 1
    total_rows = len(df_filtered)
    total_pages = math.ceil(total_rows / ROWS_PER_PAGE)
    if total_pages == 0: total_pages = 1
    current_page = st.session_state[f'{key_prefix}_page']
    if current_page > total_pages: current_page = total_pages
    if current_page < 1: current_page = 1
    st.session_state[f'{key_prefix}_page'] = current_page

    start_idx = (current_page - 1) * ROWS_PER_PAGE
    end_idx = start_idx + ROWS_PER_PAGE
    df_view = df_filtered.iloc[start_idx:end_idx]

    def highlight_rows(row):
        base_style = 'background-color: #FFFFFF; color: #3D4858; border-bottom: 1px solid #DBEAFE;'
        styles = []
        status = row.get("–°—Ç–∞—Ç—É—Å", "")
        for col_name in row.index:
            cell_style = base_style
            if col_name == "–°—Ç–∞—Ç—É—Å":
                if status == "–ù–µ–¥–æ—Å–ø–∞–º": cell_style += "color: #D32F2F; font-weight: bold;"
                elif status == "–ü–µ—Ä–µ—Å–ø–∞–º": cell_style += "color: #E65100; font-weight: bold;"
                elif status == "–ù–æ—Ä–º–∞": cell_style += "color: #2E7D32; font-weight: bold;"
            styles.append(cell_style)
        return styles

    cols_to_hide = ["is_missing", "sort_val"]
    cols_to_hide = [c for c in cols_to_hide if c in df_view.columns]

    try: styled_df = df_view.style.apply(highlight_rows, axis=1)
    except: styled_df = df_view

    st.dataframe(styled_df, use_container_width=True, height=(len(df_view) * 35) + 40, column_config={c: None for c in cols_to_hide})

    c_spacer, c_btn_prev, c_info, c_btn_next = st.columns([6, 1, 1, 1])
    with c_btn_prev:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_prev", disabled=(current_page <= 1), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] -= 1
            st.rerun()
    with c_info: st.markdown(f"<div style='text-align: center; margin-top: 10px;'><b>{current_page}</b> / {total_pages}</div>", unsafe_allow_html=True)
    with c_btn_next:
        if st.button("‚û°Ô∏è", key=f"{key_prefix}_next", disabled=(current_page >= total_pages), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] += 1
            st.rerun()
    st.markdown("---")

# ==========================================
# 6. –õ–û–ì–ò–ö–ê –î–õ–Ø PERPLEXITY (AI GEN)
# ==========================================
STATIC_DATA_GEN = {
    'IP_PROP4817': "–£—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏",
    'IP_PROP4818': "–û–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –≤ —Ä–µ–≥–∏–æ–Ω—ã —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4819': """<p>–ù–∞–¥–µ–∂–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫–∞–∑–∞ –≤ –ª—é–±—É—é —Ç–æ—á–∫—É —Å—Ç—Ä–∞–Ω—ã: "–°—Ç–∞–ª—å–º–µ—Ç—É—Ä–∞–ª" –æ—Ç–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–≤–∞—Ä 24 —á–∞—Å–∞ –≤ —Å—É—Ç–∫–∏, 7 –¥–Ω–µ–π –≤ –Ω–µ–¥–µ–ª—é. –ë–æ–ª–µ–µ 4 000 –æ—Ç–≥—Ä—É–∑–æ–∫ –≤ –≥–æ–¥. –ü—Ä–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–∏ –∑–∞–∫–∞–∑–∞ –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –≤–∞–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ä—à—Ä—É—Ç.</p>""",
    'IP_PROP4820': """<p>–ù–∞—à–∏ –∏–∑–¥–µ–ª–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è—Ö –£—Ä–∞–ª–∞, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞, –ü–æ–≤–æ–ª–∂—å—è, –°–∏–±–∏—Ä–∏. –ü–∞—Ä—Ç–Ω–µ—Ä—ã –ø–æ –ª–æ–≥–∏—Å—Ç–∏–∫–µ –ø—Ä–µ–¥–ª–æ–∂–∞—Ç –¥–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–∫–∞–∑ —Å–∞–º—ã–º —É–¥–æ–±–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º ‚Äì –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–º, –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω—ã–º, –¥–∞–∂–µ –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º. –î–ª—è –≤–∞—Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—É—é —Å—Ö–µ–º—É –ø–æ–¥ —É–¥–æ–±–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è. –ü–æ–≥—Ä—É–∑–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ç–µ—Ö–Ω–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.</p>
<div class="h4"><h4>–°–∞–º–æ–≤—ã–≤–æ–∑</h4></div><p>–ï—Å–ª–∏ –æ–±—ã—á–Ω–æ —Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∑–∞–±—Ä–∞—Ç—å —Ç–æ–≤–∞—Ä –∏–ª–∏ –¥–∞–µ—Ç–µ —ç—Ç–æ –ø—Ä–∞–≤–æ —É–ø–æ–ª–Ω–æ–º–æ—á–µ–Ω–Ω—ã–º, –∞–¥—Ä–µ—Å –∏ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã —Å–∫–ª–∞–¥–∞ –≤ —Å–≤–æ–µ–º –≥–æ—Ä–æ–¥–µ —É—Ç–æ—á–Ω—è–π—Ç–µ —É –º–µ–Ω–µ–¥–∂–µ—Ä–∞.</p>
<div class="h4"><h4>–ì—Ä—É–∑–æ–≤–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏</h4></div><p>–û—Ç–ø—Ä–∞–≤–∏–º –ø—Ä–æ–∫–∞—Ç –Ω–∞ –≤–∞—à –æ–±—ä–µ–∫—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –∞–≤—Ç–æ–ø–∞—Ä–∫–æ–º. –ü–æ–ª—É—á–µ–Ω–∏–µ –≤ —É–ø–∞–∫–æ–≤–∫–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏, –∞ –∏–º–µ–Ω–Ω–æ –Ω–∞ –¥–µ—Ä–µ–≤—è–Ω–Ω–æ–º –ø–æ–¥–¥–æ–Ω–µ.</p>
<div class="h4"><h4>–°–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å –¢–ö</h4></div><p>–î–æ—Å—Ç–∞–≤–∫–∞ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –†–æ—Å—Å–∏–∏ –∏ –°–ù–ì. –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å—Å—è, —Ç–∞–∫ –∫–∞–∫ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç, –æ–¥–Ω–∞–∫–æ, —Å—Ä–∞–≤–Ω–∏–º —Å—Ç–æ–∏–º–æ—Å—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª—É–∂–± –∏ –≤—ã–±–µ—Ä–µ–º –ª—É—á—à—É—é.</p>""",
    'IP_PROP4821': "–û–ø–ª–∞—Ç–∞ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤:",
    'IP_PROP4822': """<p>–ù–∞—à–∞ –∫–æ–º–ø–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞ –ø—Ä–∏–Ω—è—Ç—å –ª—é–±—ã–µ –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–µ –≤–∏–¥—ã –æ–ø–ª–∞—Ç—ã –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏—Ü: –ø–æ —Å—á–µ—Ç—É, –Ω–∞–ª–∏—á–Ω–∞—è –∏ –±–µ–∑–Ω–∞–ª–∏—á–Ω–∞—è, –Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–π –ø–ª–∞—Ç–µ–∂, —Ç–∞–∫–∂–µ –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞ –∏ –æ—Ç—Å—Ä–æ—á–∫–∞ –ø–ª–∞—Ç–µ–∂–∞.</p>""",
    'IP_PROP4823': """<div class="h4"><h3>–ü—Ä–∏–º–µ—Ä—ã –≤–æ–∑–º–æ–∂–Ω–æ–π –æ–ø–ª–∞—Ç—ã</h3></div><div class="an-col-12"><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–° –ø–æ–º–æ—â—å—é –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤ —Ü–µ–Ω—Ç—Ä–∞—Ö –ø—Ä–æ–¥–∞–∂</span></p></li></ul><p>–í–∞–∂–Ω–æ! –¶–µ–Ω–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—É–±–ª–∏—á–Ω–æ–π –æ—Ñ–µ—Ä—Ç–æ–π. –ü—Ä–∏—Ö–æ–¥–∏—Ç–µ –≤ –Ω–∞—à –æ—Ñ–∏—Å, —á—Ç–æ–±—ã —É—Ç–æ—á–Ω–∏—Ç—å –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ, –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ—á—Ç–∏ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å, —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –≤–æ–∑–≤—Ä–∞—Ç, —Å—á–µ—Ç, —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ª–æ–≥–∏—Å—Ç–∏–∫—É.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞ —Ä–∞—Å—á–µ—Ç–Ω—ã–π —Å—á–µ—Ç</span></p></li></ul><p>–ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É —Å—á–µ—Ç—É –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏ –±–∞–Ω–∫–∞ –∏–ª–∏ –ø—É—Ç–µ–º –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥—Å—Ç–≤ —á–µ—Ä–µ–∑ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç (—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∑–∞—â–∏—â–µ–Ω—ã, —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –æ—Ç–¥–µ–ª–µ–Ω–∏—è). –î–ª—è –ø—Ä–∞–≤–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å —Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –ø–ª–∞—Ç–µ–∂–Ω–æ–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ —Å –æ—Ç–º–µ—Ç–∫–æ–π –±–∞–Ω–∫–∞.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞–ª–∏—á–Ω—ã–º–∏ –∏–ª–∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∫–∞—Ä—Ç–æ–π –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏</span></p></li></ul><p><span style="font-weight: 400;">–ü–æ–º–æ–∂–µ–º —Å –æ–ø–ª–∞—Ç–æ–π: –æ–±—ä–µ–º –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ. –ö—Ä—É–ø–Ω—ã–º –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º ‚Äì –¥–µ–Ω—å–≥–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–µ–º–∫–∏ —Ç–æ–≤–∞—Ä–∞.</span></p><p>–ú–µ–Ω–µ–¥–∂–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.</p><p>–ó–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ —á–µ—Ä–µ–∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç:</p><p><a class="btn btn-blue" href="/catalog/">–ö–∞—Ç–∞–ª–æ–≥ (–º–∞–≥–∞–∑–∏–Ω-–º–µ–Ω—é):</a></p></div></div><br>""",
    'IP_PROP4824': "–û–ø–∏—Å–∞–Ω–∏–µ, —Å—Ç–∞—Ç—å–∏, –ø–æ–∏—Å–∫, –æ—Ç–∑—ã–≤—ã, –Ω–æ–≤–æ—Å—Ç–∏, –∞–∫—Ü–∏–∏, –∂—É—Ä–Ω–∞–ª, info:",
    'IP_PROP4825': "–ú–æ–∂–µ–º –º–µ—Ç–∞–ª–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –æ—Ü–∏–Ω–∫–æ–≤–∞—Ç—å, –Ω–∏–∫–µ–ª–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–æ–≤–æ–ª–æ—á—å",
    'IP_PROP4826': "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥",
    'IP_PROP4834': "–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –±–µ–∑ –ø—Ä–∏–º–µ—Å–µ–π",
    'IP_PROP4835': "–ü–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫",
    'IP_PROP4836': "–ö–∞—á–µ—Å—Ç–≤–æ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä",
    'IP_PROP4837': "–ü–æ—Ä—è–¥–æ–∫ –≤ –ì–û–°–¢–∞—Ö"
}

def get_page_data_for_gen(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
    except Exception as e: return None, None, f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}"
    if response.status_code != 200: return None, None, f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—É—Å–∞: {response.status_code}"
    soup = BeautifulSoup(response.text, 'html.parser')
    description_div = soup.find('div', class_='description-container')
    base_text = description_div.get_text(separator="\n", strip=True) if description_div else soup.body.get_text(separator="\n", strip=True)[:5000]
    tags_container = soup.find(class_='popular-tags-inner')
    tags_data = []
    if tags_container:
        links = tags_container.find_all('a')
        for link in links:
            tag_url = urljoin(url, link.get('href')) if link.get('href') else None
            if tag_url: tags_data.append({'name': link.get_text(strip=True), 'url': tag_url})
    return base_text, tags_data, None

def generate_five_blocks(client, base_text, tag_name):
    if not base_text: return ["Error: No base text"] * 5
    system_instruction = "–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä. –ù–∞–ø–∏—à–∏ 5 HTML –±–ª–æ–∫–æ–≤. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π markdown."
    
    user_prompt = f"""–í–í–û–î–ù–´–ï: –¢–µ–≥ "{tag_name}". –ë–∞–∑–∞: \"\"\"{base_text[:3000]}\"\"\"
    –ó–ê–î–ê–ß–ê: 5 –±–ª–æ–∫–æ–≤. –°—Ç—Ä—É–∫—Ç—É—Ä–∞: h2/h3, –∞–±–∑–∞—Ü, –≤–≤–æ–¥–Ω–∞—è —Ñ—Ä–∞–∑–∞:, —Å–ø–∏—Å–æ–∫, –∑–∞–∫–ª—é—á–µ–Ω–∏–µ. –ë–µ–∑ [1] —Å—Å—ã–ª–æ–∫. –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: |||BLOCK_SEP|||"""

    try:
        # –£–±—Ä–∞–ª–∏ seo_words –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
        response = client.chat.completions.create(model="sonar-pro", messages=[{"role": "system", "content": system_instruction}, {"role": "user", "content": user_prompt}], temperature=0.7)
        content = response.choices[0].message.content
        content = re.sub(r'\[\d+\]', '', content).replace("```html", "").replace("```", "")
        blocks = [b.strip() for b in content.split("|||BLOCK_SEP|||") if b.strip()]
        while len(blocks) < 5: blocks.append("")
        return blocks[:5]
    except Exception as e: return [f"API Error: {str(e)}"] * 5

def generate_html_table(client, user_prompt):
    # –£–±—Ä–∞–ª–∏ seo_instruction –ø—Ä–æ MANDATORY SEO –∏ –∂–∏—Ä–Ω—ã–π —à—Ä–∏—Ñ—Ç
    
    system_instruction = f"Generate HTML tables. Inline CSS: table border 2px solid black, th bg #f0f0f0. No markdown."
    try:
        response = client.chat.completions.create(model="sonar-pro", messages=[{"role": "system", "content": system_instruction}, {"role": "user", "content": user_prompt}], temperature=0.7)
        return re.sub(r'\[\d+\]', '', response.choices[0].message.content).replace("```html", "").replace("```", "").strip()
    except Exception as e: return f"Error: {e}"

# ==========================================
# 7. –ò–ù–¢–ï–†–§–ï–ô–° (TABS)
# ==========================================
tab_seo, tab_ai, tab_tags, tab_tables = st.tabs(["üìä SEO –ê–Ω–∞–ª–∏–∑", "ü§ñ AI –ì–µ–Ω–µ—Ä–∞—Ü–∏—è", "üè∑Ô∏è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–≥–æ–≤", "üß© –¢–∞–±–ª–∏—Ü—ã"])

# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 1: SEO
# ------------------------------------------
with tab_seo:
    col_main, col_sidebar = st.columns([65, 35])
    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
        my_input_type = st.radio("–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], horizontal=True, label_visibility="collapsed", key="my_page_source_radio")

        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            st.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã", placeholder="https://site.ru/catalog/tovar", label_visibility="collapsed", key="my_url_input")
        elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏–ª–∏ —Ç–µ–∫—Å—Ç", height=200, label_visibility="collapsed", placeholder="–í—Å—Ç–∞–≤—å—Ç–µ HTML", key="my_content_input")

        st.markdown("### –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å")
        st.text_input("–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞", label_visibility="collapsed", key="query_input")

        st.markdown("### –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        source_type_new = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫", ["–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)", "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"], horizontal=True, label_visibility="collapsed", key="competitor_source_radio")
        source_type = "API" if "API" in source_type_new else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"

        if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
            def update_manual_urls(): st.session_state['persistent_urls'] = st.session_state.manual_urls_widget
            st.text_area("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", height=200, key="manual_urls_widget", value=st.session_state['persistent_urls'], on_change=update_manual_urls)

        st.markdown("### –°–ø–∏—Å–∫–∏ (Stop / Exclude)")
        st.text_area("–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–æ–º–µ–Ω—ã", DEFAULT_EXCLUDE, height=100, key="settings_excludes")
        st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞", DEFAULT_STOPS, height=100, key="settings_stops")

        if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", type="primary", use_container_width=True, key="start_analysis_btn"):
            for key in list(st.session_state.keys()):
                if key.endswith('_page'): st.session_state[key] = 1
            st.session_state.start_analysis_flag = True

    with col_sidebar:
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API")
        
        if not ARSENKIN_TOKEN:
             new_arsenkin = st.text_input("Arsenkin Token", type="password", key="input_arsenkin")
             if new_arsenkin:
                 st.session_state.arsenkin_token = new_arsenkin
                 ARSENKIN_TOKEN = new_arsenkin 
        
        if not YANDEX_DICT_KEY:
             new_yandex = st.text_input("Yandex Dict Key", type="password", key="input_yandex")
             if new_yandex:
                 st.session_state.yandex_dict_key = new_yandex
                 YANDEX_DICT_KEY = new_yandex

        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
        st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
        st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
        st.selectbox("–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞", list(REGION_MAP.keys()), key="settings_region")
        st.selectbox("–ì–ª—É–±–∏–Ω–∞ —Å–±–æ—Ä–∞ (–¢–û–ü)", [10, 20, 30], index=0, key="settings_top_n")
        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å <noindex>", True, key="settings_noindex")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", False, key="settings_alt")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", False, key="settings_numbers")
        st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True, key="settings_norm")
        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã", True, key="settings_agg")

    # --- –õ–û–ì–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê (–í–ù–£–¢–†–ò –í–ö–õ–ê–î–ö–ò) ---
    if st.session_state.get('start_analysis_flag'):
        st.session_state.start_analysis_flag = False
        settings = {'noindex': st.session_state.settings_noindex, 'alt_title': st.session_state.settings_alt, 'numbers': st.session_state.settings_numbers, 'norm': st.session_state.settings_norm, 'ua': st.session_state.settings_ua, 'custom_stops': st.session_state.settings_stops.split()}

        my_data, my_domain, my_serp_pos = None, "", 0
        
        current_input_type = st.session_state.get("my_page_source_radio")
        
        if current_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
                my_data = parse_page(st.session_state.my_url_input, settings)
                if not my_data: st.error("–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."); st.stop()
                my_domain = urlparse(st.session_state.my_url_input).netloc
        elif current_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            my_data = {'url': 'Local', 'domain': 'local', 'body_text': st.session_state.my_content_input, 'anchor_text': ''}

        target_urls_raw = []
        current_source_val = st.session_state.get("competitor_source_radio")
        current_source_type = "API" if "API" in current_source_val else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"

        if current_source_type == "API":
            if not ARSENKIN_TOKEN:
                st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç API —Ç–æ–∫–µ–Ω Arsenkin. –í–≤–µ–¥–∏—Ç–µ –µ–≥–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –∏–ª–∏ –≤ secrets.toml")
                st.stop()
                
            with st.spinner("API Arsenkin..."):
                found = get_arsenkin_urls(st.session_state.query_input, st.session_state.settings_search_engine, st.session_state.settings_region, ARSENKIN_TOKEN)
                if not found: st.stop()

                excl = [d.strip() for d in st.session_state.settings_excludes.split('\n') if d.strip()]
                if st.session_state.settings_agg: excl.extend(["avito", "ozon", "wildberries", "market.yandex", "tiu", "youtube", "vk.com", "yandex", "leroymerlin", "petrovich"])

                filtered = []
                for res in found:
                    dom = urlparse(res['url']).netloc
                    if my_domain and my_domain == dom:
                        if my_serp_pos == 0 or res['pos'] < my_serp_pos: my_serp_pos = res['pos']
                        continue
                    if any(x in dom for x in excl): continue
                    filtered.append(res)
                target_urls_raw = filtered[:st.session_state.settings_top_n]
                st.session_state['persistent_urls'] = "\n".join([i['url'] for i in target_urls_raw])
        else:
            raw_urls = st.session_state.get("persistent_urls", "")
            target_urls_raw = [{'url': u.strip(), 'pos': i+1} for i, u in enumerate(raw_urls.split('\n')) if u.strip()]

        if not target_urls_raw: st.error("–ù–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤."); st.stop()

        comp_data_full = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = {executor.submit(parse_page, u['url'], settings): u['url'] for u in target_urls_raw}
            done, total = 0, len(target_urls_raw)
            prog = st.progress(0)
            for f in concurrent.futures.as_completed(futures):
                if res := f.result(): comp_data_full.append(res)
                done += 1
                prog.progress(done / total)
        prog.empty()

        with st.spinner("–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫..."):
            st.session_state.analysis_results = calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, target_urls_raw)
            st.session_state.analysis_done = True

            # ==========================================
            # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø
            # ==========================================
            res = st.session_state.analysis_results
            words_to_check = [x['word'] for x in res.get('missing_semantics_high', [])]

            if not words_to_check:
                st.session_state.categorized_products = []
                st.session_state.categorized_services = []
                st.session_state.categorized_commercial = []
                st.session_state.categorized_dimensions = []
            else:
                # –ó–¥–µ—Å—å –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤–∞—à–∞ –Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                with st.spinner("–£—Ç–æ—á–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ (Pymorphy)..."):
                    categorized = classify_semantics_with_api(words_to_check, YANDEX_DICT_KEY)

                st.session_state.categorized_products = categorized['products']
                st.session_state.categorized_services = categorized['services']
                st.session_state.categorized_commercial = categorized['commercial']
                st.session_state.categorized_dimensions = categorized['dimensions']
                
                # ==========================================================
                # –í–°–¢–ê–í–ò–¢–¨ –≠–¢–ò –°–¢–†–û–ö–ò –°–Æ–î–ê:
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º –≤–∏–¥–∂–µ—Ç –≤–æ –≤–∫–ª–∞–¥–∫–µ "–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–≥–æ–≤"
                # ==========================================================
                products_str = "\n".join(st.session_state.categorized_products)
                st.session_state['tags_products_edit_smart'] = products_str  # <--- –î–û–ë–ê–í–ò–¢–¨ –í–û–¢ –≠–¢–û

            st.rerun()

    if st.session_state.analysis_done and st.session_state.analysis_results:
        results = st.session_state.analysis_results
        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        st.markdown(f"<div style='background:{LIGHT_BG_MAIN};padding:15px;border-radius:8px;'><b>–†–µ–∑—É–ª—å—Ç–∞—Ç:</b> –®–∏—Ä–∏–Ω–∞: {results['my_score']['width']} | –ì–ª—É–±–∏–Ω–∞: {results['my_score']['depth']}</div>", unsafe_allow_html=True)

        with st.expander("üõí –†–µ–∑—É–ª—å—Ç–∞—Ç –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å–ª–æ–≤ (–° —É—á–µ—Ç–æ–º –Ø–Ω–¥–µ–∫—Å API)", expanded=True):
            c1, c2 = st.columns(2)
            c3, c4 = st.columns(2)

            with c1:
                st.info(f"üß± –¢–æ–≤–∞—Ä—ã ({len(st.session_state.categorized_products)})")
                st.caption(", ".join(st.session_state.categorized_products))

            with c2:
                st.error(f"üõ†Ô∏è –£—Å–ª—É–≥–∏ ({len(st.session_state.categorized_services)})")
                st.caption(", ".join(st.session_state.categorized_services))
            
            with c3:
                st.warning(f"üí∞ –ö–æ–º–º–µ—Ä—Ü–∏—è / –ì–µ–æ / –û–±—â–µ–µ ({len(st.session_state.categorized_commercial)})")
                st.caption(", ".join(st.session_state.categorized_commercial))

            with c4:
                dims = st.session_state.get('categorized_dimensions', [])
                st.success(f"üìè –†–∞–∑–º–µ—Ä—ã –∏ –º–∞—Ä–∫–∏ ({len(dims)})")
                st.caption(", ".join(dims))

        high = results.get('missing_semantics_high', [])
        low = results.get('missing_semantics_low', [])
        if high or low:
            with st.expander(f"üß© –£–ø—É—â–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ ({len(high)+len(low)})", expanded=False):
                if high: st.markdown(f"<div style='background:#EBF5FF;padding:10px;border-radius:5px;'><b>–í–∞–∂–Ω—ã–µ:</b> {', '.join([x['word'] for x in high])}</div>", unsafe_allow_html=True)
                if low: st.markdown(f"<div style='background:#F7FAFC;padding:10px;border-radius:5px;margin-top:5px;'><b>–î–æ–ø:</b> {', '.join([x['word'] for x in low])}</div>", unsafe_allow_html=True)

        render_paginated_table(results['depth'], "1. –ì–ª—É–±–∏–Ω–∞", "tbl_depth_1", default_sort_col="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è", use_abs_sort_default=True)
        render_paginated_table(results['hybrid'], "3. TF-IDF", "tbl_hybrid", default_sort_col="TF-IDF –¢–û–ü")
        render_paginated_table(results['relevance_top'], "4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", "tbl_rel", default_sort_col="–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)")

# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 2: AI
# ------------------------------------------
with tab_ai:
    st.title("AI –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä (Perplexity)")
    pplx_key = st.text_input("Perplexity API Key", type="password", key="pplx_key_input")
    target_url_gen = st.text_input("URL –°—Ç—Ä–∞–Ω–∏—Ü—ã (–¥–æ–Ω–æ—Ä —Ç–µ–≥–æ–≤)", key="pplx_url_input")

    if st.button("üöÄ –ù–∞—á–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é", key="btn_start_gen", disabled=not pplx_key):
        st.session_state.ai_generated_df = None
        if not openai: st.error("–ù–µ—Ç openai"); st.stop()
        client = openai.OpenAI(api_key=pplx_key, base_url="https://api.perplexity.ai")

        with st.status("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è...", expanded=True) as status:
            base_text, tags, err = get_page_data_for_gen(target_url_gen)
            if err or not tags: st.error(err or "–ù–µ—Ç —Ç–µ–≥–æ–≤"); st.stop()

            seo_list = [x['word'] for x in st.session_state.analysis_results.get('missing_semantics_high', []) if x['word'] not in GARBAGE_LATIN_STOPLIST][:15] if st.session_state.analysis_results else []

            all_rows = []
            bar = st.progress(0)
            for i, tag in enumerate(tags):
                blocks = generate_five_blocks(client, base_text, tag['name'], seo_list)
                all_rows.append({'TagName': tag['name'], 'URL': tag['url'], 'IP_PROP4839': blocks[0], 'IP_PROP4816': blocks[1], 'IP_PROP4838': blocks[2], 'IP_PROP4829': blocks[3], 'IP_PROP4831': blocks[4], **STATIC_DATA_GEN})
                bar.progress((i+1)/len(tags))

            df = pd.DataFrame(all_rows)
            st.session_state.ai_generated_df = df
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer: df.to_excel(writer, index=False)
            st.session_state.ai_excel_bytes = buffer.getvalue()
            st.rerun()

    if st.session_state.ai_generated_df is not None:
        st.download_button("üì• –°–∫–∞—á–∞—Ç—å Excel", st.session_state.ai_excel_bytes, "seo_texts.xlsx", "application/vnd.ms-excel")
        st.dataframe(st.session_state.ai_generated_df.head())

# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 3: –¢–ï–ì–ò (SMART MASS PRODUCTION v15)
# ------------------------------------------
with tab_tags:
    st.title("üè∑Ô∏è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–ª–∏—Ç–∫–∏ —Ç–µ–≥–æ–≤ (Smart SEO)")

    # --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---
    
    # –ö—ç—à –¥–ª—è —Å–ø–µ–ª–ª–µ—Ä–∞
    if 'speller_cache' not in st.session_state:
        st.session_state.speller_cache = {}

    def spell_check_yandex_cached(text):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if not text: return ""
        if text in st.session_state.speller_cache:
            return st.session_state.speller_cache[text]
            
        url = "https://speller.yandex.net/services/spellservice.json/checkText"
        params = {"text": text, "lang": "ru", "options": 518}
        try:
            r = requests.get(url, params=params, timeout=1.0)
            if r.status_code == 200:
                data = r.json()
                fixed_text = text
                for error in data:
                    if error.get('s'):
                        fixed_text = fixed_text.replace(error['word'], error['s'][0])
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                st.session_state.speller_cache[text] = fixed_text
                return fixed_text
        except:
            pass
        return text

    def smart_reverse_translit(slug):
        """
        –£–º–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ Slug -> –ß–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (v15: Hybrid GOST/Latin)
        """
        # --- 1. –¢–û–ß–ù–´–ô –°–õ–û–í–ê–†–¨ (–ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª—é–±—É—é –ª–æ–≥–∏–∫—É) ---
        TECHNICAL_DICT = {
            # –°–∞–º—ã–µ —Ö–æ–¥–æ–≤—ã–µ –ì–û–°–¢—ã (—Ç—Ä–∞–Ω—Å–ª–∏—Ç -> –∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
            'gost': '–ì–û–°–¢', 'tu': '–¢–£',
            'st3': '–°—Ç3', 'st3sp': '–°—Ç3—Å–ø', 'st3ps': '–°—Ç3–ø—Å',
            '09g2s': '09–ì2–°', '17g1s': '17–ì1–°',
            'a500c': '–ê500–°', 'a500s': '–ê500–°', 'v500s': '–í500–°', # –í–∞–∂–Ω–æ! —Å -> –° (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
            'a240': '–ê240', 'a400': '–ê400', 'a500': '–ê500',
            '12x18n10t': '12–•18–ù10–¢', '08x18n10': '08–•18–ù10',
            '40x': '40–•', '20x': '20–•', '65g': '65–ì',
            'd16t': '–î16–¢', 'amg': '–ê–ú–≥', 'ad31': '–ê–î31',
            # –ü–æ–ª–∏–º–µ—Ä—ã
            'pvc': '–ü–í–•', 'pnd': '–ü–ù–î', 'pvd': '–ü–í–î',
            # –°–æ–∫—Ä–∞—â–µ–Ω–∏—è
            'hk': '–•/–ö', 'gk': '–ì/–ö', 'bp': '–í–†'
        }

        # --- 2. –ï–î–ò–ù–ò–¶–´ –ò–ó–ú–ï–†–ï–ù–ò–Ø ---
        UNITS_MAP = {
            'mm': '–º–º', 'cm': '—Å–º', 'm': '–º', 'kg': '–∫–≥', 't': '—Ç', 
            'sht': '—à—Ç', 'rub': '—Ä—É–±'
        }

        # --- 3. –ú–ê–†–ö–ï–†–´ –ó–ê–ü–ê–î–ù–´–• –ú–ê–†–û–ö (–û—Å—Ç–∞–≤–ª—è–µ–º –ª–∞—Ç–∏–Ω–∏—Ü–µ–π) ---
        # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —ç—Ç–æ–≥–æ -> Uppercase (–±–µ–∑ —Ç—Ä–∞–Ω—Å–ª–∏—Ç–∞)
        LATIN_STARTS = ('aisi', 'astm', 'din', 'en', 'hardox', 'weldox', 'magnelis', 'ruukki', 'ssab')
        
        # –ï—Å–ª–∏ —Å–ª–æ–≤–æ —Å–æ–¥–µ—Ä–∂–∏—Ç —ç—Ç–∏ –±—É–∫–≤—ã (–º–∞—Ä–∫–µ—Ä—ã –µ–≤—Ä–æ-—Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤), —Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —ç—Ç–æ –ª–∞—Ç–∏–Ω–∏—Ü–∞
        # J (S355J2), W (Weldox), Q (S460Q), R (S235JR) - –≤ —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ –ì–û–°–¢–∞ J –∏ Q –ø–æ—á—Ç–∏ –Ω–µ —é–∑–∞—é—Ç.
        LATIN_CHARS_MARKERS = ['j', 'q', 'w'] 
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –µ–≤—Ä–æ–ø–µ–π—Å–∫–∏–µ –º–∞—Ä–∫–∏ —Å—Ç–∞–ª–∏ (S + —Ü–∏—Ñ—Ä—ã, P + —Ü–∏—Ñ—Ä—ã –∏ —Ç.–¥.)
        # –†–µ–≥—É–ª—è—Ä–∫–∞ –ª–æ–≤–∏—Ç: s355, p265, l450 –∏ —Ç.–¥.
        EURO_GRADE_PATTERN = re.compile(r'^[sple]\d{3}[a-z0-9]*$', re.IGNORECASE)

        # --- –ù–ê–ß–ê–õ–û –û–ë–†–ê–ë–û–¢–ö–ò ---
        slug = slug.lower().strip()
        slug = re.sub(r'\.html|\.php|\.htm', '', slug)
        slug = slug.replace('_', '-').replace('/', '-')
        
        parts = [p for p in slug.split('-') if p]
        final_words = []

        for part in parts:
            # A. –ü–†–û–í–ï–†–ö–ê –ü–û –°–õ–û–í–ê–†–Æ (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Ññ1)
            if part in TECHNICAL_DICT:
                final_words.append(TECHNICAL_DICT[part])
                continue

            # B. –ü–†–û–í–ï–†–ö–ê –ù–ê –†–ê–ó–ú–ï–† (100mm -> 100–º–º)
            is_unit = False
            for eng_unit, rus_unit in UNITS_MAP.items():
                if part.endswith(eng_unit) and part[:-len(eng_unit)].replace('.', '').isdigit():
                    num = part[:-len(eng_unit)]
                    final_words.append(f"{num}{rus_unit}")
                    is_unit = True
                    break
            if is_unit: continue

            # C. –ü–†–û–í–ï–†–ö–ê –ù–ê –õ–ê–¢–ò–ù–°–ö–£–Æ –ú–ê–†–ö–£ (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Ññ2)
            # 1. –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –±—Ä–µ–Ω–¥—ã/—Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã (hardox, aisi...)
            if part.startswith(LATIN_STARTS):
                final_words.append(part.upper())
                continue
            
            # 2. –ù–∞–ª–∏—á–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –ª–∞—Ç–∏–Ω—Å–∫–∏—Ö –±—É–∫–≤ (S355J2, AISI)
            if any(marker in part for marker in LATIN_CHARS_MARKERS):
                final_words.append(part.upper())
                continue
            
            # 3. –ï–≤—Ä–æ-–ø–∞—Ç—Ç–µ—Ä–Ω—ã (S355, P265...)
            if EURO_GRADE_PATTERN.match(part):
                final_words.append(part.upper())
                continue

            # D. –¢–†–ê–ù–°–õ–ò–¢–ï–†–ê–¶–ò–Ø (–ì–û–°–¢ –∏ –û–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ (Cyrillic mapping)
            text = part
            replacements = [
                ('shch', '—â'), ('sch', '—â'), ('sh', '—à'), ('ch', '—á'), ('zh', '–∂'),
                ('yu', '—é'), ('ya', '—è'), ('yo', '—ë'), ('ts', '—Ü'), ('tc', '—Ü'), ('kh', '—Ö')
            ]
            for eng, rus in replacements:
                text = text.replace(eng, rus)

            text = re.sub(r'iy(?=\s|$)', '–∏–π', text)
            text = re.sub(r'yy(?=\s|$)', '—ã–π', text)
            text = text.replace('ij', '–∏–π')
            text = re.sub(r'y(?=\s|$)', '—ã', text)

            mapping = {
                'a': '–∞', 'b': '–±', 'c': '–∫', 'd': '–¥', 'e': '–µ', 'f': '—Ñ', 'g': '–≥',
                'h': '—Ö', 'i': '–∏', 'j': '–π', 'k': '–∫', 'l': '–ª', 'm': '–º', 'n': '–Ω',
                'o': '–æ', 'p': '–ø', 'r': '—Ä', 's': '—Å', 't': '—Ç', 'u': '—É',
                'v': '–≤', 'w': '–≤', 'x': '—Ö', 'z': '–∑', 'y': '—ã', 'q': '–∫'
            }
            chars = []
            for c in text:
                chars.append(mapping.get(c, c))
            rus_text = "".join(chars)

            # E. –≠–í–†–ò–°–¢–ò–ö–ê –ì–û–°–¢–ê (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Ññ3)
            # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å–ª–∏—Ç–∞ –≤ —Å–ª–æ–≤–µ –æ—Å—Ç–∞–ª–∏—Å—å —Ü–∏—Ñ—Ä—ã -> —ç—Ç–æ –ì–û–°–¢ –º–∞—Ä–∫–∞ -> CAPS
            # –ü—Ä–∏–º–µ—Ä: 09g2s -> 09–≥2—Å -> 09–ì2–°
            if any(char.isdigit() for char in rus_text) and len(rus_text) < 10:
                 final_words.append(rus_text.upper())
            else:
                # –û–±—ã—á–Ω–æ–µ —Å–ª–æ–≤–æ -> –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –°–ø–µ–ª–ª–µ—Ä (–µ—Å–ª–∏ –¥–ª–∏–Ω–Ω–æ–µ)
                if len(rus_text) > 3:
                     rus_text = spell_check_yandex_cached(rus_text)
                final_words.append(rus_text)

        # –°–±–æ—Ä–∫–∞ —Ñ—Ä–∞–∑—ã (–ü–µ—Ä–≤–∞—è –±—É–∫–≤–∞ –∑–∞–≥–ª–∞–≤–Ω–∞—è)
        result = " ".join(final_words)
        return result[0].upper() + result[1:] if result else ""

    # --- –ò–ù–¢–ï–†–§–ï–ô–° ---
    col_t1, col_t2 = st.columns([1, 1])
    
    with col_t1:
        st.markdown("##### üîó –ò—Å—Ç–æ—á–Ω–∏–∫ (–û—Ç–∫—É–¥–∞ –ø–∞—Ä—Å–∏–º)")
        category_url = st.text_input("URL –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (–≥–¥–µ —Å–æ–±—Ä–∞—Ç—å —Å–ø–∏—Å–æ–∫ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π)", placeholder="https://site.ru/catalog/truba/")
        
        st.markdown("##### üìÇ –ë–∞–∑–∞ —Å—Å—ã–ª–æ–∫")
        uploaded_file = st.file_uploader("–§–∞–π–ª —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ (.txt)", type=["txt"], key="urls_uploader_smart")

    with col_t2:
        st.markdown("##### üìù –°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ (–ö–ª—é—á–∏ –ø–æ–∏—Å–∫–∞)")
        
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–ª—é—á –≤ session_state. 
        # –ï—Å–ª–∏ –Ω–µ—Ç (–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫) ‚Äî –∑–∞–ø–æ–ª–Ω—è–µ–º –µ–≥–æ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ categorized_products.
        # –ï—Å–ª–∏ –¥–∞ (–ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –∏–ª–∏ –≤–≤–æ–¥–∞ –≤—Ä—É—á–Ω—É—é) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.
        if "tags_products_edit_smart" not in st.session_state:
            raw_products = st.session_state.get('categorized_products', [])
            st.session_state.tags_products_edit_smart = "\n".join(raw_products) if raw_products else ""

        # 2. –°–æ–∑–¥–∞–µ–º –≤–∏–¥–∂–µ—Ç –ë–ï–ó –∞—Ä–≥—É–º–µ–Ω—Ç–∞ 'value'. 
        # Streamlit —Å–∞–º –≤–æ–∑—å–º–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ st.session_state['tags_products_edit_smart'].
        products_input = st.text_area(
            "–°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ (–±—É–¥—É—Ç –∏—Å–∫–∞—Ç—å—Å—è –≤ –±–∞–∑–µ):", 
            height=200, 
            key="tags_products_edit_smart",
            help="–°–∫—Ä–∏–ø—Ç –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å —Å—Å—ã–ª–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —ç—Ç–∏ —Å–ª–æ–≤–∞ (–≤ —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ)."
        )
        
        # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —Å–∫—Ä–∏–ø—Ç–∞
        products = [line.strip() for line in products_input.split('\n') if line.strip()]

    # --- –ó–ê–ü–£–°–ö ---
    st.markdown("---")
    if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å Smart-–≥–µ–Ω–µ—Ä–∞—Ü–∏—é", key="btn_tags_smart_gen", disabled=(not products or not uploaded_file or not category_url)):
        
        status_box = st.status("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞...", expanded=True)
        
        # 1. –ü–∞—Ä—Å–∏–Ω–≥
        status_box.write(f"üïµÔ∏è –ü–∞—Ä—Å–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {category_url}")
        target_urls_list = []
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            r = requests.get(category_url, headers=headers, timeout=15)
            if r.status_code == 200:
                soup = BeautifulSoup(r.text, 'html.parser')
                tags_container = soup.find(class_='popular-tags-inner')
                if tags_container:
                    for link in tags_container.find_all('a'):
                        href = link.get('href')
                        if href: target_urls_list.append(urljoin(category_url, href))
                else:
                    status_box.warning("–ù–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ .popular-tags-inner. –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ...")
                    main_area = soup.find('main') or soup.body
                    if main_area:
                        for link in main_area.find_all('a'):
                            href = link.get('href')
                            if href and '/catalog/' in href:
                                target_urls_list.append(urljoin(category_url, href))
        except Exception as e:
            status_box.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            st.stop()
            
        target_urls_list = list(set(target_urls_list))
        
        if not target_urls_list:
            status_box.error("–¶–µ–ª–µ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
            st.stop()
            
        status_box.write(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(target_urls_list)}")

        # 2. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞–∑—ã
        status_box.write("üìÇ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞–∑—ã —Å—Å—ã–ª–æ–∫...")
        stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
        all_txt_links = [line.strip() for line in stringio.readlines() if line.strip()]
        
        product_candidates_map = {}
        for p in products:
            tr = transliterate_text(p)
            if len(tr) >= 3:
                matches = [u for u in all_txt_links if tr in u.lower()]
                if matches: product_candidates_map[p] = matches
        
        status_box.write(f"‚úÖ –¢–æ–≤–∞—Ä—ã —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω—ã: {len(product_candidates_map)} —à—Ç.")

        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        status_box.write("üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∫–æ—Ä–æ–≤ (Smart Translit v15)...")
        final_rows = []
        prog_bar = st.progress(0)
        
        with requests.Session() as session:
            for i, target_url in enumerate(target_urls_list):
                current_page_tags = []
                
                available_products = list(product_candidates_map.keys())
                random.shuffle(available_products)
                limit = random.randint(12, 20)
                selected_products = available_products[:limit]
                
                for prod_name in selected_products:
                    candidates = product_candidates_map[prod_name]
                    norm_target = target_url.rstrip('/')
                    valid_candidates = [u for u in candidates if u.rstrip('/') != norm_target]
                    
                    if valid_candidates:
                        chosen_url = random.choice(valid_candidates)
                        
                        # SMART NAME GENERATION
                        try:
                            parsed = urlparse(chosen_url)
                            path_parts = parsed.path.strip('/').split('/')
                            slug = path_parts[-1] if path_parts[-1] else (path_parts[-2] if len(path_parts)>1 else "")
                            
                            if not slug or len(slug) < 3:
                                anchor_text = prod_name.capitalize()
                            else:
                                anchor_text = smart_reverse_translit(slug)
                        except:
                            anchor_text = prod_name.capitalize()
                        
                        current_page_tags.append({
                            'name': anchor_text,
                            'url': chosen_url
                        })
                
                if current_page_tags:
                    random.shuffle(current_page_tags)
                    html_block = '<div class="popular-tags">\n' + \
                                 "\n".join([f'    <a href="{item["url"]}" class="tag-link">{item["name"]}</a>' for item in current_page_tags]) + \
                                 '\n</div>'
                else:
                    html_block = "<!-- –ù–µ—Ç —Ç–µ–≥–æ–≤ -->"
                
                final_rows.append({
                    'Page URL': target_url,
                    'Tags HTML': html_block
                })
                
                prog_bar.progress((i + 1) / len(target_urls_list))

        prog_bar.empty()
        status_box.update(label="–ì–æ—Ç–æ–≤–æ!", state="complete", expanded=False)

        # 4. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ
        df_tags_result = pd.DataFrame(final_rows)
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            df_tags_result.to_excel(writer, index=False)
            worksheet = writer.sheets['Sheet1']
            worksheet.set_column('A:A', 60)
            worksheet.set_column('B:B', 100)
            
        excel_bytes = buffer.getvalue()
        
        st.success(f"üéâ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(final_rows)} –±–ª–æ–∫–æ–≤.")
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å Excel",
            data=excel_bytes,
            file_name="smart_tags_tiles.xlsx",
            mime="application/vnd.ms-excel"
        )

# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 4: –¢–ê–ë–õ–ò–¶–´
# ------------------------------------------
# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 4: –¢–ê–ë–õ–ò–¶–´ (MASS GEN v2)
# ------------------------------------------
with tab_tables:
    st.title("üß© –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–∞–±–ª–∏—Ü (Mass Production)")
    
    # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–æ—Å—Ç—É–ø–∞ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    c_set1, c_set2 = st.columns(2)
    with c_set1:
        pplx_key_tbl = st.text_input("Perplexity API Key", type="password", key="pplx_key_tbl")
    with c_set2:
        target_url_tbl = st.text_input("URL –†–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–∏—Å—Ç–æ—á–Ω–∏–∫ —Ç–µ–≥–æ–≤)", placeholder="https://site.ru/catalog/armatura/", key="url_source_tbl")

    st.markdown("---")

    # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏ –æ–ø–∏—Å–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü
    st.subheader("‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü")
    
    num_tables = st.selectbox("–°–∫–æ–ª—å–∫–æ —Ç–∞–±–ª–∏—Ü –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ä–ª–∞?", [1, 2, 3, 4, 5], key="num_tables_select")
    
    table_prompts = []
    
    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–µ–π –≤–≤–æ–¥–∞
    st.info("üìù –û–ø–∏—à–∏—Ç–µ, —á—Ç–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ –∫–∞–∂–¥–æ–π —Ç–∞–±–ª–∏—Ü–µ. –ò–ò –Ω–∞–ø–æ–ª–Ω–∏—Ç –∏—Ö –¥–∞–Ω–Ω—ã–º–∏, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞.")
    
    cols_prompts = st.columns(num_tables)
    for i in range(num_tables):
        with cols_prompts[i]:
            def_val = f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏" if i == 0 else f"–†–∞–∑–º–µ—Ä—ã –∏ –≤–µ—Å"
            prompt_text = st.text_area(f"–¢–∞–±–ª–∏—Ü–∞ ‚Ññ{i+1} (–û–ø–∏—Å–∞–Ω–∏–µ)", value=def_val, height=150, key=f"tbl_prompt_{i}")
            table_prompts.append(prompt_text)

    # 3. –õ–æ–≥–∏–∫–∞ –∑–∞–ø—É—Å–∫–∞
    if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–∞–±–ª–∏—Ü", key="btn_gen_tbl_mass", disabled=(not pplx_key_tbl or not target_url_tbl)):
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞
        if not openai: 
            st.error("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ openai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞/–Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
            st.stop()
            
        client = openai.OpenAI(api_key=pplx_key_tbl, base_url="https://api.perplexity.ai")
        
        status_box = st.status("‚è≥ –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏...", expanded=True)
        
        # –ê. –ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–≥–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ —Ñ—É–Ω–∫—Ü–∏—é, —á—Ç–æ –∏ –≤ AI –≤–∫–ª–∞–¥–∫–µ)
        try:
            _, tags_data, err_msg = get_page_data_for_gen(target_url_tbl)
            if err_msg or not tags_data:
                status_box.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ —Ç–µ–≥–æ–≤: {err_msg if err_msg else '–¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'}")
                st.stop()
            
            # –û–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—å –Ω–∞ —Å–ª—É—á–∞–π —Å–±–æ—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (—á—Ç–æ–±—ã –Ω–µ —Å–∂–µ—á—å –±—é–¥–∂–µ—Ç, –µ—Å–ª–∏ —Ç–µ–≥–æ–≤ 1000)
            status_box.write(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤ (—Ç–µ–≥–æ–≤): {len(tags_data)}")
        except Exception as e:
            status_box.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            st.stop()

        # –ë. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        all_table_rows = []
        progress_bar = st.progress(0)
        
        total_steps = len(tags_data)
        
        for idx, tag_item in enumerate(tags_data):
            tag_name = tag_item['name']
            tag_url = tag_item['url']
            
            status_box.write(f"‚öôÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞: {tag_name}...")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –¥–∞–Ω–Ω—ã—Ö
            row_data = {'URL': tag_url, '–ù–∞–∑–≤–∞–Ω–∏–µ': tag_name}
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —Ç–∞–±–ª–∏—Ü—É –ø–æ –æ—á–µ—Ä–µ–¥–∏
            for tbl_i, prompt_desc in enumerate(table_prompts):
                
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–æ–º—Ç –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                system_instruction = "You are a senior technical data specialist. Output ONLY HTML code. No markdown formatting, no backticks, no introduction."
                user_prompt = f"""
                CONTEXT: The specific product/sub-category is "{tag_name}".
                TASK: Generate a technical HTML table based on this description: "{prompt_desc}".
                CRITICAL REQUIREMENTS:
                1. The data inside the table MUST be specific to "{tag_name}", not generic.
                2. Style: <table style="width:100%; border-collapse: collapse; border: 2px solid black;">, headers with background #f0f0f0.
                3. Return ONLY the HTML <table>...</table> code.
                """
                
                try:
                    response = client.chat.completions.create(
                        model="sonar-pro", 
                        messages=[
                            {"role": "system", "content": system_instruction}, 
                            {"role": "user", "content": user_prompt}
                        ], 
                        temperature=0.5 # –ß—É—Ç—å –Ω–∏–∂–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –±–æ–ª—å—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
                    )
                    html_content = response.choices[0].message.content
                    
                    # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º–∞—Ä–∫–¥–∞—É–Ω–∞, –µ—Å–ª–∏ –ò–ò –≤—Å–µ –∂–µ –¥–æ–±–∞–≤–∏–ª –µ–≥–æ
                    html_content = re.sub(r'```html', '', html_content)
                    html_content = re.sub(r'```', '', html_content).strip()
                    
                    row_data[f'Table {tbl_i+1}'] = html_content
                    
                except Exception as e:
                    row_data[f'Table {tbl_i+1}'] = f"Error: {e}"
            
            all_table_rows.append(row_data)
            progress_bar.progress((idx + 1) / total_steps)

        # –í. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        status_box.update(label="‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!", state="complete", expanded=False)
        
        df_tables = pd.DataFrame(all_table_rows)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Session State –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–Ω–æ–ø–∫–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        st.session_state.tables_gen_df = df_tables
        
        # –°–æ–∑–¥–∞–Ω–∏–µ Excel –≤ –ø–∞–º—è—Ç–∏
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            df_tables.to_excel(writer, index=False)
        st.session_state.tables_excel_bytes = buffer.getvalue()
        
        st.success(f"–ì–æ—Ç–æ–≤–æ! –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Ç–∞–±–ª–∏—Ü –¥–ª—è {len(df_tables)} —Å—Ç—Ä–∞–Ω–∏—Ü.")

    # 4. –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
    if 'tables_gen_df' in st.session_state and st.session_state.tables_gen_df is not None:
        st.markdown("### üì• –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        
        st.download_button(
            label="–°–∫–∞—á–∞—Ç—å Excel —Ñ–∞–π–ª",
            data=st.session_state.tables_excel_bytes,
            file_name="generated_tables.xlsx",
            mime="application/vnd.ms-excel",
            type="primary"
        )
        
        st.dataframe(st.session_state.tables_gen_df.head(), use_container_width=True)
