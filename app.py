import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin
import inspect
import time
import json
import io
import os
import random

def transliterate_text(text):
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç '–®–≤–µ–ª–ª–µ—Ä' –≤ 'shveller', '–ê–Ω–æ–¥' –≤ 'anod'.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–∞ –≤ URL.
    """
    mapping = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }
    result = []
    for char in text.lower():
        if char in mapping:
            result.append(mapping[char])
        elif char.isalnum() or char == '-':
            result.append(char)
    return "".join(result)

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ openai
try:
    import openai
except ImportError:
    openai = None

# ==========================================
# 0. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–û–°–¢–û–Ø–ù–ò–Ø (SESSION STATE)
# ==========================================
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False

# –°–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
if 'ai_generated_df' not in st.session_state:
    st.session_state.ai_generated_df = None
if 'ai_excel_bytes' not in st.session_state:
    st.session_state.ai_excel_bytes = None

# –°–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –¢–µ–≥–æ–≤ –∏ –¢–∞–±–ª–∏—Ü
if 'tags_html_result' not in st.session_state:
    st.session_state.tags_html_result = None
if 'table_html_result' not in st.session_state:
    st.session_state.table_html_result = None

# --- –ù–û–í–´–ï –°–û–°–¢–û–Ø–ù–ò–Ø –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò ---
if 'categorized_products' not in st.session_state:
    st.session_state.categorized_products = []
if 'categorized_services' not in st.session_state:
    st.session_state.categorized_services = []
if 'categorized_commercial' not in st.session_state:
    st.session_state.categorized_commercial = []
if 'categorized_dimensions' not in st.session_state:
    st.session_state.categorized_dimensions = []

# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫
if 'persistent_urls' not in st.session_state:
    st.session_state['persistent_urls'] = ""

if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´ –ò –°–ü–ò–°–ö–ò
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO v2.3 (Secure)", page_icon="üìä")

GARBAGE_LATIN_STOPLIST = {
    'whatsapp', 'viber', 'telegram', 'skype', 'vk', 'instagram', 'facebook', 'youtube', 'twitter',
    'cookie', 'cookies', 'policy', 'privacy', 'agreement', 'terms',
    'click', 'submit', 'send', 'zakaz', 'basket', 'cart', 'order', 'call', 'back', 'callback',
    'login', 'logout', 'sign', 'register', 'auth', 'account', 'profile',
    'search', 'menu', 'nav', 'navigation', 'footer', 'header', 'sidebar',
    'img', 'jpg', 'png', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'svg',
    'ok', 'error', 'undefined', 'null', 'true', 'false', 'var', 'let', 'const', 'function', 'return',
    'ru', 'en', 'com', 'net', 'org', 'biz', 'shop', 'store',
    'phone', 'email', 'tel', 'fax', 'mob', 'address', 'copyright', 'all', 'rights', 'reserved',
    'div', 'span', 'class', 'id', 'style', 'script', 'body', 'html', 'head', 'meta', 'link'
}

# ==========================================
# 2. –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø
# ==========================================
def check_password():
    if st.session_state.get("authenticated"):
        return True

    st.markdown("""
        <style>
        .main { display: flex; flex-direction: column; justify-content: center; align-items: center; }
        .auth-logo-box { text-align: center; margin-bottom: 1rem; padding-top: 0; }
        </style>
    """, unsafe_allow_html=True)

    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="auth-logo-box"><h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3></div>', unsafe_allow_html=True)
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä–æ–ª—å –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
            if password == "jfV6Xel-Q7vp-_s2UYPO":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

# ==========================================
# 3. –ü–û–õ–£–ß–ï–ù–ò–ï API –ö–õ–Æ–ß–ï–ô (–ë–ï–ó–û–ü–ê–°–ù–û)
# ==========================================
# –ú—ã –ù–ï –ø–∏—à–µ–º –∫–ª—é—á–∏ –∑–¥–µ—Å—å. –ú—ã —á–∏—Ç–∞–µ–º –∏—Ö –∏–∑ st.secrets

# –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å Arsenkin Token
if "arsenkin_token" in st.session_state:
    ARSENKIN_TOKEN = st.session_state.arsenkin_token
else:
    # –ü—Ä–æ–±—É–µ–º –¥–æ—Å—Ç–∞—Ç—å –∏–∑ secrets.toml
    try:
        ARSENKIN_TOKEN = st.secrets["api"]["arsenkin_token"]
    except (FileNotFoundError, KeyError):
        ARSENKIN_TOKEN = None

# –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å Yandex Key
if "yandex_dict_key" in st.session_state:
    YANDEX_DICT_KEY = st.session_state.yandex_dict_key
else:
    # –ü—Ä–æ–±—É–µ–º –¥–æ—Å—Ç–∞—Ç—å –∏–∑ secrets.toml
    try:
        YANDEX_DICT_KEY = st.secrets["api"]["yandex_dict_key"]
    except (FileNotFoundError, KeyError):
        YANDEX_DICT_KEY = None


REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956},
    "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931},
    "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–ö–∏–µ–≤ (UA)": {"ya": 143, "go": 1012852},
    "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493},
    "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601}
}

DEFAULT_EXCLUDE_DOMAINS = [
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "ebay.com",
    "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", "pandao.ru",
    "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", "banki.ru",
    "regmarkets.ru", "zoon.ru", "pulscen.ru", "prodoctorov.ru", "blizko.ru",
    "domclick.ru", "satom.ru", "quto.ru", "edadeal.ru", "cataloxy.ru",
    "irr.ru", "onliner.by", "shop.by", "deal.by", "yell.ru",
    "profi.ru", "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by",
    "market.yandex.ru", "youtube.com", "gosuslugi.ru", "dzen.ru",
    "2gis.by", "wildberries.ru", "rutube.ru", "vk.com", "facebook.com"
]
DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n–∫—É–ø–∏—Ç—å\n—Ü–µ–Ω–∞\n—à—Ç\n—Å–º\n–º–º\n–∫–≥\n–∫–≤\n–º2\n—Å—Ç—Ä\n—É–ª"

# –¶–≤–µ—Ç–∞ –∏ —Å—Ç–∏–ª–∏
PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE"

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif; color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important; }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important; color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] div[role="gridcell"] {{
            background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; border-bottom: 1px solid {ROW_BORDER_COLOR} !important;
        }}
        .legend-box {{ padding: 10px; background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px; }}
        .text-red {{ color: #D32F2F; font-weight: bold; }}
        .text-green {{ color: #2E7D32; font-weight: bold; }}
        .text-bold {{ font-weight: 600; }}
        .sort-container {{ background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 8px; margin-bottom: 10px; border: 1px solid {BORDER_COLOR}; }}
        
        .stApp > header {{ background-color: transparent !important; }}
        .stTextInput input:disabled, .stTextArea textarea:disabled, .stSelectbox div[aria-disabled="true"] {{
            opacity: 1 !important; background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; cursor: text !important; -webkit-text-fill-color: {TEXT_COLOR} !important; border-color: {BORDER_COLOR} !important;
        }}
        .stButton button:disabled {{ opacity: 1 !important; background-color: {PRIMARY_COLOR} !important; color: white !important; cursor: progress !important; }}
        div[data-testid="stAppViewContainer"] {{ filter: none !important; opacity: 1 !important; transition: none !important; }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 4. –õ–û–ì–ò–ö–ê (–ë–≠–ö–ï–ù–î)
# ==========================================

try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except Exception as e:
    morph = None
    USE_NLP = False
    st.sidebar.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ NLP: {e}")

# --- –§–£–ù–ö–¶–ò–Ø –ó–ê–ü–†–û–°–ê –ö –Ø–ù–î–ï–ö–° –°–õ–û–í–ê–†–Æ ---
def get_yandex_dict_info(text, api_key):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É (–ª–µ–º–º—É) –∏ —á–∞—Å—Ç—å —Ä–µ—á–∏ (pos) —á–µ—Ä–µ–∑ API –Ø–Ω–¥–µ–∫—Å–∞.
    """
    if not api_key:
        # –ï—Å–ª–∏ –∫–ª—é—á–∞ –Ω–µ—Ç, –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–æ –∫–∞–∫ –µ—Å—Ç—å
        return {'lemma': text, 'pos': 'unknown'}
        
    url = "https://dictionary.yandex.net/api/v1/dicservice.json/lookup"
    params = {
        'key': api_key,
        'lang': 'ru-ru', 
        'text': text,
        'ui': 'ru'
    }
    try:
        r = requests.get(url, params=params, timeout=2)
        if r.status_code == 200:
            data = r.json()
            if data.get('def'):
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                first_def = data['def'][0]
                lemma = first_def.get('text', text)
                pos = first_def.get('pos', 'unknown')
                return {'lemma': lemma, 'pos': pos}
    except:
        pass
    return {'lemma': text, 'pos': 'unknown'}

# --- –§–£–ù–ö–¶–ò–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –° API –Ø–ù–î–ï–ö–°–ê (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø v2.5 - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ) ---
def classify_semantics_with_api(words_list, yandex_key):
    # 1. –°–ü–ò–°–ö–ò-–ò–°–ö–õ–Æ–ß–ï–ù–ò–Ø –ò –ü–ê–¢–¢–ï–†–ù–´
    
    dim_pattern = re.compile(r'\d+(?:[\.\,]\d+)?\s?[—Öx\*√ó]\s?\d+', re.IGNORECASE)
    grade_pattern = re.compile(r'^([–∞-—èa-z]{1,4}\-?\d+[–∞-—èa-z0-9]*)$', re.IGNORECASE)
    gost_pattern = re.compile(r'(–≥–æ—Å—Ç|din|—Ç—É|iso|—Å—Ç|—Å–ø)\s?\d+', re.IGNORECASE)

    # UI –∏ –ù–∞–≤–∏–≥–∞—Ü–∏—è
    SITE_UI_GARBAGE = {
        '–º–µ–Ω—é', '–ø–æ–∏—Å–∫', '–≥–ª–∞–≤–Ω–∞—è', '–∫–∞—Ä—Ç–∞', '—Å–∞–π—Ç', '–ª–∏—á–Ω—ã–π', '–∫–∞–±–∏–Ω–µ—Ç', 
        '–≤—Ö–æ–¥', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è', '–∫–æ—Ä–∑–∏–Ω–∞', '–∏–∑–±—Ä–∞–Ω–Ω–æ–µ', '—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ', '–ø—Ä–æ—Ñ–∏–ª—å',
        '—Ç–µ–ª–µ—Ñ–æ–Ω', '–∞–¥—Ä–µ—Å', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', 'email', '–∑–≤–æ–Ω–æ–∫', 'callback', 
        '–æ—Ç–∑—ã–≤', '–æ—Ç–∑—ã–≤—ã', '–≤–æ–ø—Ä–æ—Å', '–æ—Ç–≤–µ—Ç', '–º–µ–Ω–µ–¥–∂–µ—Ä', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è',
        '–ø–æ–ª–∏—Ç–∏–∫–∞', '–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ', '–æ—Ñ–µ—Ä—Ç–∞', 'cookie',
        '—Å–æ–≥–ª–∞—à–∞—Ç—å—Å—è', '—Å–æ–≥–ª–∞—Å–∏–µ', '–ø—Ä–∏–Ω–∏–º–∞—Ç—å', '–æ—Ç–ø—Ä–∞–≤–∏—Ç—å',
        '–æ—à–∏–±–∫–∞', '—É—Å–ø–µ—à–Ω–æ', '–∫–Ω–æ–ø–∫–∞', '—Ñ–æ—Ä–º–∞', '–ø–æ–ª–µ', '–æ–±–∑–æ—Ä', '–Ω–æ–≤–æ—Å—Ç–∏', '—Å—Ç–∞—Ç—å–∏',
        '—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞', '–æ–ø–∏—Å–∞–Ω–∏–µ', '–ø–∞—Ä–∞–º–µ—Ç—Ä', '—Å–≤–æ–π—Å—Ç–≤–æ', '–∞—Ä—Ç–∏–∫—É–ª', '–∫–æ–¥',
        '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä', '—Ñ–∏–ª—å—Ç—Ä', '—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞', '–ø–æ–∫–∞–∑–∞—Ç—å', '—Å–±—Ä–æ—Å–∏—Ç—å',
        '–∏–º—è', '—Ñ–∞–º–∏–ª–∏—è', '—Å–æ–æ–±—â–µ–Ω–∏–µ', '—Ñ–∞–π–ª', '–¥–æ–∫—É–º–µ–Ω—Ç', '—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç',
        '–∫–∞—Ç–µ–≥–æ—Ä–∏—è', '—Ä–∞–∑–¥–µ–ª', '—Å–ø–∏—Å–æ–∫', '–≤–∏–¥', '—Ç–∏–ø', '–∫–ª–∞—Å—Å', '—Å–µ—Ä–∏—è',
        '—Ä–µ–π—Ç–∏–Ω–≥', '–Ω–∞–ª–∏—á–∏–µ', '—Å–∫–ª–∞–¥', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', '–±—Ä–µ–Ω–¥', '–º–∞—Ä–∫–∞',
        '–≤–µ—Å', '–¥–ª–∏–Ω–∞', '—à–∏—Ä–∏–Ω–∞', '–≤—ã—Å–æ—Ç–∞', '—Ç–æ–ª—â–∏–Ω–∞', '–¥–∏–∞–º–µ—Ç—Ä', '—Ä–∞–∑–º–µ—Ä',
        '–æ–±—ä–µ–º', '–º–∞—Å—Å–∞', '—Ç–æ–Ω–Ω–∞', '–º–µ—Ç—Ä', '—à—Ç', '–∫–≥', '—É–ø–∞–∫–æ–≤–∫–∞', '—Ü–µ–Ω–∞',
        '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '–º–∞–≥–∞–∑–∏–Ω', '–∫–∞—Ç–∞–ª–æ–≥', '–≥–æ–¥'
    }

    # –ö–æ–º–º–µ—Ä—Ü–∏—è + –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ (–ú—É—Å–æ—Ä–Ω—ã–µ –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤)
    COMMERCIAL_WORDS = {
        '–∫—É–ø–∏—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Ü–µ–Ω–∞', '—Ü–µ–Ω—ã', '–ø—Ä–∞–π—Å', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', 
        '–ø—Ä–æ–¥–∞–∂–∞', '–Ω–µ–¥–æ—Ä–æ–≥–æ', '–¥–µ—à–µ–≤–æ', '–¥–æ—Ä–æ–≥–æ', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', 
        '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞', '–æ–ø—Ç–æ–º', '—Ä–æ–∑–Ω–∏—Ü–∞', '—Ä—É–±', '—Ä—É–±–ª–µ–π', '—É–µ',
        '–∑–∞–∫–∞–∑', '–æ–ø–ª–∞—Ç–∞', '–ø–ª–∞—Ç–µ–∂', '—Ä–∞—Å—Å—Ä–æ—á–∫–∞', '–∫—Ä–µ–¥–∏—Ç', '–ª–∏–∑–∏–Ω–≥',
        '–¥–æ—Å—Ç–∞–≤–∫–∞', '—Å–∞–º–æ–≤—ã–≤–æ–∑', '–æ—Ç–≥—Ä—É–∑–∫–∞', '–ø–æ—Å—Ç–∞–≤–∫–∞', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '–ª–æ–≥–∏—Å—Ç–∏–∫–∞',
        '–≥–∞—Ä–∞–Ω—Ç–∏—è', '–≤–æ–∑–≤—Ä–∞—Ç', '–æ–±–º–µ–Ω',
        # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï —è–≤–ª—è—é—Ç—Å—è —Ç–æ–≤–∞—Ä–æ–º
        '–≤—ã–≥–æ–¥–Ω—ã–π', '–Ω–∏–∑–∫–∏–π', '–≤—ã—Å–æ–∫–∏–π', '–ª—É—á—à–∏–π', '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π', '–Ω–∞–¥–µ–∂–Ω—ã–π',
        '–±–æ–ª—å—à–æ–π', '–º–∞–ª—ã–π', '—É–¥–æ–±–Ω—ã–π', '–±—ã—Å—Ç—Ä—ã–π', '–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π', '—Ö–æ—Ä–æ—à–∏–π',
        '–¥–æ—Å—Ç—É–ø–Ω—ã–π', '–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π', '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π',
        '–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π', '—É–Ω–∏–∫–∞–ª—å–Ω—ã–π', '—à–∏—Ä–æ–∫–∏–π', '–æ–≥—Ä–æ–º–Ω—ã–π', '—Ä–∞–∑–ª–∏—á–Ω—ã–π'
    }

    GEO_ROOTS = [
        '–º–æ—Å–∫–≤', '–ø–∏—Ç–µ—Ä', '—Å–ø–±', '–µ–∫–±', '–µ–∫–∞—Ç–µ—Ä–∏–Ω', '—Ä–æ—Å—Å–∏', '—Ä—Ñ', '–≥–æ—Ä–æ–¥', '–æ–±–ª–∞—Å—Ç',
        '–Ω–æ–≤–≥–æ—Ä–æ–¥', '–∫–∞–∑–∞–Ω', '–∫–∏–µ–≤', '–º–∏–Ω—Å–∫', '–∞–ª–º–∞—Ç—ã', '—Å–∞–º–∞—Ä–∞', '–æ–º—Å–∫', '—á–µ–ª—è–±–∏–Ω',
        '—Ä–æ—Å—Ç–æ–≤', '—É—Ñ–∞', '–≤–æ–ª–≥–æ–≥—Ä–∞–¥', '–ø–µ—Ä–º', '–∫—Ä–∞—Å–Ω–æ—è—Ä', '–≤–æ—Ä–æ–Ω–µ–∂', '—Å–∞—Ä–∞—Ç–æ–≤', '–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä',
        '—Ç—é–º–µ–Ω', '–∏–∂–µ–≤—Å–∫', '—Ç–æ–ª—å—è—Ç—Ç–∏', '–±–∞—Ä–Ω–∞—É–ª', '–∏—Ä–∫—É—Ç—Å–∫', '—É–ª—å—è–Ω–æ–≤—Å–∫', '—Ö–∞–±–∞—Ä–æ–≤—Å–∫'
    ]
    
    SERVICE_KEYWORDS = {
        '—Ä–µ–∑–∫–∞', '–≥–∏–±–∫–∞', '—Å–≤–∞—Ä–∫–∞', '–æ—Ü–∏–Ω–∫–æ–≤–∫–∞', '—Ä—É–±–∫–∞', '–º–æ–Ω—Ç–∞–∂', '—É–∫–ª–∞–¥–∫–∞', 
        '–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∏–∑–æ–ª—è—Ü–∏—è', '—Å–≤–µ—Ä–ª–µ–Ω–∏–µ', '–≥—Ä—É–Ω—Ç–æ–≤–∫–∞', '–ø–æ–∫—Ä–∞—Å–∫–∞', '—É—Å–ª—É–≥–∞',
        '–º–µ—Ç–∞–ª–ª–æ–æ–±—Ä–∞–±–æ—Ç–∫–∞', '–æ–±—Ä–∞–±–æ—Ç–∫–∞', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '—Ä–µ–º–æ–Ω—Ç', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ', '–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ'
    }

    categories = {
        'products': set(),
        'services': set(),
        'commercial': set(),
        'dimensions': set()
    }

    # 2. –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø
    api_candidates = []

    for word in words_list:
        word_lower = word.lower()

        if dim_pattern.search(word_lower) or grade_pattern.match(word_lower) or gost_pattern.search(word_lower) or word_lower.isdigit():
            categories['dimensions'].add(word_lower)
            continue
        
        if morph:
            p = morph.parse(word_lower)[0]
            lemma = p.normal_form
        else:
            lemma = word_lower

        if lemma in SITE_UI_GARBAGE or lemma in COMMERCIAL_WORDS:
            categories['commercial'].add(lemma)
            continue
        
        if any(root in lemma for root in GEO_ROOTS):
            categories['commercial'].add(lemma)
            continue

        if lemma in SERVICE_KEYWORDS or lemma.endswith('–æ–±—Ä–∞–±–æ—Ç–∫–∞'):
            categories['services'].add(lemma)
            continue

        api_candidates.append(word_lower)

    # 3. –ó–ê–ü–†–û–° –ö API YANDEX
    yandex_results = {} 
    
    if api_candidates and yandex_key:
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            future_to_word = {executor.submit(get_yandex_dict_info, w, yandex_key): w for w in api_candidates}
            for future in concurrent.futures.as_completed(future_to_word):
                orig_word = future_to_word[future]
                try:
                    res = future.result()
                    yandex_results[orig_word] = res
                except:
                    yandex_results[orig_word] = {'lemma': orig_word, 'pos': 'unknown'}
    else:
        for w in api_candidates:
             yandex_results[w] = {'lemma': w, 'pos': 'unknown'}

    # 4. –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê)
    for word in api_candidates:
        info = yandex_results.get(word, {'lemma': word, 'pos': 'unknown'})
        lemma = info['lemma']
        pos = info['pos']
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        if lemma in SITE_UI_GARBAGE or lemma in COMMERCIAL_WORDS:
            categories['commercial'].add(lemma)
            continue

        # –£—Å–ª—É–≥–∏
        is_service = False
        if lemma.endswith('–Ω–∏–µ') or lemma.endswith('–µ–Ω–∏–µ'):
            is_service = True
        elif lemma.endswith('–æ–±—Ä–∞–±–æ—Ç–∫–∞'):
            is_service = True
        elif lemma in SERVICE_KEYWORDS:
            is_service = True
        
        if is_service:
            categories['services'].add(lemma)
            continue

        # --- –ì–õ–ê–í–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï ---
        # –õ–æ–≥–∏–∫–∞ –¢–û–í–ê–†–û–í:
        # 1. –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ (noun) -> –¢–æ–≤–∞—Ä (—Ç—Ä—É–±–∞, —Å–µ—Ç–∫–∞)
        # 2. –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ (adjective) -> –¢–æ–≤–∞—Ä (–ø—Ä–æ—Ñ–∏–ª—å–Ω—ã–π, –¥—é—Ä–∞–ª–µ–≤—ã–π), –ï–°–õ–ò –µ–≥–æ –Ω–µ—Ç –≤ —Å–ø–∏—Å–∫–µ COMMERCIAL_WORDS
        # 3. –ü—Ä–∏—á–∞—Å—Ç–∏–µ (participle) -> –¢–æ–≤–∞—Ä (—Å–≤–∞—Ä–Ω–æ–π, –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω—ã–π)
        
        if pos == 'noun':
            if len(lemma) > 2:
                categories['products'].add(lemma)
        elif pos == 'adjective' or pos == 'participle': 
            # –ï—Å–ª–∏ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä COMMERCIAL_WORDS –≤—ã—à–µ, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Å–ª–æ–≤–æ
            if len(lemma) > 2:
                categories['products'].add(lemma)
        elif pos == 'unknown':
            # –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ–º, –Ω–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ (–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–µ)
            if len(lemma) > 2:
                # –í —ç—Ç–æ–º –º–µ—Å—Ç–µ –º—ã –¥–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—É COMMERCIAL_WORDS. 
                # –ï—Å–ª–∏ —Å–ª–æ–≤–æ "–¥—é—Ä–∞–ª–µ–≤—ã–π" –Ω–µ –ø–æ–ø–∞–ª–æ –≤ —Å—Ç–æ–ø-–ª–∏—Å—Ç, –æ–Ω–æ –∏–¥–µ—Ç –≤ —Ç–æ–≤–∞—Ä—ã.
                categories['products'].add(lemma)
        else:
            categories['commercial'].add(lemma)

    return {k: sorted(list(v)) for k, v in categories.items()}

# --- –§–£–ù–ö–¶–ò–Ø API ARSENKIN ---
def get_arsenkin_urls(query, engine_type, region_name, api_token, depth_val=10):
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check"
    url_get = "https://arsenkin.ru/api/tools/get"

    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-type": "application/json"
    }

    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []

    if "–Ø–Ω–¥–µ–∫—Å" in engine_type:
        se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type:
        se_params.append({"type": 11, "region": reg_ids['go']})

    payload = {
        "tools_name": "check-top",
        "data": {
            "queries": [query],
            "is_snippet": False,
            "noreask": True,
            "se": se_params,
            "depth": depth_val
        }
    }

    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        resp_json = r.json()
        if "error" in resp_json or "task_id" not in resp_json:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ API (—Å—Ç–∞—Ä—Ç): {resp_json}")
            return []
        task_id = resp_json["task_id"]
        st.toast(f"–ó–∞–¥–∞—á–∞ ID {task_id} –∑–∞–ø—É—â–µ–Ω–∞")
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–µ –∑–∞–¥–∞—á–∏: {e}")
        return []

    status = "process"
    attempts = 0
    max_attempts = 40
    progress_info = st.empty()
    bar = st.progress(0)
    res_check_data = {}

    while status == "process" and attempts < max_attempts:
        time.sleep(5)
        attempts += 1
        bar.progress(attempts / max_attempts)
        progress_info.text(f"–û–∂–∏–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ API... ({attempts*5} —Å–µ–∫)")
        try:
            r_check = requests.post(url_check, headers=headers, json={"task_id": task_id})
            res_check_data = r_check.json()
            if res_check_data.get("status") == "finish":
                status = "done"
                break
            if str(res_check_data.get("code")) == "429":
                continue
        except Exception:
            pass

    bar.empty()
    progress_info.empty()

    if status != "done":
        st.error(f"‚è≥ –í—Ä–µ–º—è –≤—ã—à–ª–æ. –°—Ç–∞—Ç—É—Å: {res_check_data.get('status', 'Unknown')}")
        return []

    res_data = {}
    try:
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id}, timeout=30)
        res_data = r_final.json()
        if res_data.get("code") != "TASK_RESULT":
            st.error(f"‚ùå –û—à–∏–±–∫–∞: API –Ω–µ –≤–µ—Ä–Ω—É–ª —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.")
            return []
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
        return []

    results_list = []
    try:
        if 'result' in res_data and 'result' in res_data['result'] and 'collect' in res_data['result']['result']:
            collect = res_data['result']['result']['collect']
        else:
            unique_urls = set()
            if 'result' in res_data and isinstance(res_data['result'], list):
                return res_data['result']  # Fallback
            return []

        final_url_list = []
        if collect and isinstance(collect, list) and len(collect) > 0 and \
           collect[0] and isinstance(collect[0], list) and len(collect[0]) > 0 and \
           collect[0][0] and isinstance(collect[0][0], list):
            final_url_list = collect[0][0]
        else:
            unique_urls = set()
            for engine_data in collect:
                if isinstance(engine_data, dict):
                    for engine_id, serps in engine_data.items():
                        if isinstance(serps, list):
                            for item in serps:
                                url = item.get('url')
                                pos = item.get('pos')
                                if url and pos:
                                    if url not in unique_urls:
                                        results_list.append({'url': url, 'pos': pos})
                                        unique_urls.add(url)
            return results_list

        if final_url_list:
            for index, url in enumerate(final_url_list):
                results_list.append({'url': url, 'pos': index + 1})
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        return []
    return results_list

def process_text_detailed(text, settings, n_gram=1):
    text = text.lower().replace('—ë', '–µ')
    pattern = r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+'
    words = re.findall(pattern, text)
    stops = set(w.lower().replace('—ë', '–µ') for w in settings['custom_stops'])
    lemmas = []
    forms_map = defaultdict(set)

    for w in words:
        if len(w) < 2: continue
        if not settings['numbers'] and w.isdigit(): continue
        if w in stops: continue

        lemma = w
        if USE_NLP and n_gram == 1:
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            lemma = p.normal_form.replace('—ë', '–µ')

        lemmas.append(lemma)
        forms_map[lemma].add(w)

    if n_gram > 1:
        ngrams = []
        for i in range(len(lemmas) - n_gram + 1):
            phrase = " ".join(lemmas[i:i+n_gram])
            ngrams.append(phrase)
        return ngrams, {}

    return lemmas, forms_map

def parse_page(url, settings):
    headers = {'User-Agent': settings['ua']}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code != 200: return None
        soup = BeautifulSoup(r.text, 'html.parser')

        tags_to_remove = []
        if settings['noindex']: tags_to_remove.append('noindex')
        for c in soup.find_all(string=lambda text: isinstance(text, Comment)): c.extract()
        if tags_to_remove:
            for t in soup.find_all(tags_to_remove): t.decompose()

        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)

        extra_text = []
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'): extra_text.append(meta_desc['content'])
        meta_kw = soup.find('meta', attrs={'name': 'keywords'})
        if meta_kw and meta_kw.get('content'): extra_text.append(meta_kw['content'])

        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])

        body_text_raw = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text = re.sub(r'\s+', ' ', body_text_raw).strip()

        if not body_text: return None
        return {'url': url, 'domain': urlparse(url).netloc, 'body_text': body_text, 'anchor_text': anchor_text}
    except:
        return None

def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    all_forms_map = defaultdict(set)

    if not my_data or not my_data.get('body_text'):
        my_lemmas, my_forms, my_anchors, my_len = [], {}, [], 0
    else:
        my_lemmas, my_forms = process_text_detailed(my_data['body_text'], settings)
        my_anchors, _ = process_text_detailed(my_data['anchor_text'], settings)
        my_len = len(my_lemmas)
        for k, v in my_forms.items(): all_forms_map[k].update(v)

    comp_data_parsed = [d for d in comp_data_full if d.get('body_text')]
    comp_docs = []
    for p in comp_data_parsed:
        body, c_forms = process_text_detailed(p['body_text'], settings)
        anchor, _ = process_text_detailed(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor, 'url': p['url'], 'domain': p['domain']})
        for k, v in c_forms.items(): all_forms_map[k].update(v)

    if not comp_docs:
        return { "depth": pd.DataFrame(), "hybrid": pd.DataFrame(), "relevance_top": pd.DataFrame(),
            "my_score": {"width": 0, "depth": 0}, "missing_semantics_high": [], "missing_semantics_low": [] }

    c_lens = [len(d['body']) for d in comp_docs]
    avg_dl = np.mean(c_lens) if c_lens else 1
    if avg_dl == 0: avg_dl = 1
    median_len = np.median(c_lens) if c_lens else 0
    norm_k_recs = (my_len / median_len) if (median_len > 0 and my_len > 0 and settings['norm']) else 1.0

    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    N = len(comp_docs)

    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
    word_counts_per_doc = []
    for d in comp_docs: word_counts_per_doc.append(Counter(d['body']))

    word_idf_map = {}
    for lemma in vocab:
        df = doc_freqs[lemma]
        if df == 0: continue
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        word_idf_map[lemma] = max(idf, 0.01)

    S_WIDTH_CORE = set()
    missing_semantics_high = []
    missing_semantics_low = []
    my_full_lemmas_set = set(my_lemmas) | set(my_anchors)
    lsi_candidates_weighted = []

    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        c_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        med_val = np.median(c_counts)
        percent = int((doc_freqs[lemma] / N) * 100)
        weight_simple = word_idf_map.get(lemma, 0) * med_val
        if med_val > 0: lsi_candidates_weighted.append((lemma, weight_simple))
        is_width_word = False
        if med_val >= 1:
            S_WIDTH_CORE.add(lemma)
            is_width_word = True

        if lemma not in my_full_lemmas_set:
            if len(lemma) < 2 or lemma.isdigit(): continue
            item = {'word': lemma, 'percent': percent, 'weight': weight_simple}
            if is_width_word: missing_semantics_high.append(item)
            elif percent >= 30: missing_semantics_low.append(item)

    missing_semantics_high.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low.sort(key=lambda x: x['percent'], reverse=True)
    lsi_candidates_weighted.sort(key=lambda x: x[1], reverse=True)
    S_DEPTH_TOP70 = set([x[0] for x in lsi_candidates_weighted[:70]])
    total_width_core_count = len(S_WIDTH_CORE)

    def calculate_bm25_okapi(doc_tokens, doc_len):
        if avg_dl == 0 or doc_len == 0: return 0
        score = 0
        counts = Counter(doc_tokens)
        k1 = 1.2
        b = 0.75
        target_words = S_WIDTH_CORE if S_WIDTH_CORE else S_DEPTH_TOP70
        for word in target_words:
            if word not in counts: continue
            tf = counts[word]
            idf = word_idf_map.get(word, 0)
            numerator = tf * (k1 + 1)
            denominator = tf + k1 * (1 - b + b * (doc_len / avg_dl))
            score += idf * (numerator / denominator)
        return score

    def calculate_width_score_val(lemmas_set):
        if total_width_core_count == 0: return 0
        intersection_count = len(lemmas_set.intersection(S_WIDTH_CORE))
        ratio = intersection_count / total_width_core_count
        if ratio >= 0.9: return 100
        else: return int(round((ratio / 0.9) * 100))

    competitor_scores_map = {}
    comp_bm25_list = []
    for i, doc in enumerate(comp_docs):
        raw_bm25 = calculate_bm25_okapi(doc['body'], c_lens[i])
        comp_bm25_list.append(raw_bm25)
        width_val = calculate_width_score_val(set(doc['body']))
        competitor_scores_map[doc['url']] = {'width_final': min(100, width_val), 'bm25_val': raw_bm25}

    median_bm25_top = np.median(comp_bm25_list) if comp_bm25_list else 0
    spam_limit = median_bm25_top * 1.25 if median_bm25_top > 0 else 1

    for url, scores in competitor_scores_map.items():
        depth_val = int(round((scores['bm25_val'] / spam_limit) * 100))
        scores['depth_final'] = min(100, depth_val)

    my_bm25 = calculate_bm25_okapi(my_lemmas, my_len)
    my_depth_score_final = min(100, int(round((my_bm25 / spam_limit) * 100)))
    my_width_score_final = min(100, calculate_width_score_val(my_full_lemmas_set))

    table_depth, table_hybrid = [], []
    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        df = doc_freqs[lemma]
        if df < 2 and lemma not in my_lemmas: continue
        my_tf_count = my_lemmas.count(lemma)
        forms_set = all_forms_map.get(lemma, set())
        forms_str = ", ".join(sorted(list(forms_set))) if forms_set else lemma
        c_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        med_total = np.median(c_counts)
        max_total = np.max(c_counts)

        base_min = min(np.mean(c_counts), med_total)
        rec_min = int(math.ceil(base_min * norm_k_recs))
        rec_max = int(round(max_total * norm_k_recs))
        if rec_max < rec_min: rec_max = rec_min
        rec_median = med_total * norm_k_recs

        status = "–ù–æ—Ä–º–∞"
        action_diff = 0
        action_text = "‚úÖ"
        if my_tf_count < rec_min:
            status = "–ù–µ–¥–æ—Å–ø–∞–º"
            action_diff = int(round(rec_min - my_tf_count))
            if action_diff == 0: action_diff = 1
            action_text = f"+{action_diff}"
        elif my_tf_count > rec_max:
            status = "–ü–µ—Ä–µ—Å–ø–∞–º"
            action_diff = int(round(my_tf_count - rec_max))
            if action_diff == 0: action_diff = 1
            action_text = f"-{action_diff}"

        depth_percent = 0
        if rec_median > 0.1: depth_percent = int(round((my_tf_count / rec_median) * 100))
        else: depth_percent = 0 if my_tf_count == 0 else 100

        weight_hybrid = word_idf_map.get(lemma, 0) * (my_tf_count / my_len if my_len > 0 else 0)
        table_depth.append({
            "–°–ª–æ–≤–æ": lemma, "–°–ª–æ–≤–æ—Ñ–æ—Ä–º—ã": forms_str, "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_tf_count,
            "–ú–µ–¥–∏–∞–Ω–∞": round(med_total, 1), "–ú–∏–Ω–∏–º—É–º (—Ä–µ–∫)": rec_min, "–ú–∞–∫—Å–∏–º—É–º (—Ä–µ–∫)": rec_max,
            "–ì–ª—É–±–∏–Ω–∞ %": min(100, depth_percent), "–°—Ç–∞—Ç—É—Å": status, "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": action_text,
            "is_missing": (status == "–ù–µ–¥–æ—Å–ø–∞–º" and my_tf_count == 0),
            "sort_val": abs(action_diff) if status != "–ù–æ—Ä–º–∞" else 0
        })
        table_hybrid.append({
            "–°–ª–æ–≤–æ": lemma, "TF-IDF –¢–û–ü": round(word_idf_map.get(lemma, 0) * (med_total / avg_dl if avg_dl > 0 else 0), 4),
            "TF-IDF —É –≤–∞—Å": round(weight_hybrid, 4), "–°–∞–π—Ç–æ–≤": df, "–ü–µ—Ä–µ—Å–ø–∞–º": max_total
        })

    table_rel = []
    for item in original_results:
        url = item['url']
        scores = competitor_scores_map.get(url, {'width_final':0, 'depth_final':0})
        table_rel.append({
            "–î–æ–º–µ–Ω": urlparse(url).netloc, "–ü–æ–∑–∏—Ü–∏—è": item['pos'],
            "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": scores['width_final'], "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": scores['depth_final']
        })

    my_label = f"{my_data['domain']} (–í—ã)" if (my_data and my_data.get('domain')) else "–í–∞—à —Å–∞–π—Ç"
    table_rel.append({ "–î–æ–º–µ–Ω": my_label, "–ü–æ–∑–∏—Ü–∏—è": my_serp_pos if my_serp_pos > 0 else len(original_results) + 1,
        "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": my_width_score_final, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": my_depth_score_final })

    return {
        "depth": pd.DataFrame(table_depth), "hybrid": pd.DataFrame(table_hybrid),
        "relevance_top": pd.DataFrame(table_rel).sort_values(by='–ü–æ–∑–∏—Ü–∏—è', ascending=True).reset_index(drop=True),
        "my_score": {"width": my_width_score_final, "depth": my_depth_score_final},
        "missing_semantics_high": missing_semantics_high, "missing_semantics_low": missing_semantics_low
    }

# ==========================================
# 5. –§–£–ù–ö–¶–ò–Ø –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø (PAGINATION)
# ==========================================
def render_paginated_table(df, title_text, key_prefix, default_sort_col=None, use_abs_sort_default=False):
    if df.empty:
        st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")
        return

    col_t1, col_t2 = st.columns([7, 3])
    with col_t1: st.markdown(f"### {title_text}")

    if f'{key_prefix}_sort_col' not in st.session_state:
        st.session_state[f'{key_prefix}_sort_col'] = default_sort_col if (default_sort_col and default_sort_col in df.columns) else df.columns[0]
    if f'{key_prefix}_sort_order' not in st.session_state:
        st.session_state[f'{key_prefix}_sort_order'] = "–£–±—ã–≤–∞–Ω–∏–µ"

    search_query = st.text_input(f"üîç –ü–æ–∏—Å–∫ ({title_text})", key=f"{key_prefix}_search")
    if search_query:
        mask = df.astype(str).apply(lambda x: x.str.contains(search_query, case=False, na=False)).any(axis=1)
        df_filtered = df[mask].copy()
    else:
        df_filtered = df.copy()

    if df_filtered.empty:
        st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return

    with st.container():
        st.markdown("<div class='sort-container'>", unsafe_allow_html=True)
        col_s1, col_s2, col_sp = st.columns([2, 2, 4])
        with col_s1:
            current_sort = st.session_state[f'{key_prefix}_sort_col']
            if current_sort not in df_filtered.columns: current_sort = df_filtered.columns[0]
            sort_col = st.selectbox("üóÇ –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ:", df_filtered.columns, key=f"{key_prefix}_sort_box", index=list(df_filtered.columns).index(current_sort))
            st.session_state[f'{key_prefix}_sort_col'] = sort_col
        with col_s2:
            sort_order = st.radio("–ü–æ—Ä—è–¥–æ–∫:", ["–£–±—ã–≤–∞–Ω–∏–µ", "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ"], horizontal=True, key=f"{key_prefix}_order_box", index=0 if st.session_state[f'{key_prefix}_sort_order'] == "–£–±—ã–≤–∞–Ω–∏–µ" else 1)
            st.session_state[f'{key_prefix}_sort_order'] = sort_order
        st.markdown("</div>", unsafe_allow_html=True)

    ascending = (sort_order == "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ")
    if use_abs_sort_default and sort_col == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è" and "sort_val" in df_filtered.columns:
         df_filtered = df_filtered.sort_values(by="sort_val", ascending=ascending)
    elif ("–î–æ–±–∞–≤–∏—Ç—å" in sort_col or "+/-" in sort_col) and df_filtered[sort_col].dtype == object:
        try:
            df_filtered['_temp_sort'] = df_filtered[sort_col].astype(str).str.replace(r'[^\d]', '', regex=True)
            df_filtered['_temp_sort'] = pd.to_numeric(df_filtered['_temp_sort'], errors='coerce').fillna(0)
            df_filtered = df_filtered.sort_values(by='_temp_sort', ascending=ascending).drop(columns=['_temp_sort'])
        except:
             df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)
    else:
        df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)

    df_filtered = df_filtered.reset_index(drop=True)
    df_filtered.index = df_filtered.index + 1

    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        export_df = df_filtered.copy()
        if "is_missing" in export_df.columns: del export_df["is_missing"]
        if "sort_val" in export_df.columns: del export_df["sort_val"]
        export_df.to_excel(writer, index=False, sheet_name='Data')
    excel_data = buffer.getvalue()

    with col_t2:
        st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel", data=excel_data, file_name=f"{key_prefix}_export.xlsx", mime="application/vnd.ms-excel", key=f"{key_prefix}_down")

    ROWS_PER_PAGE = 20
    if f'{key_prefix}_page' not in st.session_state: st.session_state[f'{key_prefix}_page'] = 1
    total_rows = len(df_filtered)
    total_pages = math.ceil(total_rows / ROWS_PER_PAGE)
    if total_pages == 0: total_pages = 1
    current_page = st.session_state[f'{key_prefix}_page']
    if current_page > total_pages: current_page = total_pages
    if current_page < 1: current_page = 1
    st.session_state[f'{key_prefix}_page'] = current_page

    start_idx = (current_page - 1) * ROWS_PER_PAGE
    end_idx = start_idx + ROWS_PER_PAGE
    df_view = df_filtered.iloc[start_idx:end_idx]

    def highlight_rows(row):
        base_style = 'background-color: #FFFFFF; color: #3D4858; border-bottom: 1px solid #DBEAFE;'
        styles = []
        status = row.get("–°—Ç–∞—Ç—É—Å", "")
        for col_name in row.index:
            cell_style = base_style
            if col_name == "–°—Ç–∞—Ç—É—Å":
                if status == "–ù–µ–¥–æ—Å–ø–∞–º": cell_style += "color: #D32F2F; font-weight: bold;"
                elif status == "–ü–µ—Ä–µ—Å–ø–∞–º": cell_style += "color: #E65100; font-weight: bold;"
                elif status == "–ù–æ—Ä–º–∞": cell_style += "color: #2E7D32; font-weight: bold;"
            styles.append(cell_style)
        return styles

    cols_to_hide = ["is_missing", "sort_val"]
    cols_to_hide = [c for c in cols_to_hide if c in df_view.columns]

    try: styled_df = df_view.style.apply(highlight_rows, axis=1)
    except: styled_df = df_view

    st.dataframe(styled_df, use_container_width=True, height=(len(df_view) * 35) + 40, column_config={c: None for c in cols_to_hide})

    c_spacer, c_btn_prev, c_info, c_btn_next = st.columns([6, 1, 1, 1])
    with c_btn_prev:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_prev", disabled=(current_page <= 1), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] -= 1
            st.rerun()
    with c_info: st.markdown(f"<div style='text-align: center; margin-top: 10px;'><b>{current_page}</b> / {total_pages}</div>", unsafe_allow_html=True)
    with c_btn_next:
        if st.button("‚û°Ô∏è", key=f"{key_prefix}_next", disabled=(current_page >= total_pages), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] += 1
            st.rerun()
    st.markdown("---")

# ==========================================
# 6. –õ–û–ì–ò–ö–ê –î–õ–Ø PERPLEXITY (AI GEN)
# ==========================================
STATIC_DATA_GEN = {
    'IP_PROP4817': "–£—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏",
    'IP_PROP4818': "–û–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –≤ —Ä–µ–≥–∏–æ–Ω—ã —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4819': """<p>–ù–∞–¥–µ–∂–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫–∞–∑–∞ –≤ –ª—é–±—É—é —Ç–æ—á–∫—É —Å—Ç—Ä–∞–Ω—ã: "–°—Ç–∞–ª—å–º–µ—Ç—É—Ä–∞–ª" –æ—Ç–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–≤–∞—Ä 24 —á–∞—Å–∞ –≤ —Å—É—Ç–∫–∏, 7 –¥–Ω–µ–π –≤ –Ω–µ–¥–µ–ª—é. –ë–æ–ª–µ–µ 4 000 –æ—Ç–≥—Ä—É–∑–æ–∫ –≤ –≥–æ–¥. –ü—Ä–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–∏ –∑–∞–∫–∞–∑–∞ –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –≤–∞–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ä—à—Ä—É—Ç.</p>""",
    'IP_PROP4820': """<p>–ù–∞—à–∏ –∏–∑–¥–µ–ª–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è—Ö –£—Ä–∞–ª–∞, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞, –ü–æ–≤–æ–ª–∂—å—è, –°–∏–±–∏—Ä–∏. –ü–∞—Ä—Ç–Ω–µ—Ä—ã –ø–æ –ª–æ–≥–∏—Å—Ç–∏–∫–µ –ø—Ä–µ–¥–ª–æ–∂–∞—Ç –¥–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–∫–∞–∑ —Å–∞–º—ã–º —É–¥–æ–±–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º ‚Äì –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–º, –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω—ã–º, –¥–∞–∂–µ –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º. –î–ª—è –≤–∞—Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—É—é —Å—Ö–µ–º—É –ø–æ–¥ —É–¥–æ–±–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è. –ü–æ–≥—Ä—É–∑–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ç–µ—Ö–Ω–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.</p>
<div class="h4"><h4>–°–∞–º–æ–≤—ã–≤–æ–∑</h4></div><p>–ï—Å–ª–∏ –æ–±—ã—á–Ω–æ —Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∑–∞–±—Ä–∞—Ç—å —Ç–æ–≤–∞—Ä –∏–ª–∏ –¥–∞–µ—Ç–µ —ç—Ç–æ –ø—Ä–∞–≤–æ —É–ø–æ–ª–Ω–æ–º–æ—á–µ–Ω–Ω—ã–º, –∞–¥—Ä–µ—Å –∏ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã —Å–∫–ª–∞–¥–∞ –≤ —Å–≤–æ–µ–º –≥–æ—Ä–æ–¥–µ —É—Ç–æ—á–Ω—è–π—Ç–µ —É –º–µ–Ω–µ–¥–∂–µ—Ä–∞.</p>
<div class="h4"><h4>–ì—Ä—É–∑–æ–≤–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏</h4></div><p>–û—Ç–ø—Ä–∞–≤–∏–º –ø—Ä–æ–∫–∞—Ç –Ω–∞ –≤–∞—à –æ–±—ä–µ–∫—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –∞–≤—Ç–æ–ø–∞—Ä–∫–æ–º. –ü–æ–ª—É—á–µ–Ω–∏–µ –≤ —É–ø–∞–∫–æ–≤–∫–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏, –∞ –∏–º–µ–Ω–Ω–æ –Ω–∞ –¥–µ—Ä–µ–≤—è–Ω–Ω–æ–º –ø–æ–¥–¥–æ–Ω–µ.</p>
<div class="h4"><h4>–°–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å –¢–ö</h4></div><p>–î–æ—Å—Ç–∞–≤–∫–∞ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –†–æ—Å—Å–∏–∏ –∏ –°–ù–ì. –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å—Å—è, —Ç–∞–∫ –∫–∞–∫ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç, –æ–¥–Ω–∞–∫–æ, —Å—Ä–∞–≤–Ω–∏–º —Å—Ç–æ–∏–º–æ—Å—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª—É–∂–± –∏ –≤—ã–±–µ—Ä–µ–º –ª—É—á—à—É—é.</p>""",
    'IP_PROP4821': "–û–ø–ª–∞—Ç–∞ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤:",
    'IP_PROP4822': """<p>–ù–∞—à–∞ –∫–æ–º–ø–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞ –ø—Ä–∏–Ω—è—Ç—å –ª—é–±—ã–µ –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–µ –≤–∏–¥—ã –æ–ø–ª–∞—Ç—ã –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏—Ü: –ø–æ —Å—á–µ—Ç—É, –Ω–∞–ª–∏—á–Ω–∞—è –∏ –±–µ–∑–Ω–∞–ª–∏—á–Ω–∞—è, –Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–π –ø–ª–∞—Ç–µ–∂, —Ç–∞–∫–∂–µ –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞ –∏ –æ—Ç—Å—Ä–æ—á–∫–∞ –ø–ª–∞—Ç–µ–∂–∞.</p>""",
    'IP_PROP4823': """<div class="h4"><h3>–ü—Ä–∏–º–µ—Ä—ã –≤–æ–∑–º–æ–∂–Ω–æ–π –æ–ø–ª–∞—Ç—ã</h3></div><div class="an-col-12"><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–° –ø–æ–º–æ—â—å—é –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤ —Ü–µ–Ω—Ç—Ä–∞—Ö –ø—Ä–æ–¥–∞–∂</span></p></li></ul><p>–í–∞–∂–Ω–æ! –¶–µ–Ω–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—É–±–ª–∏—á–Ω–æ–π –æ—Ñ–µ—Ä—Ç–æ–π. –ü—Ä–∏—Ö–æ–¥–∏—Ç–µ –≤ –Ω–∞—à –æ—Ñ–∏—Å, —á—Ç–æ–±—ã —É—Ç–æ—á–Ω–∏—Ç—å –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ, –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ—á—Ç–∏ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å, —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –≤–æ–∑–≤—Ä–∞—Ç, —Å—á–µ—Ç, —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ª–æ–≥–∏—Å—Ç–∏–∫—É.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞ —Ä–∞—Å—á–µ—Ç–Ω—ã–π —Å—á–µ—Ç</span></p></li></ul><p>–ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É —Å—á–µ—Ç—É –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏ –±–∞–Ω–∫–∞ –∏–ª–∏ –ø—É—Ç–µ–º –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥—Å—Ç–≤ —á–µ—Ä–µ–∑ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç (—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∑–∞—â–∏—â–µ–Ω—ã, —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –æ—Ç–¥–µ–ª–µ–Ω–∏—è). –î–ª—è –ø—Ä–∞–≤–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å —Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –ø–ª–∞—Ç–µ–∂–Ω–æ–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ —Å –æ—Ç–º–µ—Ç–∫–æ–π –±–∞–Ω–∫–∞.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞–ª–∏—á–Ω—ã–º–∏ –∏–ª–∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∫–∞—Ä—Ç–æ–π –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏</span></p></li></ul><p><span style="font-weight: 400;">–ü–æ–º–æ–∂–µ–º —Å –æ–ø–ª–∞—Ç–æ–π: –æ–±—ä–µ–º –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ. –ö—Ä—É–ø–Ω—ã–º –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º ‚Äì –¥–µ–Ω—å–≥–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–µ–º–∫–∏ —Ç–æ–≤–∞—Ä–∞.</span></p><p>–ú–µ–Ω–µ–¥–∂–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.</p><p>–ó–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ —á–µ—Ä–µ–∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç:</p><p><a class="btn btn-blue" href="/catalog/">–ö–∞—Ç–∞–ª–æ–≥ (–º–∞–≥–∞–∑–∏–Ω-–º–µ–Ω—é):</a></p></div></div><br>""",
    'IP_PROP4824': "–û–ø–∏—Å–∞–Ω–∏–µ, —Å—Ç–∞—Ç—å–∏, –ø–æ–∏—Å–∫, –æ—Ç–∑—ã–≤—ã, –Ω–æ–≤–æ—Å—Ç–∏, –∞–∫—Ü–∏–∏, –∂—É—Ä–Ω–∞–ª, info:",
    'IP_PROP4825': "–ú–æ–∂–µ–º –º–µ—Ç–∞–ª–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –æ—Ü–∏–Ω–∫–æ–≤–∞—Ç—å, –Ω–∏–∫–µ–ª–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–æ–≤–æ–ª–æ—á—å",
    'IP_PROP4826': "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥",
    'IP_PROP4834': "–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –±–µ–∑ –ø—Ä–∏–º–µ—Å–µ–π",
    'IP_PROP4835': "–ü–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫",
    'IP_PROP4836': "–ö–∞—á–µ—Å—Ç–≤–æ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä",
    'IP_PROP4837': "–ü–æ—Ä—è–¥–æ–∫ –≤ –ì–û–°–¢–∞—Ö"
}

def get_page_data_for_gen(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
    except Exception as e: return None, None, f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}"
    if response.status_code != 200: return None, None, f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—É—Å–∞: {response.status_code}"
    soup = BeautifulSoup(response.text, 'html.parser')
    description_div = soup.find('div', class_='description-container')
    base_text = description_div.get_text(separator="\n", strip=True) if description_div else soup.body.get_text(separator="\n", strip=True)[:5000]
    tags_container = soup.find(class_='popular-tags-inner')
    tags_data = []
    if tags_container:
        links = tags_container.find_all('a')
        for link in links:
            tag_url = urljoin(url, link.get('href')) if link.get('href') else None
            if tag_url: tags_data.append({'name': link.get_text(strip=True), 'url': tag_url})
    return base_text, tags_data, None

def generate_five_blocks(client, base_text, tag_name, seo_words=None):
    if not base_text: return ["Error: No base text"] * 5
    system_instruction = "–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä. –ù–∞–ø–∏—à–∏ 5 HTML –±–ª–æ–∫–æ–≤. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π markdown."
    keywords_instruction = ""
    if seo_words and len(seo_words) > 0:
        keywords_str = ", ".join(seo_words)
        keywords_instruction = f"–í–∫–ª—é—á–∏ —ç—Ç–∏ —Å–ª–æ–≤–∞ (—Å–∫–ª–æ–Ω—è—è –∏—Ö) –∏ –≤—ã–¥–µ–ª–∏ <b>: {keywords_str}"

    user_prompt = f"""–í–í–û–î–ù–´–ï: –¢–µ–≥ "{tag_name}". –ë–∞–∑–∞: \"\"\"{base_text[:3000]}\"\"\" {keywords_instruction}
    –ó–ê–î–ê–ß–ê: 5 –±–ª–æ–∫–æ–≤. –°—Ç—Ä—É–∫—Ç—É—Ä–∞: h2/h3, –∞–±–∑–∞—Ü, –≤–≤–æ–¥–Ω–∞—è —Ñ—Ä–∞–∑–∞:, —Å–ø–∏—Å–æ–∫, –∑–∞–∫–ª—é—á–µ–Ω–∏–µ. –ë–µ–∑ [1] —Å—Å—ã–ª–æ–∫. –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: |||BLOCK_SEP|||"""

    try:
        response = client.chat.completions.create(model="sonar-pro", messages=[{"role": "system", "content": system_instruction}, {"role": "user", "content": user_prompt}], temperature=0.7)
        content = response.choices[0].message.content
        content = re.sub(r'\[\d+\]', '', content).replace("```html", "").replace("```", "")
        blocks = [b.strip() for b in content.split("|||BLOCK_SEP|||") if b.strip()]
        while len(blocks) < 5: blocks.append("")
        return blocks[:5]
    except Exception as e: return [f"API Error: {str(e)}"] * 5

def generate_html_table(client, user_prompt, seo_keywords_data=None):
    seo_instruction = ""
    if seo_keywords_data:
        words_desc = [f"- '{item['word']}': {item['count']} times" for item in seo_keywords_data]
        seo_instruction = f"MANDATORY SEO: Use these words ({', '.join(words_desc)}). Wrap in <b>."

    system_instruction = f"Generate HTML tables. Inline CSS: table border 2px solid black, th bg #f0f0f0. {seo_instruction} No markdown."
    try:
        response = client.chat.completions.create(model="sonar-pro", messages=[{"role": "system", "content": system_instruction}, {"role": "user", "content": user_prompt}], temperature=0.7)
        return re.sub(r'\[\d+\]', '', response.choices[0].message.content).replace("```html", "").replace("```", "").strip()
    except Exception as e: return f"Error: {e}"

# ==========================================
# 7. –ò–ù–¢–ï–†–§–ï–ô–° (TABS)
# ==========================================
tab_seo, tab_ai, tab_tags, tab_tables = st.tabs(["üìä SEO –ê–Ω–∞–ª–∏–∑", "ü§ñ AI –ì–µ–Ω–µ—Ä–∞—Ü–∏—è", "üè∑Ô∏è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–≥–æ–≤", "üß© –¢–∞–±–ª–∏—Ü—ã"])

# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 1: SEO
# ------------------------------------------
with tab_seo:
    col_main, col_sidebar = st.columns([65, 35])
    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
        my_input_type = st.radio("–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], horizontal=True, label_visibility="collapsed", key="my_page_source_radio")

        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            st.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã", placeholder="https://site.ru/catalog/tovar", label_visibility="collapsed", key="my_url_input")
        elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏–ª–∏ —Ç–µ–∫—Å—Ç", height=200, label_visibility="collapsed", placeholder="–í—Å—Ç–∞–≤—å—Ç–µ HTML", key="my_content_input")

        st.markdown("### –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å")
        st.text_input("–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞", label_visibility="collapsed", key="query_input")

        st.markdown("### –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        source_type_new = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫", ["–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)", "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"], horizontal=True, label_visibility="collapsed", key="competitor_source_radio")
        source_type = "API" if "API" in source_type_new else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"

        if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
            def update_manual_urls(): st.session_state['persistent_urls'] = st.session_state.manual_urls_widget
            st.text_area("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", height=200, key="manual_urls_widget", value=st.session_state['persistent_urls'], on_change=update_manual_urls)

        st.markdown("### –°–ø–∏—Å–∫–∏ (Stop / Exclude)")
        st.text_area("–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–æ–º–µ–Ω—ã", DEFAULT_EXCLUDE, height=100, key="settings_excludes")
        st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞", DEFAULT_STOPS, height=100, key="settings_stops")

        if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", type="primary", use_container_width=True, key="start_analysis_btn"):
            for key in list(st.session_state.keys()):
                if key.endswith('_page'): st.session_state[key] = 1
            st.session_state.start_analysis_flag = True

    with col_sidebar:
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API")
        
        # –õ–û–ì–ò–ö–ê –í–í–û–î–ê –ö–õ–Æ–ß–ï–ô –ï–°–õ–ò –ò–• –ù–ï–¢ –í –°–ï–ö–†–ï–¢–ê–•
        # –ï—Å–ª–∏ –∫–ª—é—á–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ st.secrets (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –±–µ–∑ —Ñ–∞–π–ª–∞ secrets.toml),
        # –ø—Ä–µ–¥–ª–æ–∂–∏–º –≤–≤–µ—Å—Ç–∏ –∏—Ö –≤—Ä—É—á–Ω—É—é.
        
        if not ARSENKIN_TOKEN:
             new_arsenkin = st.text_input("Arsenkin Token", type="password", key="input_arsenkin")
             if new_arsenkin:
                 st.session_state.arsenkin_token = new_arsenkin
                 ARSENKIN_TOKEN = new_arsenkin # –û–±–Ω–æ–≤–ª—è–µ–º –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞
        
        if not YANDEX_DICT_KEY:
             new_yandex = st.text_input("Yandex Dict Key", type="password", key="input_yandex")
             if new_yandex:
                 st.session_state.yandex_dict_key = new_yandex
                 YANDEX_DICT_KEY = new_yandex

        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
        st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
        st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
        st.selectbox("–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞", list(REGION_MAP.keys()), key="settings_region")
        st.selectbox("–ì–ª—É–±–∏–Ω–∞ —Å–±–æ—Ä–∞ (–¢–û–ü)", [10, 20, 30], index=0, key="settings_top_n")
        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å <noindex>", True, key="settings_noindex")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", False, key="settings_alt")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", False, key="settings_numbers")
        st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True, key="settings_norm")
        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã", True, key="settings_agg")

    # --- –õ–û–ì–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê (–í–ù–£–¢–†–ò –í–ö–õ–ê–î–ö–ò) ---
    if st.session_state.get('start_analysis_flag'):
        st.session_state.start_analysis_flag = False
        settings = {'noindex': st.session_state.settings_noindex, 'alt_title': st.session_state.settings_alt, 'numbers': st.session_state.settings_numbers, 'norm': st.session_state.settings_norm, 'ua': st.session_state.settings_ua, 'custom_stops': st.session_state.settings_stops.split()}

        my_data, my_domain, my_serp_pos = None, "", 0
        
        current_input_type = st.session_state.get("my_page_source_radio")
        
        if current_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
                my_data = parse_page(st.session_state.my_url_input, settings)
                if not my_data: st.error("–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."); st.stop()
                my_domain = urlparse(st.session_state.my_url_input).netloc
        elif current_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            my_data = {'url': 'Local', 'domain': 'local', 'body_text': st.session_state.my_content_input, 'anchor_text': ''}

        target_urls_raw = []
        current_source_val = st.session_state.get("competitor_source_radio")
        current_source_type = "API" if "API" in current_source_val else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"

        if current_source_type == "API":
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º
            if not ARSENKIN_TOKEN:
                st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç API —Ç–æ–∫–µ–Ω Arsenkin. –í–≤–µ–¥–∏—Ç–µ –µ–≥–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –∏–ª–∏ –≤ secrets.toml")
                st.stop()
                
            with st.spinner("API Arsenkin..."):
                found = get_arsenkin_urls(st.session_state.query_input, st.session_state.settings_search_engine, st.session_state.settings_region, ARSENKIN_TOKEN)
                if not found: st.stop()

                excl = [d.strip() for d in st.session_state.settings_excludes.split('\n') if d.strip()]
                if st.session_state.settings_agg: excl.extend(["avito", "ozon", "wildberries", "market.yandex", "tiu", "youtube", "vk.com", "yandex", "leroymerlin", "petrovich"])

                filtered = []
                for res in found:
                    dom = urlparse(res['url']).netloc
                    if my_domain and my_domain == dom:
                        if my_serp_pos == 0 or res['pos'] < my_serp_pos: my_serp_pos = res['pos']
                        continue
                    if any(x in dom for x in excl): continue
                    filtered.append(res)
                target_urls_raw = filtered[:st.session_state.settings_top_n]
                st.session_state['persistent_urls'] = "\n".join([i['url'] for i in target_urls_raw])
        else:
            raw_urls = st.session_state.get("persistent_urls", "")
            target_urls_raw = [{'url': u.strip(), 'pos': i+1} for i, u in enumerate(raw_urls.split('\n')) if u.strip()]

        if not target_urls_raw: st.error("–ù–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤."); st.stop()

        comp_data_full = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = {executor.submit(parse_page, u['url'], settings): u['url'] for u in target_urls_raw}
            done, total = 0, len(target_urls_raw)
            prog = st.progress(0)
            for f in concurrent.futures.as_completed(futures):
                if res := f.result(): comp_data_full.append(res)
                done += 1
                prog.progress(done / total)
        prog.empty()

        with st.spinner("–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫..."):
            st.session_state.analysis_results = calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, target_urls_raw)
            st.session_state.analysis_done = True

            # ==========================================
            # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø
            # ==========================================
            res = st.session_state.analysis_results

            words_to_check = [x['word'] for x in res.get('missing_semantics_high', [])]

            if not words_to_check:
                st.session_state.categorized_products = []
                st.session_state.categorized_services = []
                st.session_state.categorized_commercial = []
                st.session_state.categorized_dimensions = []
            else:
                with st.spinner("–£—Ç–æ—á–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å –°–ª–æ–≤–∞—Ä—å..."):
                    # –ü–µ—Ä–µ–¥–∞–µ–º —Ç–µ–∫—É—â–∏–π (–≤–æ–∑–º–æ–∂–Ω–æ –≤–≤–µ–¥–µ–Ω–Ω—ã–π –≤—Ä—É—á–Ω—É—é) –∫–ª—é—á
                    categorized = classify_semantics_with_api(words_to_check, YANDEX_DICT_KEY)

                st.session_state.categorized_products = categorized['products']
                st.session_state.categorized_services = categorized['services']
                st.session_state.categorized_commercial = categorized['commercial']
                st.session_state.categorized_dimensions = categorized['dimensions']

            st.rerun()

    if st.session_state.analysis_done and st.session_state.analysis_results:
        results = st.session_state.analysis_results
        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        st.markdown(f"<div style='background:{LIGHT_BG_MAIN};padding:15px;border-radius:8px;'><b>–†–µ–∑—É–ª—å—Ç–∞—Ç:</b> –®–∏—Ä–∏–Ω–∞: {results['my_score']['width']} | –ì–ª—É–±–∏–Ω–∞: {results['my_score']['depth']}</div>", unsafe_allow_html=True)

        # --- –ë–õ–û–ö –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –ö–ê–¢–ï–ì–û–†–ò–ô (4 –ë–õ–û–ö–ê) ---
        with st.expander("üõí –†–µ–∑—É–ª—å—Ç–∞—Ç –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å–ª–æ–≤ (–° —É—á–µ—Ç–æ–º –Ø–Ω–¥–µ–∫—Å API)", expanded=True):
            c1, c2 = st.columns(2)
            c3, c4 = st.columns(2)

            # –ë–õ–û–ö 1: –¢–û–í–ê–†–´
            with c1:
                st.info(f"üß± –¢–æ–≤–∞—Ä—ã ({len(st.session_state.categorized_products)})")
                st.caption(", ".join(st.session_state.categorized_products))

            # –ë–õ–û–ö 2: –£–°–õ–£–ì–ò
            with c2:
                st.error(f"üõ†Ô∏è –£—Å–ª—É–≥–∏ ({len(st.session_state.categorized_services)})")
                st.caption(", ".join(st.session_state.categorized_services))
            
            # –ë–õ–û–ö 3: –ö–û–ú–ú–ï–†–¶–ò–Ø (–ì–ª–∞–≥–æ–ª—ã, –î–µ–Ω—å–≥–∏, –ì–æ—Ä–æ–¥–∞)
            with c3:
                st.warning(f"üí∞ –ö–æ–º–º–µ—Ä—Ü–∏—è / –ì–µ–æ / –û–±—â–µ–µ ({len(st.session_state.categorized_commercial)})")
                st.caption(", ".join(st.session_state.categorized_commercial))

            # –ë–õ–û–ö 4: –†–ê–ó–ú–ï–†–´ –ò –ú–ê–†–ö–ò
            with c4:
                dims = st.session_state.get('categorized_dimensions', [])
                st.success(f"üìè –†–∞–∑–º–µ—Ä—ã –∏ –º–∞—Ä–∫–∏ ({len(dims)})")
                st.caption(", ".join(dims))

        high = results.get('missing_semantics_high', [])
        low = results.get('missing_semantics_low', [])
        if high or low:
            with st.expander(f"üß© –£–ø—É—â–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ ({len(high)+len(low)})", expanded=False):
                if high: st.markdown(f"<div style='background:#EBF5FF;padding:10px;border-radius:5px;'><b>–í–∞–∂–Ω—ã–µ:</b> {', '.join([x['word'] for x in high])}</div>", unsafe_allow_html=True)
                if low: st.markdown(f"<div style='background:#F7FAFC;padding:10px;border-radius:5px;margin-top:5px;'><b>–î–æ–ø:</b> {', '.join([x['word'] for x in low])}</div>", unsafe_allow_html=True)

        render_paginated_table(results['depth'], "1. –ì–ª—É–±–∏–Ω–∞", "tbl_depth_1", default_sort_col="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è", use_abs_sort_default=True)
        render_paginated_table(results['hybrid'], "3. TF-IDF", "tbl_hybrid", default_sort_col="TF-IDF –¢–û–ü")
        render_paginated_table(results['relevance_top'], "4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", "tbl_rel", default_sort_col="–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)")

# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 2: AI
# ------------------------------------------
with tab_ai:
    st.title("AI –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä (Perplexity)")
    pplx_key = st.text_input("Perplexity API Key", type="password", key="pplx_key_input")
    target_url_gen = st.text_input("URL –°—Ç—Ä–∞–Ω–∏—Ü—ã (–¥–æ–Ω–æ—Ä —Ç–µ–≥–æ–≤)", key="pplx_url_input")

    if st.button("üöÄ –ù–∞—á–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é", key="btn_start_gen", disabled=not pplx_key):
        st.session_state.ai_generated_df = None
        if not openai: st.error("–ù–µ—Ç openai"); st.stop()
        client = openai.OpenAI(api_key=pplx_key, base_url="https://api.perplexity.ai")

        with st.status("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è...", expanded=True) as status:
            base_text, tags, err = get_page_data_for_gen(target_url_gen)
            if err or not tags: st.error(err or "–ù–µ—Ç —Ç–µ–≥–æ–≤"); st.stop()

            seo_list = [x['word'] for x in st.session_state.analysis_results.get('missing_semantics_high', []) if x['word'] not in GARBAGE_LATIN_STOPLIST][:15] if st.session_state.analysis_results else []

            all_rows = []
            bar = st.progress(0)
            for i, tag in enumerate(tags):
                blocks = generate_five_blocks(client, base_text, tag['name'], seo_list)
                all_rows.append({'TagName': tag['name'], 'URL': tag['url'], 'IP_PROP4839': blocks[0], 'IP_PROP4816': blocks[1], 'IP_PROP4838': blocks[2], 'IP_PROP4829': blocks[3], 'IP_PROP4831': blocks[4], **STATIC_DATA_GEN})
                bar.progress((i+1)/len(tags))

            df = pd.DataFrame(all_rows)
            st.session_state.ai_generated_df = df
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer: df.to_excel(writer, index=False)
            st.session_state.ai_excel_bytes = buffer.getvalue()
            st.rerun()

    if st.session_state.ai_generated_df is not None:
        st.download_button("üì• –°–∫–∞—á–∞—Ç—å Excel", st.session_state.ai_excel_bytes, "seo_texts.xlsx", "application/vnd.ms-excel")
        st.dataframe(st.session_state.ai_generated_df.head())

# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 3: –¢–ï–ì–ò (–û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø v3)
# ------------------------------------------
with tab_tags:
    st.title("üè∑Ô∏è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–ª–∏—Ç–∫–∏ —Ç–µ–≥–æ–≤")
    
    st.markdown("""
    **–†–µ–∂–∏–º: –£–º–Ω–∞—è –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∞ (SEO Matching)**
    –°–∫—Ä–∏–ø—Ç –±–µ—Ä–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ –ø–æ–ª—è –Ω–∏–∂–µ –∏ –∏—â–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –≤–∞—à–µ–º —Ñ–∞–π–ª–µ —Å—Å—ã–ª–æ–∫.
    """)

    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ —Å–ª–æ–≤ (–∏–∑ Session State –∏–ª–∏ –ø—É—Å—Ç–æ–π)
    auto_products = st.session_state.get('categorized_products', [])
    default_text_value = ", ".join(auto_products) if auto_products else ""

    # 2. –†–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º–æ–µ –ø–æ–ª–µ –≤–≤–æ–¥–∞ (–∫–∞–∫ –ø—Ä–æ—Å–∏–ª–∏ –Ω–∞ —Å–∫—Ä–∏–Ω–µ)
    st.markdown("### –°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–Ω—ã—Ö —Å–ª–æ–≤ (–º–æ–∂–Ω–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å)")
    tags_input_text = st.text_area(
        "–í–≤–µ–¥–∏—Ç–µ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
        value=default_text_value, 
        height=150, 
        key="tags_manual_input",
        help="–≠—Ç–∏ —Å–ª–æ–≤–∞ –±—É–¥—É—Ç –∏—Å–∫–∞—Ç—å—Å—è –≤ URL –∞–¥—Ä–µ—Å–∞—Ö —Ñ–∞–π–ª–∞."
    )

    # 3. –ó–∞–≥—Ä—É–∑—á–∏–∫ —Ñ–∞–π–ª–∞
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ (.txt)", type=["txt"], key="urls_uploader")
    
    with st.expander("–ö–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –Ω—É–∂–µ–Ω?", expanded=False):
        st.code("""https://site.ru/catalog/anod-mednyy/
https://site.ru/catalog/balka-bu/
https://site.ru/catalog/vtulki-rti/
... (–∫–∞–∂–¥–∞—è —Å—Å—ã–ª–∫–∞ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)""", language="text")

    if st.button("üöÄ –ù–∞–π—Ç–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞—Ç—å –ø–ª–∏—Ç–∫—É", key="btn_match_tags_txt"):
        # –ü–∞—Ä—Å–∏–º —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è
        if not tags_input_text.strip():
            st.warning("–°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –ø—É—Å—Ç.")
            st.stop()
            
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º –∏ —á–∏—Å—Ç–∏–º
        products_to_process = [x.strip() for x in tags_input_text.split(',') if x.strip()]

        if not uploaded_file:
            st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å–æ —Å—Å—ã–ª–∫–∞–º–∏.")
            st.stop()

        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —Å—Å—ã–ª–æ–∫
        stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
        all_urls = [line.strip() for line in stringio.readlines() if line.strip()]

        if not all_urls:
            st.error("–§–∞–π–ª —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –ø—É—Å—Ç.")
            st.stop()

        st.success(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ª–æ–≤: {len(products_to_process)} | –°—Å—ã–ª–æ–∫ –≤ —Ñ–∞–π–ª–µ: {len(all_urls)}")

        # 4. –õ–û–ì–ò–ö–ê –°–û–ü–û–°–¢–ê–í–õ–ï–ù–ò–Ø
        matched_tags = []
        progress_bar = st.progress(0)
        
        for idx, word in enumerate(products_to_process):
            translit_word = transliterate_text(word)
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –º—É—Å–æ—Ä–∞
            if len(translit_word) < 2: 
                continue 

            candidates = []
            
            for url in all_urls:
                # --- –õ–û–ì–ò–ö–ê "–ü–û–°–õ–ï catalog/" ---
                url_lower = url.lower()
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å /catalog/, –∏—â–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –Ω–µ–≥–æ
                if '/catalog/' in url_lower:
                    search_scope = url_lower.split('/catalog/', 1)[1]
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç catalog, –±–µ—Ä–µ–º –ø—É—Ç—å (path) –±–µ–∑ –¥–æ–º–µ–Ω–∞
                    try:
                        parsed = urlparse(url_lower)
                        search_scope = parsed.path
                    except:
                        search_scope = url_lower # –§–æ–ª–±—ç–∫

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–ª–∏—Ç–∞ –≤ –æ–±—Ä–µ–∑–∞–Ω–Ω—É—é —á–∞—Å—Ç—å URL
                if translit_word in search_scope:
                    candidates.append(url)
            
            # --- –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–ê ---
            if candidates:
                # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Å—Å—ã–ª–∫–∏ - –±–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—É—é
                best_match = random.choice(candidates)
                matched_tags.append({
                    'name': word.capitalize(),
                    'url': best_match,
                    'has_link': True
                })
            else:
                # –ï—Å–ª–∏ —Å—Å—ã–ª–æ–∫ –Ω–µ –Ω–∞—à–ª–∏ - –≤—Å–µ —Ä–∞–≤–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–æ, –Ω–æ –±–µ–∑ —Å—Å—ã–ª–∫–∏
                matched_tags.append({
                    'name': word.capitalize(),
                    'url': '#',
                    'has_link': False
                })
            
            progress_bar.progress((idx + 1) / len(products_to_process))
        
        progress_bar.empty()

        # 5. –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–ê (HTML)
        st.subheader(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Ç–µ–≥–æ–≤: {len(matched_tags)}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º HTML
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∞ -> <a>, –µ—Å–ª–∏ –Ω–µ—Ç -> <span> (–≤–∏–∑—É–∞–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏–π, –Ω–æ –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–π)
        html_parts = []
        for item in matched_tags:
            if item['has_link']:
                html_parts.append(f'    <a href="{item["url"]}" class="tag-link">{item["name"]}</a>')
            else:
                # –°–ø–∞–Ω –¥–ª—è —Ç–µ–≥–æ–≤ –±–µ–∑ —Å—Å—ã–ª–æ–∫
                html_parts.append(f'    <span class="tag-link tag-empty">{item["name"]}</span>')

        html_output = '<div class="popular-tags">\n' + "\n".join(html_parts) + '\n</div>'
        
        st.session_state.tags_html_result = html_output
        st.rerun()

    # –ë–ª–æ–∫ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    if st.session_state.tags_html_result:
        t_view, t_code = st.tabs(["üëÅÔ∏è –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä", "üíª HTML –ö–æ–¥"])
        
        with t_view:
            st.markdown(st.session_state.tags_html_result, unsafe_allow_html=True)
            
        with t_code:
            st.text_area("–°–∫–æ–ø–∏—Ä—É–π—Ç–µ HTML:", st.session_state.tags_html_result, height=300)
        
        if st.button("–°–±—Ä–æ—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞", key="reset_tags_txt"):
            st.session_state.tags_html_result = None
            st.rerun()

# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 4: –¢–ê–ë–õ–ò–¶–´
# ------------------------------------------
with tab_tables:
    st.title("üß© –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–∞–±–ª–∏—Ü")
    pplx_key_tbl = st.text_input("Perplexity API Key", type="password", key="pplx_key_tbl")
    prompt_tbl = st.text_area("–û–ø–∏—Å–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã")

    # –ê–≤—Ç–æ-–≤—ã–±–æ—Ä —Ç–æ–ø-4 —Å–ª–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
    top_missing = []
    if st.session_state.analysis_results:
        df = st.session_state.analysis_results['depth']
        mask = df['–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è'].astype(str).str.startswith('+')
        df_miss = df[mask].copy()
        df_miss['val'] = df_miss['–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è'].apply(lambda x: int(str(x).replace('+','')))
        top_missing = [{'word': r['–°–ª–æ–≤–æ'], 'count': r['val']} for _, r in df_miss.sort_values('val', ascending=False).head(4).iterrows()]

    if top_missing:
        st.info(f"–°–ª–æ–≤–∞ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è: {', '.join([x['word'] for x in top_missing])}")

    if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å", key="btn_gen_tbl"):
        if not openai: st.error("–ù–µ—Ç openai"); st.stop()
        client = openai.OpenAI(api_key=pplx_key_tbl, base_url="https://api.perplexity.ai")
        with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è..."):
            res = generate_html_table(client, prompt_tbl, top_missing)
            st.session_state.table_html_result = res
            st.rerun()

    if st.session_state.table_html_result:
        t1, t2 = st.tabs(["üëÅÔ∏è View", "üíª Code"])
        with t1: st.markdown(st.session_state.table_html_result, unsafe_allow_html=True)
        with t2: st.code(st.session_state.table_html_result, language='html')








