import streamlit as st
import pymorphy3 as pymorphy2
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin, unquote
import inspect
import time
import json
import io
import csv
from google import genai
import os
import requests
proxy_url = "http://QYnojH:Uekp4k@196.18.3.35:8000" 

os.environ["http_proxy"] = proxy_url
os.environ["https_proxy"] = proxy_url

try:
    my_ip = requests.get("https://api.ipify.org", timeout=5).text
    st.info(f"üïµÔ∏è –í–ê–® IP –î–õ–Ø –°–ö–†–ò–ü–¢–ê: {my_ip}")
except Exception as e:
    st.error(f"‚ùå –ü—Ä–æ–∫—Å–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {e}")
    
import random
import streamlit.components.v1 as components
import copy
import plotly.graph_objects as go
import pickle
import datetime
# ==========================================
# –Ø–î–†–û –ë–ê–ó–´ –î–ê–ù–ù–´–• (–ö–≠–®–ò–†–û–í–ê–ù–ò–ï SEO-–ê–ù–ê–õ–ò–ó–ê –ù–ê 90 –î–ù–ï–ô)
# ==========================================
import sqlite3
import json
import datetime

def init_seo_db():
    conn = sqlite3.connect('seo_cache.db', timeout=10)
    c = conn.cursor()
    c.execute('''
        CREATE TABLE IF NOT EXISTS seo_analysis (
            query TEXT PRIMARY KEY,
            timestamp TEXT,
            parsed_data TEXT
        )
    ''')
    conn.commit()
    conn.close()

init_seo_db()

def get_cached_analysis(query):
    if not query: return None
    try:
        conn = sqlite3.connect('seo_cache.db', timeout=10)
        c = conn.cursor()
        
        # –ê–≤—Ç–æ-—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—å—è (90 –¥–Ω–µ–π)
        expiry_date = (datetime.datetime.now() - datetime.timedelta(days=90)).strftime("%Y-%m-%d %H:%M:%S")
        c.execute('DELETE FROM seo_analysis WHERE timestamp < ?', (expiry_date,))
        conn.commit()
        
        c.execute('SELECT timestamp, parsed_data FROM seo_analysis WHERE query = ?', (query.lower().strip(),))
        row = c.fetchone()
        conn.close()
        
        if row:
            return json.loads(row[1])
    except sqlite3.OperationalError:
        return None 
    return None

def save_cached_analysis(query, data_for_graph):
    try:
        conn = sqlite3.connect('seo_cache.db', timeout=10)
        c = conn.cursor()
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        c.execute('''
            INSERT OR REPLACE INTO seo_analysis (query, timestamp, parsed_data)
            VALUES (?, ?, ?)
        ''', (query.lower().strip(), timestamp, json.dumps(data_for_graph)))
        conn.commit()
        conn.close()
    except: pass

# ==========================================
# –î–í–ò–ñ–û–ö –ì–ï–ù–ï–†–ê–¶–ò–ò –û–¢–ó–´–í–û–í (–ë–ï–ó –ò–ò)
# ==========================================
import pymorphy3
import random
import re
import pandas as pd
import datetime

@st.cache_resource
def init_morph():
    return pymorphy3.MorphAnalyzer()

morph = init_morph()

LSI_BRIDGES = [
    {"template": "–û—Ç–¥–µ–ª—å–Ω–æ —Ö–æ—á—É –æ—Ç–º–µ—Ç–∏—Ç—å **{}**.", "case": "accs"},
    {"template": "–¢–∞–∫–∂–µ –ø–æ—Ä–∞–¥–æ–≤–∞–ª–æ –Ω–∞–ª–∏—á–∏–µ **{}**.", "case": "gent"},
    {"template": "–û–±—Ä–∞—Ç–∏–ª–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ **{}** ‚Äì –≤—Å—ë –æ—Ç–ª–∏—á–Ω–æ.", "case": "accs"},
    {"template": "–ö—Å—Ç–∞—Ç–∏, —Å **{}** —Ç–æ–∂–µ –Ω–∏–∫–∞–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –≤–æ–∑–Ω–∏–∫–ª–æ.", "case": "ablt"},
    {"template": "–ö —Å–ª–æ–≤—É, **{}** —Ç—É—Ç –Ω–∞ –≤—ã—Å—à–µ–º —É—Ä–æ–≤–Ω–µ.", "case": "nomn"}
]

def inflect_lsi_phrase(phrase, target_case):
    words = str(phrase).split()
    inflected_words = []
    for word in words:
        parsed_word = morph.parse(word)[0]
        try:
            inf_word = parsed_word.inflect({target_case})
            if inf_word:
                inflected_words.append(inf_word.word)
            else:
                inflected_words.append(word)
        except AttributeError:
            inflected_words.append(word)
    return " ".join(inflected_words)

def generate_random_date():
    start_date = datetime.datetime(2026, 1, 1)
    end_date = datetime.datetime(2026, 2, 10)
    random_days = random.randrange((end_date - start_date).days + 1)
    return (start_date + datetime.timedelta(days=random_days)).strftime("%d.%m.%Y")

def build_review_from_repo(template, variables_dict, repo_fio, lsi_words):
    def replace_var(match):
        var_name = match.group(1).strip()
        if var_name == "–¥–∞—Ç–∞":
            return generate_random_date()
        if var_name in variables_dict:
            return str(random.choice(variables_dict[var_name])).strip()
        return match.group(0)

    draft = re.sub(r'\{([^}]+)\}', replace_var, str(template))
    
    forbidden_roots = [
        "—É–∫—Ä–∞–∏–Ω", "ukrain", "ua", "–≤—Å—É", "–∑—Å—É", "–∞—Ç–æ", "—Å–≤–æ", "–≤–æ–π–Ω",
        "–∫–∏–µ–≤", "–ª—å–≤–æ–≤", "—Ö–∞—Ä—å–∫–æ–≤", "–æ–¥–µ—Å—Å", "–¥–Ω–µ–ø—Ä", "–º–∞—Ä–∏—É–ø–æ–ª—å",
        "–¥–æ–Ω–µ—Ü", "–ª—É–≥–∞–Ω—Å", "–¥–Ω—Ä", "–ª–Ω—Ä", "–¥–æ–Ω–±–∞—Å—Å", "–º–µ–ª–∏—Ç–æ–ø–æ–ª—å",
        "–±–µ—Ä–¥—è–Ω—Å–∫", "–±–∞—Ö–º—É—Ç", "–∑–∞–ø–æ—Ä–æ–∂—å", "—Ö–µ—Ä—Å–æ–Ω", "–∫—Ä—ã–º",
        "–ø–æ–ª–∏—Ç–∏–∫", "—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü"
    ]
    clean_lsi = [w for w in lsi_words if not any(root in str(w).lower() for root in forbidden_roots) and len(str(w)) > 2]
    
    used_lsi = []
    if clean_lsi:
        lsi_word = random.choice(clean_lsi)
        bridge = random.choice(LSI_BRIDGES)
        inflected_lsi = inflect_lsi_phrase(lsi_word, bridge["case"])
        lsi_sentence = bridge["template"].format(inflected_lsi)
        
        sentences = [s.strip() for s in draft.split('.') if s.strip()]
        insert_pos = random.randint(1, max(1, len(sentences)))
        sentences.insert(insert_pos, lsi_sentence)
        draft = ". ".join(sentences) + "."
        used_lsi.append(inflected_lsi)

    draft = re.sub(r'\s+', ' ', draft)
    draft = draft.replace(' .', '.').replace(' ,', ',').replace(' - ', ' ‚Äì ')
    sentences = draft.split('. ')
    draft = '. '.join([s.capitalize() for s in sentences]).strip()
    
    # –°–±–æ—Ä–∫–∞ –§–ò–û
    random_name = "–ê–Ω–æ–Ω–∏–º"
    available_genders = [g for g in ['MALE', 'FEMALE'] if repo_fio[g]['names'] and repo_fio[g]['surnames']]
    if available_genders:
        chosen_gender = random.choice(available_genders)
        rand_name = random.choice(repo_fio[chosen_gender]['names'])
        rand_surname = random.choice(repo_fio[chosen_gender]['surnames'])
        
        if repo_fio[chosen_gender]['patronymics'] and random.random() > 0.5:
            rand_patronymic = random.choice(repo_fio[chosen_gender]['patronymics'])
            random_name = f"{rand_name} {rand_patronymic} {rand_surname}"
        else:
            random_name = f"{rand_name} {rand_surname}"

    return random_name, draft, used_lsi
# ==========================================

# ==========================================
# FIX FOR PYTHON 3.11+
# ==========================================
if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

try:
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º pymorphy3, –Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞–∫ pymorphy2, 
    # —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å –≤–µ—Å—å –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥
    import pymorphy3 as pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except ImportError:
    # –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ —Å—Ç–æ–∏—Ç —Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è
    try:
        import pymorphy2
        morph = pymorphy2.MorphAnalyzer()
        USE_NLP = True
    except Exception as e:
        st.error(f"‚ùå –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å pymorphy. –î–µ—Ç–∞–ª–∏: {e}")
        morph = None
        USE_NLP = False

try:
    import openai
except ImportError:
    openai = None

try:
    from google import genai
except ImportError:
    genai = None


# ... (—Ç—É—Ç –∏–¥—É—Ç –∏–º–ø–æ—Ä—Ç—ã) ...
import datetime

# === –í–°–¢–ê–í–ò–¢–¨ –°–Æ–î–ê (–°–¢–†–û–ö–ê ~40) ===
if 'SUPER_GLOBAL_KEY' not in st.session_state:
    st.session_state.SUPER_GLOBAL_KEY = ""
    # –ü—Ä–æ–±—É–µ–º –ø–æ–¥—Ç—è–Ω—É—Ç—å –∏–∑ secrets —Å—Ä–∞–∑—É –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    try: st.session_state.SUPER_GLOBAL_KEY = st.secrets["GEMINI_KEY"]
    except: pass
# ==================================

# ==========================================
# 0. –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ==========================================

def transliterate_text(text):
    mapping = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }
    result = []
    for char in text.lower():
        if char in mapping:
            result.append(mapping[char])
        elif char.isalnum() or char == '-':
            result.append(char)
    return "".join(result)

def get_h1_from_url(url):
    """–ü–∞—Ä—Å–∏—Ç H1 —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–∫—Å–∏, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –≥–ª–æ–±–∞–ª—å–Ω–æ –≤ –≤–∞—à–µ–º —Å–∫—Ä–∏–ø—Ç–µ
        r = requests.get(url, headers=headers, timeout=10)
        if r.status_code == 200:
            soup = BeautifulSoup(r.text, 'html.parser')
            h1 = soup.find('h1')
            if h1:
                return h1.get_text(strip=True)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ H1: {e}")
    return ""

def force_cyrillic_name_global(slug_text):
    raw = unquote(slug_text).lower()
    raw = raw.replace('.html', '').replace('.php', '')
    if re.search(r'[–∞-—è]', raw):
        return raw.replace('-', ' ').replace('_', ' ').capitalize()

    words = re.split(r'[-_]', raw)
    rus_words = []
    
    exact_map = {
        'nikel': '–Ω–∏–∫–µ–ª—å', 'stal': '—Å—Ç–∞–ª—å', 'med': '–º–µ–¥—å', 'latun': '–ª–∞—Ç—É–Ω—å',
        'bronza': '–±—Ä–æ–Ω–∑–∞', 'svinec': '—Å–≤–∏–Ω–µ—Ü', 'titan': '—Ç–∏—Ç–∞–Ω', 'tsink': '—Ü–∏–Ω–∫',
        'dural': '–¥—é—Ä–∞–ª—å', 'dyural': '–¥—é—Ä–∞–ª—å', 'chugun': '—á—É–≥—É–Ω',
        'alyuminiy': '–∞–ª—é–º–∏–Ω–∏–π', 'al': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è', 'alyuminievaya': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è',
        'nerzhaveyushchiy': '–Ω–µ—Ä–∂–∞–≤–µ—é—â–∏–π', 'nerzhaveyka': '–Ω–µ—Ä–∂–∞–≤–µ–π–∫–∞',
        'profil': '–ø—Ä–æ—Ñ–∏–ª—å', 'shveller': '—à–≤–µ–ª–ª–µ—Ä', 'ugolok': '—É–≥–æ–ª–æ–∫',
        'polosa': '–ø–æ–ª–æ—Å–∞', 'krug': '–∫—Ä—É–≥', 'kvadrat': '–∫–≤–∞–¥—Ä–∞—Ç',
        'list': '–ª–∏—Å—Ç', 'truba': '—Ç—Ä—É–±–∞', 'setka': '—Å–µ—Ç–∫–∞',
        'provoloka': '–ø—Ä–æ–≤–æ–ª–æ–∫–∞', 'armatura': '–∞—Ä–º–∞—Ç—É—Ä–∞', 'balka': '–±–∞–ª–∫–∞',
        'katanka': '–∫–∞—Ç–∞–Ω–∫–∞', 'otvod': '–æ—Ç–≤–æ–¥', 'perehod': '–ø–µ—Ä–µ—Ö–æ–¥',
        'flanec': '—Ñ–ª–∞–Ω–µ—Ü', 'zaglushka': '–∑–∞–≥–ª—É—à–∫–∞', 'metiz': '–º–µ—Ç–∏–∑—ã',
        'profnastil': '–ø—Ä–æ—Ñ–Ω–∞—Å—Ç–∏–ª', 'shtrips': '—à—Ç—Ä–∏–ø—Å', 'lenta': '–ª–µ–Ω—Ç–∞',
        'shina': '—à–∏–Ω–∞', 'prutok': '–ø—Ä—É—Ç–æ–∫', 'shestigrannik': '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫',
        'vtulka': '–≤—Ç—É–ª–∫–∞', 'kabel': '–∫–∞–±–µ–ª—å', 'panel': '–ø–∞–Ω–µ–ª—å',
        'detal': '–¥–µ—Ç–∞–ª—å', 'set': '—Å–µ—Ç—å', 'cep': '—Ü–µ–ø—å', 'svyaz': '—Å–≤—è–∑—å',
        'rezba': '—Ä–µ–∑—å–±–∞', 'gost': '–ì–û–°–¢',
        'polipropilenovye': '–ø–æ–ª–∏–ø—Ä–æ–ø–∏–ª–µ–Ω–æ–≤—ã–µ', 'truby': '—Ç—Ä—É–±—ã',
        'ocinkovannaya': '–æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω–∞—è', 'riflenyy': '—Ä–∏—Ñ–ª–µ–Ω—ã–π'
    }

    for w in words:
        if not w: continue
        if w in exact_map:
            rus_words.append(exact_map[w])
            continue
        
        processed_w = w
        if processed_w.endswith('yy'): processed_w = processed_w[:-2] + '—ã–π'
        elif processed_w.endswith('iy'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('ij'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('yi'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('aya'): processed_w = processed_w[:-3] + '–∞—è'
        elif processed_w.endswith('oye'): processed_w = processed_w[:-3] + '–æ–µ'
        elif processed_w.endswith('ye'): processed_w = processed_w[:-2] + '—ã–µ'

        replacements = [
            ('shch', '—â'), ('sch', '—â'), ('yo', '—ë'), ('zh', '–∂'), ('ch', '—á'), ('sh', '—à'), 
            ('yu', '—é'), ('ya', '—è'), ('kh', '—Ö'), ('ts', '—Ü'), ('ph', '—Ñ'),
            ('a', '–∞'), ('b', '–±'), ('v', '–≤'), ('g', '–≥'), ('d', '–¥'), ('e', '–µ'), 
            ('z', '–∑'), ('i', '–∏'), ('j', '–π'), ('k', '–∫'), ('l', '–ª'), ('m', '–º'), 
            ('n', '–Ω'), ('o', '–æ'), ('p', '–ø'), ('r', '—Ä'), ('s', '—Å'), ('t', '—Ç'), 
            ('u', '—É'), ('f', '—Ñ'), ('h', '—Ö'), ('c', '–∫'), ('w', '–≤'), ('y', '—ã'), ('x', '–∫—Å')
        ]
        
        temp_res = processed_w
        for eng, rus in replacements:
            temp_res = temp_res.replace(eng, rus)
        
        rus_words.append(temp_res)

    draft_phrase = " ".join(rus_words)
    draft_phrase = draft_phrase.replace('–ø—Ä–æ—Ñ–∏–ª', '–ø—Ä–æ—Ñ–∏–ª—å').replace('–ø—Ä–æ—Ñ–∏–ª—å–Ω', '–ø—Ä–æ—Ñ–∏–ª—å–Ω')
    draft_phrase = draft_phrase.replace('–µ–ª–Ω—ã–π', '–µ–ª—å–Ω—ã–π').replace('–∞–ª–Ω—ã–π', '–∞–ª—å–Ω—ã–π')
    draft_phrase = draft_phrase.replace('–µ–ª–Ω–∞—è', '–µ–ª—å–Ω–∞—è').replace('–∞–ª–Ω–∞—è', '–∞–ª—å–Ω–∞—è')
    draft_phrase = draft_phrase.replace('—Å—Ç–∞–ª–Ω', '—Å—Ç–∞–ª—å–Ω').replace('–º–µ–¥—å–Ω', '–º–µ–¥–Ω')
    draft_phrase = draft_phrase.replace('–π–∞', '—è').replace('–π–æ', '—ë')

    return draft_phrase.capitalize()

def get_breadcrumb_only(url, ua_settings="Mozilla/5.0"):
    try:
        session = requests.Session()
        retry = Retry(connect=3, read=3, redirect=3, backoff_factor=0.5)
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        
        headers = {'User-Agent': ua_settings}
        r = session.get(url, headers=headers, timeout=25)
        if r.status_code != 200: 
            return None
        
        soup = BeautifulSoup(r.text, 'html.parser')
        
        breadcrumbs = soup.find(class_=re.compile(r'breadcrumb|breadcrumbs|nav-path|nav-chain|bx-breadcrumb', re.I))
        if not breadcrumbs:
            breadcrumbs = soup.find(id=re.compile(r'breadcrumb|breadcrumbs|nav-path', re.I))

        if breadcrumbs:
            full_text = breadcrumbs.get_text(separator='|||', strip=True)
            parts = [p.strip() for p in full_text.split('|||') if p.strip()]
            clean_parts = [p for p in parts if p not in ['/', '\\', '>', '¬ª', '‚Ä¢', '-', '|']]
            
            if clean_parts:
                last_item = clean_parts[-1]
                if len(last_item) > 2 and last_item.lower() != "–≥–ª–∞–≤–Ω–∞—è":
                    return last_item
    except:
        return None
    return None

def render_clean_block(title, icon, words_list):
    unique_words = sorted(list(set(words_list))) if words_list else []
    count = len(unique_words)
    
    if count > 0:
        content_html = ", ".join(unique_words)
        # –ö–∞—Ä—Ç–æ—á–∫–∞ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è
        html_code = f"""
        <details class="details-card">
            <summary class="card-summary">
                <div>
                    <span class="arrow-icon">‚ñ∂</span>
                    {icon} {title}
                </div>
                <span class="count-tag">{count}</span>
            </summary>
            <div class="card-content">
                {content_html}
            </div>
        </details>
        """
    else:
        # –ï—Å–ª–∏ –ø—É—Å—Ç–æ - –∫–∞—Ä—Ç–æ—á–∫–∞ –Ω–µ–∞–∫—Ç–∏–≤–Ω–∞ (–±–µ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
        html_code = f"""
        <div class="details-card">
            <div class="card-summary" style="cursor: default; color: #9ca3af;">
                <div>{icon} {title}</div>
                <span class="count-tag">0</span>
            </div>
        </div>
        """
    
    st.markdown(html_code, unsafe_allow_html=True)

def render_relevance_chart(df_rel, unique_key="default"):
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ None, —á—Ç–æ–±—ã –≥—Ä–∞—Ñ–∏–∫ –Ω–µ –ø–∞–¥–∞–ª –ø—Ä–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    if df_rel is None or (isinstance(df_rel, pd.DataFrame) and df_rel.empty):
        return

    # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø—É—Å—Ç—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    if '–ü–æ–∑–∏—Ü–∏—è' not in df_rel.columns:
        return
    

    # 1. –ñ–ï–°–¢–ö–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ > 0
    # –í–∞—à —Å–∞–π—Ç (–ø–æ–∑–∏—Ü–∏—è 0) —É–¥–∞–ª—è–µ—Ç—Å—è –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
    df = df_rel[df_rel['–ü–æ–∑–∏—Ü–∏—è'] > 0].copy()
    
    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø—É—Å—Ç–∞ - –≤—ã—Ö–æ–¥–∏–º
    if df.empty:
        return

    df = df.sort_values(by='–ü–æ–∑–∏—Ü–∏—è')
    x_indices = np.arange(len(df))
    
    tick_links = []
    
    for _, row in df.iterrows():
        # –ß–∏—Å—Ç–∏–º –∏–º—è –¥–æ–º–µ–Ω–∞
        raw_name = row['–î–æ–º–µ–Ω'].replace(' (–í—ã)', '').strip()
        clean_domain = raw_name.replace('www.', '').split('/')[0]
        
        # –§–æ—Ä–º–∞—Ç: "1. site.ru" (–±–µ–∑ #)
        label_text = f"{row['–ü–æ–∑–∏—Ü–∏—è']}. {clean_domain}"
        
        # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å, —Ç–∞–∫ –∫–∞–∫ —à—Ä–∏—Ñ—Ç —Ç–µ–ø–µ—Ä—å –∫—Ä—É–ø–Ω–µ–µ
        if len(label_text) > 25: label_text = label_text[:23] + ".."
        
        url_target = row.get('URL', f"https://{raw_name}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º CSS-–∫–ª–∞—Å—Å .chart-link –≤–º–µ—Å—Ç–æ style="..." –¥–ª—è —Ä–∞–±–æ—Ç—ã hover
        link_html = f"<a href='{url_target}' target='_blank' class='chart-link'>{label_text}</a>"
        tick_links.append(link_html)

    # –ú–µ—Ç—Ä–∏–∫–∏
    df['Total_Rel'] = (df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] + df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)']) / 2
    
    # –¢—Ä–µ–Ω–¥
    z = np.polyfit(x_indices, df['Total_Rel'], 1)
    p = np.poly1d(z)
    df['Trend'] = p(x_indices)

    # 2. –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
    fig = go.Figure()

    # --- –ü–ê–õ–ò–¢–†–ê (Premium) ---
    COLOR_MAIN = '#4F46E5'  # –ò–Ω–¥–∏–≥–æ
    COLOR_WIDTH = '#0EA5E9' # –ì–æ–ª—É–±–æ–π
    COLOR_DEPTH = '#E11D48' # –ú–∞–ª–∏–Ω–æ–≤—ã–π
    COLOR_TREND = '#15803d' # –ó–µ–ª–µ–Ω—ã–π (Forest Green)

    COMMON_CONFIG = dict(
        mode='lines+markers',
        line=dict(width=3, shape='spline'), 
        marker=dict(size=8, line=dict(width=2, color='white'), symbol='circle')
    )

    # 1. –û–ë–©–ê–Ø
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['Total_Rel'],
        name='–û–±—â–∞—è',
        line=dict(color=COLOR_MAIN, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_MAIN, **COMMON_CONFIG['marker']),
        mode='lines+markers'
    ))

    # 2. –®–ò–†–ò–ù–ê
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'],
        name='–®–∏—Ä–∏–Ω–∞',
        line=dict(color=COLOR_WIDTH, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_WIDTH, **COMMON_CONFIG['marker']),
        mode='lines+markers'
    ))

    # 3. –ì–õ–£–ë–ò–ù–ê
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'],
        name='–ì–ª—É–±–∏–Ω–∞',
        line=dict(color=COLOR_DEPTH, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_DEPTH, **COMMON_CONFIG['marker']),
        mode='lines+markers'
    ))

    # 4. –¢–†–ï–ù–î
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['Trend'],
        name='–¢—Ä–µ–Ω–¥',
        line=dict(color=COLOR_TREND, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_TREND, **COMMON_CONFIG['marker']),
        mode='lines+markers',
        opacity=0.8
    ))

# 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Layout (–ö–û–ú–ü–ê–ö–¢–ù–ê–Ø –í–ï–†–°–ò–Ø)
    fig.update_layout(
        template="plotly_white",
        legend=dict(
            orientation="h",
            yanchor="bottom", y=1.02, # –õ–µ–≥–µ–Ω–¥–∞ –ø—Ä—è–º–æ –Ω–∞–¥ –≥—Ä–∞—Ñ–∏–∫–æ–º
            xanchor="center", x=0.5,
            font=dict(size=12, color="#111827", family="Inter, sans-serif")
        ),
        xaxis=dict(
            showgrid=True,
            gridcolor='#F3F4F6',
            linecolor='#E5E7EB',
            tickmode='array',
            tickvals=x_indices,
            ticktext=tick_links, 
            
            tickfont=dict(size=11), # –ß—É—Ç—å –º–µ–Ω—å—à–µ —à—Ä–∏—Ñ—Ç –ø–æ–¥–ø–∏—Å–µ–π
            tickangle=-45, 
            
            fixedrange=True,
            dtick=1, 
            range=[-0.5, len(df) - 0.5], 
            automargin=False 
        ),
        yaxis=dict(
            range=[0, 115], 
            showgrid=True, 
            gridcolor='#F3F4F6', 
            gridwidth=1,
            zeroline=False,
            fixedrange=True
        ),
        # === –í–û–¢ –¢–£–¢ –ú–ï–ù–Ø–ï–ú –†–ê–ó–ú–ï–†–´ ===
        # l/r - –±–æ–∫–∞, t - –≤–µ—Ä—Ö, b - –Ω–∏–∑ (–ø–æ–¥ –ø–æ–¥–ø–∏—Å–∏)
        margin=dict(l=10, r=10, t=30, b=110),
        
        hovermode="x unified",
        
        # –û–±—â–∞—è –≤—ã—Å–æ—Ç–∞ –≥—Ä–∞—Ñ–∏–∫–∞ (–±—ã–ª–æ 550)
        height=400 
    )
    
    # use_container_width=True —Ä–∞—Å—Ç—è–≥–∏–≤–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –Ω–∞ –≤—Å—é —à–∏—Ä–∏–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    st.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False}, key=f"rel_chart_{unique_key}")

def analyze_serp_anomalies(df_rel):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–í–µ—Ä—Å–∏—è v5 - Robust).
    –ü–æ—Ä–æ–≥: 75% –æ—Ç –ª–∏–¥–µ—Ä–∞. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∏–ø–∏–∑–∞—Ü–∏—è.
    """
    if df_rel.empty:
        return [], [], {"type": "none", "msg": ""}

    # –ò—Å–∫–ª—é—á–∞–µ–º "–í–∞—à —Å–∞–π—Ç" –∏–∑ —Ä–∞—Å—á–µ—Ç–æ–≤ —ç—Ç–∞–ª–æ–Ω–∞
    df = df_rel[~df_rel['–î–æ–º–µ–Ω'].str.contains("\(–í—ã\)", na=False)].copy()
    
    if df.empty:
        return [], [], {"type": "none", "msg": ""}

    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –¥–µ–ª–∞–µ–º —á–∏—Å–ª–∞–º–∏ (–∑–∞—â–∏—Ç–∞ –æ—Ç —Å–±–æ–µ–≤)
    df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] = pd.to_numeric(df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'], errors='coerce').fillna(0)
    df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'] = pd.to_numeric(df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'], errors='coerce').fillna(0)

    # –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª
    df['Total'] = (df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] + df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)']) / 2
    
    # 1. –ò–©–ï–ú –õ–ò–î–ï–†–ê
    max_score = df['Total'].max()
    if max_score < 1: max_score = 1 # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
    
    # 2. –ñ–ï–°–¢–ö–ò–ô –ü–û–†–û–ì: 75% –æ—Ç –ª–∏–¥–µ—Ä–∞.
    # –ï—Å–ª–∏ –õ–∏–¥–µ—Ä=100, –ø–æ—Ä–æ–≥=75. –í—Å–µ —á—Ç–æ < 75 - —É–¥–∞–ª—è–µ–º.
    threshold = max(max_score * 0.75, 40) 
    
    anomalies = []
    normal_urls = []
    
    debug_counts = 0
    
    for _, row in df.iterrows():
        # –î–æ—Å—Ç–∞–µ–º —Å—Å—ã–ª–∫—É. –ó–∞—â–∏—Ç–∞ –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤.
        current_url = str(row.get('URL', '')).strip()
        if not current_url or current_url.lower() == 'nan':
             current_url = f"https://{row['–î–æ–º–µ–Ω']}" 

        score = row['Total']
        
        # –ê–ù–ê–õ–ò–ó
        if score < threshold:
            reason = f"–°–∫–æ—Ä {int(score)} < {int(threshold)} (–õ–∏–¥–µ—Ä {int(max_score)})"
            anomalies.append({'url': current_url, 'reason': reason, 'score': score})
            debug_counts += 1
        else:
            normal_urls.append(current_url)

    # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—è–º–∏
    if anomalies:
        st.toast(f"üóëÔ∏è –§–∏–ª—å—Ç—Ä (–õ–∏–¥–µ—Ä {int(max_score)} / –ü–æ—Ä–æ–≥ {int(threshold)}). –ò—Å–∫–ª—é—á–µ–Ω–æ: {len(anomalies)}", icon="‚ö†Ô∏è")
    else:
        # –ï—Å–ª–∏ –Ω–∏–∫–æ–≥–æ –Ω–µ –∏—Å–∫–ª—é—á–∏–ª–∏, –ø–∏—à–µ–º –ø–æ—á–µ–º—É
        st.toast(f"‚úÖ –í—Å–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –æ–∫. (–õ–∏–¥–µ—Ä {int(max_score)} / –ü–æ—Ä–æ–≥ {int(threshold)}). –ú–∏–Ω. –±–∞–ª–ª: {int(df['Total'].min())}", icon="‚ÑπÔ∏è")
    
    # –¢—Ä–µ–Ω–¥
    x = np.arange(len(df)); y = df['Total'].values
    slope = np.polyfit(x, y, 1)[0] if len(x) > 1 else 0
    trend_msg = "üìâ –ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–æ–ø" if slope < -1 else ("üìà –ü–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç—ã–π —Ç–æ–ø" if slope > 1 else "‚û°Ô∏è –†–æ–≤–Ω—ã–π —Ç–æ–ø")

    return normal_urls, anomalies, {"type": "info", "msg": trend_msg}

@st.cache_data
def load_lemmatized_dictionaries():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_path = os.path.join(script_dir, "data")
    
    # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞
    sets = {
        "products": set(),
        "commercial": set(),
        "specs": set(),
        "geo": set(),
        "services": set(),
        "sensitive": set()
    }

    # –ö–∞—Ä—Ç–∞ —Ñ–∞–π–ª–æ–≤
    files_map = {
        "metal_products.json": "products",
        "commercial_triggers.json": "commercial",
        "geo_locations.json": "geo",
        "services_triggers.json": "services",
        "tech_specs.json": "specs",
        "SENSITIVE_STOPLIST.json": "sensitive"
    }

    for filename, set_key in files_map.items():
        full_path = os.path.join(base_path, filename)
        if not os.path.exists(full_path):
            continue
        
        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                data = json.load(f) 
                
                words_bucket = []
                if isinstance(data, dict):
                    for cat_list in data.values():
                        words_bucket.extend(cat_list)
                elif isinstance(data, list):
                    words_bucket = data
                
                for phrase in words_bucket:
                    w_clean = str(phrase).lower().strip().replace('—ë', '–µ')
                    if not w_clean: continue
                    sets[set_key].add(w_clean)
                    if morph:
                        normal_form = morph.parse(w_clean)[0].normal_form.replace('—ë', '–µ')
                        sets[set_key].add(normal_form)
                    if ' ' in w_clean:
                        parts = w_clean.split()
                        for p in parts:
                            sets[set_key].add(p)
                            if morph: 
                                sets[set_key].add(morph.parse(p)[0].normal_form.replace('—ë', '–µ'))
        except: pass

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 6 –Ω–∞–±–æ—Ä–æ–≤
    return sets["products"], sets["commercial"], sets["specs"], sets["geo"], sets["services"], sets["sensitive"]

def classify_semantics_with_api(words_list, yandex_key):
    # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º 6 —Å–ª–æ–≤–∞—Ä–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Ä–Ω—É–ª–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
    PRODUCTS_SET, COMM_SET, SPECS_SET, GEO_SET, SERVICES_SET, SENS_SET = load_lemmatized_dictionaries()
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ —Ñ–∞–π–ª–∞ –∏ –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –≤ –∫–æ–¥–µ
    FULL_SENSITIVE = SENS_SET.union(SENSITIVE_STOPLIST)

    if 'debug_geo_count' not in st.session_state:
        st.session_state.debug_geo_count = len(GEO_SET)
    
    st.sidebar.info(f"–°–ª–æ–≤–∞—Ä–∏ (–∏–∑ —Ñ–∞–π–ª–æ–≤):\nüì¶ –¢–æ–≤–∞—Ä—ã: {len(PRODUCTS_SET)}\nüí∞ –ö–æ–º–º–µ—Ä—Ü–∏—è: {len(COMM_SET)}\nüõ†Ô∏è –£—Å–ª—É–≥–∏: {len(SERVICES_SET)}\nüåç –ì–æ—Ä–æ–¥–∞: {len(GEO_SET)}")

    dim_pattern = re.compile(r'\d+(?:[\.\,]\d+)?\s?[—Öx\*√ó]\s?\d+', re.IGNORECASE)
    grade_pattern = re.compile(r'^([–∞-—èa-z]{1,4}\-?\d+[–∞-—èa-z0-9]*)$', re.IGNORECASE)
    
    categories = {'products': set(), 'services': set(), 'commercial': set(), 
                  'dimensions': set(), 'geo': set(), 'general': set(), 'sensitive': set()}
    
    for word in words_list:
        word_lower = word.lower()
        
        # 1. –°–¢–û–ü-–°–õ–û–í–ê
        is_sensitive = False
        if word_lower in FULL_SENSITIVE: is_sensitive = True
        else:
            for stop_w in FULL_SENSITIVE:
                if len(stop_w) > 3 and stop_w in word_lower: is_sensitive = True; break
        if is_sensitive: categories['sensitive'].add(word_lower); continue
        
        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
        lemma = word_lower
        if morph:
            p = morph.parse(word_lower)[0]
            lemma = p.normal_form

        # 2. –†–ê–ó–ú–ï–†–´ / –ì–û–°–¢
        if word_lower in SPECS_SET or lemma in SPECS_SET:
            categories['dimensions'].add(word_lower); continue
        if dim_pattern.search(word_lower) or grade_pattern.match(word_lower) or word_lower.isdigit():
            categories['dimensions'].add(word_lower); continue

        # 3. –¢–û–í–ê–†–´ (–£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞)
        if word_lower in PRODUCTS_SET or lemma in PRODUCTS_SET:
            categories['products'].add(word_lower); continue
        
        is_product_root = False
        for prod in PRODUCTS_SET:
            check_root = prod[:-1] if len(prod) > 4 else prod
            if len(check_root) > 3 and check_root in word_lower:
                categories['products'].add(word_lower)
                is_product_root = True
                break
        if is_product_root: continue

        # 4. –ì–ï–û
        if lemma in GEO_SET or word_lower in GEO_SET:
            categories['geo'].add(word_lower); continue
        
        # 5. –£–°–õ–£–ì–ò
        if lemma in SERVICES_SET or word_lower in SERVICES_SET:
             categories['services'].add(word_lower); continue
        if lemma.endswith('–æ–±—Ä–∞–±–æ—Ç–∫–∞') or lemma.endswith('–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ') or lemma == "—Ä–µ–∑–∫–∞":
            categories['services'].add(word_lower); continue

        # 6. –ö–û–ú–ú–ï–†–¶–ò–Ø
        if lemma in COMM_SET or word_lower in COMM_SET:
            categories['commercial'].add(word_lower); continue
            
        # 7. –û–ë–©–ò–ï
        categories['general'].add(word_lower)

    return {k: sorted(list(v)) for k, v in categories.items()}

# ==========================================
# STATE INIT
# ==========================================
if 'sidebar_gen_df' not in st.session_state: st.session_state.sidebar_gen_df = None
if 'sidebar_excel_bytes' not in st.session_state: st.session_state.sidebar_excel_bytes = None
if 'analysis_results' not in st.session_state: st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state: st.session_state.analysis_done = False
if 'ai_generated_df' not in st.session_state: st.session_state.ai_generated_df = None
if 'ai_excel_bytes' not in st.session_state: st.session_state.ai_excel_bytes = None
if 'tags_html_result' not in st.session_state: st.session_state.tags_html_result = None
if 'table_html_result' not in st.session_state: st.session_state.table_html_result = None
if 'tags_generated_df' not in st.session_state: st.session_state.tags_generated_df = None
if 'tags_excel_data' not in st.session_state: st.session_state.tags_excel_data = None
if 'reviews_results' not in st.session_state: st.session_state.reviews_results = []
if 'reviews_queue' not in st.session_state: st.session_state.reviews_queue = []
if 'reviews_automode_active' not in st.session_state: st.session_state.reviews_automode_active = False
if 'reviews_current_index' not in st.session_state: st.session_state.reviews_current_index = 0
if 'reviews_per_query' not in st.session_state: st.session_state.reviews_per_query = 3
if 'pending_widget_updates' not in st.session_state: st.session_state.pending_widget_updates = {}

# Current lists
if 'categorized_products' not in st.session_state: st.session_state.categorized_products = []
if 'categorized_services' not in st.session_state: st.session_state.categorized_services = []
if 'categorized_commercial' not in st.session_state: st.session_state.categorized_commercial = []
if 'categorized_dimensions' not in st.session_state: st.session_state.categorized_dimensions = []
if 'categorized_geo' not in st.session_state: st.session_state.categorized_geo = []
if 'categorized_general' not in st.session_state: st.session_state.categorized_general = []
if 'categorized_sensitive' not in st.session_state: st.session_state.categorized_sensitive = []

# Original lists (for restoration)
if 'orig_products' not in st.session_state: st.session_state.orig_products = []
if 'orig_services' not in st.session_state: st.session_state.orig_services = []
if 'orig_commercial' not in st.session_state: st.session_state.orig_commercial = []
if 'orig_dimensions' not in st.session_state: st.session_state.orig_dimensions = []
if 'orig_geo' not in st.session_state: st.session_state.orig_geo = []
if 'orig_general' not in st.session_state: st.session_state.orig_general = []

if 'auto_tags_words' not in st.session_state: st.session_state.auto_tags_words = []
if 'auto_promo_words' not in st.session_state: st.session_state.auto_promo_words = []
if 'persistent_urls' not in st.session_state: st.session_state['persistent_urls'] = ""

st.set_page_config(layout="wide", page_title="GAR PRO v2.6 (Mass Promo)", page_icon="üìä")

GARBAGE_LATIN_STOPLIST = {
    'whatsapp', 'viber', 'telegram', 'skype', 'vk', 'instagram', 'facebook', 'youtube', 'twitter',
    'cookie', 'cookies', 'policy', 'privacy', 'agreement', 'terms',
    'click', 'submit', 'send', 'zakaz', 'basket', 'cart', 'order', 'call', 'back', 'callback',
    'login', 'logout', 'sign', 'register', 'auth', 'account', 'profile',
    'search', 'menu', 'nav', 'navigation', 'footer', 'header', 'sidebar',
    'img', 'jpg', 'png', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'svg',
    'ok', 'error', 'undefined', 'null', 'true', 'false', 'var', 'let', 'const', 'function', 'return',
    'ru', 'en', 'com', 'net', 'org', 'biz', 'shop', 'store',
    'phone', 'email', 'tel', 'fax', 'mob', 'address', 'copyright', 'all', 'rights', 'reserved',
    'div', 'span', 'class', 'id', 'style', 'script', 'body', 'html', 'head', 'meta', 'link'
}

SENSITIVE_STOPLIST_RAW = {
    "—É–∫—Ä–∞–∏–Ω–∞", "ukraine", "ua", "–≤—Å—É", "–∑—Å—É", "–∞—Ç–æ",
    "–∫–∏–µ–≤", "–ª—å–≤–æ–≤", "—Ö–∞—Ä—å–∫–æ–≤", "–æ–¥–µ—Å—Å–∞", "–¥–Ω–µ–ø—Ä", "–º–∞—Ä–∏—É–ø–æ–ª—å",
    "–¥–æ–Ω–µ—Ü–∫", "–ª—É–≥–∞–Ω—Å–∫", "–¥–Ω—Ä", "–ª–Ω—Ä", "–¥–æ–Ω–±–∞—Å—Å", 
    "–º–µ–ª–∏—Ç–æ–ø–æ–ª—å", "–±–µ—Ä–¥—è–Ω—Å–∫", "–±–∞—Ö–º—É—Ç", "–∑–∞–ø–æ—Ä–æ–∂—å–µ", "—Ö–µ—Ä—Å–æ–Ω",
    "–∫—Ä—ã–º", "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å", "—Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å"
}
STOP_POS = {'PREP', 'CONJ', 'PRCL', 'INTJ', 'NPRO'}
SENSITIVE_STOPLIST = {w.lower() for w in SENSITIVE_STOPLIST_RAW}

def check_password():
    if st.session_state.get("authenticated"):
        return True
    st.markdown("""<style>.main { display: flex; flex-direction: column; justify-content: center; align-items: center; } .auth-logo-box { text-align: center; margin-bottom: 1rem; padding-top: 0; }</style>""", unsafe_allow_html=True)
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="auth-logo-box"><h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3></div>', unsafe_allow_html=True)
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "ZVC01w4_pIquj0bMiaAu":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

if "arsenkin_token" in st.session_state:
    ARSENKIN_TOKEN = st.session_state.arsenkin_token
else:
    try: ARSENKIN_TOKEN = st.secrets["api"]["arsenkin_token"]
    except (FileNotFoundError, KeyError): ARSENKIN_TOKEN = None

if "yandex_dict_key" in st.session_state:
    YANDEX_DICT_KEY = st.session_state.yandex_dict_key
else:
    try: YANDEX_DICT_KEY = st.secrets["api"]["yandex_dict_key"]
    except (FileNotFoundError, KeyError): YANDEX_DICT_KEY = None

REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956},
    "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931},
    "–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É": {"ya": 39, "go": 1012028},
    "–£—Ñ–∞": {"ya": 172, "go": 1012091},
    "–ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫": {"ya": 62, "go": 1012001},
    "–í–æ—Ä–æ–Ω–µ–∂": {"ya": 193, "go": 1012134},
    "–ü–µ—Ä–º—å": {"ya": 50, "go": 1012015},
    "–í–æ–ª–≥–æ–≥—Ä–∞–¥": {"ya": 38, "go": 1012131},
    "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–°–∞—Ä–∞—Ç–æ–≤": {"ya": 194, "go": 1012046},
    "–¢—é–º–µ–Ω—å": {"ya": 283, "go": 1012089},
    "–¢–æ–ª—å—è—Ç—Ç–∏": {"ya": 240, "go": 1012080},
    "–ò–∂–µ–≤—Å–∫": {"ya": 44, "go": 1011979},
    "–ë–∞—Ä–Ω–∞—É–ª": {"ya": 197, "go": 1011855},
    "–ò—Ä–∫—É—Ç—Å–∫": {"ya": 63, "go": 1011977},
    "–£–ª—å—è–Ω–æ–≤—Å–∫": {"ya": 195, "go": 1012092},
    "–•–∞–±–∞—Ä–æ–≤—Å–∫": {"ya": 76, "go": 1011973},
    "–í–ª–∞–¥–∏–≤–æ—Å—Ç–æ–∫": {"ya": 75, "go": 1012129},
    "–Ø—Ä–æ—Å–ª–∞–≤–ª—å": {"ya": 16, "go": 1012140},
    "–ú–∞—Ö–∞—á–∫–∞–ª–∞": {"ya": 28, "go": 1011993},
    "–¢–æ–º—Å–∫": {"ya": 67, "go": 1012082},
    "–û—Ä–µ–Ω–±—É—Ä–≥": {"ya": 48, "go": 1012009},
    "–ö–µ–º–µ—Ä–æ–≤–æ": {"ya": 64, "go": 1011985},
    "–ù–æ–≤–æ–∫—É–∑–Ω–µ—Ü–∫": {"ya": 237, "go": 1011987},
    "–†—è–∑–∞–Ω—å": {"ya": 11, "go": 1012033},
    "–ù–∞–±–µ—Ä–µ–∂–Ω—ã–µ –ß–µ–ª–Ω—ã": {"ya": 234, "go": 1011905},
    "–ü–µ–Ω–∑–∞": {"ya": 49, "go": 1012013},
    "–õ–∏–ø–µ—Ü–∫": {"ya": 9, "go": 1011991},
    "–¢—É–ª–∞": {"ya": 15, "go": 1012085},
    "–ö–∏—Ä–æ–≤": {"ya": 46, "go": 1011989},
    "–ß–µ–±–æ–∫—Å–∞—Ä—ã": {"ya": 45, "go": 1011880},
    "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥": {"ya": 22, "go": 1011981},
    "–ö—É—Ä—Å–∫": {"ya": 8, "go": 1011988},
    "–£–ª–∞–Ω-–£–¥—ç": {"ya": 68, "go": 1012090},
    "–°—Ç–∞–≤—Ä–æ–ø–æ–ª—å": {"ya": 36, "go": 1012070},
    "–°–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å": {"ya": 959, "go": 1012048},
    "–°–æ—á–∏": {"ya": 239, "go": 1012053},
    "–†–æ—Å—Å–∏—è": {"ya": 225, "go": 2643},
    "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493},
    "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601},
    "–ê—Å—Ç–∞–Ω–∞ (KZ)": {"ya": 163, "go": 1014620}
}

DEFAULT_EXCLUDE_DOMAINS = {
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "aliexpress.ru", 
    "ebay.com", "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", 
    "pandao.ru", "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", 
    "banki.ru", "regmarkets.ru", "zoon.ru", "pulscen.ru", "prodoctorov.ru", 
    "blizko.ru", "domclick.ru", "satom.ru", "quto.ru", "edadeal.ru", 
    "cataloxy.ru", "irr.ru", "onliner.by", "shop.by", "deal.by", "yell.ru", 
    "profi.ru", "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by", 
    "market.yandex.ru", "youtube.com", "www.youtube.com", "gosuslugi.ru", 
    "www.gosuslugi.ru", "dzen.ru", "2gis.by", "wildberries.ru", "rutube.ru", 
    "vk.com", "facebook.com", "chipdip.ru"
    }

DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n—Å—Ç—Ä\n—É–ª\n—à—Ç\n—Å–º\n–º–º\n–º–ª\n–∫–≥\n–∫–≤\n–º¬≤\n—Å–º¬≤\n–º2\n—Å–º2"

PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE"

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif; color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important; }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important; color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] div[role="gridcell"] {{
            background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; border-bottom: 1px solid {ROW_BORDER_COLOR} !important;
        }}
        .legend-box {{ padding: 10px; background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px; }}
        .text-red {{ color: #D32F2F; font-weight: bold; }}
        .text-green {{ color: #2E7D32; font-weight: bold; }}
        .text-bold {{ font-weight: 600; }}
        .sort-container {{ background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 8px; margin-bottom: 10px; border: 1px solid {BORDER_COLOR}; }}
        
        .stApp > header {{ background-color: transparent !important; }}
        .stTextInput input:disabled, .stTextArea textarea:disabled, .stSelectbox div[aria-disabled="true"] {{
            opacity: 1 !important; background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; cursor: text !important; -webkit-text-fill-color: {TEXT_COLOR} !important; border-color: {BORDER_COLOR} !important;
        }}
        .stButton button:disabled {{ opacity: 1 !important; background-color: {PRIMARY_COLOR} !important; color: white !important; cursor: progress !important; }}
        div[data-testid="stAppViewContainer"] {{ filter: none !important; opacity: 1 !important; transition: none !important; }}
        /* –°—Ç–∏–ª–∏ –¥–ª—è —Å—Å—ã–ª–æ–∫ –≤–Ω—É—Ç—Ä–∏ –≥—Ä–∞—Ñ–∏–∫–∞ Plotly */
        .chart-link {{
            color: #277EFF !important;
            font-weight: 600 !important;
            text-decoration: none !important;
            border-bottom: 4px solid #CBD5E1 !important; 
            display: inline-block !important;
            transition: border-color 0.2s ease !important;
        }}
        .chart-link:hover {{
            border-bottom-color: #277EFF !important;
            cursor: pointer !important;
        }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# PARSING & METRICS
# ==========================================
# ... (–û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ —Ñ—É–Ω–∫—Ü–∏–π –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)

def get_yandex_dict_info(text, api_key):
    if not api_key: return {'lemma': text, 'pos': 'unknown'}
    url = "https://dictionary.yandex.net/api/v1/dicservice.json/lookup"
    params = {'key': api_key, 'lang': 'ru-ru', 'text': text, 'ui': 'ru'}
    try:
        r = requests.get(url, params=params, timeout=2)
        if r.status_code == 200:
            data = r.json()
            if data.get('def'):
                first_def = data['def'][0]
                return {'lemma': first_def.get('text', text), 'pos': first_def.get('pos', 'unknown')}
    except: pass
    return {'lemma': text, 'pos': 'unknown'}

def get_arsenkin_urls(query, engine_type, region_name, api_token, depth_val=10):
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check"
    url_get = "https://arsenkin.ru/api/tools/get"
    headers = {"Authorization": f"Bearer {api_token}", "Content-type": "application/json"}
    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []
    if "–Ø–Ω–¥–µ–∫—Å" in engine_type: se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type: se_params.append({"type": 11, "region": reg_ids['go']})

    payload = {"tools_name": "check-top", "data": {"queries": [query], "is_snippet": False, "noreask": True, "se": se_params, "depth": depth_val}}
    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        resp_json = r.json()
        if "error" in resp_json or "task_id" not in resp_json: st.error(f"‚ùå –û—à–∏–±–∫–∞ API: {resp_json}"); return []
        task_id = resp_json["task_id"]
        st.toast(f"–ó–∞–¥–∞—á–∞ ID {task_id} –∑–∞–ø—É—â–µ–Ω–∞")
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}"); return []

    status = "process"
    attempts = 0
    # Timeout increased to 10 minutes (120 * 5s)
    while status == "process" and attempts < 120:
        time.sleep(5); attempts += 1
        try:
            r_check = requests.post(url_check, headers=headers, json={"task_id": task_id})
            res_check_data = r_check.json()
            if res_check_data.get("status") == "finish": status = "done"; break
        except: pass

    if status != "done": st.error(f"‚è≥ –¢–∞–π–º-–∞—É—Ç API"); return []

    try:
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id}, timeout=200)
        res_data = r_final.json()
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}"); return []

    results_list = []
    try:
        collect = res_data.get('result', {}).get('result', {}).get('collect')
        if not collect: return []
        final_url_list = []
        if isinstance(collect, list) and len(collect) > 0 and isinstance(collect[0], list): final_url_list = collect[0][0]
        else:
             unique_urls = set()
             for engine_data in collect:
                 if isinstance(engine_data, dict):
                     for _, serps in engine_data.items():
                         for item in serps:
                             if item.get('url') and item.get('url') not in unique_urls:
                                 results_list.append({'url': item['url'], 'pos': item['pos']})
                                 unique_urls.add(item['url'])
             return results_list

        if final_url_list:
            for index, url in enumerate(final_url_list): results_list.append({'url': url, 'pos': index + 1})
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}"); return []
    return results_list

def process_text_detailed(text, settings, n_gram=1):
    text = text.lower().replace('—ë', '–µ')
    words = re.findall(r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+', text)
    stops = set(w.lower().replace('—ë', '–µ') for w in settings['custom_stops'])
    lemmas = []
    forms_map = defaultdict(set)

    # –°–ø–∏—Å–æ–∫ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –í–´–ö–ò–î–´–í–ê–ï–ú (—Å–æ—é–∑—ã, –ø—Ä–µ–¥–ª–æ–≥–∏, –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è –∏ —Ç.–¥.)
    # –î–æ–±–∞–≤—å INTJ (–º–µ–∂–¥–æ–º–µ—Ç–∏—è), –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –µ—â–µ —á–∏—â–µ
    BAD_POS = {'PREP', 'CONJ', 'PRCL', 'NPRO', 'INTJ'}

    for w in words:
        # 1. –§–∏–ª—å—Ç—Ä –ø–æ –¥–ª–∏–Ω–µ: —É–±–∏—Ä–∞–µ–º –≤—Å—ë, —á—Ç–æ –∫–æ—Ä–æ—á–µ 3 —Å–∏–º–≤–æ–ª–æ–≤ (–±—ã–ª–æ < 2)
        if len(w) < 3: 
            continue
            
        # 2. –§–∏–ª—å—Ç—Ä —Ü–∏—Ñ—Ä –∏ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤
        if not settings['numbers'] and w.isdigit(): continue
        if w in stops: continue
        
        lemma = w
        if USE_NLP and n_gram == 1:
            p = morph.parse(w)[0]
            # –ï—Å–ª–∏ —á–∞—Å—Ç—å —Ä–µ—á–∏ –≤ —Å–ø–∏—Å–∫–µ –º—É—Å–æ—Ä–∞ (STOP_POS) ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–æ–≤–æ
            if p.tag.POS in STOP_POS: 
                continue
            lemma = p.normal_form.replace('—ë', '–µ')
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –ª–µ–º–º–∞ —Å—Ç–∞–ª–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–π –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
            if len(lemma) < 3:
                continue

        lemmas.append(lemma)
        forms_map[lemma].add(w)
        
    return lemmas, forms_map # –ù–µ –∑–∞–±—É–¥—å –ø—Ä–æ return, –µ—Å–ª–∏ –æ–Ω –±—ã–ª –≤ –∫–æ–Ω—Ü–µ

def check_positions_NO_ALT(query, target_url, region_name, api_token):
    """
    –ê–±—Å–æ–ª—é—Ç–Ω–æ –Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è.
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç alt_urls.
    """
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check"
    url_get = "https://arsenkin.ru/api/tools/get"
    headers = {"Authorization": f"Bearer {api_token}", "Content-type": "application/json"}
    
    # –†–µ–≥–∏–æ–Ω
    reg_ids = REGION_MAP.get(region_name, {"ya": 213})
    region_id_int = int(reg_ids['ya'])
    
    # === JSON –°–¢–†–û–ì–û –ë–ï–ó ALT_URLS ===
    payload = {
        "tools_name": "positions",
        "data": {
            "queries": [str(query)],
            "url": str(target_url).strip(),
            # –°–¢–†–û–ö–ê alt_urls –ü–û–õ–ù–û–°–¢–¨–Æ –£–î–ê–õ–ï–ù–ê –û–¢–°–Æ–î–ê
            "subdomain": True,
            "se": [{"type": 2, "region": region_id_int}],
            "format": 0
        }
    }

    try:
        # 1. –ó–ê–ü–£–°–ö
        r = requests.post(url_set, headers=headers, json=payload, timeout=20)
        
        # –ï—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –≤–µ—Ä–Ω—É–ª 500 –∏–ª–∏ 400
        if r.status_code != 200:
            return 0, {"error": f"HTTP {r.status_code}", "text": r.text}
            
        resp = r.json()
        if "error" in resp: return 0, resp
        
        task_id = resp.get("task_id")
        if not task_id: return 0, {"error": "No Task ID", "resp": resp}
        
        # 2. –û–ñ–ò–î–ê–ù–ò–ï
        for i in range(40):
            time.sleep(2)
            r_c = requests.post(url_check, headers=headers, json={"task_id": task_id})
            if r_c.json().get("status") == "finish":
                break
        else:
            return 0, {"error": "Timeout"}

        # 3. –†–ï–ó–£–õ–¨–¢–ê–¢
        r_g = requests.post(url_get, headers=headers, json={"task_id": task_id})
        data = r_g.json()
        
        res_list = data.get("result", [])
        if not res_list: return 0, data
            
        item = res_list[0]
        pos = item.get('position')
        if pos is None: pos = item.get('pos')
        
        if str(pos) in ['0', '-', '', 'None']:
            return 0, item 
            
        return int(pos), None

    except Exception as e:
        return 0, {"error": f"Crash: {str(e)}"}

def parse_page(url, settings, query_context=""):
    import streamlit as st
    try:
        from curl_cffi import requests as cffi_requests
        headers = {
            'User-Agent': settings['ua'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        }
        r = cffi_requests.get(url, headers=headers, timeout=20, impersonate="chrome110")
        if r.status_code == 403: raise Exception("CURL_CFFI –ø–æ–ª—É—á–∏–ª 403 Forbidden")
        if r.status_code != 200: return None
        content = r.content
        encoding = r.encoding if r.encoding else 'utf-8'
    except Exception:
        try:
            import requests
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            session = requests.Session()
            headers = {'User-Agent': settings['ua']}
            r = session.get(url, headers=headers, timeout=20, verify=False)
            if r.status_code != 200: return None
            content = r.content
            encoding = r.apparent_encoding
        except Exception: return None

    try:
        # 1. –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Soup (–ü–æ–ª–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞)
        soup = BeautifulSoup(content, 'html.parser', from_encoding=encoding)
        
        # === –ù–û–í–û–ï: –°–æ–±–∏—Ä–∞–µ–º Title –∏ Description –æ—Ç–¥–µ–ª—å–Ω–æ ===
        page_title = soup.title.string.strip() if soup.title and soup.title.string else ""
        
        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})
        page_desc = meta_desc_tag['content'].strip() if meta_desc_tag and meta_desc_tag.get('content') else ""
        # ====================================================

        # === –õ–û–ì–ò–ö–ê –¢–ê–ë–õ–ò–¶–´ 2 (–ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ URL/–°—Å—ã–ª–∫–µ) ===
        product_titles = []
        search_roots = set()
        if query_context:
            clean_q = query_context.lower().replace('–∫—É–ø–∏—Ç—å', '').replace('—Ü–µ–Ω–∞', '').replace(' –≤ ', ' ')
            words = re.findall(r'[–∞-—èa-z]+', clean_q)
            for w in words:
                if len(w) > 3: search_roots.add(w[:-1])
                else: search_roots.add(w)
        
        parsed_current = urlparse(url)
        current_path_clean = parsed_current.path.rstrip('/')
        seen_titles = set()
        
        for a in soup.find_all('a', href=True):
            txt = a.get_text(strip=True)
            raw_href = a['href']
            if len(txt) < 5 or len(txt) > 300: continue
            if raw_href.startswith('#') or raw_href.startswith('javascript'): continue
            
            abs_href = urljoin(url, raw_href)
            parsed_href = urlparse(abs_href)
            href_path_clean = parsed_href.path.rstrip('/')
            
            is_child_path = href_path_clean.startswith(current_path_clean)
            is_deeper = len(href_path_clean) > len(current_path_clean)
            is_not_query_param_only = (href_path_clean != current_path_clean)

            if is_child_path and is_deeper and is_not_query_param_only:
                txt_lower = txt.lower()
                href_lower = abs_href.lower()
                has_keywords = False
                if search_roots:
                    for root in search_roots:
                        if root in txt_lower or root in href_lower:
                            has_keywords = True; break
                else:
                    if re.search(r'\d', txt): has_keywords = True

                is_buy_button = txt_lower in {'–∫—É–ø–∏—Ç—å', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ', '–≤ –∫–æ—Ä–∑–∏–Ω—É', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Ü–µ–Ω–∞'}
                if has_keywords and not is_buy_button:
                    if txt not in seen_titles:
                        product_titles.append(txt)
                        seen_titles.add(txt)
        # ========================================================
        
        h1_tag = soup.find('h1')
        h1_text = h1_tag.get_text(strip=True) if h1_tag else ""

        # 2. –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –¢–∞–±–ª–∏—Ü—ã 2 (–£–¥–∞–ª—è–µ–º –±–ª–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤)
        soup_no_grid = BeautifulSoup(content, 'html.parser', from_encoding=encoding)
        grid_div = soup_no_grid.find('div', class_='an-container-fluid an-container-xl')
        if grid_div: grid_div.decompose()
        
        # === [–í–ê–ñ–ù–û] –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ö–û–ù–¢–ï–ù–¢–ê –ü–û –ì–ê–õ–û–ß–ö–ê–ú ===
        tags_to_remove = []
        if settings['noindex']: tags_to_remove.append('noindex')
        
        for s in [soup, soup_no_grid]:
            for c in s.find_all(string=lambda text: isinstance(text, Comment)): c.extract()
            if tags_to_remove:
                for t in s.find_all(tags_to_remove): t.decompose()
            for script in s(["script", "style", "svg", "path", "noscript"]): script.decompose()

        # –¢–µ–∫—Å—Ç —Å—Å—ã–ª–æ–∫ (–∞–Ω–∫–æ—Ä—ã)
        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)
        
        # –°–±–æ—Ä –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ì–û —Ç–µ–∫—Å—Ç–∞ (Description, Alt, Title)
        extra_text = []
        # Description –¥–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Ç–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–∂–µ
        if page_desc: extra_text.append(page_desc)

        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])

        # –°–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç
        body_text_raw = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text = re.sub(r'\s+', ' ', body_text_raw).strip()

        body_text_no_grid_raw = soup_no_grid.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text_no_grid = re.sub(r'\s+', ' ', body_text_no_grid_raw).strip()

        if not body_text: return None
            
        return {
            'url': url, 
            'domain': urlparse(url).netloc, 
            'body_text': body_text, 
            'body_text_no_grid': body_text_no_grid,
            'anchor_text': anchor_text,
            'h1': h1_text,
            'product_titles': product_titles,
            # !!! –ù–û–í–´–ï –ü–û–õ–Ø –î–õ–Ø DASHBOARD !!!
            'meta_title': page_title,
            'meta_desc': page_desc
        }
    except Exception:
        return None

def analyze_meta_gaps(comp_data_full, my_data, settings):
    """
    –£–ú–ù–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† META-–¢–ï–ì–û–í v2.1
    1. –£—á–∏—Ç—ã–≤–∞–µ—Ç –≤–µ—Å –ø–æ–∑–∏—Ü–∏–∏ (—Å–ª–æ–≤–∞ —Ç–æ–ø–æ–≤ –≤–∞–∂–Ω–µ–µ).
    2. –ü–æ—Ä–æ–≥ –≤—Ö–æ–∂–¥–µ–Ω–∏—è: –°–¢–†–û–ì–û 50% (—Å–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —É –ø–æ–ª–æ–≤–∏–Ω—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤).
    3. –§–∏–ª—å—Ç—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–≥–∏ –∏ —Å–æ—é–∑—ã.
    """
    if not comp_data_full: return None
    
    # === 1. –ù–ê–°–¢–†–û–ô–ö–ò –ê–õ–ì–û–†–ò–¢–ú–ê ===
    TOTAL_COMPS = len(comp_data_full)
    
    # !!! –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–¢–†–û–ì–û 50% !!!
    MIN_OCCURRENCE_PCT = 0.4 
    
    # –ú–∏–Ω–∏–º—É–º 2 —Å–∞–π—Ç–∞, –¥–∞–∂–µ –µ—Å–ª–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤—Å–µ–≥–æ 3
    MIN_COUNT = max(2, int(TOTAL_COMPS * MIN_OCCURRENCE_PCT))

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (–ß–∏—Å—Ç–∫–∞ –º—É—Å–æ—Ä–∞)
def fast_tokenize(text):
    if not text: return set()
    
    # 1. –¢–≤–æ–π —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ + –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
    stop_garbage = {
        '–≤', '–Ω–∞', '–∏', '—Å', '—Å–æ', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '—É', 
        '–æ', '–æ–±', '–∑–∞', '–Ω–∞–¥', '–ø–æ–¥', '–ø—Ä–∏', '–ø—Ä–æ', '–±–µ–∑', '—á–µ—Ä–µ–∑', '–º–µ–∂–¥—É',
        '–∞', '–Ω–æ', '–∏–ª–∏', '–¥–∞', '–∫–∞–∫', '—á—Ç–æ', '—á—Ç–æ–±—ã', '–µ—Å–ª–∏', '—Ç–æ', '–ª–∏', '–±—ã', '–∂–µ', 
        '–≥', '–æ–±–ª', '—Ä', '—Ä—É–±', '–º–º', '—Å–º', '–º', '–∫–≥', '—Ç', '—à—Ç', '–¥–Ω',
        '–≤–µ—Å—å', '–≤—Å–µ', '–≤—Å—ë', '—Å–≤–æ–π', '–≤–∞—à', '–Ω–∞—à', '–º—ã', '–≤—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏',
        '–∫—É–ø–∏—Ç—å', '—Ü–µ–Ω–∞', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø—Ä–æ–¥–∞–∂–∞', '–Ω–µ–¥–æ—Ä–æ–≥–æ', 
        '–º–æ—Å–∫–≤–∞', '—Å–ø–±',
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–Ω–æ –ø–æ —Ç–≤–æ–µ–º—É —Å–ø–∏—Å–∫—É:
        '—Ä—É–±–ª–µ–π', '—Å—Ç—Ä', '—É–ª', '–∫–≤', '–º¬≤', '—Å–º¬≤', '–º2', '—Å–º2'
    }

    # –£–±–∏—Ä–∞–µ–º –∫–æ–º–º–µ—Ä—Ü–∏—é, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (–∫–∞–∫ –≤ —Ç–≤–æ–µ–º –∏—Å—Ö–æ–¥–Ω–∏–∫–µ)
    if '–∫—É–ø–∏—Ç—å' in stop_garbage: stop_garbage.remove('–∫—É–ø–∏—Ç—å') 
    if '—Ü–µ–Ω–∞' in stop_garbage: stop_garbage.remove('—Ü–µ–Ω–∞')
    
    if settings.get('custom_stops'):
        stop_garbage.update(set(settings['custom_stops']))

    lemmas = set()
    # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –†–ï–ì–£–õ–Ø–†–ö–ê: –¥–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ü–∏—Ñ—Ä –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤ –ø–ª–æ—â–∞–¥–µ–π ¬≤
    words = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9¬≤]+', text.lower())
    
    for w in words:
        # –§–∏–ª—å—Ç—Ä –¥–ª–∏–Ω—ã
        if len(w) < 2: continue 
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –î–û –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (–Ω–∞ —Å–ª—É—á–∞–π "—Ä—É–±", "–º2")
        if w in stop_garbage: continue
        
        if morph:
            try:
                p = morph.parse(w)[0]
                # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏
                if p.tag.POS in {'PREP', 'CONJ', 'PRCL', 'NPRO', 'INTJ'}:
                    continue
                
                normal_form = p.normal_form
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ü–û–°–õ–ï –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (–Ω–∞ —Å–ª—É—á–∞–π "—Ä—É–±–ª–µ–π" -> "—Ä—É–±–ª—å")
                # –ß—Ç–æ–±—ã "—Ä—É–±–ª—å" —Ç–æ–∂–µ –æ—Ç—Å–µ–∫–∞–ª—Å—è, –µ—Å–ª–∏ –≤ —Å–ø–∏—Å–∫–µ –µ—Å—Ç—å "—Ä—É–±"
                if normal_form in stop_garbage:
                    continue
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è (—Ä., —Ä—É–±. –∏ —Ç.–¥.)
                if any(normal_form.startswith(s) for s in ['—Ä—É–±–ª', '–º–µ—Ç—Ä', '—Å–∞–Ω—Ç–∏–º', '–∫–∏–ª–æ–≥—Ä']):
                    if w in stop_garbage or normal_form in stop_garbage:
                        continue

                lemmas.add(normal_form)
            except: 
                lemmas.add(w)
        else:
            lemmas.add(w)
            
    return lemmas

    # === 2. –°–ë–û–† –î–ê–ù–ù–´–• –° –í–ï–°–ê–ú–ò ===
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞: word -> {'count': 0, 'score': 0.0}
    stats_map = {
        'title': defaultdict(lambda: {'count': 0, 'score': 0.0}),
        'desc': defaultdict(lambda: {'count': 0, 'score': 0.0}),
        'h1': defaultdict(lambda: {'count': 0, 'score': 0.0})
    }
    
    detailed_rows = []

    for i, item in enumerate(comp_data_full):
        # –í–µ—Å –ø–æ–∑–∏—Ü–∏–∏: 1-–µ –º–µ—Å—Ç–æ = –≤–µ—Å–æ–º–µ–µ, —á–µ–º 10-–µ
        rank_weight = 1.0 + ( (TOTAL_COMPS - i) / TOTAL_COMPS ) * 1.5
        
        t_tok = fast_tokenize(item.get('meta_title', ''))
        d_tok = fast_tokenize(item.get('meta_desc', ''))
        h_tok = fast_tokenize(item.get('h1', ''))
        
        for w in t_tok:
            stats_map['title'][w]['count'] += 1
            stats_map['title'][w]['score'] += rank_weight
            
        for w in d_tok:
            stats_map['desc'][w]['count'] += 1
            stats_map['desc'][w]['score'] += rank_weight
            
        for w in h_tok:
            stats_map['h1'][w]['count'] += 1
            stats_map['h1'][w]['score'] += rank_weight

        detailed_rows.append({
            'URL': item['url'],
            'Title': item.get('meta_title', ''),
            'Description': item.get('meta_desc', ''),
            'H1': item.get('h1', '')
        })

    # === 3. –ê–ù–ê–õ–ò–ó –†–ê–ó–†–´–í–û–í (GAPS) ===
    
    my_tokens = {
        'title': fast_tokenize(my_data.get('meta_title', '')),
        'desc': fast_tokenize(my_data.get('meta_desc', '')),
        'h1': fast_tokenize(my_data.get('h1', ''))
    }

    def process_category(cat_key):
        data = stats_map[cat_key]
        important_words = []
        
        for word, metrics in data.items():
            # 1. –û—Ç—Å–µ–∫–∞–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ä–µ–∂–µ, —á–µ–º —É 50% –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
            if metrics['count'] < MIN_COUNT:
                continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–æ –∏ –µ–≥–æ "–≤–∞–∂–Ω–æ—Å—Ç—å" (Score)
            important_words.append((word, metrics['score']))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (Score)
        important_words.sort(key=lambda x: x[1], reverse=True)
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —è–¥—Ä–æ (–¢–æ–ø-15 —Å–ª–æ–≤, –ø—Ä–æ—à–µ–¥—à–∏—Ö —Ñ–∏–ª—å—Ç—Ä 50%)
        core_semantics = [x[0] for x in important_words[:15]]
        
        if not core_semantics:
            return 100, [] 
            
        matches = 0
        missing = []
        
        for w in core_semantics:
            if w in my_tokens[cat_key]:
                matches += 1
            else:
                missing.append(w)
        
        if len(core_semantics) > 0:
            score = int((matches / len(core_semantics)) * 100)
        else:
            score = 100
            
        return score, missing

    s_t, m_t = process_category('title')
    s_d, m_d = process_category('desc')
    s_h, m_h = process_category('h1')

    return {
        'scores': {'title': s_t, 'desc': s_d, 'h1': s_h},
        'missing': {'title': m_t, 'desc': m_d, 'h1': m_h},
        'detailed': detailed_rows,
        'my_data': {
            'Title': my_data.get('meta_title', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'),
            'Description': my_data.get('meta_desc', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'),
            'H1': my_data.get('h1', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω')
        }
    }
        
def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    import math
    import pandas as pd
    import numpy as np
    from collections import Counter, defaultdict
    import re
    from urllib.parse import urlparse

    if morph is None:
        st.error("CRITICAL: –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç!")
        return { "depth": pd.DataFrame(), "hybrid": pd.DataFrame(), "relevance_top": pd.DataFrame(), "my_score": {"width": 0, "depth": 0}, "missing_semantics_high": [], "missing_semantics_low": [] }

    # –ö–∞—Ä—Ç–∞ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏
    POS_MAP = {
        'NOUN': '–°—É—â', 'ADJF': '–ü—Ä–∏–ª', 'ADJS': '–ü—Ä–∏–ª',
        'VERB': '–ì–ª', 'INFN': '–ì–ª', 'PRTF': '–ü—Ä–∏—á', 'PRTS': '–ü—Ä–∏—á',
        'GRND': '–î–µ–µ–ø—Ä', 'NUMR': '–ß–∏—Å–ª', 'ADVB': '–ù–∞—Ä–µ—á',
        'NPRO': '–ú–µ—Å—Ç–æ–∏–º', 'PREP': '–ü—Ä–µ–¥–ª–æ–≥', 'CONJ': '–°–æ—é–∑', 'PRCL': '–ß–∞—Å—Ç–∏—Ü–∞', 'INTJ': '–ú–µ–∂–¥–æ–º'
    }

    # === 1. –ê–ù–ê–õ–ò–ó–ê–¢–û–† (–° –ß–ò–°–¢–ö–û–ô –ú–£–°–û–†–ê) ===
    def analyze_text_structure(text):
        if not text: return [], {}, 0
        
        # –ß–ï–†–ù–´–ô –°–ü–ò–°–û–ö (–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –º—É—Å–æ—Ä)
        trash_stop_list = {
            '—Ä—É–±', '—Ä—É–±–ª–µ–π', '–∫–≥', '—É–ª', '–Ω–∞—à', '–≤–∞—à', 'ru', 'com', 'net', 'org', 
            '—Å—Ç—Ä', '—à—Ç', '—Å–º', '–º–º', '–º–ª', '–∫–≤', '—Ç–µ–ª', '—Ñ–∞–∫—Å', '–ø–Ω', '–≤—Ç', '—Å—Ä', '—á—Ç', '–ø—Ç', '—Å–±', '–≤—Å',
            'description', 'keywords', 'content', 'viewport', 'charset', 'utf-8', 'html', 'body', 
            'div', 'span', 'class', 'style', 'script', 'function', 'return', 'var', 'let', 'const',
            '—Ü–µ–Ω–∞', '–∫—É–ø–∏—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '–∫–æ—Ä–∑–∏–Ω–∞', '–∫–∞—Ç–∞–ª–æ–≥', '–≥', '–æ–±–ª', '–¥', 'pro', 'max', 'min',
            'width', 'height', 'px', 'em', 'rem', 'color', 'background', 'border', 'padding', 'margin',
            'true', 'false', 'null', 'undefined', 'nan', 'id', 'src', 'href', 'link', 'rel', 'type',
            'mil', 'armox', 'target', 'blank', 'self', 'parent', 'top'
        }

        words = re.findall(r'[–∞-—è–ê-–Ø—ë–Åa-zA-Z0-9\-]+', text.lower())
        
        lemma_pos_list = []
        forms_map = defaultdict(set)
        valid_word_count = 0

        for w in words:
            if len(w) < 3 or w in trash_stop_list: continue
            if not settings['numbers'] and w.isdigit(): continue
            
            p = morph.parse(w)[0]
            if p.tag.POS in {'PREP', 'CONJ', 'PRCL', 'INTJ', 'NPRO'}: continue
            
            lemma = p.normal_form.replace('—ë', '–µ')
            if lemma in trash_stop_list or len(lemma) < 3: continue

            pos_tag = p.tag.POS
            pos_ru = POS_MAP.get(pos_tag, '–ü—Ä–æ—á–µ–µ')
            
            key = (lemma, pos_ru)
            lemma_pos_list.append(key)
            forms_map[key].add(w)
            valid_word_count += 1
            
        return lemma_pos_list, forms_map, valid_word_count

    # === 2. –°–ë–û–† –°–´–†–´–• –î–ê–ù–ù–´–• ===
    # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º —Ä–∞–∑–¥–µ–ª—å–Ω–æ: (–±—Ä–æ–Ω–µ–≤–æ–π, –°—É—â) –∏ (–±—Ä–æ–Ω–µ–≤–æ–π, –ü—Ä–∏–ª)
    global_stats_raw = defaultdict(lambda: {
        'sum_tf': 0.0, 
        'forms': set(), 
        'counts_list': [] # –°–ø–∏—Å–æ–∫ –≤—Ö–æ–∂–¥–µ–Ω–∏–π –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º [0, 5, 2, ...]
    })

    all_text_blocks = [] 
    N_sites = len(comp_data_full) if len(comp_data_full) > 0 else 1
    PASSAGE_SIZE = 20 

    for p in comp_data_full:
        if not p.get('body_text'): 
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –¥–ª–∏–Ω–∞ counts_list –±—ã–ª–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π
            continue
            
        doc_tokens, doc_forms, doc_len = analyze_text_structure(p['body_text'])
        
        if doc_len > 0:
            doc_counter = Counter(doc_tokens) # –ö–ª—é—á–∏ –∑–¥–µ—Å—å (lemma, pos)
            
            # –í–ê–ñ–ù–û: –ù–∞–º –Ω—É–∂–Ω–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å counts_list –¥–ª—è –≤—Å–µ—Ö –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–ª—é—á–µ–π
            # –ù–æ –º—ã –ø–æ–∫–∞ –Ω–µ –∑–Ω–∞–µ–º –≤—Å–µ—Ö –∫–ª—é—á–µ–π. –ü–æ—ç—Ç–æ–º—É —Å–æ–±–∏—Ä–∞–µ–º –ª–æ–∫–∞–ª—å–Ω–æ, –∞ –ø–æ—Ç–æ–º —Å–æ–ª—å–µ–º.
            # –£–ø—Ä–æ—Å—Ç–∏–º: global_stats_raw —Å–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ. counts_list –±—É–¥–µ–º –Ω–∞—Ä–∞—â–∏–≤–∞—Ç—å.
            # –ù–æ —á—Ç–æ–±—ã –ø–æ–∑–∏—Ü–∏–∏ —Å–æ–≤–ø–∞–¥–∞–ª–∏ —Å N_sites, –Ω—É–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω—É–ª—è–º–∏.
            pass

    # –ü–ï–†–ï–ü–ò–°–ê–ù–ù–´–ô –°–ë–û–† –î–ê–ù–ù–´–• –î–õ–Ø –ö–û–†–†–ï–ö–¢–ù–û–ì–û –°–õ–ò–Ø–ù–ò–Ø
    # 1. –°–Ω–∞—á–∞–ª–∞ –Ω–∞–π–¥–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏ (lemma, pos) –≤–æ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
    # 2. –ü–æ—Ç–æ–º –ø—Ä–æ–π–¥–µ–º—Å—è –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏ –∑–∞–ø–æ–ª–Ω–∏–º –º–∞—Ç—Ä–∏—Ü—É
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞: { (lemma, pos): [count_doc1, count_doc2, ...] }
    matrix_counts = defaultdict(lambda: [0] * N_sites)
    matrix_forms = defaultdict(set)
    matrix_sum_tf = defaultdict(float)
    
    # –î–ª—è IDF (–ø–∞—Å—Å–∞–∂–∏)
    # –ó–¥–µ—Å—å —É–ø—Ä–æ—Å—Ç–∏–º: —Å—á–∏—Ç–∞–µ–º IDF –ø–æ –ª–µ–º–º–∞–º —Å—Ä–∞–∑—É, —á—Ç–æ–±—ã –Ω–µ –º—É—á–∏—Ç—å—Å—è —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º –ø–∞—Å—Å–∞–∂–µ–π
    lemma_docs_count = Counter()

    for idx, p in enumerate(comp_data_full):
        if not p.get('body_text'): continue
        doc_tokens, doc_forms, doc_len = analyze_text_structure(p['body_text'])
        
        if doc_len > 0:
            # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ª–µ–º–º—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –¥–ª—è IDF
            unique_lemmas_here = set(t[0] for t in doc_tokens)
            lemma_docs_count.update(unique_lemmas_here)
            
            # –ü–æ–¥—Å—á–µ—Ç –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã
            doc_counter = Counter(doc_tokens)
            for (lemma, pos), count in doc_counter.items():
                matrix_counts[(lemma, pos)][idx] = count
                matrix_sum_tf[(lemma, pos)] += (count / doc_len)
                matrix_forms[(lemma, pos)].update(doc_forms[(lemma, pos)])

    # –í–ê–® –°–ê–ô–¢ (–°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ)
    my_counts_map_raw = Counter() # –ö–ª—é—á: (lemma, pos)
    my_clean_domain = "local"
    if my_data and my_data.get('body_text'):
        my_tokens, my_forms, my_len = analyze_text_structure(my_data['body_text'])
        my_counts_map_raw = Counter(my_tokens)
        if my_data.get('domain'):
            my_clean_domain = my_data.get('domain').lower().replace('www.', '').split(':')[0]

    # === üî• –≠–¢–ê–ü –°–õ–ò–Ø–ù–ò–Ø (MERGE) üî• ===
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∫–ª—é—á–∏ –ø–æ –ª–µ–º–º–µ
    grouped_keys = defaultdict(list)
    for (lemma, pos) in matrix_counts.keys():
        grouped_keys[lemma].append(pos)

    final_stats = {}

    for lemma, pos_list in grouped_keys.items():
        if not pos_list: continue
        
        # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–±–µ–¥–∏—Ç–µ–ª—è –ø–æ —Å—É–º–º–∞—Ä–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ (sum_tf)
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞—Å—Ç–∏ —Ä–µ—á–∏: —É –∫–æ–≥–æ sum_tf –±–æ–ª—å—à–µ
        sorted_pos = sorted(pos_list, key=lambda p: matrix_sum_tf[(lemma, p)], reverse=True)
        winner_pos = sorted_pos[0]
        
        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è
        merged_counts = list(matrix_counts[(lemma, winner_pos)]) # –ö–æ–ø–∏—è —Å–ø–∏—Å–∫–∞
        merged_sum_tf = matrix_sum_tf[(lemma, winner_pos)]
        merged_forms = matrix_forms[(lemma, winner_pos)].copy()
        
        # 3. –ü–†–ò–ü–õ–Æ–°–û–í–´–í–ê–ï–ú –ü–†–û–ò–ì–†–ê–í–®–ò–•
        for loser_pos in sorted_pos[1:]:
            # –°–∫–ª–∞–¥—ã–≤–∞–µ–º TF
            merged_sum_tf += matrix_sum_tf[(lemma, loser_pos)]
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ñ–æ—Ä–º—ã
            merged_forms.update(matrix_forms[(lemma, loser_pos)])
            
            # –°–∫–ª–∞–¥—ã–≤–∞–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏—è –ø–æ –∫–∞–∂–¥–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É (–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Å–ª–æ–∂–µ–Ω–∏–µ)
            loser_counts = matrix_counts[(lemma, loser_pos)]
            for i in range(N_sites):
                merged_counts[i] += loser_counts[i]

        # 4. –°—á–∏—Ç–∞–µ–º docs_containing –¥–ª—è –û–ë–™–ï–î–ò–ù–ï–ù–ù–û–ì–û —Å–ª–æ–≤–∞
        # (—Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–º–µ—é—Ç count > 0 –ø–æ—Å–ª–µ —Å–ª–∏—è–Ω–∏—è)
        merged_docs_containing = sum(1 for c in merged_counts if c > 0)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤—ã–π —Å–ø–∏—Å–æ–∫ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        # –ù–∞–ø—Ä–∏–º–µ—Ä: "–ü—Ä–∏–ª, –°—É—â"
        display_pos = ", ".join(sorted_pos)
        
        final_stats[lemma] = {
            'pos': display_pos,
            'sum_tf': merged_sum_tf,
            'forms': merged_forms,
            'counts_list': merged_counts,
            'docs_containing': merged_docs_containing
        }

    # === 3. –†–ê–°–ß–ï–¢ –¢–ê–ë–õ–ò–¶ ===
    table_depth = []
    table_hybrid = []
    missing_semantics_high = []
    missing_semantics_low = []
    words_with_median_gt_0 = set()
    my_found_words = set()

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
    sorted_lemmas = sorted(final_stats.keys())

    # –ü–∞—Å—Å–∞–∂–∏ —Å—á–∏—Ç–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω–æ —á–µ—Ä–µ–∑ lemma_docs_count (—ç—Ç–æ IDF)
    # –ù–æ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å merged_docs_containing –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–º–∫–∞—Ö TF-IDF
    # N_passages –≤ —ç—Ç–æ–π –≤–µ—Ä—Å–∏–∏ –∑–∞–º–µ–Ω–∏–º –Ω–∞ N_sites –¥–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ IDF –¥–æ–∫—É–º–µ–Ω—Ç–∞
    
    for lemma in sorted_lemmas:
        data = final_stats[lemma]
        df_docs = data['docs_containing']
        if df_docs == 0: continue
        
        # IDF (–ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º, –Ω–µ –ø–æ –ø–∞—Å—Å–∞–∂–∞–º, —Ç–∞–∫ –Ω–∞–¥–µ–∂–Ω–µ–µ –ø—Ä–∏ —Å–ª–∏—è–Ω–∏–∏)
        idf = math.log(N_sites / (1 + df_docs)) + 1
        avg_tf = data['sum_tf'] / N_sites
        tf_idf_value = avg_tf * idf
        
        # –°–ß–ò–¢–ê–ï–ú –í–•–û–ñ–î–ï–ù–ò–Ø –£ –í–ê–° (–¢–û–ñ–ï –°–£–ú–ú–ò–†–£–ï–ú –í–°–ï –í–ê–†–ò–ê–ù–¢–´)
        my_total_count = 0
        for (m_lemma, m_pos), cnt in my_counts_map_raw.items():
            if m_lemma == lemma:
                my_total_count += cnt

        # 3.1. TF-IDF
        table_hybrid.append({
            "–°–ª–æ–≤–æ": lemma,
            "–ß–∞—Å—Ç—å —Ä–µ—á–∏": data['pos'], # –í—ã–≤–µ–¥–µ—Ç "–ü—Ä–∏–ª, –°—É—â"
            "TF-IDF –¢–û–ü": tf_idf_value, 
            "IDF": idf, 
            "–ö–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤": df_docs,
            "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_total_count
        })

        # 3.2. –ì–õ–£–ë–ò–ù–ê
        raw_counts = data['counts_list']
        # raw_counts —É–∂–µ –∏–º–µ–µ—Ç –¥–ª–∏–Ω—É N_sites
        rec_median = int(np.median(raw_counts) + 0.5)
        obs_max = max(raw_counts) if raw_counts else 0
        
        if not (obs_max == 0 and my_total_count == 0):
            if rec_median >= 1:
                words_with_median_gt_0.add(lemma)
                if my_total_count > 0: my_found_words.add(lemma)

            if my_total_count == 0:
                weight = tf_idf_value * (rec_median if rec_median > 0 else 0.5)
                item = {'word': lemma, 'weight': weight}
                if rec_median >= 1: missing_semantics_high.append(item)
                else: missing_semantics_low.append(item)

            forms_str = ", ".join(sorted(list(data['forms'])))[:100]
            diff = rec_median - my_total_count
            status = "–ù–æ—Ä–º–∞" if diff == 0 else ("–ù–µ–¥–æ—Å–ø–∞–º" if diff > 0 else "–ü–µ—Ä–µ—Å–ø–∞–º")
            action_text = "‚úÖ" if diff == 0 else (f"+{diff}" if diff > 0 else f"{diff}")

            table_depth.append({
                "–°–ª–æ–≤–æ": lemma, "–°–ª–æ–≤–æ—Ñ–æ—Ä–º—ã": forms_str, "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_total_count,
                "–ú–µ–¥–∏–∞–Ω–∞": rec_median, "–ú–∞–∫—Å–∏–º—É–º (–∫–æ–Ω–∫—É—Ä.)": obs_max,
                "–°—Ç–∞—Ç—É—Å": status, "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": action_text,
                "is_missing": (my_total_count == 0), "sort_val": abs(diff)
            })

    # --- 4. –§–ò–ù–ê–õ ---
    df_hybrid = pd.DataFrame(table_hybrid)
    if not df_hybrid.empty:
        df_hybrid = df_hybrid.sort_values(by="TF-IDF –¢–û–ü", ascending=False).head(1000)
        df_hybrid["TF-IDF –¢–û–ü"] = df_hybrid["TF-IDF –¢–û–ü"].apply(lambda x: float(f"{x:.6f}"))
        df_hybrid["IDF"] = df_hybrid["IDF"].round(2)

    total_needed = len(words_with_median_gt_0)
    total_found = len(my_found_words)
    my_width_score = int(min(100, (total_found / total_needed) * 105)) if total_needed > 0 else 0
    
    table_rel = []
    my_site_found = False
    for item in original_results:
        url = item['url']
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –≤ comp_data_full
        try:
            # –ò—â–µ–º –∏–Ω–¥–µ–∫—Å –ø–æ URL
            idx = next(i for i, x in enumerate(comp_data_full) if x['url'] == url)
        except StopIteration:
            continue

        doc_data = comp_data_full[idx]
        width_val = 0
        
        if doc_data and doc_data.get('body_text'):
             # –°—á–∏—Ç–∞–µ–º —à–∏—Ä–∏–Ω—É –ø–æ —É–∂–µ —Å–ª–∏—Ç—ã–º –¥–∞–Ω–Ω—ã–º
             # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –∏–∑ words_with_median_gt_0 –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —ç—Ç–æ–º –¥–æ–∫–µ
             # matrix_counts[(lemma, winner_pos)][idx] > 0 ???
             # –ù–µ—Ç, –ø—Ä–æ—â–µ –≤–∑—è—Ç—å —Å—ã—Ä—ã–µ –ª–µ–º–º—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
             toks, _, _ = analyze_text_structure(doc_data['body_text'])
             lemmas_only = set(t[0] for t in toks)
             inter = lemmas_only.intersection(words_with_median_gt_0)
             width_val = int(min(100, (len(inter) / total_needed) * 105)) if total_needed > 0 else 0
        
        d_name = urlparse(url).netloc
        if my_clean_domain != "local" and my_clean_domain in d_name:
            d_name += " (–í—ã)"; my_site_found = True
        table_rel.append({ "–î–æ–º–µ–Ω": d_name, "URL": url, "–ü–æ–∑–∏—Ü–∏—è": item['pos'], "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": width_val, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": width_val })

    if not my_site_found:
        my_u_val = my_data.get('url', '#') if my_data else '#'
        table_rel.append({ "–î–æ–º–µ–Ω": "–í–∞—à —Å–∞–π—Ç", "URL": my_u_val, "–ü–æ–∑–∏—Ü–∏—è": my_serp_pos, "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": my_width_score, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": my_width_score })

    missing_semantics_high.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low.sort(key=lambda x: x['weight'], reverse=True)
    
    good_urls, bad_urls_dicts, trend_info = analyze_serp_anomalies(pd.DataFrame(table_rel))

    return { 
        "depth": pd.DataFrame(table_depth), 
        "hybrid": df_hybrid, 
        "relevance_top": pd.DataFrame(table_rel).sort_values(by='–ü–æ–∑–∏—Ü–∏—è'), 
        "my_score": {"width": my_width_score, "depth": my_width_score}, 
        "missing_semantics_high": missing_semantics_high, 
        "missing_semantics_low": missing_semantics_low[:500],
        "debug_width": {"found": total_found, "needed": total_needed}
    }
    
def get_hybrid_word_type(word, main_marker_root, specs_dict=None):
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä 3.1 (–§–∏–∫—Å –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤).
    """
    w = word.lower()
    specs_dict = specs_dict or set()
    
    # 1. –ú–ê–†–ö–ï–†
    if w == main_marker_root: return "1. üíé –ú–∞—Ä–∫–µ—Ä (–¢–æ–≤–∞—Ä)"
    if morph:
        norm = morph.parse(w)[0].normal_form
        if norm == main_marker_root: return "1. üíé –ú–∞—Ä–∫–µ—Ä (–¢–æ–≤–∞—Ä)"

    # 2. –°–¢–ê–ù–î–ê–†–¢–´
    if re.search(r'(gost|din|iso|en|tu|astm|aisi|–≥–æ—Å—Ç|–æ—Å—Ç|—Ç—É|–¥–∏–Ω)', w):
        return "6. üìú –°—Ç–∞–Ω–¥–∞—Ä—Ç"

    # 3. –†–ê–ó–ú–ï–†–´ / –¢–ï–•. –ü–ê–†–ê–ú–ï–¢–†–´
    # –ê. –ì–æ–ª—ã–µ —Ü–∏—Ñ—Ä—ã (10, 50.5)
    if re.fullmatch(r'\d+([.,]\d+)?', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    # –ë. –†–∞–∑–º–µ—Ä—ã —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ (10—Ö20, 10*20, 10-20, 10/20) <--- –î–û–ë–ê–í–ò–õ –¢–ò–†–ï –ò –°–õ–ï–®
    if re.search(r'^\d+[x—Ö*\-/]\d+', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    # –í. –ï–¥–∏–Ω–∏—Ü—ã (–º–º, –∫–≥)
    if re.search(r'\d+(–º–º|mm|–º|m|kg|–∫–≥|bar|–±–∞—Ä|–∞—Ç–º)$', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    # –ì. –ü—Ä–µ—Ñ–∏–∫—Å—ã (–î—É, –†—É, SDR)
    if re.match(r'^(d|dn|pn|sn|sdr|–¥—É|—Ä—É|√∏)\d+', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"

    # 4. –ú–ê–†–ö–ò / –°–ü–õ–ê–í–´
    if w in specs_dict: return "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –º–∞—Ä–æ–∫ (–ë—É–∫–≤—ã+–¶–∏—Ñ—Ä—ã)
    if re.search(r'\d', w): return "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"

    # 5. –õ–ê–¢–ò–ù–ò–¶–ê (–ë—Ä–µ–Ω–¥—ã)
    if re.search(r'^[a-z\-]+$', w): return "7. üî† –õ–∞—Ç–∏–Ω–∏—Ü–∞/–ë—Ä–µ–Ω–¥"

    # 6. –¢–ï–ö–°–¢
    if morph:
        p = morph.parse(w)[0]
        tag = p.tag
        if {'PREP'} in tag or {'CONJ'} in tag: return "SKIP"
        if {'ADJF'} in tag or {'PRTF'} in tag or {'ADJS'} in tag: return "2. üé® –°–≤–æ–π—Å—Ç–≤–∞"
        if {'NOUN'} in tag: return "4. üîó –î–æ–ø–æ–ª–Ω–µ–Ω–∏—è"

    if w.endswith(('–∏–π', '—ã–π', '–∞—è', '–æ–µ', '—ã–µ', '–∞—è')): return "2. üé® –°–≤–æ–π—Å—Ç–≤–∞"
    return "4. üîó –î–æ–ø–æ–ª–Ω–µ–Ω–∏—è"
    
def calculate_naming_metrics(comp_data_full, my_data, settings):
    """
    –¢–∞–±–ª–∏—Ü–∞ 2. –ë–µ–∑ "–æ–±—Ä–µ–∑–∞–Ω–∏—è" —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤.
    """
    # –ü–æ–¥–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è
    SPECS_SET = st.session_state.get('categorized_dimensions', set())
    if not SPECS_SET: _, _, SPECS_SET, _, _, _ = load_lemmatized_dictionaries()

    # 1. –ú–æ–π —Å–∞–π—Ç
    my_tokens = []
    if my_data and my_data.get('body_text_no_grid'):
        # –°–≤–æ—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –î—É50
        raw_w = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', my_data['body_text_no_grid'].lower())
        for w in raw_w:
            # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–ª–æ–≤–∞
            if not re.search(r'\d', w) and morph:
                my_tokens.append(morph.parse(w)[0].normal_form)
            else:
                my_tokens.append(w)

    # 2. –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã
    all_words_flat = []
    site_vocab_map = []
    
    for p in comp_data_full:
        titles = p.get('product_titles', [])
        valid_titles = [t for t in titles if 5 < len(t) < 150]
        
        if not valid_titles:
            site_vocab_map.append(set())
            continue
            
        curr_site_tokens = set()
        for t in valid_titles:
            words = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower())
            for w in words:
                if len(w) < 2: continue
                
                # –õ–û–ì–ò–ö–ê –°–û–•–†–ê–ù–ï–ù–ò–Ø –§–û–†–ú–´:
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Ü–∏—Ñ—Ä–∞ -> —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å (d50 -> d50)
                if re.search(r'\d', w):
                    token = w
                elif re.search(r'^[a-z]+$', w): # –õ–∞—Ç–∏–Ω–∏—Ü–∞ -> –∫–∞–∫ –µ—Å—Ç—å
                    token = w
                elif morph: # –†—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ -> –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º (—Å—Ç–∞–ª—å–Ω–∞—è -> —Å—Ç–∞–ª—å–Ω–æ–π)
                    token = morph.parse(w)[0].normal_form
                else:
                    token = w
                
                all_words_flat.append(token)
                curr_site_tokens.add(token)
                
        site_vocab_map.append(curr_site_tokens)

    if not all_words_flat: return pd.DataFrame()
    N_sites = len(site_vocab_map)

    # 3. –ú–∞—Ä–∫–µ—Ä (–°–∞–º–æ–µ —á–∞—Å—Ç–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–ª–æ–≤–æ)
    counts = Counter([w for w in all_words_flat if not re.search(r'\d', w)])
    main_marker_root = ""
    # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
    for w, c in counts.most_common(10):
        if morph and 'NOUN' in morph.parse(w)[0].tag: main_marker_root = w; break
    if not main_marker_root and counts: main_marker_root = counts.most_common(1)[0][0]

    # 4. –°–±–æ—Ä —Ç–∞–±–ª–∏—Ü—ã
    vocab = sorted(list(set(all_words_flat)))
    table_rows = []
    
    for token in vocab:
        if token in GARBAGE_LATIN_STOPLIST: continue
        
        # –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å
        sites_with_word = sum(1 for s_set in site_vocab_map if token in s_set)
        freq_percent = int((sites_with_word / N_sites) * 100)
        
        # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø
        cat = get_hybrid_word_type(token, main_marker_root, SPECS_SET)
        
        if cat == "SKIP": continue
        
        # –§–∏–ª—å—Ç—Ä—ã –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        # –ú–∞—Ä–∫–∏ –∏ –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç 5%
        is_spec = "–ú–∞—Ä–∫–∞" in cat or "–°—Ç–∞–Ω–¥–∞—Ä—Ç" in cat
        if is_spec and freq_percent < 5: continue
        
        # –û–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞ –æ—Ç 15%
        if not is_spec and "–†–∞–∑–º–µ—Ä—ã" not in cat and freq_percent < 15: continue
        
        # –†–∞–∑–º–µ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ —Ä–µ–∞–ª—å–Ω–æ —á–∞—Å—Ç—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ö–æ–¥–æ–≤–æ–π –¥–∏–∞–º–µ—Ç—Ä)
        # –ò–Ω–∞—á–µ —Ç–∞–±–ª–∏—Ü–∞ –±—É–¥–µ—Ç –∑–∞–±–∏—Ç–∞ —Ü–∏—Ñ—Ä–∞–º–∏ 10, 11, 12...
        if "–†–∞–∑–º–µ—Ä—ã" in cat and freq_percent < 15: continue

        rec_median = 1 if freq_percent > 30 else 0
        my_tf = my_tokens.count(token)
        diff = rec_median - my_tf
        action_text = f"+{diff}" if diff > 0 else ("‚úÖ" if diff == 0 else f"{diff}")
        
        table_rows.append({
            "–¢–∏–ø —Ö–∞—Ä-–∫–∏": cat[3:],
            "–°–ª–æ–≤–æ": token, # –í—ã–≤–æ–¥–∏–º —Ç–æ–∫–µ–Ω –∫–∞–∫ –µ—Å—Ç—å (—Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏ –±—É–∫–≤–∞–º–∏)
            "–ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å (%)": f"{freq_percent}%",
            "–£ –í–∞—Å": my_tf,
            "–ú–µ–¥–∏–∞–Ω–∞": rec_median,
            "–î–æ–±–∞–≤–∏—Ç—å": action_text,
            "raw_freq": freq_percent,
            "cat_sort": int(cat[0])
        })
        
    df = pd.DataFrame(table_rows)
    if not df.empty:
        df = df.sort_values(by=["cat_sort", "raw_freq"], ascending=[True, False])
        
    return df

def analyze_ideal_name(comp_data_full):
    """
    –°—Ç—Ä–æ–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å —É—á–µ—Ç–æ–º –ú–∞—Ä–æ–∫ –∏ –ì–û–°–¢–æ–≤.
    """
    # –ü–æ–¥–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
    SPECS_SET = st.session_state.get('categorized_dimensions', set())
    if not SPECS_SET: _, _, SPECS_SET, _, _, _ = load_lemmatized_dictionaries()

    titles = []
    for d in comp_data_full:
        ts = d.get('product_titles', [])
        titles.extend([t for t in ts if 5 < len(t) < 150])
    
    if not titles: return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö", []

    # –ú–∞—Ä–∫–µ—Ä
    all_w = []
    for t in titles: all_w.extend(re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower()))
    c = Counter(all_w)
    main_marker_root = ""
    for w, _ in c.most_common(5):
        if not re.search(r'\d', w):
             if morph and 'NOUN' in morph.parse(w)[0].tag: main_marker_root = w; break
             elif not morph: main_marker_root = w; break
    if not main_marker_root and c: main_marker_root = c.most_common(1)[0][0]

    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    structure_counter = Counter()
    vocab_by_type = defaultdict(Counter)
    
    sample = titles[:500]
    
    for t in sample:
        words = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower())
        pattern = []
        
        for w in words:
            if len(w) < 2: continue
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º —Å–ª–æ–≤–∞—Ä—è
            cat_full = get_hybrid_word_type(w, main_marker_root, SPECS_SET)
            if cat_full == "SKIP": continue
            
            # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∏–º—è —Ç–∏–ø–∞ ("–°–≤–æ–π—Å—Ç–≤–∞", "–ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤", "–°—Ç–∞–Ω–¥–∞—Ä—Ç")
            # "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤" -> "–ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"
            try:
                cat_short = cat_full.split('.', 1)[1].strip().split(' ', 1)[1] # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –∏–∫–æ–Ω–∫–∏
            except:
                cat_short = cat_full # Fallback
            
            vocab_by_type[cat_short][w] += 1
            
            if not pattern or pattern[-1] != cat_short:
                pattern.append(cat_short)
        
        if pattern:
            structure_str = " + ".join(pattern)
            structure_counter[structure_str] += 1
            
    # –°–±–æ—Ä–∫–∞
    if not structure_counter: return "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞", []
    
    best_struct_str, _ = structure_counter.most_common(1)[0]
    best_struct_list = best_struct_str.split(" + ")
    
    final_parts = []
    used_words = set()
    
    for block in best_struct_list:
        # –î–ª—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç–∞–≤–∏–º –∑–∞–≥–ª—É—à–∫—É
        if "–†–∞–∑–º–µ—Ä—ã" in block or "–°—Ç–∞–Ω–¥–∞—Ä—Ç" in block or "–ú–∞—Ä–∫–∞" in block:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –æ–Ω –æ—á–µ–Ω—å –ø–æ–ø—É–ª—è—Ä–µ–Ω
            top_cand = vocab_by_type[block].most_common(1)
            if top_cand and top_cand[0][1] > (len(sample) * 0.3): # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —É 30%
                 final_parts.append(top_cand[0][0])
            else:
                 final_parts.append(f"[{block.upper()}]")
            continue
            
        # –î–ª—è —Å–ª–æ–≤ (–ú–∞—Ä–∫–µ—Ä, –°–≤–æ–π—Å—Ç–≤–∞) –±–µ—Ä–µ–º –¢–û–ü-1
        candidates = vocab_by_type[block].most_common(3)
        for w, cnt in candidates:
            if w not in used_words:
                if "–ú–∞—Ä–∫–µ—Ä" in block: w = w.capitalize()
                final_parts.append(w)
                used_words.add(w)
                break
                
    ideal_name = " ".join(final_parts)
    
    # –û—Ç—á–µ—Ç
    report = []
    report.append(f"**–°—Ö–µ–º–∞:** {best_struct_str}")
    report.append("")
    report.append("**–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**")
    for block in best_struct_list:
        if "–†–∞–∑–º–µ—Ä—ã" in block: continue
        top = [f"{w}" for w, c in vocab_by_type[block].most_common(3)]
        report.append(f"- **{block}**: {', '.join(top)}")
            
    return ideal_name, report

def run_seo_analysis_background(query, api_token):
    """
    –§–æ–Ω–æ–≤—ã–π –∑–∞–ø—É—Å–∫ SEO-–∞–Ω–∞–ª–∏–∑–∞.
    –ü–û–õ–ù–ê–Ø –°–ò–ú–£–õ–Ø–¶–ò–Ø –†–ê–ë–û–¢–´ –í–ö–õ–ê–î–ö–ò 1 (–û–ë–ù–û–í–õ–Ø–ï–¢ UI).
    """
    # 1. –ò—Å–ø–æ–ª—å–∑—É–µ–º –¢–ï –ñ–ï –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, —á—Ç–æ –∏ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ 1 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    settings = {
        'noindex': True, 
        'alt_title': False, 
        'numbers': False, 
        'norm': True, 
        'ua': "Mozilla/5.0 (Windows NT 10.0; Win64; x64)", 
        'custom_stops': []
    }
    
    # –†–µ–∂–∏–º "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
    my_data = {'url': 'Local', 'domain': 'local', 'body_text': '', 'anchor_text': ''}
    
    if not api_token: return []
    
    try:
        # === 1. –ò–ú–ò–¢–ê–¶–ò–Ø –ù–ê–ñ–ê–¢–ò–Ø "–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API" ===
        raw_top = get_arsenkin_urls(query, "–Ø–Ω–¥–µ–∫—Å", "–ú–æ—Å–∫–≤–∞", api_token, depth_val=10)
        if not raw_top: return []
        
        candidates = [item for item in raw_top if not any(x in item['url'] for x in ["avito", "ozon", "wildberries", "market", "tiu"])]
        candidates = candidates[:10]
        if not candidates: return []

        comp_data = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = {executor.submit(parse_page, item['url'], settings, query): item for item in candidates}
            for f in concurrent.futures.as_completed(futures):
                try:
                    res = f.result()
                    if res:
                        res['pos'] = futures[f]['pos']
                        comp_data.append(res)
                except: pass
        
        if not comp_data: return []

        # === 2. –†–ê–°–ß–ï–¢ –ú–ï–¢–†–ò–ö (—Å –æ—á–∏—Å—Ç–∫–æ–π –æ—Ç –¥—É–±–ª–µ–π, –∫–∞–∫ –º—ã –Ω–∞—Å—Ç—Ä–æ–∏–ª–∏ –≤—ã—à–µ) ===
        targets = [{'url': d['url'], 'pos': d['pos']} for d in comp_data]
        results = calculate_metrics(comp_data, my_data, settings, 0, targets)
        
        # =========================================================
        # üî• –û–ë–ù–û–í–õ–ï–ù–ò–ï STATE (–ß—Ç–æ–±—ã –≤–∫–ª–∞–¥–∫–∞ 1 —É–≤–∏–¥–µ–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç)
        # =========================================================
        
        st.session_state['analysis_done'] = True
        st.session_state['analysis_results'] = results
        st.session_state['raw_comp_data'] = comp_data 
        
        # 1. –í–ø–∏—Å—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å –≤ –ø–æ–ª–µ –≤–≤–æ–¥–∞ –Ω–∞ 1 –≤–∫–ª–∞–¥–∫–µ
        st.session_state['query_input'] = query
        
        # 2. –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º —Ä–µ–∂–∏–º "–í–∞—à–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞" -> "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
        if 'my_page_source_radio' not in st.session_state:
            st.session_state['my_page_source_radio'] = "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
        
        # 3. –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ -> "API"
        # –≠—Ç–æ —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ, —á—Ç–æ–±—ã UI –Ω–µ –∏—Å–∫–∞–ª —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫
        st.session_state['competitor_source_radio'] = "–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)"
        
        # 4. –û–±–Ω–æ–≤–ª—è–µ–º –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
        st.session_state['naming_table_df'] = calculate_naming_metrics(comp_data, my_data, settings)
        st.session_state['ideal_h1_result'] = analyze_ideal_name(comp_data)
        st.session_state['full_graph_data'] = results['relevance_top']
        
        # 5. –¢—Ä–µ–Ω–¥—ã
        _, _, trend = analyze_serp_anomalies(results['relevance_top'])
        st.session_state['serp_trend_info'] = trend

        # =========================================================

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 15 –ª—É—á—à–∏—Ö —Å–ª–æ–≤ (TF-IDF) –¥–ª—è LSI –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
        df_hybrid = results.get('hybrid')
        if df_hybrid is not None and not df_hybrid.empty:
            return df_hybrid.head(15)['–°–ª–æ–≤–æ'].tolist()
            
    except Exception as e:
        print(f"Background SEO Error: {e}")
        return []
    
    return []

def render_paginated_table(df, title_text, key_prefix, default_sort_col=None, use_abs_sort_default=False, default_sort_order="–£–±—ã–≤–∞–Ω–∏–µ", show_controls=True):
    if df.empty: st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö."); return
    col_t1, col_t2 = st.columns([7, 3])
    with col_t1: st.markdown(f"### {title_text}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ñ–æ–ª—Ç–æ–≤ –≤ Session State
    if f'{key_prefix}_sort_col' not in st.session_state: 
        st.session_state[f'{key_prefix}_sort_col'] = default_sort_col if (default_sort_col and default_sort_col in df.columns) else df.columns[0]
    
    if f'{key_prefix}_sort_order' not in st.session_state: 
        st.session_state[f'{key_prefix}_sort_order'] = default_sort_order

    search_query = st.text_input(f"üîç –ü–æ–∏—Å–∫ ({title_text})", key=f"{key_prefix}_search")
    if search_query:
        mask = df.astype(str).apply(lambda x: x.str.contains(search_query, case=False, na=False)).any(axis=1)
        df_filtered = df[mask].copy()
    else: df_filtered = df.copy()

    if df_filtered.empty: st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."); return

    # === –õ–û–ì–ò–ö–ê –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø –ö–û–ù–¢–†–û–õ–û–í ===
    if show_controls:
        with st.container():
            st.markdown("<div class='sort-container'>", unsafe_allow_html=True)
            col_s1, col_s2, col_sp = st.columns([2, 2, 4])
            with col_s1:
                current_sort = st.session_state[f'{key_prefix}_sort_col']
                if current_sort not in df_filtered.columns: current_sort = df_filtered.columns[0]
                sort_col = st.selectbox("üóÇ –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ:", df_filtered.columns, key=f"{key_prefix}_sort_box", index=list(df_filtered.columns).index(current_sort))
                st.session_state[f'{key_prefix}_sort_col'] = sort_col
            with col_s2:
                def_index = 0 if st.session_state[f'{key_prefix}_sort_order'] == "–£–±—ã–≤–∞–Ω–∏–µ" else 1
                sort_order = st.radio("–ü–æ—Ä—è–¥–æ–∫:", ["–£–±—ã–≤–∞–Ω–∏–µ", "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ"], horizontal=True, key=f"{key_prefix}_order_box", index=def_index)
                st.session_state[f'{key_prefix}_sort_order'] = sort_order
            st.markdown("</div>", unsafe_allow_html=True)
    else:
        # –ï—Å–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª—ã —Å–∫—Ä—ã—Ç—ã, –±–µ—Ä–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ session_state (–¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ)
        sort_col = st.session_state[f'{key_prefix}_sort_col']
        sort_order = st.session_state[f'{key_prefix}_sort_order']

    ascending = (sort_order == "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ")
    
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    if use_abs_sort_default and sort_col == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è" and "sort_val" in df_filtered.columns: 
        df_filtered = df_filtered.sort_values(by="sort_val", ascending=ascending)
    elif ("–î–æ–±–∞–≤–∏—Ç—å" in sort_col or "+/-" in sort_col) and df_filtered[sort_col].dtype == object:
        try:
            df_filtered['_temp_sort'] = df_filtered[sort_col].astype(str).str.replace(r'[^\d]', '', regex=True)
            df_filtered['_temp_sort'] = pd.to_numeric(df_filtered['_temp_sort'], errors='coerce').fillna(0)
            df_filtered = df_filtered.sort_values(by='_temp_sort', ascending=ascending).drop(columns=['_temp_sort'])
        except: df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)
    else: 
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–ª–æ–Ω–∫–∏ (–Ω–∞ —Å–ª—É—á–∞–π —Å–º–µ–Ω—ã –¥–∞–Ω–Ω—ã—Ö)
        if sort_col in df_filtered.columns:
            df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)

    df_filtered = df_filtered.reset_index(drop=True); df_filtered.index = df_filtered.index + 1
    
    # –≠–∫—Å–ø–æ—Ä—Ç
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        export_df = df_filtered.copy()
        if "is_missing" in export_df.columns: del export_df["is_missing"]
        if "sort_val" in export_df.columns: del export_df["sort_val"]
        export_df.to_excel(writer, index=False, sheet_name='Data')
    excel_data = buffer.getvalue()
    with col_t2: st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel", data=excel_data, file_name=f"{key_prefix}_export.xlsx", mime="application/vnd.ms-excel", key=f"{key_prefix}_down")

    # –ü–∞–≥–∏–Ω–∞—Ü–∏—è
    ROWS_PER_PAGE = 20
    if f'{key_prefix}_page' not in st.session_state: st.session_state[f'{key_prefix}_page'] = 1
    total_rows = len(df_filtered); total_pages = math.ceil(total_rows / ROWS_PER_PAGE)
    if total_pages == 0: total_pages = 1
    current_page = st.session_state[f'{key_prefix}_page']
    if current_page > total_pages: current_page = total_pages
    if current_page < 1: current_page = 1
    st.session_state[f'{key_prefix}_page'] = current_page
    start_idx = (current_page - 1) * ROWS_PER_PAGE
    end_idx = start_idx + ROWS_PER_PAGE
    df_view = df_filtered.iloc[start_idx:end_idx]

    def highlight_rows(row):
        base_style = 'background-color: #FFFFFF; color: #3D4858; border-bottom: 1px solid #DBEAFE;'
        styles = []
        status = row.get("–°—Ç–∞—Ç—É—Å", "")
        for col_name in row.index:
            cell_style = base_style
            if col_name == "–°—Ç–∞—Ç—É—Å":
                if status == "–ù–µ–¥–æ—Å–ø–∞–º": cell_style += "color: #D32F2F; font-weight: bold;"
                elif status == "–ü–µ—Ä–µ—Å–ø–∞–º": cell_style += "color: #E65100; font-weight: bold;"
                elif status == "–ù–æ—Ä–º–∞": cell_style += "color: #2E7D32; font-weight: bold;"
            styles.append(cell_style)
        return styles

    cols_to_hide = [c for c in ["is_missing", "sort_val"] if c in df_view.columns]
    try: styled_df = df_view.style.apply(highlight_rows, axis=1)
    except: styled_df = df_view
    st.dataframe(styled_df, use_container_width=True, height=(len(df_view) * 35) + 40, column_config={c: None for c in cols_to_hide})
    
    c_spacer, c_btn_prev, c_info, c_btn_next = st.columns([6, 1, 1, 1])
    with c_btn_prev:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_prev", disabled=(current_page <= 1), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] -= 1
            st.rerun()
    with c_info: st.markdown(f"<div style='text-align: center; margin-top: 10px;'><b>{current_page}</b> / {total_pages}</div>", unsafe_allow_html=True)
    with c_btn_next:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_next", disabled=(current_page >= total_pages), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] += 1
            st.rerun()
    st.markdown("---")
# ==========================================
# PERPLEXITY GEN
# ==========================================
STATIC_DATA_GEN = {
    'IP_PROP4817': "–£—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏",
    'IP_PROP4818': "–û–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –≤ —Ä–µ–≥–∏–æ–Ω—ã —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4819': """<p>–ù–∞–¥–µ–∂–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫–∞–∑–∞ –≤ –ª—é–±—É—é —Ç–æ—á–∫—É —Å—Ç—Ä–∞–Ω—ã: "–°—Ç–∞–ª—å–º–µ—Ç—É—Ä–∞–ª" –æ—Ç–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–≤–∞—Ä 24 —á–∞—Å–∞ –≤ —Å—É—Ç–∫–∏, 7 –¥–Ω–µ–π –≤ –Ω–µ–¥–µ–ª—é. –ë–æ–ª–µ–µ 4 000 –æ—Ç–≥—Ä—É–∑–æ–∫ –≤ –≥–æ–¥. –ü—Ä–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–∏ –∑–∞–∫–∞–∑–∞ –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –≤–∞–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ä—à—Ä—É—Ç.</p>""",
    'IP_PROP4820': """<p>–ù–∞—à–∏ –∏–∑–¥–µ–ª–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è—Ö –£—Ä–∞–ª–∞, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞, –ü–æ–≤–æ–ª–∂—å—è, –°–∏–±–∏—Ä–∏. –ü–∞—Ä—Ç–Ω–µ—Ä—ã –ø–æ –ª–æ–≥–∏—Å—Ç–∏–∫–µ –ø—Ä–µ–¥–ª–æ–∂–∞—Ç –¥–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–∫–∞–∑ —Å–∞–º—ã–º —É–¥–æ–±–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º ‚Äì –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–º, –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω—ã–º, –¥–∞–∂–µ –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º. –î–ª—è –≤–∞—Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—É—é —Å—Ö–µ–º—É –ø–æ–¥ —É–¥–æ–±–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è. –ü–æ–≥—Ä—É–∑–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ç–µ—Ö–Ω–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.</p><div class="h4"><h4>–°–∞–º–æ–≤—ã–≤–æ–∑</h4></div><p>–ï—Å–ª–∏ –æ–±—ã—á–Ω–æ —Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∑–∞–±—Ä–∞—Ç—å —Ç–æ–≤–∞—Ä –∏–ª–∏ –¥–∞–µ—Ç–µ —ç—Ç–æ –ø—Ä–∞–≤–æ —É–ø–æ–ª–Ω–æ–º–æ—á–µ–Ω–Ω—ã–º, –∞–¥—Ä–µ—Å –∏ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã —Å–∫–ª–∞–¥–∞ –≤ —Å–≤–æ–µ–º –≥–æ—Ä–æ–¥–µ —É—Ç–æ—á–Ω—è–π—Ç–µ —É –º–µ–Ω–µ–¥–∂–µ—Ä–∞.</p><div class="h4"><h4>–ì—Ä—É–∑–æ–≤–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏</h4></div><p>–û—Ç–ø—Ä–∞–≤–∏–º –ø—Ä–æ–∫–∞—Ç –Ω–∞ –≤–∞—à –æ–±—ä–µ–∫—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –∞–≤—Ç–æ–ø–∞—Ä–∫–æ–º. –ü–æ–ª—É—á–µ–Ω–∏–µ –≤ —É–ø–∞–∫–æ–≤–∫–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏, –∞ –∏–º–µ–Ω–Ω–æ –Ω–∞ –¥–µ—Ä–µ–≤—è–Ω–Ω–æ–º –ø–æ–¥–¥–æ–Ω–µ.</p><div class="h4"><h4>–°–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å –¢–ö</h4></div><p>–î–æ—Å—Ç–∞–≤–∫–∞ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –†–æ—Å—Å–∏–∏ –∏ –°–ù–ì. –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å—Å—è, —Ç–∞–∫ –∫–∞–∫ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç, –æ–¥–Ω–∞–∫–æ, —Å—Ä–∞–≤–Ω–∏–º —Å—Ç–æ–∏–º–æ—Å—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª—É–∂–± –∏ –≤—ã–±–µ—Ä–µ–º –ª—É—á—à—É—é.</p>""",
    'IP_PROP4821': "–û–ø–ª–∞—Ç–∞ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤:",
    'IP_PROP4822': """<p>–ù–∞—à–∞ –∫–æ–º–ø–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞ –ø—Ä–∏–Ω—è—Ç—å –ª—é–±—ã–µ –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–µ –≤–∏–¥—ã –æ–ø–ª–∞—Ç—ã –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏—Ü: –ø–æ —Å—á–µ—Ç—É, –Ω–∞–ª–∏—á–Ω–∞—è –∏ –±–µ–∑–Ω–∞–ª–∏—á–Ω–∞—è, –Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–π –ø–ª–∞—Ç–µ–∂, —Ç–∞–∫–∂–µ –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞ –∏ –æ—Ç—Å—Ä–æ—á–∫–∞ –ø–ª–∞—Ç–µ–∂–∞.</p>""",
    'IP_PROP4823': """<div class="h4"><h3>–ü—Ä–∏–º–µ—Ä—ã –≤–æ–∑–º–æ–∂–Ω–æ–π –æ–ø–ª–∞—Ç—ã</h3></div><div class="an-col-12"><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–° –ø–æ–º–æ—â—å—é –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤ —Ü–µ–Ω—Ç—Ä–∞—Ö –ø—Ä–æ–¥–∞–∂</span></p></li></ul><p>–í–∞–∂–Ω–æ! –¶–µ–Ω–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—É–±–ª–∏—á–Ω–æ–π –æ—Ñ–µ—Ä—Ç–æ–π. –ü—Ä–∏—Ö–æ–¥–∏—Ç–µ –≤ –Ω–∞—à –æ—Ñ–∏—Å, —á—Ç–æ–±—ã —É—Ç–æ—á–Ω–∏—Ç—å –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ, –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ—á—Ç–∏ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å, —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –≤–æ–∑–≤—Ä–∞—Ç, —Å—á–µ—Ç, —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ª–æ–≥–∏—Å—Ç–∏–∫—É.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞ —Ä–∞—Å—á–µ—Ç–Ω—ã–π —Å—á–µ—Ç</span></p></li></ul><p>–ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É —Å—á–µ—Ç—É –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏ –±–∞–Ω–∫–∞ –∏–ª–∏ –ø—É—Ç–µ–º –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥—Å—Ç–≤ —á–µ—Ä–µ–∑ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç (—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∑–∞—â–∏—â–µ–Ω—ã, —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –æ—Ç–¥–µ–ª–µ–Ω–∏—è). –î–ª—è –ø—Ä–∞–≤–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å —Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –ø–ª–∞—Ç–µ–∂–Ω–æ–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ —Å –æ—Ç–º–µ—Ç–∫–æ–π –±–∞–Ω–∫–∞.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞–ª–∏—á–Ω—ã–º–∏ –∏–ª–∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∫–∞—Ä—Ç–æ–π –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏</span></p></li></ul><p><span style="font-weight: 400;">–ü–æ–º–æ–∂–µ–º —Å –æ–ø–ª–∞—Ç–æ–π: –æ–±—ä–µ–º –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ. –ö—Ä—É–ø–Ω—ã–º –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º ‚Äì –¥–µ–Ω—å–≥–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–µ–º–∫–∏ —Ç–æ–≤–∞—Ä–∞.</span></p><p>–ú–µ–Ω–µ–¥–∂–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.</p><p>–ó–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ —á–µ—Ä–µ–∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç:</p><p><a class="btn btn-blue" href="/catalog/">–ö–∞—Ç–∞–ª–æ–≥ (–º–∞–≥–∞–∑–∏–Ω-–º–µ–Ω—é):</a></p></div></div><br>""",
    'IP_PROP4824': "–ö–æ–Ω—Ç–∞–∫—Ç—ã –¥–ª—è —Å–≤—è–∑–∏",
    'IP_PROP4825': "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä",
    'IP_PROP4826': "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥",
    'IP_PROP4834': "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä",
    'IP_PROP4835': "–¢–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4836': "–ì–∏–±–∫–∏–µ —É—Å–ª–æ–≤–∏—è —Ä–∞—Å—á–µ—Ç–∞",
    'IP_PROP4837': "–ü–æ—Ä—è–¥–æ–∫ –≤ –ì–û–°–¢–∞—Ö"
}

def get_page_data_for_gen(url):
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π requests, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–ª –Ω–æ—Ä–º–∞–ª—å–Ω–æ
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        # verify=False –ø–æ–º–æ–≥–∞–µ—Ç –æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö SSL –æ—à–∏–±–æ–∫, –Ω–æ –Ω–µ –ª–æ–º–∞–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∫—É
        response = requests.get(url, headers=headers, timeout=20, verify=False)
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞ —á–∞—Å—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –Ω–∞ —Ä—É-—Å–∞–π—Ç–∞—Ö
        if response.encoding != 'utf-8':
            response.encoding = response.apparent_encoding
    except Exception as e: 
        return None, None, None, f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}"
    
    if response.status_code != 200: 
        return None, None, None, f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—É—Å–∞: {response.status_code}"
    
    try:
        soup = BeautifulSoup(response.text, 'html.parser')
    except:
        return None, None, None, "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"
    
    # 1. –ó–ê–ì–û–õ–û–í–û–ö
    description_div = soup.find('div', class_='description-container')
    target_h2 = None
    if description_div:
        target_h2 = description_div.find('h2')
    
    if not target_h2:
        target_h2 = soup.find('h2')
    
    # –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ï—Å–ª–∏ H2 –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None, —á—Ç–æ–±—ã —Å–∫—Ä–∏–ø—Ç –≤–∑—è–ª –∏–º—è —Ç–æ–≤–∞—Ä–∞ –∏–∑ —Å—Å—ã–ª–∫–∏
    page_header = target_h2.get_text(strip=True) if target_h2 else None

    # 2. –§–∞–∫—Ç—É—Ä–∞ (—Ç–µ–∫—Å—Ç)
    if description_div:
        base_text = description_div.get_text(separator="\n", strip=True)
    else:
        # –ß–∏—Å—Ç–∏–º —Å–∫—Ä–∏–ø—Ç—ã, —á—Ç–æ–±—ã –≤ —Ç–µ–∫—Å—Ç –Ω–µ –ø–æ–ø–∞–ª –º—É—Å–æ—Ä
        for s in soup(['script', 'style']): s.decompose()
        base_text = soup.body.get_text(separator="\n", strip=True)[:6000]
    
    # 3. –¢–µ–≥–∏
    tags_container = soup.find(class_='popular-tags-inner')
    tags_data = []
    if tags_container:
        links = tags_container.find_all('a')
        for link in links:
            tag_url = urljoin(url, link.get('href')) if link.get('href') else None
            if tag_url: tags_data.append({'name': link.get_text(strip=True), 'url': tag_url})
            
    return base_text, tags_data, page_header, None

def generate_ai_content_blocks(api_key, base_text, tag_name, forced_header, num_blocks=5, seo_words=None):
    if not base_text: return ["Error: No base text"] * num_blocks
    
    from openai import OpenAI
    client = OpenAI(api_key=api_key, base_url="https://litellm.tokengate.ru/v1")
    
    seo_words = seo_words or []
    seo_instruction_block = ""
    
    # === 1. –ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û SEO (–ò–°–ü–†–ê–í–õ–ï–ù–û: –£–ë–†–ê–ù–û –¢–†–ï–ë–û–í–ê–ù–ò–ï –í–´–î–ï–õ–ï–ù–ò–Ø) ===
    if seo_words:
        seo_list_str = ", ".join(seo_words)
        seo_instruction_block = f"""
--- –í–ê–ñ–ù–ê–Ø –ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û SEO-–°–õ–û–í–ê–ú ---
–¢–µ–±–µ –Ω—É–∂–Ω–æ –≤–Ω–µ–¥—Ä–∏—Ç—å –≤ —Ç–µ–∫—Å—Ç —Å–ª–µ–¥—É—é—â–∏–µ —Å–ª–æ–≤–∞ –≤ –ª—é–±–æ–π –ø–æ–¥—Ö–æ–¥—è—â–µ–π –ø–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ª–µ–º–º–µ: {{{seo_list_str}}}

–ü–†–ê–í–ò–õ–ê –í–ù–ï–î–†–ï–ù–ò–Ø –ò –í–´–î–ï–õ–ï–ù–ò–Ø:
1. –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï: –†–∞—Å–∫–∏–¥–∞–π —Å–ª–æ–≤–∞ –ø–æ –≤—Å–µ–º {num_blocks} –±–ª–æ–∫–∞–º.
2. –°–¢–†–û–ì–û –ó–ê–ü–†–ï–©–ï–ù–û: –ù–µ –≤—ã–¥–µ–ª—è–π –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∂–∏—Ä–Ω—ã–º —à—Ä–∏—Ñ—Ç–æ–º (**—Ç–µ–∫—Å—Ç** –∏–ª–∏ <b>—Ç–µ–∫—Å—Ç</b>). –í–ø–∏—Å—ã–≤–∞–π –∏—Ö –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç.
4. –ï–°–¢–ï–°–¢–í–ï–ù–ù–û–°–¢–¨: –ú–µ–Ω—è–π —Å–ª–æ–≤–æ—Ñ–æ—Ä–º—ã –ø–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç. –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∏ –ª–æ–≥–∏—á–Ω—ã–º, –Ω–µ –ø–∏—à–∏ —á—É—à—å.
–ü–†–ò–ú–ï–†–´ –¢–û–ì–û, –ö–ê–ö –ù–ê–î–û –ò –ù–ï –ù–ê–î–û –î–ï–õ–ê–¢–¨:
1. –ö–ª—é—á: "—Ç–æ–Ω–Ω–∞"
   ‚ùå –ü–õ–û–•–û: "–¶–µ–Ω–∞ –∑–∞ —Ç–æ–Ω–Ω–∞..." (–û—à–∏–±–∫–∞ –ø–∞–¥–µ–∂–∞)
   ‚úÖ –•–û–†–û–®–û: "–¶–µ–Ω–∞ –∑–∞ —Ç–æ–Ω–Ω—É..." (–í–∏–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂)
   
2. –ö–ª—é—á: "–∫–∞—á–µ—Å—Ç–≤–æ"
   ‚ùå –ü–õ–û–•–û: "–ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–∞..." (–ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å)
   ‚úÖ –•–û–†–û–®–û: "–ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ..."

3. –ö–ª—é—á: "–ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å"
   ‚ùå –ü–õ–û–•–û: "–í–æ –º–Ω–æ–≥–∏—Ö –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å..."
   ‚úÖ –•–û–†–û–®–û: "–í–æ –º–Ω–æ–≥–∏—Ö –æ—Ç—Ä–∞—Å–ª—è—Ö –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏..."
-------------------------------------------
"""

    # === 2. –°–ò–°–¢–ï–ú–ù–ê–Ø –†–û–õ–¨ (–í–∞—à –≤–∞—Ä–∏–∞–Ω—Ç) ===
    system_instruction = (
        "–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä –∏ –≤–µ—Ä—Å—Ç–∞–ª—å—â–∏–∫. "
        "–¢–≤–æ—è —Ü–µ–ª—å ‚Äî –ø–∏—Å–∞—Ç—å –≥–ª—É–±–æ–∫–∏–π, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –ø–æ–ª–µ–∑–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–æ–≤, –Ω–∞—Å—ã—â–µ–Ω–Ω—ã–π —Ñ–∞–∫—Ç–∞–º–∏ –∏ —Ü–∏—Ñ—Ä–∞–º–∏. "
        "–¢—ã –≤—ã–¥–∞–µ—à—å –¢–û–õ–¨–ö–û HTML-–∫–æ–¥. "
        "–°—Ç–∏–ª—å: –î–µ–ª–æ–≤–æ–π, —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π, –Ω–æ \"—á–µ–ª–æ–≤–µ—á–Ω—ã–π\" –∏ –ø–æ–Ω—è—Ç–Ω—ã–π. –ò–∑–±–µ–≥–∞–π –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º–æ–≤ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. "
        "–§–∞–∫—Ç—ã –∏ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞: –í—Å–µ —Å—É–∂–¥–µ–Ω–∏—è –ø–æ–¥–∫—Ä–µ–ø–ª—è–π –∏–∑–º–µ—Ä–∏–º—ã–º–∏ —Ñ–∞–∫—Ç–∞–º–∏, —Ü–∏—Ñ—Ä–∞–º–∏, —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –ì–û–°–¢—ã, –º–∞—Ä–∫–∏ —Å—Ç–∞–ª–∏ –∏ –¥—Ä—É–≥–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤—ã. –ò—Å–ø–æ–ª—å–∑—É–π –ø–æ–∏—Å–∫–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π. "
        "–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å: –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–¥–∞–≤–∞—Ç—å. –ì–æ–≤–æ—Ä–∏ –æ—Ç –ª–∏—Ü–∞ –∫–æ–º–ø–∞–Ω–∏–∏-–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è/–ø–æ—Å—Ç–∞–≤—â–∏–∫–∞. –í–º–µ—Å—Ç–æ \"–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫\" –∏—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—â–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –∏ —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É. "
        "–§–æ—Ä–º—É–ª–∞ –ì–ª–∞–≤—Ä–µ–¥–∞ –¥–ª—è B2B: –í —Ç–µ–∫—Å—Ç–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã: —á—Ç–æ —ç—Ç–æ? –∫–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç? –∫–æ–º—É –ø–æ–¥–æ–π–¥–µ—Ç? –∫–∞–∫–∏–µ –µ—Å—Ç—å —Ä–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç–∏? –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å–∫—Ä–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞—Ö –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞, —Å–∫–ª–∞–¥—Å–∫–∏—Ö –∑–∞–ø–∞—Å–∞—Ö –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è –ø–æ–¥ –∑–∞–∫–∞–∑. "
        "–°–¢–†–û–ì–ò–ï –ó–ê–ü–†–ï–¢–´: "
        "1. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –£–∫—Ä–∞–∏–Ω—ã, —É–∫—Ä–∞–∏–Ω—Å–∫–∏—Ö –≥–æ—Ä–æ–¥–æ–≤ (–ö–∏–µ–≤, –õ—å–≤–æ–≤ –∏ –¥—Ä.), "
        "–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–º—ã, –≤–∞–ª—é—Ç—É –≥—Ä–∏–≤–Ω—É. –ö–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–æ–≥–æ –¥–ª—è –†–§. "
        "2. –ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∏ –≤ —Ç–µ–∫—Å—Ç–µ, –Ω–∏ –≤ —Å–ø–∏—Å–∫–∞—Ö. –ß–∏—Å—Ç–∏ —Ç–µ–∫—Å—Ç –æ—Ç –Ω–∏—Ö –ø–æ–ª–Ω–æ—Å—Ç—å—é. "
        "3. –ò–º–µ–Ω–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ, –Ω–∞–∑–≤–∞–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤ –ø–∏—à–∏ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã. –ú–∞—Ä–∫–∏ –ø–∏—à–∏ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –º–∞—Ä–æ—á–Ω–∏–∫–∞–º–∏. –ì–û–°–¢ –≤—Å–µ–≥–¥–∞ –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏."
    )

    # === 3. –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–¨–°–ö–ò–ô –ü–†–û–ú–¢ (–í–∞—à –≤–∞—Ä–∏–∞–Ω—Ç + –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ) ===
    user_prompt = f"""
    –ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:
    –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: "{tag_name}"
    –ë–∞–∑–æ–≤—ã–π —Ç–µ–∫—Å—Ç (—Ñ–∞–∫—Ç—É—Ä–∞): \"\"\"{base_text[:3500]}\"\"\"
    
    {seo_instruction_block}
    
    –ó–ê–î–ê–ß–ê:
    –ù–∞–ø–∏—à–∏ {num_blocks} HTML-–±–ª–æ–∫–æ–≤, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º: |||BLOCK_SEP|||
    
    –û–ë–©–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
    1. –û–ë–™–ï–ú: –ö–∞–∂–¥—ã–π –±–ª–æ–∫ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∞–∫—Å–∏–º—É–º 800 —Å–∏–º–≤–æ–ª–æ–≤. –†–∞—Å–∫—Ä—ã–≤–∞–π —Ç–µ–º—É –ø–æ–¥—Ä–æ–±–Ω–æ.
    2. –ß–ò–°–¢–û–¢–ê: –ò—Å–∫–ª—é—á–∏ –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.
    3. –ü–û–õ–¨–ó–ê: –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –≥—Ä–∞–º–æ—Ç–Ω—ã–º –∏ –ø–æ–ª–µ–∑–Ω—ã–º –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –ø–æ –∑–∞–∫—É–ø–∫–∞–º. –ò–∑–±–µ–≥–∞–π "–≤–æ–¥—ã".
    
    –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –°–¢–†–£–ö–¢–£–†–ï –ö–ê–ñ–î–û–ì–û –ë–õ–û–ö–ê:
    –ö–∞–∂–¥—ã–π –∏–∑ {num_blocks} –±–ª–æ–∫–æ–≤ –¥–æ–ª–∂–µ–Ω —Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –ø–æ—Ä—è–¥–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤:
    1. –ó–∞–≥–æ–ª–æ–≤–æ–∫ (<h2> —Ç–æ–ª—å–∫–æ –¥–ª—è 1-–≥–æ –±–ª–æ–∫–∞, <h3> –¥–ª—è –±–ª–æ–∫–æ–≤ 2-{num_blocks}).
    2. –ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞ (<p>) - —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π.
    3. –í–≤–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –ø–æ–¥–≤–æ–¥—è—â–µ–µ –∫ —Å–ø–∏—Å–∫—É (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:", "–°—Ñ–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è:").
    4. –ú–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ (<ul> c <li>).
    5. –í—Ç–æ—Ä–æ–π (–∑–∞–≤–µ—Ä—à–∞—é—â–∏–π) –∞–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞ (<p>) - —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π.
    
    –¢–ï–ú–´ –ë–õ–û–ö–û–í:
    --- –ë–õ–û–ö 1 (–í–≤–æ–¥–Ω—ã–π) ---
    - –ó–∞–≥–æ–ª–æ–≤–æ–∫: <h2>{forced_header}</h2>
    - –û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞, –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ, –∫–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏.
    
    --- –ë–õ–û–ö–ò 2, 3, 4, 5 (–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏) ---
    - –ó–∞–≥–æ–ª–æ–≤–∫–∏: <h3> (–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ, –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ, –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏, –°–æ—Ä—Ç–∞–º–µ–Ω—Ç –∏ —Ç.–¥.).
    - –ò—Å–ø–æ–ª—å–∑—É–π —Ñ–∞–∫—Ç—É—Ä—É –∏–∑ "–ë–∞–∑–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞".
    
    –§–ò–ù–ê–õ–¨–ù–´–ï –£–°–õ–û–í–ò–Ø:
    - –ù–∏–∫–∞–∫–∏—Ö –≤–≤–æ–¥–Ω—ã—Ö —Å–ª–æ–≤ —Ç–∏–ø–∞ "–í–æ—Ç –≤–∞—à –∫–æ–¥".
    - –ù–∏–∫–∞–∫–æ–≥–æ Markdown (```).
    - –ù–ò–ö–ê–ö–û–ì–û –ñ–ò–†–ù–û–ì–û –¢–ï–ö–°–¢–ê.
    - –¢–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π HTML, —Ä–∞–∑–±–∏—Ç—ã–π —á–µ—Ä–µ–∑ |||BLOCK_SEP|||.
    """
    
    try:
        response = client.chat.completions.create(
            model="google/gemini-2.5-pro",
            messages=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3 
        )
        content = response.choices[0].message.content
        
        # === –ß–ò–°–¢–ö–ê –û–¢ MARKDOWN –ò –ú–£–°–û–†–ê ===
        content = re.sub(r'^```[a-zA-Z]*\s*', '', content.strip())
        content = re.sub(r'\s*```$', '', content.strip())
        
        blocks = [b.strip() for b in content.split("|||BLOCK_SEP|||") if b.strip()]
        
        cleaned_blocks = []
        for b in blocks:
            cb = re.sub(r'^```[a-zA-Z]*', '', b).strip().lstrip('`.').strip()
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª—å H2, –µ—Å–ª–∏ –ò–ò –µ–≥–æ –≤—Å–µ-—Ç–∞–∫–∏ –Ω–∞–ø–∏—Å–∞–ª
            cb = re.sub(r'^<h2.*?>.*?</h2>', '', cb, flags=re.DOTALL | re.IGNORECASE).strip()
            
            # === –§–ò–ó–ò–ß–ï–°–ö–û–ï –£–î–ê–õ–ï–ù–ò–ï –ñ–ò–†–ù–û–ì–û –¢–ï–ö–°–¢–ê ===
            # 1. –£–¥–∞–ª—è–µ–º Markdown –∂–∏—Ä–Ω—ã–π (**—Ç–µ–∫—Å—Ç**)
            cb = cb.replace("**", "")
            # 2. –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏ –∂–∏—Ä–Ω–æ–≥–æ (<b>, </b>, <strong>, </strong>)
            cb = re.sub(r'</?(b|strong)>', '', cb, flags=re.IGNORECASE)
            
            if cb: cleaned_blocks.append(cb)
            
        while len(cleaned_blocks) < num_blocks: cleaned_blocks.append("")
        
        # === –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –í–°–¢–ê–í–ö–ê –ó–ê–ì–û–õ–û–í–ö–ê ===
        if cleaned_blocks:
            final_h2_text = forced_header if forced_header else tag_name
            cleaned_blocks[0] = f"<h2>{final_h2_text}</h2>\n{cleaned_blocks[0]}"

        return cleaned_blocks[:num_blocks]
        
    except Exception as e:
        return [f"API Error: {str(e)}"] * num_blocks

# ==========================================
# –ù–û–í–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø LSI –ì–ï–ù–ï–†–ê–¢–û–†–ê (–í–°–¢–ê–í–ò–¢–¨ –°–Æ–î–ê)
# ==========================================

def inflect_lsi_phrase(phrase, target_case):
    morph = pymorphy2.MorphAnalyzer()
    words = str(phrase).split()
    inflected_words = []
    for word in words:
        parsed_word = morph.parse(word)[0]
        try:
            inf_word = parsed_word.inflect({target_case})
            inflected_words.append(inf_word.word if inf_word else word)
        except: inflected_words.append(word)
    return " ".join(inflected_words)

def generate_random_date():
    start = datetime.datetime(2026, 1, 1)
    end = datetime.datetime(2026, 2, 10)
    delta = end - start
    return (start + datetime.timedelta(days=random.randrange(delta.days + 1))).strftime("%d.%m.%Y")

def build_review_from_repo(template, variables_dict, repo_fio, lsi_words):
    # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ LSI —Å–ª–æ–≤–æ
    lsi_word = random.choice(lsi_words) if lsi_words else ""
    
    # –ü—ã—Ç–∞–µ–º—Å—è –≤—Å—Ç–∞–≤–∏—Ç—å LSI —Å–ª–æ–≤–æ –í–ù–£–¢–†–ò —à–∞–±–ª–æ–Ω–∞ –≤–º–µ—Å—Ç–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    if lsi_word:
        parsed = morph.parse(lsi_word)[0]
        lsi_gender = parsed.tag.gender
        lsi_number = parsed.tag.number
        
        # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, {—Ç–æ–≤–∞—Ä_—Å—É—â_–º—É–∂})
        placeholders = re.findall(r'\{([^}]+)\}', template)
        found_slot = None
        for p in placeholders:
            is_product = any(x in p for x in ['—Ç–æ–≤–∞—Ä', '—Å—É—â', '–≤–∏–¥_–ø—Ä–æ–∫–∞—Ç–∞'])
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–æ–¥ –∏ —á–∏—Å–ª–æ, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ "–∫—É–ø–∏–ª–∏ —Ç—Ä—É–±–∞ (–º—É–∂.—Ä–æ–¥ —Å–ª–æ—Ç)"
            gender_ok = True
            if '_–º—É–∂' in p and lsi_gender != 'masc': gender_ok = False
            if '_–∂–µ–Ω' in p and lsi_gender != 'femn': gender_ok = False
            if '_–º–Ω—á' in p and lsi_number != 'plur': gender_ok = False
            
            if is_product and gender_ok:
                found_slot = p
                break
        
        if found_slot:
            # –°–∫–ª–æ–Ω—è–µ–º LSI –ø–æ–¥ –ø–∞–¥–µ–∂ —Å–ª–æ—Ç–∞
            target_case = 'nomn'
            case_map = {'_–≤–∏–Ω': 'accs', '_–í–ü': 'accs', '_—Ä–æ–¥': 'gent', '_—Ç–≤–æ—Ä–∏—Ç': 'ablt', '_–∏–º': 'nomn'}
            for sfx, c in case_map.items():
                if sfx in found_slot:
                    target_case = c
                    break
            inflected = inflect_lsi_phrase(lsi_word, target_case)
            template = template.replace(f"{{{found_slot}}}", f"**{inflected}**", 1)
        else:
            # –ï—Å–ª–∏ —Å–ª–æ—Ç–∞ –Ω–µ—Ç, –¥–µ–ª–∞–µ–º —Ä–∞–Ω–¥–æ–º–Ω—É—é –≤—Å—Ç–∞–≤–∫—É —á–µ—Ä–µ–∑ —Ç–≤–æ–∏ –∂–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
            v_intro = random.choice(variables_dict.get('–≤–≤–æ–¥–Ω–æ–µ_—Å–ª–æ–≤–æ', ['–ö—Å—Ç–∞—Ç–∏']))
            v_eval = random.choice(variables_dict.get('–æ—Ü–µ–Ω–∫–∞1_—Ö–∞—Ä_—Ç–æ–≤–∞—Ä_–µ–¥_–∏–º', ['–Ω–∞ –≤—ã—Å–æ—Ç–µ']))
            template += f" {v_intro}, **{lsi_word}** {v_eval}."

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    def replace_var(match):
        v = match.group(1).strip()
        if v == "–¥–∞—Ç–∞":
            # –¢–≤–æ–∏ –¥–∞—Ç—ã 2026 –≥–æ–¥–∞
            start = datetime.date(2026, 1, 1)
            return (start + datetime.timedelta(days=random.randint(0, 40))).strftime("%d.%m.%Y")
        if v in variables_dict:
            return str(random.choice(variables_dict[v])).strip()
        return match.group(0)

    final_draft = re.sub(r'\{([^}]+)\}', replace_var, template)
    
    # –§–ò–û –∏–∑ —Ç–≤–æ–µ–≥–æ —Ñ–∞–π–ª–∞
    fio_row = repo_fio.sample(1).iloc[0]
    author = f"{fio_row['–ò–º—è']} {fio_row['–§–∞–º–∏–ª–∏—è']}"
    
    return author, final_draft
# ==========================================
# –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ì–ï–ù–ï–†–ê–¶–ò–ò FAQ (–ñ–ò–†–ù–´–ô –®–†–ò–§–¢ + –ê–í–¢–û–¢–ò–†–ï)
# ==========================================
def generate_faq_gemini(api_key, h1, lsi_words, target_count=5):
    import json
    from openai import OpenAI
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç —á–µ—Ä–µ–∑ –≤–∞—à —Å—Ç–æ—Ä–æ–Ω–Ω–∏–π —Å–µ—Ä–≤–∏—Å
        client = OpenAI(api_key=api_key, base_url="https://litellm.tokengate.ru/v1")
    except Exception as e:
        return [{"–í–æ–ø—Ä–æ—Å": "–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ API", "–û—Ç–≤–µ—Ç": str(e)}]
    
    # === –ù–û–í–´–ô –ë–õ–û–ö: –ñ–ï–°–¢–ö–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø LSI –°–õ–û–í ===
    forbidden_roots = [
        "—É–∫—Ä–∞–∏–Ω", "ukrain", "ua", "–≤—Å—É", "–∑—Å—É", "–∞—Ç–æ", "—Å–≤–æ", "–≤–æ–π–Ω",
        "–∫–∏–µ–≤", "–ª—å–≤–æ–≤", "—Ö–∞—Ä—å–∫–æ–≤", "–æ–¥–µ—Å—Å", "–¥–Ω–µ–ø—Ä", "–º–∞—Ä–∏—É–ø–æ–ª—å",
        "–¥–æ–Ω–µ—Ü", "–ª—É–≥–∞–Ω—Å", "–¥–Ω—Ä", "–ª–Ω—Ä", "–¥–æ–Ω–±–∞—Å—Å", "–º–µ–ª–∏—Ç–æ–ø–æ–ª—å",
        "–±–µ—Ä–¥—è–Ω—Å–∫", "–±–∞—Ö–º—É—Ç", "–∑–∞–ø–æ—Ä–æ–∂—å", "—Ö–µ—Ä—Å–æ–Ω", "–∫—Ä—ã–º",
        "–ø–æ–ª–∏—Ç–∏–∫", "—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü"
    ]
    
    clean_lsi = []
    for w in lsi_words:
        w_lower = str(w).lower()
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤ –Ω—ë–º –ù–ï–¢ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö –∫–æ—Ä–Ω–µ–π
        if not any(root in w_lower for root in forbidden_roots):
            clean_lsi.append(w)
            
    lsi_text = ", ".join(clean_lsi)
    # ===============================================
    
    # --- –≠–¢–ê–ü 1: –ß–ï–†–ù–û–í–ò–ö ---
    prompt_1 = f"""
    –¢—ã —ç–∫—Å–ø–µ—Ä—Ç –≤ SEO –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–µ –∫–ª–∏–µ–Ω—Ç–æ–≤.
    –°–æ—Å—Ç–∞–≤—å FAQ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã "{h1}".
    
    –°–¢–†–û–ñ–ê–ô–®–ò–ô –ó–ê–ü–†–ï–¢:
    –ö–∞—Ç–µ–≥–æ—Ä–∏—á–µ—Å–∫–∏ –∑–∞–ø—Ä–µ—â–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±—ã–µ —Å–ª–æ–≤–∞, —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏ —Ç–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –£–∫—Ä–∞–∏–Ω–æ–π, —É–∫—Ä–∞–∏–Ω—Å–∫–∏–º–∏ –≥–æ—Ä–æ–¥–∞–º–∏, –≤–æ–π–Ω–æ–π –∏ –æ—Å—Ç—Ä–æ–π –ø–æ–ª–∏—Ç–∏–∫–æ–π. –£–ø–æ–º–∏–Ω–∞—Ç—å –†–æ—Å—Å–∏—é –∏ –¥—Ä—É–≥–∏–µ —Å—Ç—Ä–∞–Ω—ã ‚Äî –ú–û–ñ–ù–û. –ï—Å–ª–∏ –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º —Å–ø–∏—Å–∫–µ LSI —Å–ª—É—á–∞–π–Ω–æ –æ—Å—Ç–∞–ª–æ—Å—å –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Å–ª–æ–≤–æ ‚Äî –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä—É–π –µ–≥–æ!
    
    –£–°–õ–û–í–ò–Ø:
    1. –í–æ—Ç —Å–ø–∏—Å–æ–∫ LSI-—Å–ª–æ–≤: {lsi_text}
    –û—Ç—Ñ–∏–ª—å—Ç—Ä—É–π –º—É—Å–æ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞. –û—Ä–≥–∞–Ω–∏—á–Ω–æ –≤–ø–∏—à–∏ —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ —Å–º—ã—Å–ª—É.
    2. –í–´–î–ï–õ–ò –ñ–ò–†–ù–´–ú –®–†–ò–§–¢–û–ú (**—Å–ª–æ–≤–æ**) –≤—Å–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ LSI-—Å–ª–æ–≤–∞ –ø—Ä—è–º–æ –≤ —Ç–µ–∫—Å—Ç–µ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤!
    3. –ù–∞–ø–∏—à–∏ —Ä–æ–≤–Ω–æ {target_count} –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤.
    4. –í–ï–†–ù–ò –°–¢–†–û–ì–û –í –§–û–†–ú–ê–¢–ï JSON! –ú–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤: [{{"–í–æ–ø—Ä–æ—Å": "...", "–û—Ç–≤–µ—Ç": "..."}}]
    """
    try:
        res_1 = client.chat.completions.create(
            model="google/gemini-2.5-pro",
            messages=[{"role": "user", "content": prompt_1}],
            temperature=0.3
        )
        draft_text = res_1.choices[0].message.content.strip()
        
        # –û—á–∏—Å—Ç–∫–∞ Markdown –¥–ª—è JSON
        if draft_text.startswith("```json"): draft_text = draft_text[7:]
        if draft_text.startswith("```"): draft_text = draft_text[3:]
        if draft_text.endswith("```"): draft_text = draft_text[:-3]
        draft_text = draft_text.strip()
        
        # --- –≠–¢–ê–ü 2: –†–ï–î–ê–ö–¢–£–†–ê –ò –§–ê–ö–¢–ß–ï–ö–ò–ù–ì ---
        prompt_2 = f"""
        –Ø —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª —á–µ—Ä–Ω–æ–≤–∏–∫ FAQ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã "{h1}". –í–æ—Ç –æ–Ω (JSON):
        {draft_text}

        –í—ã—Å—Ç—É–ø–∏ –≤ —Ä–æ–ª–∏ —Å—Ç—Ä–æ–≥–æ–≥–æ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞.
        –ü–†–ê–í–ò–õ–ê:
        1. –°–¢–†–û–ì–ò–ô –ó–ê–ü–†–ï–¢: –í—ã—á–∏—Å—Ç–∏ –ª—é–±—ã–µ —Å–ª–µ–¥—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –£–∫—Ä–∞–∏–Ω—ã, –µ—ë –≥–æ—Ä–æ–¥–æ–≤, –≤–æ–π–Ω—ã –∏ –ø–æ–ª–∏—Ç–∏–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –ø–æ—è–≤–∏–ª–∏—Å—å. (–£–ø–æ–º–∏–Ω–∞–Ω–∏—è –†–æ—Å—Å–∏–∏ –¥–æ–ø—É—Å—Ç–∏–º—ã).
        2. –£–¥–∞–ª–∏ —Ç–∏–ø–∏—á–Ω—ã–µ —Ñ—Ä–∞–∑—ã –ò–ò ("–í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å", "–ö–æ–Ω–µ—á–Ω–æ, –≤–æ—Ç").
        3. –í—ã—á–∏—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –±—Ä–µ–¥–æ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –ø–æ —Ç–µ–º–µ "{h1}".
        4. –í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –∑–≤—É—á–∞—Ç—å –∂–∏–≤–æ, –∫–∞–∫ –∏—Ö —Ä–µ–∞–ª—å–Ω–æ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç –ª—é–¥–∏.
        5. –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –∏ –ø–æ–ª–µ–∑–Ω—ã–º–∏.
        6. –°–æ—Ö—Ä–∞–Ω–∏ –∑–∞–¥–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ ({target_count}).
        7. –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –°–û–•–†–ê–ù–ò –ò–õ–ò –î–û–ë–ê–í–¨ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –∂–∏—Ä–Ω—ã–º —à—Ä–∏—Ñ—Ç–æ–º (**—Å–ª–æ–≤–æ**) –¥–ª—è –≤—Å–µ—Ö LSI-—Å–ª–æ–≤!
        8. –í–ï–†–ù–ò –¢–û–õ–¨–ö–û –ì–û–õ–´–ô JSON-–ú–ê–°–°–ò–í! –ë–µ–∑ markdown-—Ä–∞–∑–º–µ—Ç–∫–∏ –±–ª–æ–∫–∞.
        """
        
        res_2 = client.chat.completions.create(
            model="google/gemini-2.5-pro",
            messages=[{"role": "user", "content": prompt_2}],
            temperature=0.3
        )
        final_text = res_2.choices[0].message.content.strip()
        
        # –û—á–∏—Å—Ç–∫–∞ Markdown –¥–ª—è JSON
        if final_text.startswith("```json"): final_text = final_text[7:]
        if final_text.startswith("```"): final_text = final_text[3:]
        if final_text.endswith("```"): final_text = final_text[:-3]
        final_text = final_text.strip()
        
        # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –≤ –æ–±—ä–µ–∫—Ç—ã Python
        parsed_data = json.loads(final_text)
        
        # --- –≠–¢–ê–ü 3: –°–ö–†–ò–ü–¢–û–í–ê–Ø –ê–í–¢–û–ó–ê–ú–ï–ù–ê –¢–ò–†–ï ---
        # –ñ–µ—Å—Ç–∫–æ –∑–∞–º–µ–Ω—è–µ–º –¥–ª–∏–Ω–Ω–æ–µ —Ç–∏—Ä–µ (‚Äî) –∏ –¥–µ—Ñ–∏—Å—ã —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ ( - ) –Ω–∞ —Ü–∏—Ñ—Ä–æ–≤–æ–µ —Ç–∏—Ä–µ (‚Äì, Alt+0150)
        for item in parsed_data:
            if "–í–æ–ø—Ä–æ—Å" in item:
                item["–í–æ–ø—Ä–æ—Å"] = item["–í–æ–ø—Ä–æ—Å"].replace("‚Äî", "‚Äì").replace(" - ", " ‚Äì ")
            if "–û—Ç–≤–µ—Ç" in item:
                item["–û—Ç–≤–µ—Ç"] = item["–û—Ç–≤–µ—Ç"].replace("‚Äî", "‚Äì").replace(" - ", " ‚Äì ")
                
        return parsed_data
        
    except Exception as e:
        return [{"–í–æ–ø—Ä–æ—Å": "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "–û—Ç–≤–µ—Ç": str(e)}]


def generate_full_article_v2(api_key, h1_marker, h2_topic, lsi_list):
    if not api_key: return "Error: No API Key"
    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key, base_url="https://litellm.tokengate.ru/v1")
    except ImportError: return "Error: Library 'openai' not installed"
    
    lsi_string = ", ".join(lsi_list)
    
    stop_words_list = (
        "—è–≤–ª—è–µ—Ç—Å—è, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π, –∫–ª—é—á–µ–≤–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç, —à–∏—Ä–æ–∫–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è, "
        "–æ–±–ª–∞–¥–∞—é—Ç, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—Ç—Å—è, –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ, —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä, "
        "–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π, —É–Ω–∏–∫–∞–ª—å–Ω—ã–π, –¥–∞–Ω–Ω—ã–π, —ç—Ç–æ—Ç, –∏–∑–¥–µ–ª–∏—è, –º–∞—Ç–µ—Ä–∏–∞–ª—ã, "
        "–≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –¥–æ—Å—Ç—É–ø–Ω–∞—è —Ü–µ–Ω–∞, –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥, "
        "–¥–æ—Å—Ç–∞–≤–∫–∞, –æ–ø–ª–∞—Ç–∞, —É—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏, –∑–≤–æ–Ω–∏—Ç–µ, –º–µ–Ω–µ–¥–∂–µ—Ä"
    )

    contact_html_block = (
        '–ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é —Å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–º –ø–æ –Ω–æ–º–µ—Ä—É '
        '<nobr><a href="tel:#PHONE#" onclick="ym(document.querySelector(\'#ya_counter\').getAttribute(\'data-counter\'),\'reachGoal\',\'tel\');gtag(\'event\', \'Click po nomeru telefona\', {{\'event_category\' : \'Click\', \'event_label\' : \'po nomeru telefona\'}});gtag(\'event\', \'Lead_Goal\', {{\'event_category\' : \'Click\', \'event_label\' : \'Leads Goal\'}});" class="a_404 ct_phone">#PHONE#</a></nobr>, '
        '–ª–∏–±–æ –ø–∏—à–∏—Ç–µ –Ω–∞ –ø–æ—á—Ç—É <a href="mailto:#EMAIL#" onclick="ym(document.querySelector(\'#ya_counter\').getAttribute(\'data-counter\'),\'reachGoal\',\'email\');gtag(\'event\', \'Click napisat nam\', {{\'event_category\' : \'Click\', \'event_label\' : \'napisat nam\'}});gtag(\'event\', \'Lead_Goal\', {{\'event_category\' : \'Click\', \'event_label\' : \'Leads Goal\'}});" class="a_404">#EMAIL#</a>.'
    )

    system_instruction = (
        "–¢—ã ‚Äî —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ä–µ–¥–∞–∫—Ç–æ—Ä. –¢—ã –ø–∏—à–µ—à—å —Ñ–∞–∫—Ç–∞–º–∏, —Å–≤—è–∑–Ω—ã–º —Ä—É—Å—Å–∫–∏–º —è–∑—ã–∫–æ–º. "
        "–¢—ã —Å–æ–±–ª—é–¥–∞–µ—à—å HTML-—Å—Ç—Ä—É–∫—Ç—É—Ä—É (—Å–ø–∏—Å–∫–∏ <ul>, —Ç–∞–±–ª–∏—Ü—ã). "
        "–¢—ã —É–º–µ–µ—à—å –≥—Ä–∞–º–æ—Ç–Ω–æ –≤–ø–∏—Å—ã–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –º–µ–Ω—è—è –∏—Ö —Ñ–æ—Ä–º—É –∏ –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤. "
        "–ï—Å–ª–∏ –Ω—É–∂–Ω–æ –≤–ø–∏—Å–∞—Ç—å –º–Ω–æ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ ‚Äî —Ç—ã —É–≤–µ–ª–∏—á–∏–≤–∞–µ—à—å –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞, —á—Ç–æ–±—ã –æ–Ω–∏ —Å–º–æ—Ç—Ä–µ–ª–∏—Å—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ."
    )
    
    # –í –í–ê–®–ï–ú –ü–†–û–ú–¢–ï:
    # {exact_h2} –≤ —Å—Ç–∞—Ä–æ–º –∫–æ–¥–µ ‚Äî —ç—Ç–æ —Ç–æ, –ø—Ä–æ —á—Ç–æ –ø–∏—Å–∞—Ç—å —Å—Ç–∞—Ç—å—é. 
    # –í –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–µ: 
    # - {h1_marker} ‚Äî —ç—Ç–æ –ø—Ä–µ–¥–º–µ—Ç —Å—Ç–∞—Ç—å–∏ (–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –¥–ª—è H3 –∏ —Ç–µ–∫—Å—Ç–∞).
    # - {h2_topic} ‚Äî —ç—Ç–æ —Ç–æ—á–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ H2.
    
    user_prompt = f"""
    –ó–ê–î–ê–ß–ê: –ù–∞–ø–∏—à–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é —Å—Ç–∞—Ç—å—é.
    
    [I] –ì–õ–ê–í–ù–´–ï –ü–†–ê–í–ò–õ–ê –†–ê–ë–û–¢–´ –° –ö–õ–Æ–ß–û–ú ("{h1_marker}"):
    
    1. –í –ó–ê–ì–û–õ–û–í–ö–ê–• H3 (–°–¢–†–û–ì–û –¶–ï–õ–ò–ö–û–ú):
       - –ó–¥–µ—Å—å —Ñ—Ä–∞–∑–∞ "{h1_marker}" –¥–æ–ª–∂–Ω–∞ —Å—Ç–æ—è—Ç—å –¶–ï–õ–ò–ö–û–ú (—Ä—è–¥–æ–º).
       - –ú–û–ñ–ù–û: –°–∫–ª–æ–Ω—è—Ç—å (–ú–æ–Ω—Ç–∞–∂ —Ç—Ä—É–±—ã —Å—Ç–∞–ª—å–Ω–æ–π).
       - –ù–ï–õ–¨–ó–Ø: –†–∞–∑—Ä—ã–≤–∞—Ç—å —Å–ª–æ–≤–∞ –∏–ª–∏ –∑–∞–º–µ–Ω—è—Ç—å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏.
       
    2. –í –¢–ï–ö–°–¢–ï –ò –ê–ë–ó–ê–¶–ê–• (–ú–Ø–ì–ö–û–ï –í–•–û–ñ–î–ï–ù–ò–ï):
       - –û–±—â–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≤ –∏–∑ –∫–ª—é—á–∞ ‚Äî 1.5%.
       - –í–ê–ñ–ù–û: –í —Ç–µ–∫—Å—Ç–µ –¢–´ –û–ë–Ø–ó–ê–ù –†–ê–ó–ë–ò–í–ê–¢–¨ —Ñ—Ä–∞–∑—É, –º–µ–Ω—è—Ç—å –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤.
       - –ü–õ–û–•–û: "–ö—É–ø–∏—Ç—å —Ç—Ä—É–±—É —Å—Ç–∞–ª—å–Ω—É—é –º–æ–∂–Ω–æ..." (—Å–ø–∞–º).
       - –•–û–†–û–®–û: "–°—Ç–∞–ª—å–Ω–∞—è –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å —Ç—Ä—É–±—ã –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç..." (—Ä–∞–∑–±–∏–ª —Å–ª–æ–≤–∞).
       - –•–û–†–û–®–û: "–î–ª—è —ç—Ç–æ–π —Ç—Ä—É–±—ã —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–∞ —Å—Ç–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞..." (–ø–æ–º–µ–Ω—è–ª –º–µ—Å—Ç–∞–º–∏).
       
    [II] –õ–û–ì–ò–ö–ê HTML (–°–¢–†–û–ì–û):
    
    1. –°–ü–ò–°–ö–ò:
       - <ol>: –¢–û–õ–¨–ö–û –¥–ª—è –ø–æ—à–∞–≥–æ–≤—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤.
       - <ul>: –î–õ–Ø –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö, –°–§–ï–†, –°–í–û–ô–°–¢–í (–°–ø–∏—Å–∫–∏ ‚Ññ1, ‚Ññ2, ‚Ññ3 ‚Äî –°–¢–†–û–ì–û <ul>).
       - –í–ê–ñ–ù–û: –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –¥–≤–æ–µ—Ç–æ—á–∏–µ (:) –í–ù–£–¢–†–ò –ø—É–Ω–∫—Ç–æ–≤ —Å–ø–∏—Å–∫–∞.
       
    2. –¢–ê–ë–õ–ò–¶–ê:
       - –ö–ª–∞—Å—Å: "brand-accent-table".
       - –®–∞–ø–∫–∞ —á–µ—Ä–µ–∑ <thead> –∏ <th>.

    [III] –°–¢–†–£–ö–¢–£–†–ê –¢–ï–ö–°–¢–ê:
    
    1.1. –ó–∞–≥–æ–ª–æ–≤–æ–∫: <h2>{h2_topic}</h2>.
    
    1.2. –ë–≠–ù–ì–ï–†: 3-4 —Å–≤—è–∑–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –û–ø–∏—à–∏ —Ç–æ–≤–∞—Ä "{h1_marker}" –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º —è–∑—ã–∫–æ–º (—á—Ç–æ —ç—Ç–æ, –ì–û–°–¢, –º–∞—Ç–µ—Ä–∏–∞–ª).
    
    1.3. –ê–±–∑–∞—Ü 1 + –ö–æ–Ω—Ç–∞–∫—Ç—ã: 
    {contact_html_block}
    
    1.4. –ü–æ–¥–≤–æ–¥–∫–∞ –∫ —Å–ø–∏—Å–∫—É 1 (:).
    
    1.5. –°–ø–∏—Å–æ–∫ ‚Ññ1 (6 –ø—É–Ω–∫—Ç–æ–≤): –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ü–ê–†–ê–ú–ï–¢–†–´.
    (–§–æ—Ä–º–∞—Ç: <ul>). "–ò" –º–µ–Ω—è–π –Ω–∞ –∑–∞–ø—è—Ç—É—é. –¶–∏—Ñ—Ä—ã –∑–Ω–∞—á–∞—â–∏–µ.
       
    1.6. –ê–±–∑–∞—Ü 2. –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞.
    
    1.7. –¢–ê–ë–õ–ò–¶–ê –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö (–°–ü–†–ê–í–û–ß–ù–ê–Ø):
    4-5 —Å—Ç—Ä–æ–∫. –ë–µ–∑ –¥—É–±–ª–µ–π —Å–ø–∏—Å–∫–∞ ‚Ññ1.
    –ò–°–ü–û–õ–¨–ó–£–ô –≠–¢–û–¢ –ö–û–î:
    <table class="brand-accent-table">
        <thead>
            <tr>
                <th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th>
                <th>–ó–Ω–∞—á–µ–Ω–∏–µ</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>–ì–û–°–¢ / –¢–£</td><td>[–î–∞–Ω–Ω—ã–µ]</td></tr>
            <tr><td>–ú–∞—Ä–∫–∞ —Å–ø–ª–∞–≤–∞</td><td>[–î–∞–Ω–Ω—ã–µ]</td></tr>
            <tr><td>[–ü–∞—Ä–∞–º–µ—Ç—Ä 3]</td><td>[–î–∞–Ω–Ω—ã–µ]</td></tr>
            <tr><td>[–ü–∞—Ä–∞–º–µ—Ç—Ä 4]</td><td>[–î–∞–Ω–Ω—ã–µ]</td></tr>
        </tbody>
    </table>
    
    1.8. –ü–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫ H3 (–®–ê–ë–õ–û–ù): 
    "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è {h1_marker} (—Ä–æ–¥. –ø–∞–¥–µ–∂, —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã)"
    (–¢—É—Ç –∫–ª—é—á —Ü–µ–ª–∏–∫–æ–º!).
    
    1.9. –ê–±–∑–∞—Ü 3. –í–∏–¥—ã, —Ç–∏–ø—ã. (–¢—É—Ç —Ä–∞–∑–±–∏–≤–∞–π –∫–ª—é—á).
    
    1.10. –ü–æ–¥–≤–æ–¥–∫–∞ –∫ —Å–ø–∏—Å–∫—É 2 (:).
    
    1.11. –°–ø–∏—Å–æ–∫ ‚Ññ2 (6 –ø—É–Ω–∫—Ç–æ–≤): –°–§–ï–†–´ –ü–†–ò–ú–ï–ù–ï–ù–ò–Ø.
    (–§–æ—Ä–º–∞—Ç: <ul>).
       
    1.12. –ê–±–∑–∞—Ü 4. –£—Å–ª–æ–≤–∏—è —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏.
                          
    1.13. –ü–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫ H3 (–®–ê–ë–õ–û–ù):
    "–ú–æ–Ω—Ç–∞–∂ {h1_marker} (—Ä–æ–¥. –ø–∞–¥–µ–∂)" –ò–õ–ò "–û–±—Ä–∞–±–æ—Ç–∫–∞ {h1_marker} (—Ä–æ–¥. –ø–∞–¥–µ–∂)".
    (–¢—É—Ç –∫–ª—é—á —Ü–µ–ª–∏–∫–æ–º!).
    
    1.14. –ê–±–∑–∞—Ü 5. –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è —Ä–∞–±–æ—Ç—ã.
    
    1.15. –ü–æ–¥–≤–æ–¥–∫–∞ –∫ —Å–ø–∏—Å–∫—É 3 (:).
    
    1.16. –°–ø–∏—Å–æ–∫ ‚Ññ3 (6 –ø—É–Ω–∫—Ç–æ–≤): –≠–ö–°–ü–õ–£–ê–¢–ê–¶–ò–û–ù–ù–´–ï –°–í–û–ô–°–¢–í–ê.
    (–ë–µ–∑ —Å–æ—é–∑–æ–≤ "–∏"). –§–æ—Ä–º–∞—Ç: <ul>.
       
    1.17. –ê–±–∑–∞—Ü 6. –†–µ–∑—é–º–µ –∏ –æ—Ç–≥—Ä—É–∑–∫–∞.

    [IV] –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û (LSI –Ø–î–†–û):
    –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤: {{{lsi_string}}}
    
    –ü–†–ê–í–ò–õ–ê LSI:
    1. –ò–°–ü–û–õ–¨–ó–£–ô –í–ï–°–¨ –°–ü–ò–°–û–ö (–û–±—â–∏–µ + –°–µ–º–∞–Ω—Ç–∏–∫–∞).
    2. –ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ ‚Äî –†–û–í–ù–û 1 –†–ê–ó (–Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π).
    3. –í—ã–¥–µ–ª—è–π –∫–∞–∂–¥–æ–µ –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ —Ç–µ–≥–æ–º <b>–∂–∏—Ä–Ω—ã–π</b>.
    4. –ï—Å–ª–∏ —Å–ª–æ–≤ –º–Ω–æ–≥–æ ‚Äî –£–í–ï–õ–ò–ß–ò–í–ê–ô –û–ë–™–ï–ú –¢–ï–ö–°–¢–ê. –ü–∏—à–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —á—Ç–æ–±—ã —Å–ª–æ–≤–∞ –≤–ø–∏—Å—ã–≤–∞–ª–∏—Å—å –ø–ª–∞–≤–Ω–æ, –∞ –Ω–µ "–æ–±—Ä—É–±–∫–∞–º–∏". –°–º—ã—Å–ª –∏ —Å–≤—è–∑–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏.

    [V] –°–¢–û–ü-–°–õ–û–í–ê: ({stop_words_list}).
    
    –í–´–í–û–î: –¢–û–õ–¨–ö–û HTML –ö–û–î.
    """
    
    try:
        response = client.chat.completions.create(
            model="google/gemini-2.5-pro",
            messages=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.25
        )
        content = response.choices[0].message.content
        content = re.sub(r'^```html', '', content.strip())
        content = re.sub(r'^```', '', content.strip())
        content = re.sub(r'```$', '', content.strip())
        
        # --- –û–ß–ò–°–¢–ö–ê ---
        # –¢–µ–≥–∏ <b> –ù–ï —É–¥–∞–ª—è–µ–º!
        content = content.replace(' - ', ' &ndash; ')
        content = content.replace('‚Äî', '&ndash;')
        content = content.replace('‚Äì', '&ndash;')
        content = content.replace('&mdash;', '&ndash;')
        content = content.replace('**', '').replace('__', '')
        
        return content
    except Exception as e:
        return f"API Error: {str(e)}"

def scrape_h1_h2_from_url(url):
    """
    –ó–∞—Ö–æ–¥–∏—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É, –∑–∞–±–∏—Ä–∞–µ—Ç H1 (–∫–∞–∫ –º–∞—Ä–∫–µ—Ä) –∏ –ø–µ—Ä–≤—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π H2.
    """
    # 1. –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ curl_cffi (—á—Ç–æ–±—ã –æ–±–æ–π—Ç–∏ 403)
    try:
        from curl_cffi import requests as cffi_requests
        r = cffi_requests.get(
            url, 
            impersonate="chrome110", 
            headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'},
            timeout=20
        )
        content = r.content
        encoding = r.encoding if r.encoding else 'utf-8'
    except:
        # 2. Fallback –Ω–∞ requests
        try:
            import requests
            import urllib3
            urllib3.disable_warnings()
            r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20, verify=False)
            content = r.content
            encoding = r.apparent_encoding
        except Exception as e:
            return None, None, f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}"

    if r.status_code != 200:
        return None, None, f"HTTP Error {r.status_code}"

    try:
        soup = BeautifulSoup(content, 'html.parser', from_encoding=encoding)
        
        # --- –ò–©–ï–ú H1 ---
        h1_tag = soup.find('h1')
        h1_text = h1_tag.get_text(strip=True) if h1_tag else ""
        
        # --- –ò–©–ï–ú H2 ---
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ H2 –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–≥–æ –±–ª–æ–∫–∞ (—á–∞—Å—Ç–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞)
        h2_text = ""
        content_div = soup.find('div', class_=re.compile(r'(desc|content|text|article)'))
        if content_div:
            h2_tag = content_div.find('h2')
            if h2_tag: h2_text = h2_tag.get_text(strip=True)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ, –±–µ—Ä–µ–º –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–≤—ã–π –ø–æ–ø–∞–≤—à–∏–π—Å—è H2 –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        if not h2_text:
            h2_tag = soup.find('h2')
            if h2_tag: h2_text = h2_tag.get_text(strip=True)
            
        # –ï—Å–ª–∏ H2 –≤–æ–æ–±—â–µ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º H1 –∫–∞–∫ —Ç–µ–º—É (fallback)
        if not h2_text:
            h2_text = h1_text

        if not h1_text:
            return None, None, "H1 –Ω–µ –Ω–∞–π–¥–µ–Ω"

        return h1_text, h2_text, "OK"

    except Exception as e:
        return None, None, f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}"

# === –§–ò–ö–° –û–®–ò–ë–ö–ò –í–ò–î–ñ–ï–¢–û–í (StreamlitAPIException) ===
if 'pending_widget_updates' in st.session_state:
    for k, v in st.session_state['pending_widget_updates'].items():
        st.session_state[k] = v
    del st.session_state['pending_widget_updates']
# ==========================================
# 7. UI TABS RESTRUCTURED
# ==========================================
def global_stop_callback():
    st.session_state.lsi_automode_active = False
    st.session_state.faq_automode_active = False
    st.session_state.reviews_automode_active = False # –î–æ–±–∞–≤–ª–µ–Ω–æ
    st.session_state.auto_run_active = False
    st.session_state.start_analysis_flag = False

tab_seo_main, tab_wholesale_main, tab_projects, tab_monitoring, tab_lsi_gen, tab_faq_gen, tab_reviews_gen = st.tabs(["üìä SEO –ê–Ω–∞–ª–∏–∑", "üè≠ –û–ø—Ç–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä", "üìÅ –ü—Ä–æ–µ–∫—Ç—ã", "üìâ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ–∑–∏—Ü–∏–π", "üìù LSI –¢–µ–∫—Å—Ç—ã", "‚ùì FAQ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä", "üí¨ –û—Ç–∑—ã–≤—ã"])

# ------------------------------------------
# TAB 1: SEO ANALYSIS (KEPT AS IS)
# ------------------------------------------
with tab_seo_main:
    col_main, col_sidebar = st.columns([65, 35])
    
    # === –õ–ï–í–ê–Ø –ö–û–õ–û–ù–ö–ê (–û–°–ù–û–í–ù–ê–Ø) ===
    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
        
        # –°–±—Ä–æ—Å –∫—ç—à–∞ –¥–ª—è —Å–ª–æ–≤–∞—Ä–µ–π
        if st.button("üßπ –û–±–Ω–æ–≤–∏—Ç—å —Å–ª–æ–≤–∞—Ä–∏ (–ö—ç—à)", key="clear_cache_btn"):
            st.cache_data.clear()
            st.rerun()

        my_input_type = st.radio("–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], horizontal=True, label_visibility="collapsed", key="my_page_source_radio")
        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            st.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã", placeholder="https://site.ru/catalog/tovar", label_visibility="collapsed", key="my_url_input")
        elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏–ª–∏ —Ç–µ–∫—Å—Ç", height=200, label_visibility="collapsed", placeholder="–í—Å—Ç–∞–≤—å—Ç–µ HTML", key="my_content_input")

        st.markdown("### –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å")
        st.text_input("–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞", label_visibility="collapsed", key="query_input")
        
        st.markdown("### –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        
        # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ-–ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è ---
        if st.session_state.get('force_radio_switch'):
            st.session_state["competitor_source_radio"] = "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"
            st.session_state['force_radio_switch'] = False
        # -----------------------------------------------

        source_type_new = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫", ["–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)", "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"], horizontal=True, label_visibility="collapsed", key="competitor_source_radio")
        source_type = "API" if "API" in source_type_new else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"
        
        if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
            # --- –í–°–¢–ê–í–ò–¢–¨ –≠–¢–û–¢ –ë–õ–û–ö –¢–£–¢ ---
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç —Ñ–∏–ª—å—Ç—Ä–∞
            if 'temp_update_urls' in st.session_state:
                st.session_state['persistent_urls'] = st.session_state['temp_update_urls']
                del st.session_state['temp_update_urls']

            # –ö–Ω–æ–ø–∫–∞ —Å–±—Ä–æ—Å–∞
            if st.session_state.get('analysis_done'):
                col_reset, _ = st.columns([1, 4])
                with col_reset:
                    if st.button("üîÑ –ù–æ–≤—ã–π –ø–æ–∏—Å–∫ (–°–±—Ä–æ—Å)", type="secondary"):
                        keys_to_clear = ['analysis_done', 'analysis_results', 'persistent_urls', 'excluded_urls_auto', 'detected_anomalies']
                        for k in keys_to_clear:
                            if k in st.session_state: del st.session_state[k]
                        st.rerun()

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–µ—Å–ª–∏ –Ω–µ—Ç)
            if 'persistent_urls' not in st.session_state:
                st.session_state['persistent_urls'] = ""

            has_exclusions = st.session_state.get('excluded_urls_auto') and len(st.session_state.get('excluded_urls_auto')) > 5
            
            if has_exclusions:
                c_url_1, c_url_2 = st.columns(2)
                with c_url_1:
                    # –ü–†–û–°–¢–û –í–ò–î–ñ–ï–¢. –ë–µ–∑ value=..., —Ç–∞–∫ –∫–∞–∫ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º key.
                    # –ó–Ω–∞—á–µ–Ω–∏–µ —Å–∞–º–æ –ø–æ–¥—Ç—è–Ω–µ—Ç—Å—è –∏–∑ st.session_state['persistent_urls']
                    st.text_area(
                        "‚úÖ –ê–∫—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã (–î–ª—è –∞–Ω–∞–ª–∏–∑–∞)", 
                        height=200, 
                        key="persistent_urls" 
                    )
                with c_url_2:
                    st.text_area(
                        "üö´ –ê–≤—Ç–æ-–∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ", 
                        height=200, 
                        value=st.session_state.get('excluded_urls_auto', ""),
                        disabled=True # –°–¥–µ–ª–∞–ª –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–º, —á—Ç–æ–±—ã –Ω–µ –ø—É—Ç–∞—Ç—å
                    )
            else:
                st.text_area(
                    "–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", 
                    height=200, 
                    key="persistent_urls"
                )

        # –ì–†–ê–§–ò–ö
        if st.session_state.get('analysis_done') and st.session_state.get('analysis_results'):
            results = st.session_state.analysis_results
            if 'relevance_top' in results and not results['relevance_top'].empty:
                st.markdown("<br>", unsafe_allow_html=True)
                with st.expander("üìä –ì—Ä–∞—Ñ–∏–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–ù–∞–∂–º–∏—Ç–µ, —á—Ç–æ–±—ã —Ä–∞—Å–∫—Ä—ã—Ç—å)", expanded=False):                    
                    graph_data = st.session_state.get('full_graph_data', results['relevance_top'])
                    render_relevance_chart(graph_data, unique_key="main")
                st.markdown("<br>", unsafe_allow_html=True)

        # --- –ö–ù–û–ü–ö–ê –ó–ê–ü–£–°–ö–ê ---
        def run_analysis_callback():
            saved_filter_state = st.session_state.get('settings_auto_filter', True)
            keys_to_clear = [
                'analysis_results', 'analysis_done', 'naming_table_df',
                'ideal_h1_result', 'gen_result_df', 'unified_excel_data',
                'detected_anomalies', 'serp_trend_info',
                'excluded_urls_auto'
            ]
            for k in keys_to_clear:
                if k in st.session_state: del st.session_state[k]
            st.session_state.settings_auto_filter = saved_filter_state
            for k in list(st.session_state.keys()):
                if k.endswith('_page'): st.session_state[k] = 1
            st.session_state.start_analysis_flag = True

        st.markdown("<br>", unsafe_allow_html=True) # –û—Ç—Å—Ç—É–ø –ø–µ—Ä–µ–¥ –∫–Ω–æ–ø–∫–æ–π
        st.button(
            "–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", 
            type="primary", 
            use_container_width=True, 
            key="start_analysis_btn",
            on_click=run_analysis_callback 
        )

    # === –ü–†–ê–í–ê–Ø –ö–û–õ–û–ù–ö–ê (–°–ê–ô–î–ë–ê–†) ===
    with col_sidebar:
        if not ARSENKIN_TOKEN:
             new_arsenkin = st.text_input("Arsenkin Token", type="password", key="input_arsenkin")
             if new_arsenkin: st.session_state.arsenkin_token = new_arsenkin; ARSENKIN_TOKEN = new_arsenkin 
        if not YANDEX_DICT_KEY:
             new_yandex = st.text_input("Yandex Dict Key", type="password", key="input_yandex")
             if new_yandex: st.session_state.yandex_dict_key = new_yandex; YANDEX_DICT_KEY = new_yandex
        
        st.markdown("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
        st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
        st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
        st.selectbox("–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞", list(REGION_MAP.keys()), key="settings_region")
        st.selectbox("–ö–æ–ª-–≤–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", [10, 20], index=0, key="settings_top_n")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–µ–∫–±–æ–∫—Å–æ–≤
        if "settings_noindex" not in st.session_state: st.session_state.settings_noindex = True
        if "settings_alt" not in st.session_state: st.session_state.settings_alt = False
        if "settings_numbers" not in st.session_state: st.session_state.settings_numbers = False
        if "settings_norm" not in st.session_state: st.session_state.settings_norm = True
        if "settings_auto_filter" not in st.session_state: st.session_state.settings_auto_filter = True

        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å <noindex>", key="settings_noindex")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", key="settings_alt")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", key="settings_numbers")
        st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", key="settings_norm")
        st.checkbox("–ê–≤—Ç–æ-—Ñ–∏–ª—å—Ç—Ä —Å–ª–∞–±—ã—Ö —Å–∞–π—Ç–æ–≤", key="settings_auto_filter", help="–°–∞–π—Ç—ã —Å –Ω–∏–∑–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã –≤ —Å–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö.")
        
        # === [–ò–ó–ú–ï–ù–ï–ù–ò–ï] –°–ü–ò–°–ö–ò –ü–ï–†–ï–ù–ï–°–ï–ù–´ –°–Æ–î–ê ===
        st.markdown("---")
        st.markdown("üõë **–ò—Å–∫–ª—é—á–µ–Ω–∏—è**")
        
        if "settings_excludes" not in st.session_state: st.session_state.settings_excludes = DEFAULT_EXCLUDE
        if "settings_stops" not in st.session_state: st.session_state.settings_stops = DEFAULT_STOPS

        st.text_area("–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–æ–º–µ–Ω—ã", height=100, key="settings_excludes", help="–î–æ–º–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–∞—Ä—Å–µ—Ä –ø—Ä–æ–ø—É—Å—Ç–∏—Ç —Å—Ä–∞–∑—É.")
        st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞", height=100, key="settings_stops", help="–°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–ø–∞–¥—É—Ç –≤ –∞–Ω–∞–ª–∏–∑.")
# ==========================================
    # –ë–õ–û–ö 1: –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
    # ==========================================
    if st.session_state.analysis_done and st.session_state.analysis_results:
        results = st.session_state.analysis_results
        
        d_score = results['my_score']['depth']
        w_score = results['my_score']['width']
        
        # –¶–≤–µ—Ç–∞ –±–∞–ª–ª–æ–≤
        w_color = "#2E7D32" if w_score >= 80 else ("#E65100" if w_score >= 50 else "#D32F2F")
        
        if 75 <= d_score <= 88:
            d_color = "#2E7D32"; d_status = "–ò–î–ï–ê–õ (–¢–æ–ø)"
        elif 88 < d_score <= 100:
            d_color = "#D32F2F"; d_status = "–ü–ï–†–ï–°–ü–ê–ú (–†–∏—Å–∫)"
        elif 55 <= d_score < 75:
            d_color = "#F9A825"; d_status = "–°—Ä–µ–¥–Ω—è—è"
        else:
            d_color = "#D32F2F"; d_status = "–ù–∏–∑–∫–∞—è"

        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        
        # –°—Ç–∏–ª–∏
        st.markdown("""
        <style>
            details > summary { list-style: none; }
            details > summary::-webkit-details-marker { display: none; }
            .details-card { background-color: #f8f9fa; border: 1px solid #e9ecef; border-radius: 8px; margin-bottom: 10px; }
            .card-summary { padding: 12px 15px; cursor: pointer; font-weight: 700; display: flex; justify-content: space-between; }
            .count-tag { background: #e5e7eb; padding: 2px 8px; border-radius: 10px; font-size: 12px; }
            .flat-card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; height: 340px; display: flex; flex-direction: column; }
            .flat-header { height: 50px; padding: 0 20px; font-weight: 700; border-bottom: 1px solid #f3f4f6; display: flex; align-items: center; justify-content: space-between; }
            .flat-content { flex-grow: 1; padding: 15px 20px; overflow-y: auto; font-size: 13px; line-height: 1.4; }
            .flat-footer { height: 150px; padding: 12px 20px; border-top: 1px solid #f3f4f6; background: #fafafa; }
            .flat-len-badge { padding: 2px 8px; border-radius: 4px; font-weight: 700; font-size: 10px; }
            .flat-miss-tag { border: 1px solid #fecaca; color: #991b1b; padding: 2px 6px; font-size: 11px; border-radius: 4px; margin: 2px; display: inline-block; }
        </style>
        """, unsafe_allow_html=True)

# –í—ã–≤–æ–¥ –û–¢–õ–ê–î–ö–ò –¥–ª—è –®–∏—Ä–∏–Ω—ã (—á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –ø–æ—á–µ–º—É 95)
        if 'debug_width' in results:
            found = results['debug_width']['found']
            needed = results['debug_width']['needed']
            pct = int((found / needed * 100)) if needed > 0 else 0
            st.caption(f"üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –®–∏—Ä–∏–Ω—ã: –ù–∞–π–¥–µ–Ω–æ **{found}** –∏–∑ **{needed}** –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ ({pct}%).")
        
        # –í—ã–≤–æ–¥ –±–∞–ª–ª–æ–≤
        st.markdown(f"""
        <div style='display: flex; gap: 20px; flex-wrap: wrap; margin-bottom: 20px;'>
            <div style='flex: 1; background:{LIGHT_BG_MAIN}; padding:15px; border-radius:8px; border-left: 5px solid {w_color};'>
                <div style='font-size: 12px; color: #666;'>–®–ò–†–ò–ù–ê (–û—Ö–≤–∞—Ç —Ç–µ–º)</div>
                <div style='font-size: 24px; font-weight: bold; color: {w_color};'>{w_score}/100</div>
            </div>
            <div style='flex: 1; background:{LIGHT_BG_MAIN}; padding:15px; border-radius:8px; border-left: 5px solid {d_color};'>
                <div style='font-size: 12px; color: #666;'>–ì–õ–£–ë–ò–ù–ê (–¶–µ–ª—å: ~80)</div>
                <div style='font-size: 24px; font-weight: bold; color: {d_color};'>{d_score}/100 <span style='font-size:14px; font-weight:normal;'>({d_status})</span></div>
            </div>
        </div>
        """, unsafe_allow_html=True)

        # --- –†–ê–°–ß–ï–¢ META (–ß—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å –∏—Ö –ø–µ—Ä–≤—ã–º–∏) ---
        my_data_saved = st.session_state.get('saved_my_data')
        meta_res = None
        
        if 'raw_comp_data' in st.session_state and my_data_saved:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            s_meta = {
                'noindex': True, 'alt_title': False, 'numbers': False, 'norm': True, 
                'ua': "Mozilla/5.0", 'custom_stops': st.session_state.get('settings_stops', "").split()
            }
            meta_res = analyze_meta_gaps(st.session_state['raw_comp_data'], my_data_saved, s_meta)

        # --- –í–´–í–û–î META DASHBOARD (–ö–ê–†–¢–û–ß–ö–ò) ---
        if meta_res:
            st.markdown("### üß¨ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ Title, Description –∏ H1")
            
            # –•–µ–ª–ø–µ—Ä—ã –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏
            def check_len_status(text, type_key):
                length = len(text) if text else 0
                limits = {'Title': (30, 70), 'Description': (150, 250), 'H1': (20, 60)}
                mn, mx = limits.get(type_key, (0,0))
                if mn <= length <= mx: return length, "–•–û–†–û–®–û", "#059669", "#ECFDF5"
                return length, "–ü–õ–û–•–û", "#DC2626", "#FEF2F2"

            def render_flat_card(col, label, type_key, icon, txt, score, missing):
                length, status, col_txt, col_bg = check_len_status(txt, type_key)
                rel_col = "#10B981" if score >= 90 else ("#F59E0B" if score >= 50 else "#EF4444")
                
                miss_html = ""
                if missing:
                    tags = "".join([f'<span class="flat-miss-tag">{w}</span>' for w in missing[:10]])
                    miss_html = f"<div style='margin-top:5px;'>{tags}</div>"
                else:
                    miss_html = "<div style='color:#059669; font-weight:bold; margin-top:10px;'>‚úî –í—Å—ë –æ—Ç–ª–∏—á–Ω–æ</div>"

                html = f"""
                <div class="flat-card">
                    <div class="flat-header">
                        <div>{icon} {label}</div>
                        <span class="flat-len-badge" style="background:{col_bg}; color:{col_txt}">{length} –∑–Ω.</span>
                    </div>
                    <div class="flat-content">{txt if txt else '<span style="color:#ccc">–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö</span>'}</div>
                    <div class="flat-footer">
                        <div style="display:flex; justify-content:space-between; font-weight:bold; font-size:11px; color:#9ca3af;">
                            <span>–†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–¨</span> 
                            <span style="color:{rel_col}">{score}%</span>
                        </div>
                        <div style="width:100%; height:6px; background:#e5e7eb; border-radius:3px; margin-top:5px; overflow:hidden;">
                            <div style="width:{score}%; height:100%; background:{rel_col};"></div>
                        </div>
                        {miss_html}
                    </div>
                </div>
                """
                col.markdown(html, unsafe_allow_html=True)

            c1, c2, c3 = st.columns(3)
            m_s = meta_res['scores']; m_m = meta_res['missing']; m_d = meta_res['my_data']
            
            render_flat_card(c1, "Title", "Title", "üìë", m_d['Title'], m_s['title'], m_m['title'])
            render_flat_card(c2, "Description", "Description", "üìù", m_d['Description'], m_s['desc'], m_m['desc'])
            render_flat_card(c3, "H1 –ó–∞–≥–æ–ª–æ–≤–æ–∫", "H1", "#Ô∏è‚É£", m_d['H1'], m_s['h1'], m_m['h1'])
            
            st.markdown("<br>", unsafe_allow_html=True)

# 1. –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ï –Ø–î–†–û
        with st.expander("üõí –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ", expanded=True):
            if not st.session_state.get('orig_products') and not st.session_state.get('categorized_general'):
                st.info("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑.")
            else:
                # --- –§–£–ù–ö–¶–ò–Ø –ü–ï–†–ï–°–ß–ï–¢–ê (CALLBACK) ---
                def sync_semantics_with_stoplist():
                    # 1. –°—á–∏—Ç—ã–≤–∞–µ–º, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Å—Ç–∞–≤–∏–ª/–Ω–∞–ø–∏—Å–∞–ª –≤ –ø–æ–ª–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
                    raw_input = st.session_state.get('sensitive_words_input_final', "")
                    # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç (–º–Ω–æ–∂–µ—Å—Ç–≤–æ) –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞, –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
                    current_stop_set = set(w.strip().lower() for w in raw_input.split('\n') if w.strip())

                    # 2. –ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã–µ —Å–ø–∏—Å–∫–∏ –∏–∑ –ú–∞—Å—Ç–µ—Ä-—Å–ø–∏—Å–∫–æ–≤ (orig_...)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º: –µ—Å–ª–∏ —Å–ª–æ–≤–∞ –Ω–µ—Ç –≤ —Å—Ç–æ–ø-–ª–∏—Å—Ç–µ ‚Äî –æ–Ω–æ –∏–¥–µ—Ç –≤ —Ä–∞–±–æ—Ç—É
                    st.session_state.categorized_products = [w for w in st.session_state.orig_products if w.lower() not in current_stop_set]
                    st.session_state.categorized_services = [w for w in st.session_state.orig_services if w.lower() not in current_stop_set]
                    st.session_state.categorized_commercial = [w for w in st.session_state.orig_commercial if w.lower() not in current_stop_set]
                    st.session_state.categorized_geo = [w for w in st.session_state.orig_geo if w.lower() not in current_stop_set]
                    st.session_state.categorized_dimensions = [w for w in st.session_state.orig_dimensions if w.lower() not in current_stop_set]
                    st.session_state.categorized_general = [w for w in st.session_state.orig_general if w.lower() not in current_stop_set]

                    # 3. –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º (—á—Ç–æ–±—ã –º—É—Å–æ—Ä –Ω–µ –ø–æ–ø–∞–ª –≤ —Ç–µ–≥–∏)
                    all_active_products = st.session_state.categorized_products
                    if len(all_active_products) < 20:
                        st.session_state.auto_tags_words = all_active_products
                        st.session_state.auto_promo_words = []
                    else:
                        mid = math.ceil(len(all_active_products) / 2)
                        st.session_state.auto_tags_words = all_active_products[:mid]
                        st.session_state.auto_promo_words = all_active_products[mid:]
                    
                    st.toast("–°–ø–∏—Å–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã!", icon="‚úÖ")

                # --- –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –ö–ê–†–¢–û–ß–ï–ö ---
                c1, c2, c3 = st.columns(3)
                with c1: render_clean_block("–¢–æ–≤–∞—Ä—ã", "üß±", st.session_state.categorized_products)
                with c2: render_clean_block("–ì–µ–æ", "üåç", st.session_state.categorized_geo)
                with c3: render_clean_block("–ö–æ–º–º–µ—Ä—Ü–∏—è", "üí∞", st.session_state.categorized_commercial)
                
                c4, c5, c6 = st.columns(3)
                with c4: render_clean_block("–£—Å–ª—É–≥–∏", "üõ†Ô∏è", st.session_state.categorized_services)
                with c5: render_clean_block("–†–∞–∑–º–µ—Ä—ã/–ì–û–°–¢", "üìè", st.session_state.categorized_dimensions)
                with c6: render_clean_block("–û–±—â–∏–µ", "üìÇ", st.session_state.categorized_general)

                # --- –ë–õ–û–ö –°–¢–û–ü-–°–õ–û–í (–†–ï–î–ê–ö–¢–ò–†–£–ï–ú–´–ô) ---
                st.markdown("<br>", unsafe_allow_html=True)
                st.markdown("#### üõë –°—Ç–æ–ø-–ª–∏—Å—Ç")
                st.caption("–°—é–¥–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ø–∞–ª–∏ —Å–ª–æ–≤–∞ –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Å–ø–∏—Å–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤, –æ–Ω–∏ –Ω–µ –±—É–¥—É—Ç –≤–ª–∏—è—Ç—å –Ω–∞ —Ä–∞—Å—á–µ—Ç—ã –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ. –î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ –∏–ª–∏ —É–¥–∞–ª–∏—Ç–µ –ª–∏—à–Ω–∏–µ.")

                col_text, col_btn = st.columns([4, 1])
                
                with col_text:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º key, —á—Ç–æ–±—ã –∑–Ω–∞—á–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω—è–ª–æ—Å—å –≤ session_state
                    st.text_area(
                        "–°–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π",
                        height=150,
                        key="sensitive_words_input_final", 
                        label_visibility="collapsed"
                    )
                
                with col_btn:
                    st.write("") # –û—Ç—Å—Ç—É–ø
                    st.button(
                        "üîÑ –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∏ –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å", 
                        type="primary", 
                        use_container_width=True,
                        on_click=sync_semantics_with_stoplist
                    )
                    st.info("–£–¥–∞–ª–∏—Ç–µ —Å–ª–æ–≤–æ –∏–∑ —Å–ø–∏—Å–∫–∞ —Å–ª–µ–≤–∞, —á—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å –µ–≥–æ –≤ –≥—Ä—É–ø–ø—ã –≤—ã—à–µ.")

        # 2. –¢–ê–ë–õ–ò–¶–ê –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò
        with st.expander("üèÜ 4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–¢–∞–±–ª–∏—Ü–∞)", expanded=True):
            render_paginated_table(results['relevance_top'], "4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", "tbl_rel", 
                                   default_sort_col="–ü–æ–∑–∏—Ü–∏—è", default_sort_order="–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ", show_controls=False)

        st.markdown("<br>", unsafe_allow_html=True)
        st.caption("üëá –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")

        # 3. –ù–ê–ô–ú–ò–ù–ì
        with st.expander("üè∑Ô∏è –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ç–æ–≤–∞—Ä–æ–≤", expanded=False):
            if 'naming_table_df' in st.session_state and not st.session_state.naming_table_df.empty:
                st.dataframe(st.session_state.naming_table_df, use_container_width=True, hide_index=True)
            else:
                st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")

        # 4. –î–ï–¢–ê–õ–ò META (–¢–ê–ë–õ–ò–¶–ê) - –í–û–¢ –¢–£–¢ –ë–´–õ–ê –û–®–ò–ë–ö–ê
        with st.expander("üïµÔ∏è –ú–µ—Ç–∞-–¥–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤", expanded=False):
            # –í—Å—Ç–∞–≤–ª—è–µ–º –∑–∞—â–∏—Ç—É: –µ—Å–ª–∏ meta_res –Ω–µ—Ç, –Ω–µ —Å—Ç—Ä–æ–∏–º —Ç–∞–±–ª–∏—Ü—É
            if meta_res and 'detailed' in meta_res:
                df_meta_table = pd.DataFrame(meta_res['detailed'])
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É "–í–∞—à —Å–∞–π—Ç"
                my_row = pd.DataFrame([{
                    'URL': '–í–ê–® –°–ê–ô–¢', 
                    'Title': meta_res['my_data']['Title'], 
                    'Description': meta_res['my_data']['Description'], 
                    'H1': meta_res['my_data']['H1']
                }])
                df_meta_table = pd.concat([my_row, df_meta_table], ignore_index=True)
                
                st.dataframe(
                    df_meta_table, 
                    use_container_width=True, 
                    column_config={
                        "URL": st.column_config.LinkColumn("–°—Å—ã–ª–∫–∞"),
                        "Title": st.column_config.TextColumn("Title", width="medium"),
                        "Description": st.column_config.TextColumn("Description", width="large"),
                        "H1": st.column_config.TextColumn("H1", width="small"),
                    }
                )
            else:
                st.warning("–î–∞–Ω–Ω—ã–µ –ø–æ –º–µ—Ç–∞-—Ç–µ–≥–∞–º –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã (–≤–æ–∑–º–æ–∂–Ω–æ, –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ).")

# ==================================================================
            # üî• HOOK –î–õ–Ø LSI –ì–ï–ù–ï–†–ê–¢–û–†–ê (–í–ö–õ–ê–î–ö–ê 5) - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø v2
            # ==================================================================
            if st.session_state.get('lsi_automode_active'):
                
                # 1. –î–æ—Å—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–µ–∫—É—â–µ–π –∑–∞–¥–∞—á–∏
                current_idx = st.session_state.get('lsi_processing_task_id')
                
                # –ó–∞—â–∏—Ç–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –æ—á–µ—Ä–µ–¥–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞
                if 'bg_tasks_queue' not in st.session_state or current_idx is None or current_idx >= len(st.session_state.bg_tasks_queue):
                    st.session_state.lsi_automode_active = False
                    st.success("–í—Å–µ –∑–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã (–∏–ª–∏ –æ—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞)!")
                    st.stop()

                task = st.session_state.bg_tasks_queue[current_idx]
                
                # 2. –î–æ—Å—Ç–∞–µ–º LSI (TF-IDF) –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
                lsi_words = []
                
                # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–ï–†–ï–ú –ò–ó SESSION_STATE, –ê –ù–ï –ò–ó –õ–û–ö–ê–õ–¨–ù–û–ô –ü–ï–†–ï–ú–ï–ù–ù–û–ô ---
                results_data = st.session_state.get('analysis_results')
                
                if results_data and results_data.get('hybrid') is not None and not results_data['hybrid'].empty:
                    # –ë–µ—Ä–µ–º —Ç–æ–ø-15 —Å–ª–æ–≤
                    lsi_words = results_data['hybrid'].head(15)['–°–ª–æ–≤–æ'].tolist()
                # --------------------------------------------------------------------------
                
                # 3. –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
                common_lsi = ["–≥–∞—Ä–∞–Ω—Ç–∏—è", "–¥–æ—Å—Ç–∞–≤–∫–∞", "—Ü–µ–Ω–∞", "–∫—É–ø–∏—Ç—å", "–æ–ø—Ç–æ–º", "–≤ –Ω–∞–ª–∏—á–∏–∏"] 
                combined_lsi = list(set(common_lsi + lsi_words))
                
# 4. –ì–ï–ù–ï–†–ò–†–£–ï–ú –°–¢–ê–¢–¨–Æ
                # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—â–µ–º –∫–ª—é—á –≤–µ–∑–¥–µ ---
                api_key_gen = st.session_state.get('gemini_key_persistent')
                if not api_key_gen:
                    api_key_gen = st.session_state.get('bulk_api_key_v3')
                if not api_key_gen:
                    try: api_key_gen = st.secrets["GEMINI_KEY"]
                    except: pass
                
                html_out = ""
                status_code = "Error"
                
                if not api_key_gen:
                    html_out = "–û—à–∏–±–∫–∞: –ù–µ—Ç API –∫–ª—é—á–∞ Gemini (–≤–≤–µ–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ 5). –ù–∞–∂–º–∏—Ç–µ Enter –ø–æ—Å–ª–µ –≤–≤–æ–¥–∞ –∫–ª—é—á–∞!"
                    status_code = "Key Error"
                else:
                    try:
                        html_out = generate_full_article_v2(api_key_gen, task['h1'], task['h2'], combined_lsi)
                        status_code = "OK"
                    except Exception as e:
                        html_out = f"Error generating: {e}"
                        status_code = "Gen Error"

                # 5. –°–û–•–†–ê–ù–Ø–ï–ú –†–ï–ó–£–õ–¨–¢–ê–¢ –í –°–ü–ò–°–û–ö –í–ö–õ–ê–î–ö–ò 5
                if 'bg_results' not in st.session_state:
                    st.session_state.bg_results = []
                    
                found_existing = False
                for existing_res in st.session_state.bg_results:
                    if existing_res['h1'] == task['h1'] and existing_res['h2'] == task['h2']:
                        existing_res['content'] = html_out
                        existing_res['lsi_added'] = lsi_words
                        existing_res['status'] = status_code
                        found_existing = True
                        break
                
                if not found_existing:
                    st.session_state.bg_results.append({
                        "h1": task['h1'],
                        "h2": task['h2'],
                        "source_url": task.get('source_url', '-'),
                        "lsi_added": lsi_words,
                        "content": html_out,
                        "status": status_code
                    })

                # 6. –ü–õ–ê–ù–ò–†–£–ï–ú –°–õ–ï–î–£–Æ–©–£–Æ –ó–ê–î–ê–ß–£
                next_task_idx = current_idx + 1
                
                if next_task_idx < len(st.session_state.bg_tasks_queue):
                    next_task = st.session_state.bg_tasks_queue[next_task_idx]
                    
                    st.toast(f"‚úÖ –ì–æ—Ç–æ–≤–æ: {task['h1']}. –ü–µ—Ä–µ—Ö–æ–¥ –∫: {next_task['h1']}...")
                    
                    # === –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û–ï –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï ===
                    # –ï—Å–ª–∏ –∫–ª—é—á –≤–¥—Ä—É–≥ —É–¥–∞–ª–∏–ª—Å—è, –Ω–æ –µ—Å—Ç—å –≤ secrets –∏–ª–∏ –¥—Ä—É–≥–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π - –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
                    if 'bulk_api_key_v3' not in st.session_state:
                         # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤ persist –∏–ª–∏ secrets
                         recovered = st.session_state.get('gemini_key_persistent') or st.secrets.get("GEMINI_KEY", "")
                         if recovered:
                             st.session_state.bulk_api_key_v3 = recovered

# === –¢–û–ß–ï–ß–ù–ê–Ø –û–ß–ò–°–¢–ö–ê –°–¢–ê–†–´–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
                    keys_to_clear = [
                        'analysis_results', 'analysis_done', 'naming_table_df', 
                        'ideal_h1_result', 'raw_comp_data', 'full_graph_data',
                        'detected_anomalies', 'serp_trend_info', 'excluded_urls_auto'
                    ]
                    for k in keys_to_clear:
                        st.session_state.pop(k, None)
                        
                    # –£–°–¢–ê–ù–û–í–ö–ê –ü–ê–†–ê–ú–ï–¢–†–û–í –î–õ–Ø –°–õ–ï–î–£–Æ–©–ï–ì–û
                    st.session_state['pending_widget_updates'] = {
                        'query_input': next_task['h1'],
                        'competitor_source_radio': "–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)",
                        'my_page_source_radio': "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã",
                        'my_url_input': ""
                    }
                    st.session_state['lsi_processing_task_id'] = next_task_idx
                    st.session_state['start_analysis_flag'] = True 
                    st.session_state['analysis_done'] = False
                    
                    time.sleep(0.5)
                    st.rerun()
                    
                else:
                    st.session_state.lsi_automode_active = False
                    st.balloons()
                    st.success("üèÅ –í–°–ï –ó–ê–î–ê–ß–ò –í –û–ß–ï–†–ï–î–ò –û–ë–†–ê–ë–û–¢–ê–ù–´! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –≤–∫–ª–∞–¥–∫–µ 5.")
            
            # ==================================================================

# 5. –£–ü–£–©–ï–ù–ù–ê–Ø –°–ï–ú–ê–ù–¢–ò–ö–ê + –ú–û–¢–û–† –ê–í–¢–û–ú–ê–¢–ò–ó–ê–¶–ò–ò
        high = results.get('missing_semantics_high', [])
        low = results.get('missing_semantics_low', [])
        
        if high or low:
            total_missing = len(high) + len(low)
            with st.expander(f"üß© –£–ø—É—â–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ ({total_missing})", expanded=False):
                if high: 
                    words_high = ", ".join([x['word'] for x in high])
                    st.markdown(f"<div style='background:#EBF5FF; padding:12px; border-radius:8px; border:1px solid #BFDBFE; color:#1E40AF; margin-bottom:10px;'><b>üî• –í–∞–∂–Ω—ã–µ:</b><br>{words_high}</div>", unsafe_allow_html=True)
                if low: 
                    words_low = ", ".join([x['word'] for x in low])
                    st.markdown(f"<div style='background:#F8FAFC; padding:12px; border-radius:8px; border:1px solid #E2E8F0; color:#475569;'><b>üî∏ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ:</b><br>{words_low}</div>", unsafe_allow_html=True)

            # --- –í–û–¢ –≠–¢–û–¢ –ö–£–°–û–ö –û–ñ–ò–í–õ–Ø–ï–¢ –°–ö–†–ò–ü–¢ ---
            if st.session_state.get('lsi_automode_active'):
                with st.status("üõ†Ô∏è –†–∞–±–æ—Ç–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∫–∞: –ø–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö...", expanded=True) as status:
                    
                    # 1. –ó–∞–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞
                    st.write("üìù –°—á–∏—Ç—ã–≤–∞—é —Ç–∞–±–ª–∏—Ü—É TF-IDF...")
                    current_lsi_words = [x['word'] for x in high] if high else []
                    
                    # 2. –ò—â–µ–º —Ç–µ–∫—É—â—É—é –∑–∞–¥–∞—á—É
                    t_id = st.session_state.get('lsi_processing_task_id', 0)
                    if t_id < len(st.session_state.bg_tasks_queue):
                        task = st.session_state.bg_tasks_queue[t_id]
                        
                        # 3. –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤–æ –≤–∫–ª–∞–¥–∫—É 5
                        st.write(f"üìÇ –°–æ—Ö—Ä–∞–Ω—è—é LSI –¥–ª—è: **{task['h1']}**")
                        
                        new_rec = {
                            "h1": task['h1'],
                            "h2": task['h2'],
                            "lsi_added": current_lsi_words,
                            "content": "",
                            "status": "Ready",
                            "date": "10.02.2026"
                        }
                        
                        if 'bg_results' not in st.session_state:
                            st.session_state.bg_results = []
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏
                        is_exist = False
                        for r in st.session_state.bg_results:
                            if r['h1'] == task['h1']:
                                r['lsi_added'] = current_lsi_words
                                is_exist = True
                                break
                        if not is_exist:
                            st.session_state.bg_results.append(new_rec)
                        
                        # 4. –ì–ï–ù–ï–†–ê–¶–ò–Ø (–°—Ä–∞–∑—É –∑–¥–µ—Å—å!)
                        st.write("üß† –û—Ç–ø—Ä–∞–≤–ª—è—é LSI –∏ –¢–ó –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—å Gemini...")
                        try:
                            # –£–±–µ–¥–∏—Å—å, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –∏–º–µ–Ω–Ω–æ —Ç–∞–∫
                            text = generate_article_with_gemini(task['h1'], current_lsi_words)
                            
                            # –ù–∞—Ö–æ–¥–∏–º –∑–∞–ø–∏—Å—å –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç
                            for r in st.session_state.bg_results:
                                if r['h1'] == task['h1']:
                                    r['content'] = text
                                    r['status'] = "Done"
                            st.write("‚úÖ –¢–µ–∫—Å—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ!")
                        except Exception as e:
                            st.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {e}")

                        # 5. –ü–ï–†–ï–•–û–î –ö –°–õ–ï–î–£–Æ–©–ï–ú–£
                        next_id = t_id + 1
                        if next_id < len(st.session_state.bg_tasks_queue):
                            st.write(f"‚è≠Ô∏è –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –∫–ª—é—á: **{st.session_state.bg_tasks_queue[next_id]['h1']}**")
                            
                            st.session_state.lsi_processing_task_id = next_id
                            st.session_state.query_input = st.session_state.bg_tasks_queue[next_id]['h1']
                            st.session_state.start_analysis_flag = True
                            st.session_state.analysis_done = False # –°–±—Ä–∞—Å—ã–≤–∞–µ–º, —á—Ç–æ–±—ã –ø–æ—à–µ–ª –Ω–æ–≤—ã–π –ø–æ–∏—Å–∫
                            
                            time.sleep(2)
                            status.update(label="üîÑ –ó–∞–ø—É—Å–∫–∞—é –Ω–æ–≤—ã–π —Ü–∏–∫–ª –∞–Ω–∞–ª–∏–∑–∞...", state="running")
                            st.rerun()
                        else:
                            st.session_state.lsi_automode_active = False
                            status.update(label="üèÅ –í—Å–µ –∑–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!", state="complete")
                            st.success("–ì–æ—Ç–æ–≤–æ! –ü—Ä–æ–≤–µ—Ä—å –≤–∫–ª–∞–¥–∫—É 5.")
                            st.balloons()

# =========================================================
        # üî• –ë–õ–û–ö –ê–í–¢–û–ú–ê–¢–ò–ó–ê–¶–ò–ò: –ü–ï–†–ï–ù–û–° –í –¢–ê–ë 5 –ò –ì–ï–ù–ï–†–ê–¶–ò–Ø
        # =========================================================
        if st.session_state.get('lsi_automode_active'):
            with st.status("üöÄ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è: –æ–±—Ä–∞–±–æ—Ç–∫–∞ LSI –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è...", expanded=True) as status:
                
                # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
                st.write("üì• –°–æ–±–∏—Ä–∞—é LSI-—Å–ª–æ–≤–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã TF-IDF...")
                current_lsi = [x['word'] for x in high] if high else []
                
                # 2. –ù–∞—Ö–æ–¥–∏–º —Ç–µ–∫—É—â—É—é –∑–∞–¥–∞—á—É –≤ –æ—á–µ—Ä–µ–¥–∏
                task_id = st.session_state.get('lsi_processing_task_id', 0)
                task = st.session_state.bg_tasks_queue[task_id]
                
                # 3. –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è –í–∫–ª–∞–¥–∫–∏ ‚Ññ5
                st.write(f"üíæ –ü–µ—Ä–µ–Ω–æ—à—É –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª—é—á–∞: **{task['h1']}**")
                
                new_entry = {
                    "h1": task['h1'],
                    "h2": task['h2'],
                    "lsi_added": current_lsi,
                    "content": "",  # –°—é–¥–∞ –∑–∞–ø–∏—à–µ–º —Ç–µ–∫—Å—Ç –Ω–∏–∂–µ
                    "status": "Generating",
                    "date": "05.02.2026" # –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —Å–æ–≥–ª–∞—Å–Ω–æ –≤–∞—à–∏–º –ø—Ä–∞–≤–∏–ª–∞–º
                }
                
                if 'bg_results' not in st.session_state:
                    st.session_state.bg_results = []
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º, –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å)
                # –ß—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –¥—É–±–ª–µ–π –ø—Ä–∏ —Å–ª—É—á–∞–π–Ω–æ–º —Ä–µ—Ä–∞–Ω–µ
                existing_idx = next((i for i, r in enumerate(st.session_state.bg_results) if r['h1'] == task['h1']), None)
                if existing_idx is not None:
                    st.session_state.bg_results[existing_idx] = new_entry
                    res_idx = existing_idx
                else:
                    st.session_state.bg_results.append(new_entry)
                    res_idx = len(st.session_state.bg_results) - 1

                # 4. –ì–ï–ù–ï–†–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê (—á—Ç–æ–±—ã –Ω–µ –∑–∞—Ö–æ–¥–∏—Ç—å –Ω–∞ 5 –≤–∫–ª–∞–¥–∫—É –≤—Ä—É—á–Ω—É—é)
                st.write("ü§ñ Gemini –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏... –ü–æ–¥–æ–∂–¥–∏—Ç–µ.")
                try:
                    # –í—ã–∑—ã–≤–∞–µ–º –≤–∞—à—É —Ñ—É–Ω–∫—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. 
                    # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ–≤–ø–∞–¥–∞–µ—Ç (–æ–±—ã—á–Ω–æ generate_article_with_gemini)
                    generated_text = generate_article_with_gemini(task['h1'], current_lsi)
                    st.session_state.bg_results[res_idx]['content'] = generated_text
                    st.session_state.bg_results[res_idx]['status'] = "Done"
                    st.write("‚úÖ –¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")
                except Exception as e:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ Gemini: {e}")
                    st.session_state.bg_results[res_idx]['status'] = "Error"

                # 5. –ü–ï–†–ï–•–û–î –ö –°–õ–ï–î–£–Æ–©–ï–ú–£ –ö–õ–Æ–ß–£
                next_task_idx = task_id + 1
                if next_task_idx < len(st.session_state.bg_tasks_queue):
                    st.write(f"‚è≠Ô∏è –°–ª–µ–¥—É—é—â–∏–π –∫–ª—é—á –≤ –æ—á–µ—Ä–µ–¥–∏: **{st.session_state.bg_tasks_queue[next_task_idx]['h1']}**")
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
                    st.session_state.lsi_processing_task_id = next_task_idx
                    st.session_state.query_input = st.session_state.bg_tasks_queue[next_task_idx]['h1']
                    st.session_state.start_analysis_flag = True 
                    st.session_state.analysis_done = False # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥, —á—Ç–æ–±—ã –∑–∞–ø—É—Å—Ç–∏—Ç—å –Ω–æ–≤—ã–π –ø–æ–∏—Å–∫
                    
                    status.update(label="üîÑ –ü–µ—Ä–µ—Ö–æ–∂—É –∫ –∞–Ω–∞–ª–∏–∑—É —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–ª—é—á–∞...", state="running")
                    time.sleep(2)
                    st.rerun()
                else:
                    st.session_state.lsi_automode_active = False
                    status.update(label="üèÅ –í–°–ï –ó–ê–î–ê–ß–ò –ó–ê–í–ï–†–®–ï–ù–´!", state="complete")
                    st.balloons()
                    st.success("–í—Å–µ –∫–ª—é—á–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –≤–∫–ª–∞–¥–∫–µ 5.")

# 6. –ì–õ–£–ë–ò–ù–ê (–ó–ê–ö–†–´–¢–û)
        with st.expander("üìâ 1. –ì–ª—É–±–∏–Ω–∞ (–î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞)", expanded=False):
            render_paginated_table(
                results['depth'], 
                "–ì–ª—É–±–∏–Ω–∞", 
                "tbl_depth_1", 
                default_sort_col="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è", 
                use_abs_sort_default=True
            )

        # 7. TF-IDF (–ó–ê–ö–†–´–¢–û)
        with st.expander("üßÆ 3. TF-IDF –ê–Ω–∞–ª–∏–∑", expanded=False):
            render_paginated_table(
                results['hybrid'], 
                "3. TF-IDF", 
                "tbl_hybrid", 
                default_sort_col="TF-IDF –¢–û–ü", 
                show_controls=False 
            )
# ==========================================
    # –ë–õ–û–ö 2: –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ï –ò –†–ê–°–ß–ï–¢
    # ==========================================
    if st.session_state.get('start_analysis_flag'):
        st.session_state.start_analysis_flag = False
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        settings = {
            'noindex': st.session_state.settings_noindex, 
            'alt_title': st.session_state.settings_alt, 
            'numbers': st.session_state.settings_numbers, 
            'norm': st.session_state.settings_norm, 
            'ua': st.session_state.settings_ua, 
            'custom_stops': st.session_state.settings_stops.split()
        }
        
        my_data, my_domain, my_serp_pos = None, "", 0
        current_input_type = st.session_state.get("my_page_source_radio")
        
        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –í–ê–®–ï–ô —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if current_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
                my_data = parse_page(st.session_state.my_url_input, settings, st.session_state.query_input)
                if not my_data: st.error("–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."); st.stop()
                my_domain = urlparse(st.session_state.my_url_input).netloc
        elif current_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            my_data = {'url': 'Local', 'domain': 'local', 'body_text': st.session_state.my_content_input, 'anchor_text': ''}

        st.session_state['saved_my_data'] = my_data 
            
# 2. –°–±–æ—Ä –ö–ê–ù–î–ò–î–ê–¢–û–í –ò –ü–†–û–í–ï–†–ö–ê –ö–≠–®–ê –ë–î
        current_source_val = st.session_state.get("competitor_source_radio")
        user_target_top_n = st.session_state.settings_top_n
        download_limit = 30 # –í–°–ï–ì–î–ê –ö–ê–ß–ê–ï–ú 30 –¥–ª—è TF-IDF
        
        cached_data_for_graph = None
        if "API" in current_source_val and current_input_type == "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã":
            cached_data_for_graph = get_cached_analysis(st.session_state.query_input)

        if cached_data_for_graph:
            st.toast(f"‚ö° –ù–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö! –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–ø—É—â–µ–Ω", icon="üóÑÔ∏è")
            data_for_graph = cached_data_for_graph
            targets_for_graph = [{'url': d['url'], 'pos': d['pos']} for d in data_for_graph]
        else:
            candidates_pool = []
            if "API" in current_source_val:
                if not ARSENKIN_TOKEN: st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç API —Ç–æ–∫–µ–Ω Arsenkin."); st.stop()
                with st.spinner(f"API Arsenkin (–ó–∞–ø—Ä–æ—Å –¢–æ–ø-30)..."):
                    raw_top = get_arsenkin_urls(st.session_state.query_input, st.session_state.settings_search_engine, st.session_state.settings_region, ARSENKIN_TOKEN, depth_val=30)
                    if not raw_top: st.stop()
                    
                    excl = [d.strip() for d in st.session_state.settings_excludes.split('\n') if d.strip()]
                    agg_list = [
                        "avito", "ozon", "wildberries", "market.yandex", "tiu", "youtube", "vk.com", "yandex",
                        "leroymerlin", "petrovich", "satom", "pulscen", "blizko", "deal.by", "satu.kz", "prom.ua",
                        "wikipedia", "dzen", "rutube", "kino", "otzovik", "irecommend", "profi.ru", "zoon", "2gis",
                        "megamarket.ru", "lamoda.ru", "utkonos.ru", "vprok.ru", "allbiz.ru", "all-companies.ru",
                        "orgpage.ru", "list-org.com", "rusprofile.ru", "e-katalog.ru", "kufar.by", "wildberries.kz",
                        "ozon.kz", "kaspi.kz", "pulscen.kz", "allbiz.kz", "wildberries.uz", "olx.uz", "pulscen.uz",
                        "allbiz.uz", "wildberries.kg", "pulscen.kg", "allbiz.kg", "all.biz", "b2b-center.ru"
                    ]
                    excl.extend(agg_list)
                    for res in raw_top:
                        dom = urlparse(res['url']).netloc.lower()
                        if my_domain and (my_domain in dom or dom in my_domain):
                            if my_serp_pos == 0 or res['pos'] < my_serp_pos: 
                                my_serp_pos = res['pos']
                        is_garbage = False
                        for x in excl:
                            if x.lower() in dom:
                                is_garbage = True
                                break
                        if is_garbage: continue
                        candidates_pool.append(res)
            else:
                raw_input_urls = st.session_state.get("persistent_urls", "")
                candidates_pool = [{'url': u.strip(), 'pos': i+1} for i, u in enumerate(raw_input_urls.split('\n')) if u.strip()]

            if not candidates_pool: st.error("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤."); st.stop()
            
            # 3. –°–ö–ê–ß–ò–í–ê–ù–ò–ï (–í—Å–µ—Ö 30)
            comp_data_valid = []
            with st.status(f"üïµÔ∏è –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (–í—Å–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidates_pool)})...", expanded=True) as status:
                with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
                    futures = {
                        executor.submit(parse_page, item['url'], settings, st.session_state.query_input): item 
                        for item in candidates_pool
                    }
                    done_count = 0
                    for f in concurrent.futures.as_completed(futures):
                        original_item = futures[f]
                        try:
                            res = f.result()
                            if res:
                                res['pos'] = original_item['pos']
                                comp_data_valid.append(res)
                        except: pass
                        done_count += 1
                        status.update(label=f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {done_count}/{len(candidates_pool)} | –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ: {len(comp_data_valid)}")

                comp_data_valid.sort(key=lambda x: x['pos'])
                data_for_graph = comp_data_valid[:download_limit]
                targets_for_graph = [{'url': d['url'], 'pos': d['pos']} for d in data_for_graph]
                
                # +++ –°–û–•–†–ê–ù–Ø–ï–ú –í –ë–î –¢–û–õ–¨–ö–û –ß–¢–û –°–ö–ê–ß–ê–ù–ù–û–ï +++
                if "API" in current_source_val and current_input_type == "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã":
                    save_cached_analysis(st.session_state.query_input, data_for_graph)
                
                # +++ –°–û–•–†–ê–ù–Ø–ï–ú –í –ë–î –¢–û–õ–¨–ö–û –ß–¢–û –°–ö–ê–ß–ê–ù–ù–û–ï +++
                if "API" in current_source_val and current_input_type == "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã":
                    save_cached_analysis(st.session_state.query_input, data_for_graph)

        # 5. –†–ê–°–ß–ï–¢ –ú–ï–¢–†–ò–ö (–î–í–û–ô–ù–û–ô –ü–†–û–ì–û–ù)
        with st.spinner("–ê–Ω–∞–ª–∏–∑ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è..."):
            
            # --- –≠–¢–ê–ü 1: –ß–µ—Ä–Ω–æ–≤–æ–π –ø—Ä–æ–≥–æ–Ω (–ø–æ –≤—Å–µ–º 30 —Å–∞–π—Ç–∞–º) ---
            # –≠—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ –∏ –Ω–∞–π—Ç–∏ –∞–Ω–æ–º–∞–ª–∏–∏
            results_full = calculate_metrics(data_for_graph, my_data, settings, my_serp_pos, targets_for_graph)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ü–û–õ–ù–´–ï –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ (—á—Ç–æ–±—ã –Ω–∞ –Ω–µ–º –±—ã–ª–∏ –≤—Å–µ)
            st.session_state['full_graph_data'] = results_full['relevance_top']
            
            # –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π –ø–æ –ø–æ–ª–Ω–æ–º—É —Å–ø–∏—Å–∫—É
            df_rel_check = results_full['relevance_top']
            good_urls, bad_urls_dicts, trend = analyze_serp_anomalies(df_rel_check)
            st.session_state['serp_trend_info'] = trend
            
            # --- –≠–¢–ê–ü 2: –û—Ç–±–æ—Ä —á–∏—Å—Ç–æ–≤—ã—Ö (–¢–æ–ø-10/20 –±–µ–∑ –º—É—Å–æ—Ä–∞) ---
            
# 1. –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–µ—Ö —Å–∞–π—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –≤ —Å–ø–∏—Å–∫–µ –ø–ª–æ—Ö–∏—Ö
            bad_urls_set = set(item['url'] for item in bad_urls_dicts)
            
            # === –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –§–ò–õ–¨–¢–†–ê–¶–ò–ò ===
            # –ï—Å–ª–∏ —ç—Ç–æ API - –º—ã —Ñ–∏–ª—å—Ç—Ä—É–µ–º –∏ —Ä–µ–∂–µ–º —Ç–æ–ø.
            # –ï—Å–ª–∏ —ç—Ç–æ –†–£–ß–ù–û–ô —Ä–µ–∂–∏–º - –º—ã –ù–ï —Ñ–∏–ª—å—Ç—Ä—É–µ–º (–¥–æ–≤–µ—Ä—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é).
            if "API" in current_source_val:
                clean_data_pool = [d for d in data_for_graph if d['url'] not in bad_urls_set]
                final_clean_data = clean_data_pool[:user_target_top_n]
            else:
                # –í —Ä—É—á–Ω–æ–º —Ä–µ–∂–∏–º–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï–• —Å–∫–∞—á–∞–Ω–Ω—ã—Ö, –Ω–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º "—Å–ª–∞–±—ã—Ö"
                final_clean_data = data_for_graph 
            
            # <--- –í–ê–ñ–ù–û: –°—Ç—Ä–æ–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–¥–µ—Ç –°–¢–†–û–ì–û –ü–û–°–õ–ï –±–ª–æ–∫–∞ if/else --->
            st.session_state['raw_comp_data'] = final_clean_data
            # ------------------------------------------------------------------

            final_clean_targets = [{'url': d['url'], 'pos': d['pos']} for d in final_clean_data]
            
            # 3. –§–ò–ù–ê–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ (–¢–æ–ª—å–∫–æ –ø–æ —ç–ª–∏—Ç–µ)
            results_final = calculate_metrics(final_clean_data, my_data, settings, my_serp_pos, final_clean_targets)
            
# 3. –§–ò–ù–ê–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ (–¢–æ–ª—å–∫–æ –ø–æ —ç–ª–∏—Ç–µ)
            results_final = calculate_metrics(final_clean_data, my_data, settings, my_serp_pos, final_clean_targets)
            st.session_state.analysis_results = results_final
            
            # --- –û—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ (–Ω–µ–π–º–∏–Ω–≥, —Å–µ–º–∞–Ω—Ç–∏–∫–∞) ---
            naming_df = calculate_naming_metrics(final_clean_data, my_data, settings)
            st.session_state.naming_table_df = naming_df 
            st.session_state.ideal_h1_result = analyze_ideal_name(final_clean_data)
            st.session_state.analysis_done = True
            
# ==========================================
        # üî• –ü–û–õ–ù–´–ô –î–í–ò–ñ–û–ö –û–¢–ó–´–í–û–í (–í–ï–†–°–ò–Ø 6: –£–ü–£–©–ï–ù–ù–ê–Ø –°–ï–ú–ê–ù–¢–ò–ö–ê + –¢–û–í–ê–†–´)
        # ==========================================
        if st.session_state.get('reviews_automode_active'):
            try:
               # === –ù–û–í–´–ô –°–ë–û–† –°–õ–û–í –î–õ–Ø –û–¢–ó–´–í–û–í ===
                # 1. –ë–µ—Ä–µ–º –¢–û–õ–¨–ö–û "–æ—Å–Ω–æ–≤–Ω—ã–µ –≤–∞–∂–Ω—ã–µ" (high) –∏–∑ –ø–µ—Ä–≤–æ–π –≤–∫–ª–∞–¥–∫–∏
                res_seo = st.session_state.get('analysis_results', {})
                missing_high = [x['word'] for x in res_seo.get('missing_semantics_high', [])]
                
                # 2. –ë–µ—Ä–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–ª–µ—Ç–µ–ª–∏ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ 2-–π –≤–∫–ª–∞–¥–∫–µ (—Ç–µ–≥–∏, –ø—Ä–æ–º–æ, —Ç–µ–∫—Å—Ç)
                unused_from_gen = list(st.session_state.get('global_unused_for_reviews', set()))
                
                # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö –≤ –µ–¥–∏–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (—É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã)
                raw_candidates = list(set(missing_high + unused_from_gen))
                
                # 4. (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä –Ω–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤, —á—Ç–æ–±—ã –æ–Ω–∏ –Ω–µ –ø–µ—Ä–µ–Ω–æ—Å–∏–ª–∏—Å—å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–æ–µ–∫—Ç—ã
                # st.session_state['global_unused_for_reviews'] = set()
                # ===================================
                
                # –ü–æ–¥—Ç—è–≥–∏–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –Ω–∞ –≤–∫–ª–∞–¥–∫–µ 1)
                known_products = set(st.session_state.get('categorized_products', []))
                
                # === –ë–õ–û–ö–ò–†–û–í–ö–ê –ì–ï–û –ò –ú–£–°–û–†–ê ===
                known_geo = set(st.session_state.get('categorized_geo', []))
                known_geo.update(st.session_state.get('orig_geo', []))
                
                try:
                    # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –ø–æ–¥—Ç—è–≥–∏–≤–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –≥–æ—Ä–æ–¥–æ–≤, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                    _, _, _, dict_geo, _, _ = load_lemmatized_dictionaries()
                    known_geo.update(dict_geo)
                except:
                    pass
                
                lsi_nouns = []
                
                # –ß–ï–†–ù–´–ô –°–ü–ò–°–û–ö (–°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≥–ª—É–ø–æ –∑–≤—É—á–∞—Ç –≤ –æ—Ç–∑—ã–≤–∞—Ö)
                STOP_NOUNS = {
                    '–∫–æ–¥', '—Å–∞–π—Ç', '–∫–∞—Ç–∞–ª–æ–≥', '–º–µ–Ω—é', '–∫–æ—Ä–∑–∏–Ω–∞', '–ø–æ–∏—Å–∫', '—Å—Å—ã–ª–∫–∞', '—Å—Ç—Ä–∞–Ω–∏—Ü–∞', 
                    '–≤–µ—Ä—Å–∏—è', '–æ—à–∏–±–∫–∞', '—Ä—É–±', '–≥—Ä–Ω', '—à—Ç', '—Ä–∞–∑', '–¥–≤–∞', '—Ç—Ä–∏', '–Ω–æ–º–µ—Ä', 
                    '—Ç–µ–ª–µ—Ñ–æ–Ω', '–∞–¥—Ä–µ—Å', 'email', '—Ñ–∏–ª—å—Ç—Ä', '—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞', '–∞—Ä—Ç–∏–∫—É–ª', '–Ω–∞–ª–∏—á–∏–µ',
                    '–∞–∫—Ü–∏—è', '—Å–∫–∏–¥–∫–∞', '—Ö–∏—Ç', '–Ω–æ–≤–∏–Ω–∫–∞', '–∫–ª–∏–∫', '–≤—Ö–æ–¥', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è', '–≥–ª–∞–≤–Ω–∞—è',
                    '–∫–∞—Ä—Ç–∞', '–Ω–æ–≤–æ—Å—Ç—å', '—Å—Ç–∞—Ç—å—è', '–æ—Ç–∑—ã–≤', '–≤–∞–∫–∞–Ω—Å–∏—è', '–æ–ø–ª–∞—Ç–∞', '–¥–æ—Å—Ç–∞–≤–∫–∞',
                    '–≥–æ—Ä–æ–¥', '—Ä–µ–≥–∏–æ–Ω', '—Ä–æ—Å—Å–∏—è', '–º–æ—Å–∫–≤–∞', '—Å–ø–±', '–¥–æ—Å—Ç–∞–≤–∫–∞', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ', '–∑–∞–≤–æ–¥'
                }

                if raw_candidates:
                    for w in raw_candidates:
                        w_clean = str(w).lower().strip()
                        
                        # –ë–ê–ó–û–í–´–ô –§–ò–õ–¨–¢–†: –¥–ª–∏–Ω–∞ > 2, –Ω–µ—Ç –ª–∞—Ç–∏–Ω–∏—Ü—ã/—Ü–∏—Ñ—Ä, –Ω–µ—Ç –≤ —Å—Ç–æ–ø-–ª–∏—Å—Ç–µ, –ù–ï–¢ –í –ì–ï–û
                        if (len(w_clean) > 2 
                            and not re.search(r'[a-zA-Z0-9]', w_clean) 
                            and w_clean not in STOP_NOUNS
                            and w_clean not in known_geo):
                            
                            parsed = morph.parse(w_clean)[0]
                            
                            # –£–°–õ–û–í–ò–ï –û–¢–ë–û–†–ê:
                            # 1. –õ–∏–±–æ —ç—Ç–æ –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ (NOUN)
                            # 2. –õ–∏–±–æ —ç—Ç–æ —Å–ª–æ–≤–æ —É–∂–µ –ª–µ–∂–∏—Ç –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ "–¢–æ–≤–∞—Ä—ã" (categorized_products)
                            is_noun = 'NOUN' in parsed.tag
                            
                            # === –£–ú–ù–´–ô –§–ò–õ–¨–¢–† PYMORPHY ===
                            # Geox - —Ç–æ–ø–æ–Ω–∏–º—ã (–≥–æ—Ä–æ–¥–∞, —Ä–µ–∫–∏), Name - –∏–º–µ–Ω–∞, Surn - —Ñ–∞–º–∏–ª–∏–∏
                            is_name_or_geo = any(tag in parsed.tag for tag in ['Name', 'Surn', 'Patr', 'Geox'])
                            
                            is_known_product = w_clean in known_products
                            
                            # –ë–µ—Ä–µ–º, –µ—Å–ª–∏ —ç—Ç–æ —Ç–æ–≤–∞—Ä, –õ–ò–ë–û –µ—Å–ª–∏ —ç—Ç–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ, –Ω–æ –ù–ï –≥–æ—Ä–æ–¥/–∏–º—è
                            if is_known_product or (is_noun and not is_name_or_geo):
                                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –ï—Å–ª–∏ —ç—Ç–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–≤–∞—Ä - —Å—Ç–∞–≤–∏–º —Ñ–ª–∞–≥ priority
                                priority = 1 if is_known_product else 0
                                
                                lsi_nouns.append({
                                    'word': w_clean,
                                    'parse': parsed,
                                    'priority': priority
                                })
                
                # –°–û–†–¢–ò–†–û–í–ö–ê: –°–Ω–∞—á–∞–ª–∞ —Å–ª–æ–≤–∞ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ "–¢–æ–≤–∞—Ä—ã", –ø–æ—Ç–æ–º –ø—Ä–æ—Å—Ç–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑ —É–ø—É—â–µ–Ω–Ω–æ–≥–æ
                # Shuffle –¥–µ–ª–∞–µ–º –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–∞–Ω–¥–æ–º, –Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                lsi_nouns.sort(key=lambda x: x['priority'], reverse=True)
                
                # –ï—Å–ª–∏ —Å–ª–æ–≤ –º–∞–ª–æ, –º–æ–∂–Ω–æ –ø–µ—Ä–µ–º–µ—à–∞—Ç—å —Ç–æ–ø-20, —á—Ç–æ–±—ã –Ω–µ —à–ª–∏ –≤—Å–µ–≥–¥–∞ –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ —Ç–æ–≤–∞—Ä—ã
                if len(lsi_nouns) > 5:
                    top_slice = lsi_nouns[:10]
                    random.shuffle(top_slice)
                    lsi_nouns[:10] = top_slice

                curr_idx = st.session_state.get('reviews_current_index', 0)
                queue = st.session_state.get('reviews_queue', [])
                
                if curr_idx < len(queue):
                    task = queue[curr_idx]
                else:
                    st.session_state.reviews_automode_active = False
                    st.rerun()

                # 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤
                import os
                if os.path.exists("dicts/fio.csv"):
                    df_fio = pd.read_csv("dicts/fio.csv", sep=";")
                else:
                    df_fio = pd.DataFrame([{"–§–∞–º–∏–ª–∏—è": "–ü–æ–∫—É–ø–∞—Ç–µ–ª—å", "–ò–º—è": ""}])

                if os.path.exists("dicts/templates.csv"):
                    df_templates = pd.read_csv("dicts/templates.csv", sep=";")
                else:
                    df_templates = pd.DataFrame([
                        {"–®–∞–±–ª–æ–Ω": "–ó–∞–∫–∞–∑—ã–≤–∞–ª–∏ {—Ç–æ–≤–∞—Ä}. –í—Å–µ —Ö–æ—Ä–æ—à–æ."}, 
                        {"–®–∞–±–ª–æ–Ω": "–ö–∞—á–µ—Å—Ç–≤–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ, {—Ç–æ–≤–∞—Ä} –ø—Ä–∏—à–µ–ª –≤–æ–≤—Ä–µ–º—è."},
                        {"–®–∞–±–ª–æ–Ω": "–ù–æ—Ä–º–∞–ª—å–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è. –ë—ã–ª –Ω—É–∂–µ–Ω {—Ç–æ–≤–∞—Ä}, –ø–æ–º–æ–≥–ª–∏ –ø–æ–¥–æ–±—Ä–∞—Ç—å."}
                    ])
                
                var_dict = {}
                if os.path.exists("dicts/vars.csv"):
                    df_vars = pd.read_csv("dicts/vars.csv", sep=";")
                    for _, row in df_vars.iterrows():
                        v_name = str(row['–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è']).strip()
                        if pd.notna(row['–ó–Ω–∞—á–µ–Ω–∏—è']):
                            var_dict[f"{{{v_name}}}"] = [v.strip() for v in str(row['–ó–Ω–∞—á–µ–Ω–∏—è']).split('|')]
                
                if "{—Ç–æ–≤–∞—Ä}" not in var_dict:
                    var_dict["{—Ç–æ–≤–∞—Ä}"] = ["–∑–∞–∫–∞–∑", "—Ç–æ–≤–∞—Ä", "–ø—Ä–æ–¥—É–∫—Ü–∏—é"]

                # –§—Ä–∞–∑—ã-–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã (–¥–ª—è –≤—Å—Ç–∞–≤–∫–∏, –µ—Å–ª–∏ —à–∞–±–ª–æ–Ω –Ω–µ –ø–æ–¥–æ—à–µ–ª)
                LSI_SENTENCES = [
                    {"tpl": "–û—Ç–¥–µ–ª—å–Ω–æ –æ—Ç–º–µ—á—É –∫–∞—á–µ—Å—Ç–≤–æ {}.", "case": "gent"}, 
                    {"tpl": "–ü–æ—Ä–∞–¥–æ–≤–∞–ª–∞ —Ü–µ–Ω–∞ –Ω–∞ {}.", "case": "accs"},       
                    {"tpl": "–ü—Ä–∏–æ–±—Ä–µ–ª–∏ {}.", "case": "accs"},         
                    {"tpl": "–ü—Ä–æ–±–ª–µ–º —Å {} –Ω–µ –≤–æ–∑–Ω–∏–∫–ª–æ.", "case": "ablt"},    
                    {"tpl": "–°–µ–π—á–∞—Å {} –≤ –Ω–∞–ª–∏—á–∏–∏.", "case": "nomn"}          
                ]

                with st.spinner(f"üì¶ –°–±–æ—Ä–∫–∞ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è: {task.get('q', '–∑–∞–ø—Ä–æ—Å–∞')}..."):
                    for _ in range(st.session_state.get('reviews_per_query', 3)):
                        # –§–ò–û
                        f_row = df_fio.sample(n=1).iloc[0]
                        c_fio = f"{f_row.get('–ò–º—è', '')} {f_row.get('–§–∞–º–∏–ª–∏—è', '')}".strip()
                        if not c_fio: c_fio = "–ö–ª–∏–µ–Ω—Ç"

                        # –®–∞–±–ª–æ–Ω
                        final_text = random.choice(df_templates['–®–∞–±–ª–æ–Ω'].values)
                        used_lsi_word = None
                        
                        # --- –í–ù–ï–î–†–ï–ù–ò–ï LSI (–° –ü–†–ò–û–†–ò–¢–ï–¢–û–ú –¢–û–í–ê–†–û–í) ---
                        
                        # 1. –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–ª–æ–≤–æ –¥–ª—è –∑–∞–º–µ–Ω—ã —Ç–µ–≥–∞ {—Ç–æ–≤–∞—Ä}
                        # –ë–µ—Ä–µ–º —Å–ª–æ–≤–æ –∏–∑ lsi_nouns (–æ–Ω–∏ —É–∂–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: –¢–æ–≤–∞—Ä—ã –≤–≤–µ—Ä—Ö—É, –ø–æ—Ç–æ–º –ø—Ä–æ—Å—Ç–æ –£–ø—É—â–µ–Ω–Ω–æ–µ)
                        if "{—Ç–æ–≤–∞—Ä}" in final_text and lsi_nouns:
                            # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ –∏–∑ —Ç–æ–ø-5 –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–ª–æ–≤, —á—Ç–æ–±—ã –±—ã–ª–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
                            top_n = min(len(lsi_nouns), 10)
                            lsi_obj = lsi_nouns[random.randint(0, top_n - 1)] 
                            
                            replacement = f"**{lsi_obj['word']}**"
                            final_text = final_text.replace("{—Ç–æ–≤–∞—Ä}", replacement, 1)
                            used_lsi_word = True
                        
                        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
                        tags = re.findall(r"\{[–∞-—è–ê-–Ø—ë–Åa-zA-Z0-9_]+\}", final_text)
                        for t in tags:
                            if t in var_dict:
                                final_text = final_text.replace(t, random.choice(var_dict[t]), 1)
                            elif t == "{–¥–∞—Ç–∞}":
                                dt = (datetime.datetime.now() - datetime.timedelta(days=random.randint(1, 60))).strftime("%d.%m.%Y")
                                final_text = final_text.replace("{–¥–∞—Ç–∞}", dt)

                        # 2. –ï—Å–ª–∏ {—Ç–æ–≤–∞—Ä} –Ω–µ –±—ã–ª –∑–∞–º–µ–Ω–µ–Ω (–Ω–µ—Ç —Ç–µ–≥–∞ –∏–ª–∏ —Å–ª–æ–≤), –≤—Å—Ç–∞–≤–ª—è–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                        if not used_lsi_word and lsi_nouns:
                            top_n = min(len(lsi_nouns), 10)
                            lsi_obj = lsi_nouns[random.randint(0, top_n - 1)]
                            
                            parsed_word = lsi_obj['parse']
                            tpl_obj = random.choice(LSI_SENTENCES)
                            
                            try:
                                inflected = parsed_word.inflect({tpl_obj['case']})
                                w_res = inflected.word if inflected else lsi_obj['word']
                            except:
                                w_res = lsi_obj['word']
                            
                            w_res_bold = f"**{w_res}**"
                            add_sentence = tpl_obj['tpl'].format(w_res_bold)
                            
                            sentences = [s.strip() for s in final_text.split('.') if len(s) > 1]
                            if sentences:
                                idx = random.randint(0, len(sentences))
                                sentences.insert(idx, add_sentence)
                            else:
                                sentences = [final_text, add_sentence]
                                
                            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–≥–∏—Å—Ç—Ä–∞ (Capitalization)
                            capitalized_sentences = []
                            for s in sentences:
                                clean_s = s.strip()
                                if clean_s:
                                    cap_s = clean_s[0].upper() + clean_s[1:]
                                    capitalized_sentences.append(cap_s)
                            
                            final_text = ". ".join(capitalized_sentences) + "."

                        # –§–∏–Ω–∞–ª—å–Ω–∞—è —á–∏—Å—Ç–∫–∞
                        final_text = re.sub(r"\{[^}]+\}", "", final_text)
                        final_text = final_text.replace("..", ".").replace(" .", ".").replace(" ,", ",")
                        final_text = re.sub(r'\s+', ' ', final_text).strip()
                        
                        if final_text:
                            final_text = final_text[0].upper() + final_text[1:]

                        st.session_state.reviews_results.append({
                            "–§–ò–û": c_fio,
                            "–ó–∞–ø—Ä–æ—Å": task.get('q', '-'),
                            "URL": task.get('url', '-'),
                            "–û—Ç–∑—ã–≤": final_text
                        })

                # –ü–µ—Ä–µ—Ö–æ–¥ –¥–∞–ª—å—à–µ
                n_idx = curr_idx + 1
                if n_idx < len(queue):
                    st.session_state.reviews_current_index = n_idx
                    nxt = queue[n_idx]
                    
                    keys_to_clear = [
                        'analysis_results', 'analysis_done', 'naming_table_df', 
                        'ideal_h1_result', 'raw_comp_data', 'full_graph_data',
                        'detected_anomalies', 'serp_trend_info', 'excluded_urls_auto'
                    ]
                    for k in keys_to_clear:
                        st.session_state.pop(k, None)
                    
                    st.session_state['pending_widget_updates'] = {
                        'query_input': nxt.get('q'),
                        'my_url_input': nxt.get('url', ''),
                        'my_page_source_radio': "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ" if nxt.get('url') != 'manual' else "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
                    }
                    st.session_state.start_analysis_flag = True
                    st.toast(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {nxt.get('q')}")
                    import time
                    time.sleep(0.5)
                    st.rerun()
                else:
                    st.session_state.reviews_automode_active = False
                    st.success("‚úÖ –ì–æ—Ç–æ–≤–æ!")
                    st.balloons()
            
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                st.session_state.reviews_automode_active = False
        # ==========================================
        # üî• –ë–õ–û–ö: –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –°–ï–ú–ê–ù–¢–ò–ö–ò (–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô)
        # ==========================================
        words_to_check = [x['word'] for x in results_final.get('missing_semantics_high', [])]
        if len(words_to_check) < 5:
            words_to_check.extend([x['word'] for x in results_final.get('missing_semantics_low', [])[:20]])

        if not words_to_check:
            st.session_state.categorized_products = []
            st.session_state.categorized_services = []
            st.session_state.categorized_commercial = []
            st.session_state.categorized_dimensions = []
            st.session_state.categorized_geo = []
            st.session_state.categorized_general = []
            st.session_state.categorized_sensitive = []
        else:
            if 'categorized_products' not in st.session_state or not st.session_state.categorized_products:
                with st.spinner("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏..."):
                    categorized = classify_semantics_with_api(words_to_check, YANDEX_DICT_KEY)
                
                st.session_state.categorized_products = categorized['products']
                st.session_state.categorized_services = categorized['services']
                st.session_state.categorized_commercial = categorized['commercial']
                st.session_state.categorized_geo = categorized['geo']
                st.session_state.categorized_dimensions = categorized['dimensions']
                st.session_state.categorized_general = categorized['general']
                st.session_state.categorized_sensitive = categorized['sensitive']

                st.session_state.orig_products = categorized['products'] + categorized['sensitive']
                st.session_state.orig_services = categorized['services'] + categorized['sensitive']
                st.session_state.orig_commercial = categorized['commercial'] + categorized['sensitive']
                st.session_state.orig_geo = categorized['geo'] + categorized['sensitive']
                st.session_state.orig_dimensions = categorized['dimensions'] + categorized['sensitive']
                st.session_state.orig_general = categorized['general'] + categorized['sensitive']

        # –ì–æ—Ç–æ–≤–∏–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –≤–∏–¥–∂–µ—Ç–æ–≤
        if 'pending_widget_updates' not in st.session_state:
            st.session_state['pending_widget_updates'] = {}
        
        updates = st.session_state['pending_widget_updates']

        if words_to_check and 'categorized_sensitive' in locals() or 'categorized' in locals():
            # –ï—Å–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ —á—Ç–æ –ø—Ä–æ—à–ª–∞, –±–µ—Ä–µ–º –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            sens = categorized['sensitive'] if 'categorized' in locals() else st.session_state.categorized_sensitive
            updates['sensitive_words_input_final'] = "\n".join(sens)
        
        all_found_products = st.session_state.get('categorized_products', [])
        count_prods = len(all_found_products)
        
        if count_prods > 0:
            if count_prods < 20:
                st.session_state.auto_tags_words = all_found_products
                st.session_state.auto_promo_words = []
            else:
                half_count = int(math.ceil(count_prods / 2))
                st.session_state.auto_tags_words = all_found_products[:half_count]
                st.session_state.auto_promo_words = all_found_products[half_count:]
            
            updates['tags_products_edit_final'] = "\n".join(st.session_state.auto_tags_words)
            updates['promo_keywords_area_final'] = "\n".join(st.session_state.auto_promo_words)
        
        st.session_state['pending_widget_updates'] = updates

        # --- –ì–†–ê–§–ò–ö–ò ---
        current_source_val = st.session_state.get('competitor_source_radio', '')
        if "API" in current_source_val and 'full_graph_data' in st.session_state:
            df_rel_check = st.session_state['full_graph_data']
        else:
            df_rel_check = st.session_state.analysis_results['relevance_top']
        
        # 2. –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π
        good_urls, bad_urls_dicts, trend = analyze_serp_anomalies(df_rel_check)
        st.session_state['serp_trend_info'] = trend
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞
        is_filter_enabled = st.session_state.get("settings_auto_filter", True)
        
        def get_strict_key(u):
            if not u: return ""
            return str(u).lower().strip().replace("https://", "").replace("http://", "").replace("www.", "").rstrip('/')

        final_clean_text = ""
        
        # --- –õ–û–ì–ò–ö–ê –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø ---
        if is_filter_enabled and bad_urls_dicts:
            # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–ª–æ—Ö–∏—Ö
            st.session_state['detected_anomalies'] = bad_urls_dicts
            
            blacklist_keys = set()
            excluded_display_list = []
            for item in bad_urls_dicts:
                raw_u = item.get('url', '')
                if raw_u:
                    blacklist_keys.add(get_strict_key(raw_u))
                    excluded_display_list.append(str(raw_u).strip())
            
            st.session_state['excluded_urls_auto'] = "\n".join(excluded_display_list)
            
            # 2. –°–æ–±–∏—Ä–∞–µ–º —Ö–æ—Ä–æ—à–∏—Ö
            clean_active_list = []
            seen_keys = set()
            for u in good_urls:
                key = get_strict_key(u)
                if key and key not in blacklist_keys and key not in seen_keys:
                    clean_active_list.append(str(u).strip())
                    seen_keys.add(key)
            
            final_clean_text = "\n".join(clean_active_list)
            st.toast(f"–§–∏–ª—å—Ç—Ä —Å—Ä–∞–±–æ—Ç–∞–ª. –ò—Å–∫–ª—é—á–µ–Ω–æ: {len(blacklist_keys)}", icon="‚úÇÔ∏è")
        
        else:
            # –§–∏–ª—å—Ç—Ä –≤—ã–∫–ª—é—á–µ–Ω –∏–ª–∏ –ø–ª–æ—Ö–∏—Ö –Ω–µ—Ç - –±–µ—Ä–µ–º –≤—Å—ë
            clean_all = []
            seen_all = set()
            combined_pool = good_urls + [x['url'] for x in (bad_urls_dicts or [])]
            for u in combined_pool:
                key = get_strict_key(u)
                if key and key not in seen_all:
                    clean_all.append(str(u).strip())
                    seen_all.add(key)
            
            final_clean_text = "\n".join(clean_all)
            # –ß–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—ã–µ –æ—à–∏–±–∫–∏
            st.session_state.pop('excluded_urls_auto', None)
            st.session_state.pop('detected_anomalies', None)

        # === –§–ò–ù–ê–õ–¨–ù–ê–Ø –ó–ê–ü–ò–°–¨ –ò –ü–ï–†–ï–ó–ê–ì–†–£–ó–ö–ê ===
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –í–†–ï–ú–ï–ù–ù–£–Æ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        st.session_state['temp_update_urls'] = final_clean_text
        
        # –°—Ç–∞–≤–∏–º —Ñ–ª–∞–≥ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–∞–¥–∏–æ-–∫–Ω–æ–ø–∫–∏
        st.session_state['force_radio_switch'] = True

# ==================================================================
        # üî• HOOK –î–õ–Ø LSI –ì–ï–ù–ï–†–ê–¢–û–†–ê (–í–ö–õ–ê–î–ö–ê 5)
        # –ï—Å–ª–∏ —ç—Ç–æ—Ç –∞–Ω–∞–ª–∏–∑ –±—ã–ª –∑–∞–∫–∞–∑–∞–Ω –í–∫–ª–∞–¥–∫–æ–π 5, –º—ã –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –∏ –∏–¥–µ–º –¥–∞–ª—å—à–µ
        # ==================================================================
        if st.session_state.get('lsi_automode_active'):
            
            # 1. –î–æ—Å—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–µ–∫—É—â–µ–π –∑–∞–¥–∞—á–∏
            current_idx = st.session_state.get('lsi_processing_task_id')
            task = st.session_state.bg_tasks_queue[current_idx]
            
            # 2. –î–æ—Å—Ç–∞–µ–º LSI (TF-IDF) –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –í–∫–ª–∞–¥–∫–∏ 1
            lsi_words = []
            if results_final.get('hybrid') is not None and not results_final['hybrid'].empty:
                lsi_words = results_final['hybrid'].head(15)['–°–ª–æ–≤–æ'].tolist()
            
            # 3. –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ (–Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Ö –≤ session_state –≤–æ –≤–∫–ª–∞–¥–∫–µ 5)
            # (–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –æ–Ω–∏ –µ—Å—Ç—å, –∏–ª–∏ –±–µ—Ä–µ–º –¥–µ—Ñ–æ–ª—Ç)
# 3. –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞ –∏–∑ –ø–æ–ª—è –≤–≤–æ–¥–∞
            raw_common = st.session_state.get('common_lsi_input', "–≥–∞—Ä–∞–Ω—Ç–∏—è, –∑–≤–æ–Ω–∏—Ç–µ, –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è, –∫—É–ø–∏—Ç—å, –æ–ø–ª–∞—Ç–∞, –æ–ø—Ç–æ–º, –æ—Ç–≥—Ä—É–∑–∫–∞, –ø–æ–¥ –∑–∞–∫–∞–∑, –ø–æ—Å—Ç–∞–≤–∫–∞, –ø—Ä–∞–π—Å-–ª–∏—Å—Ç, —Ü–µ–Ω—ã")
            common_lsi = [w.strip() for w in raw_common.split(",") if w.strip()]
            combined_lsi = list(set(common_lsi + lsi_words))
            
# 4. –ì–ï–ù–ï–†–ò–†–£–ï–ú –°–¢–ê–¢–¨–Æ
            # –ß–∏—Ç–∞–µ–º –∏–∑ SUPER_GLOBAL_KEY (–∫–æ—Ç–æ—Ä—ã–π –º—ã —Å–æ–∑–¥–∞–ª–∏ –≤ –®–∞–≥–µ 1)
            api_key_gen = st.session_state.get('SUPER_GLOBAL_KEY')
            
            # –§–æ–ª–±—ç–∫: –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –µ–≥–æ –Ω–µ—Ç, –ø—Ä–æ–±—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
            if not api_key_gen:
                api_key_gen = st.session_state.get('bulk_api_key_v3')
            
            try:
                html_out = generate_full_article_v2(api_key_gen, task['h1'], task['h2'], combined_lsi)
                status_code = "OK"
            except Exception as e:
                html_out = f"Error: {e}"
                status_code = "Error"

            # 5. –°–û–•–†–ê–ù–Ø–ï–ú –†–ï–ó–£–õ–¨–¢–ê–¢ –í –°–ü–ò–°–û–ö –í–ö–õ–ê–î–ö–ò 5
            st.session_state.bg_results.append({
                "h1": task['h1'],
                "h2": task['h2'],
                "source_url": task.get('source_url', '-'),
                "lsi_added": lsi_words,
                "content": html_out,
                "status": status_code
            })

            # 6. –ü–õ–ê–ù–ò–†–£–ï–ú –°–õ–ï–î–£–Æ–©–£–Æ –ó–ê–î–ê–ß–£
            finished_ids = set(f"{r['h1']}|{r['h2']}" for r in st.session_state.bg_results)
            next_task_idx = -1
            
            for i, t in enumerate(st.session_state.bg_tasks_queue):
                unique_id = f"{t['h1']}|{t['h2']}"
                if unique_id not in finished_ids:
                    next_task_idx = i
                    break
            
            st.write(f"DEBUG: –ù–∞–π–¥–µ–Ω–∞ —Å–ª–µ–¥—É—é—â–∞—è –∑–∞–¥–∞—á–∞ –ø–æ–¥ –∏–Ω–¥–µ–∫—Å–æ–º: {next_task_idx}")

            if next_task_idx != -1:
# === –¢–û–ß–ï–ß–ù–ê–Ø –û–ß–ò–°–¢–ö–ê –°–¢–ê–†–´–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
                keys_to_clear = [
                    'analysis_results', 'analysis_done', 'naming_table_df', 
                    'ideal_h1_result', 'raw_comp_data', 'full_graph_data',
                    'detected_anomalies', 'serp_trend_info', 'excluded_urls_auto'
                ]
                for k in keys_to_clear:
                    st.session_state.pop(k, None)

                # 3. –ë–ï–†–ï–ú –ù–û–í–£–Æ –ó–ê–î–ê–ß–£
                next_task = st.session_state.bg_tasks_queue[next_task_idx]
                
                # –°—Ç–∞–≤–∏–º —Å—Ç–∞—Ç—É—Å "–í —Ä–∞–±–æ—Ç–µ" –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã –æ—á–µ—Ä–µ–¥–∏
                st.session_state.bg_tasks_queue[next_task_idx]['status'] = "üîç –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞..."
                
                # === –¢–û–ß–ï–ß–ù–ê–Ø –û–ß–ò–°–¢–ö–ê –°–¢–ê–†–´–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
                keys_to_clear = [
                    'analysis_results', 'analysis_done', 'naming_table_df', 
                    'ideal_h1_result', 'raw_comp_data', 'full_graph_data',
                    'detected_anomalies', 'serp_trend_info', 'excluded_urls_auto'
                ]
                for k in keys_to_clear:
                    st.session_state.pop(k, None)
                    
                # –ü—Ä–æ–ø–∏—Å—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–µ—Ä–≤–æ–π –≤–∫–ª–∞–¥–∫–∏ (–ü–∞—Ä—Å–µ—Ä)
                st.session_state['pending_widget_updates'] = {
                    'query_input': next_task['h1'],
                    'competitor_source_radio': "–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)",
                    'my_page_source_radio': "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã",
                    'my_url_input': ""
                }
                
                # –í–∫–ª—é—á–∞–µ–º "–∞–≤—Ç–æ–ø–∏–ª–æ—Ç"
                st.session_state['start_analysis_flag'] = True
                st.session_state['analysis_done'] = False
                st.session_state['lsi_processing_task_id'] = next_task_idx
                
                st.toast(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞–±–æ—Ç—É –Ω–∞–¥: {next_task['h1']}")
                time.sleep(1)
                st.rerun()

# ------------------------------------------
# TAB 2: WHOLESALE GENERATOR (COMBINED)
# ------------------------------------------
with tab_wholesale_main:
    st.header("üè≠ –ï–¥–∏–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    
    # ==========================================
    # 0. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• (–ò–ó –¢–ï–ö–£–©–ï–ì–û –°–û–°–¢–û–Ø–ù–ò–Ø)
    # ==========================================
    cat_products = st.session_state.get('categorized_products', [])
    cat_services = st.session_state.get('categorized_services', [])
    
    # 1. –î–ª—è –¢–µ–≥–æ–≤ –∏ –ü—Ä–æ–º–æ (–°–∞–π–¥–±–∞—Ä –∏—Å–∫–ª—é—á–µ–Ω)
    structure_keywords = cat_products + cat_services
    count_struct = len(structure_keywords)

    if 'auto_tags_words' in st.session_state and st.session_state.auto_tags_words:
         tags_list_source = st.session_state.auto_tags_words
         promo_list_source = st.session_state.auto_promo_words
    else:
         if count_struct > 0:
            if count_struct < 10:
                tags_list_source = structure_keywords
                promo_list_source = []
            else:
                # –î–µ–ª–∏–º –≤—Å–µ–≥–¥–∞ –ø–æ–ø–æ–ª–∞–º (–¢–µ–≥–∏ / –ü—Ä–æ–º–æ), –°–∞–π–¥–±–∞—Ä –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
                mid = math.ceil(count_struct / 2)
                tags_list_source = structure_keywords[:mid]
                promo_list_source = structure_keywords[mid:]
         else:
             tags_list_source = []
             promo_list_source = []
    
    # –°–∞–π–¥–±–∞—Ä –≤—Å–µ–≥–¥–∞ –ø—É—Å—Ç–æ–π
    sidebar_default_text = ""

    tags_default_text = ", ".join(tags_list_source)
    promo_default_text = ", ".join(promo_list_source)

    # 2. –î–ª—è –¢–∞–±–ª–∏—Ü (–†–∞–∑–º–µ—Ä—ã/–ì–û–°–¢)
    cat_dimensions = st.session_state.get('categorized_dimensions', [])
    tech_context_default = ", ".join(cat_dimensions) if cat_dimensions else ""

    # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ö–æ–º–º–µ—Ä—Ü–∏–∏/–û–±—â–∏—Ö –∏ –ì–ï–û
    cat_commercial = st.session_state.get('categorized_commercial', [])
    cat_general = st.session_state.get('categorized_general', [])
    cat_geo = st.session_state.get('categorized_geo', [])
    
    # –ò–°–ö–õ–Æ–ß–ê–ï–ú –ì–ï–û –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    text_context_list_raw = cat_commercial + cat_general
    text_context_default = ", ".join(text_context_list_raw)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ñ–æ–ª—Ç –¥–ª—è –ì–ï–û –±–ª–æ–∫–∞
    geo_context_default = ", ".join(cat_geo)

    # --- –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ê–ö–¢–ò–í–ù–û–°–¢–ò –ú–û–î–£–õ–ï–ô ---
    auto_check_text = bool(text_context_list_raw)
    auto_check_tags = bool(tags_list_source)
    auto_check_tables = bool(cat_dimensions)
    auto_check_promo = bool(promo_list_source)
    auto_check_geo = bool(cat_geo)

    # ==========================================
    # 1. –í–í–û–î–ù–´–ï –î–ê–ù–ù–´–ï
    # ==========================================
    with st.container(border=True):
        st.subheader("1. –ò—Å—Ç–æ—á–Ω–∏–∫ –∏ –î–æ—Å—Ç—É–ø—ã")
        
        col_source, col_key = st.columns([3, 1])
        
        use_manual_html = st.checkbox("üìù –í—Å—Ç–∞–≤–∏—Ç—å HTML –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", key="cb_manual_html_mode", value=False)
        
        with col_source:
            if use_manual_html:
                manual_html_source = st.text_area(
                    "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (HTML)", 
                    height=200, 
                    placeholder="<html>...</html>", 
                    help="–°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Å—é–¥–∞ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."
                )
                main_category_url = None
            else:
                main_category_url = st.text_input(
                    "URL –ö–∞—Ç–µ–≥–æ—Ä–∏–∏", 
                    placeholder="https://site.ru/catalog/...", 
                    help="–°–∫—Ä–∏–ø—Ç —Å–æ–±–µ—Ä–µ—Ç —Ç–æ–≤–∞—Ä—ã —Å —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
                )
                manual_html_source = None

        with col_key:
            try:
                key_from_secrets = st.secrets["GEMINI_KEY"]
            except (FileNotFoundError, KeyError):
                key_from_secrets = ""

            default_key = st.session_state.get('gemini_key_cache', key_from_secrets)
            gemini_api_key = st.text_input("Google Gemini API Key", value=default_key, type="password")

    # ==========================================
    # 2. –í–´–ë–û–† –ú–û–î–£–õ–ï–ô
    # ==========================================
    st.subheader("2. –ö–∞–∫–∏–µ –±–ª–æ–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º?")
    st.info("‚ÑπÔ∏è **–ê–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞:** –ì–∞–ª–æ—á–∫–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ç–∞–º, –≥–¥–µ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—à–ª–∏—Å—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–ª–æ–≤–∞.")
    col_ch1, col_ch2, col_ch3, col_ch4, col_ch5, col_ch6 = st.columns(6)
    
    with col_ch1: use_text = st.checkbox("ü§ñ AI –¢–µ–∫—Å—Ç—ã", value=auto_check_text)
    with col_ch2: use_tags = st.checkbox("üè∑Ô∏è –¢–µ–≥–∏", value=auto_check_tags)
    with col_ch3: use_tables = st.checkbox("üß© –¢–∞–±–ª–∏—Ü—ã", value=auto_check_tables)
    with col_ch4: use_promo = st.checkbox("üî• –ü—Ä–æ–º–æ", value=auto_check_promo)
    
    # –û–¢–ö–õ–Æ–ß–ê–ï–ú –°–ê–ô–î–ë–ê–† –ó–î–ï–°–¨
    with col_ch5: use_sidebar = st.checkbox("üìë –°–∞–π–¥–±–∞—Ä (–û—Ç–∫–ª)", value=False, disabled=True, key="sidebar_disabled_ui")
    
    with col_ch6: use_geo = st.checkbox("üåç –ì–µ–æ-–±–ª–æ–∫", value=auto_check_geo)

    # ==========================================
    # 3. –ù–ê–°–¢–†–û–ô–ö–ò –ú–û–î–£–õ–ï–ô
    # ==========================================
    global_tags_list = []
    global_promo_list = []
    global_sidebar_list = []
    global_geo_list = []
    tags_file_content = ""
    table_prompts = []
    df_db_promo = None
    promo_title = "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º"
    sidebar_content = ""
    text_context_final_list = []
    tech_context_final_str = ""
    
    # –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–ª–æ–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞ (–ø–æ –¥–µ—Ñ–æ–ª—Ç—É 5)
    num_text_blocks_val = 5 

    if any([use_text, use_tags, use_tables, use_promo, use_sidebar, use_geo]):
        st.subheader("3. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥—É–ª–µ–π")

        # --- AI TEXT ---
        if use_text:
            with st.container(border=True):
                st.markdown("#### ü§ñ 1. AI –¢–µ–∫—Å—Ç—ã")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–±–æ—Ä –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–ª–æ–∫–æ–≤
                col_txt1, col_txt2 = st.columns([1, 4])
                with col_txt1:
                    num_text_blocks_val = st.selectbox("–ö–æ–ª-–≤–æ –±–ª–æ–∫–æ–≤", [1, 2, 3, 4, 5], index=4, key="sb_num_blocks")
                
                with col_txt2:
                    ai_words_input = st.text_area(
                        "–°–ª–æ–≤–∞ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è (–ö–æ–º–º–µ—Ä—Ü–∏—è + –û–±—â–∏–µ)", 
                        value=text_context_default, 
                        height=100, 
                        key="ai_text_context_editable",
                        help="–≠—Ç–∏ —Å–ª–æ–≤–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –ø–æ—Å—Ç–∞—Ä–∞–µ—Ç—Å—è –≤–Ω–µ–¥—Ä–∏—Ç—å –≤ —Ç–µ–∫—Å—Ç."
                    )
                
                text_context_final_list = [x.strip() for x in re.split(r'[,\n]+', ai_words_input) if x.strip()]

        # --- TAGS ---
        if use_tags:
            with st.container(border=True):
                st.markdown("#### üè∑Ô∏è 2. –¢–µ–≥–∏")
                kws_input_tags = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=tags_default_text, 
                    height=100, 
                    key="kws_tags_auto"
                )
                global_tags_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_tags) if x.strip()]
                if not global_tags_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                col_t1, col_t2 = st.columns([1, 2])
                with col_t1: u_manual = st.checkbox("–°–≤–æ—è –±–∞–∑–∞ —Å—Å—ã–ª–æ–∫ (.txt)", key="cb_tags_vert")
                with col_t2:
                    default_tags_path = "data/links_base.txt"
                    if not u_manual and os.path.exists(default_tags_path):
                        st.success(f"‚úÖ –ë–∞–∑–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (`links_base.txt`)")
                        with open(default_tags_path, "r", encoding="utf-8") as f: tags_file_content = f.read()
                    elif u_manual:
                        up_t = st.file_uploader("–§–∞–π–ª .txt", type=["txt"], key="up_tags_vert", label_visibility="collapsed")
                        if up_t: tags_file_content = up_t.getvalue().decode("utf-8")
                    else: st.error("‚ùå –§–∞–π–ª –±–∞–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω!")

# --- –§–£–ù–ö–¶–ò–Ø –ì–õ–£–ë–û–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê –ö–û–ù–¢–ï–ö–°–¢–ê –î–õ–Ø –¢–ê–ë–õ–ò–¶ ---
        def generate_context_aware_headers(count, query, dimensions_list, general_list):
            """
            –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ò –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (—Ä–∞–∑–º–µ—Ä—ã, –æ–±—â–∏–µ), 
            —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —Ç–∏–ø—ã —Ç–∞–±–ª–∏—Ü –Ω—É–∂–Ω—ã.
            """
            query_lower = query.lower()
            
            # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–∫–∏ —Å–ª–æ–≤ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            dims_str = " ".join(dimensions_list).lower()
            gen_str = " ".join(general_list).lower()
            full_context = f"{dims_str} {gen_str} {query_lower}"
            
            # --- 1. –î–ï–¢–ï–ö–¢–û–†–´ –°–ò–ì–ù–ê–õ–û–í (–ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Å–µ–º–∞–Ω—Ç–∏–∫–µ) ---
            
            # –ü—Ä–∏–∑–Ω–∞–∫ —Ä–∞–∑–º–µ—Ä–æ–≤: –µ—Å—Ç—å —Ü–∏—Ñ—Ä—ã —Å '—Ö' (10—Ö20), —Å–ª–æ–≤–∞ –º–º, –∫–≥, —Ç–æ–Ω–Ω–∞, —Ä–∞–∑–º–µ—Ä
            has_sizes_signal = (
                len(dimensions_list) > 0 or 
                bool(re.search(r'\d+[x—Ö*]\d+', full_context)) or 
                any(x in full_context for x in ['—Ä–∞–∑–º–µ—Ä', '–≥–∞–±–∞—Ä–∏—Ç', '—Ç–æ–ª—â–∏–Ω', '–¥–∏–∞–º–µ—Ç—Ä', '—Ä–∞—Å–∫—Ä–æ–π', '–≤–µ—Å', '–º–∞—Å—Å'])
            )
            
            # –ü—Ä–∏–∑–Ω–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤: –ì–û–°–¢, –û–°–¢, –¢–£, DIN, AISI
            has_gost_signal = any(x in full_context for x in ['–≥–æ—Å—Ç', 'din', 'aisi', 'astm', '—Ç—É ', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç'])
            
            # –ü—Ä–∏–∑–Ω–∞–∫ –º–∞—Ä–æ–∫/–º–∞—Ç–µ—Ä–∏–∞–ª–∞: —Å—Ç–∞–ª—å, —Å–ø–ª–∞–≤, –º–∞—Ä–∫–∞, —Å—Ç.3, 09–≥2—Å
            has_grade_signal = any(x in full_context for x in ['–º–∞—Ä–∫', '—Å–ø–ª–∞–≤', '—Å—Ç–∞–ª—å', '—Å—Ç.', '–º–∞—Ç–µ—Ä–∏–∞–ª', '—Ö–∏–º–∏—á', '—Å–æ—Å—Ç–∞–≤'])
            
            # –ü—Ä–∏–∑–Ω–∞–∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è: –¥–ª—è —á–µ–≥–æ, —Å—Ñ–µ—Ä–∞, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            has_usage_signal = any(x in full_context for x in ['–ø—Ä–∏–º–µ–Ω–µ–Ω', '—Å—Ñ–µ—Ä', '–Ω–∞–∑–Ω–∞—á–µ–Ω', '–∏—Å–ø–æ–ª—å–∑'])

            # --- 2. –°–ë–û–†–ö–ê –û–ß–ï–†–ï–î–ò (PRIORITY QUEUE) ---
            # –ú—ã —Ä–∞—Å—Å—Ç–∞–≤–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—ã –≤ –ø–æ—Ä—è–¥–∫–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
            priority_stack = []
            
            # –ï—Å–ª–∏ —ç—Ç–æ –º–µ—Ç–∞–ª–ª–æ–ø—Ä–æ–∫–∞—Ç (–µ—Å—Ç—å –º–∞—Ä–∫–∏/—Å–ø–ª–∞–≤—ã), –æ–±—ã—á–Ω–æ —Å–Ω–∞—á–∞–ª–∞ —Å—Ç–∞–≤—è—Ç –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–ª–∏ –ú–∞—Ä–∫–∏
            if has_grade_signal:
                priority_stack.append("–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã")
                
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã - —ç—Ç–æ —Å—É–ø–µ—Ä –≤–∞–∂–Ω–æ, —Å—Ç–∞–≤–∏–º –≤ –Ω–∞—á–∞–ª–æ
            if has_sizes_signal:
                # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –ú–∞—Ä–∫–∏, —Ç–æ –†–∞–∑–º–µ—Ä—ã –≤—Ç–æ—Ä—ã–º–∏. –ï—Å–ª–∏ –Ω–µ—Ç - –ø–µ—Ä–≤—ã–º–∏.
                priority_stack.append("–¢–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤")
                
            # –ï—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ì–û–°–¢–æ–≤
            if has_gost_signal:
                priority_stack.append("–ì–û–°–¢—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã")
                
            # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—Ä–æ —Ö–∏–º —Å–æ—Å—Ç–∞–≤ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ)
            if "—Ö–∏–º" in full_context and "—Å–æ—Å—Ç–∞–≤" in full_context:
                 # –í—Å—Ç–∞–≤–ª—è–µ–º "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤" –≤–º–µ—Å—Ç–æ "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã" –∏–ª–∏ —Ä—è–¥–æ–º
                 if "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã" in priority_stack:
                     idx = priority_stack.index("–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã")
                     priority_stack.insert(idx+1, "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤")
                 else:
                     priority_stack.append("–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤")

            # --- 3. –ó–ê–ü–û–õ–ù–ï–ù–ò–ï –ü–£–°–¢–û–¢ (DEFAULTS) ---
            # –ï—Å–ª–∏ –º—ã –≤—ã–±—Ä–∞–ª–∏ 5 —Ç–∞–±–ª–∏—Ü, –∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞—à–ª–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ 2, –Ω—É–∂–Ω–æ –¥–æ–±–∏—Ç—å –æ—Å—Ç–∞–ª—å–Ω–æ–µ
            defaults = [
                "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞
                "–°–≤–æ–π—Å—Ç–≤–∞",
                "–°—Ñ–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è",
                "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–¥–µ–ª–∏—è",
                "–ê–Ω–∞–ª–æ–≥–∏"
            ]
            
            final_headers = []
            # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ, —á—Ç–æ –Ω–∞—à–ª–∏ —É–º–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
            for p in priority_stack:
                if p not in final_headers: final_headers.append(p)
            
            # –î–æ–±–∏–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏
            for d in defaults:
                if d not in final_headers: final_headers.append(d)
                
            # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –≤—Å—ë —Ä–∞–≤–Ω–æ –º–∞–ª–æ (—Ä–µ–¥–∫–∏–π —Å–ª—É—á–∞–π)
            while len(final_headers) < count:
                final_headers.append("–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏")
                
            return final_headers[:count]

        # --- –ë–õ–û–ö –ò–ù–¢–ï–†–§–ï–ô–°–ê TABLES ---
        if use_tables:
            with st.container(border=True):
                st.markdown("#### üß© 3. –¢–∞–±–ª–∏—Ü—ã")
                
                # –î–ê–ù–ù–´–ï –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê (–ë–µ—Ä–µ–º –∏–∑ session_state, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞)
                raw_query = st.session_state.get('query_input', '')
                found_dims = st.session_state.get('categorized_dimensions', []) # –°–ª–æ–≤–∞—Ä—å —Ä–∞–∑–º–µ—Ä–æ–≤
                found_general = st.session_state.get('categorized_general', []) # –°–ª–æ–≤–∞—Ä—å –æ–±—â–∏—Ö —Å–ª–æ–≤
                
                col_ctx, col_cnt = st.columns([3, 1]) 
                
                with col_ctx:
                    tech_context_final_str = st.text_area(
                        "–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ç–∞–±–ª–∏—Ü (–ú–∞—Ä–∫–∏, –ì–û–°–¢, –†–∞–∑–º–µ—Ä—ã)", 
                        value=tech_context_default, # –ó–¥–µ—Å—å –ª–µ–∂–∞—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
                        height=68, 
                        key="table_context_editable",
                        help="–≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–º–æ–≥—É—Ç AI —Å–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É."
                    )
                
                with col_cnt:
                    cnt_options = [1, 2, 3, 4, 5]
                    cnt = st.selectbox("–ö–æ–ª-–≤–æ —Ç–∞–±–ª–∏—Ü", cnt_options, index=1, key="num_tbl_vert_select")

                # --- –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê ---
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢–û–ì–û, –ß–¢–û –ù–ê–®–õ–ò –í –°–ï–ú–ê–ù–¢–ò–ö–ï
                smart_headers_list = generate_context_aware_headers(cnt, raw_query, found_dims, found_general)

                table_presets = [
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–°–≤–æ–π—Å—Ç–≤–∞", "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–¥–µ–ª–∏—è",
                    "–û–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ", "–¢–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤", "–°–æ—Ä—Ç–∞–º–µ–Ω—Ç",
                    "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤", "–§–∏–∑–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞", "–ú–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞",
                    "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã", "–°–æ—Å—Ç–∞–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞", "–ì–û–°–¢—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã",
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –ì–û–°–¢", "–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ",
                    "–°—Ñ–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", "–£—Å–ª–æ–≤–∏—è —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏", "–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è",
                    "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", "–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏", "–ê–Ω–∞–ª–æ–≥–∏",
                    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π", "–†–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç–∏"
                ]
                
                table_prompts = []
                st.write("") 
                
                cols = st.columns(cnt)
                
                for i, col in enumerate(cols):
                    with col:
                        st.caption(f"**–¢–∞–±–ª–∏—Ü–∞ {i+1}**")
                        
                        # –ê–≤—Ç–æ-–≤—ã–±–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
                        suggested_topic = smart_headers_list[i]
                        
                        try: default_idx = table_presets.index(suggested_topic)
                        except: default_idx = 0
                        
                        is_manual = st.checkbox("–°–≤–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫", key=f"cb_tbl_manual_{i}")
                        
                        if is_manual:
                            selected_topic = st.text_input(
                                f"–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª. {i+1}", value="", 
                                key=f"tbl_topic_custom_{i}", label_visibility="collapsed"
                            )
                            if not selected_topic.strip(): selected_topic = "–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏" 
                        else:
                            selected_topic = st.selectbox(
                                f"–¢–µ–º–∞ —Ç–∞–±–ª. {i+1}", 
                                table_presets, 
                                index=default_idx, # <--- –£–ú–ù–´–ô –ò–ù–î–ï–ö–°
                                key=f"tbl_topic_select_{i}",
                                label_visibility="collapsed"
                            )
                        
                        table_prompts.append(selected_topic)

# --- PROMO (–° –ê–ù–ê–õ–ò–ó–û–ú –ö–û–ú–ú–ï–†–ß–ï–°–ö–ò–• –§–ê–ö–¢–û–†–û–í) ---
        if use_promo:
            with st.container(border=True):
                st.markdown("#### üî• 4. –ü—Ä–æ–º–æ-–±–ª–æ–∫")
                
                kws_input_promo = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=promo_default_text, 
                    height=100, 
                    key="kws_promo_auto"
                )
                global_promo_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_promo) if x.strip()]
                if not global_promo_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                
                col_p1, col_p2 = st.columns([1, 2])
                with col_p1:
                    promo_presets = [
                        "–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ", "–ü–æ—Ö–æ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã", "–í–∞—Å –º–æ–∂–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å",
                        "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º", "–î—Ä—É–≥–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", "–í–∞–º –º–æ–∂–µ—Ç –ø—Ä–∏–≥–æ–¥–∏—Ç—å—Å—è",
                        "–¢–∞–∫–∂–µ –≤ —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ", "–° —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º –ø–æ–∫—É–ø–∞—é—Ç", "–ß–∞—Å—Ç–æ –ø–æ–∫—É–ø–∞—é—Ç –≤–º–µ—Å—Ç–µ",
                        "–°–æ–ø—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–≤–∞—Ä—ã", "–•–∏—Ç—ã –ø—Ä–æ–¥–∞–∂", "–í—ã–±–æ—Ä –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π",
                        "–õ–∏–¥–µ—Ä—ã —Å–ø—Ä–æ—Å–∞", "–ü–æ–ø—É–ª—è—Ä–Ω–æ–µ —Å–µ–π—á–∞—Å", "–¢–æ–ø —Ç–æ–≤–∞—Ä–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏",
                        "–õ—É—á—à–∞—è —Ü–µ–Ω–∞", "–°–ø–µ—Ü–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", "–£—Å–ø–µ–π—Ç–µ –∑–∞–∫–∞–∑–∞—Ç—å",
                        "–ù–µ –∑–∞–±—É–¥—å—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å", "–í—ã –Ω–µ–¥–∞–≤–Ω–æ —Å–º–æ—Ç—Ä–µ–ª–∏"
                    ]

                    # --- –õ–û–ì–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê –ö–û–ú–ú–ï–†–¶–ò–ò ---
                    # –ë–µ—Ä–µ–º –∑–∞–ø—Ä–æ—Å + —Å–ø–∏—Å–æ–∫ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ (Tab 1)
                    raw_query = st.session_state.get('query_input', '').lower()
                    comm_words = st.session_state.get('categorized_commercial', [])
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                    comm_context = f"{raw_query} {' '.join(comm_words)}".lower()
                    
                    target_header = "–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ" # –î–µ—Ñ–æ–ª—Ç (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π)

                    # 1. –Ø–≤–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è (–µ—Å—Ç—å —Å–ª–æ–≤–∞ '—Ü–µ–Ω–∞', '–∫—É–ø–∏—Ç—å' –≤ —Å–µ–º–∞–Ω—Ç–∏–∫–µ –∏–ª–∏ –∑–∞–ø—Ä–æ—Å–µ)
                    is_commercial = any(x in comm_context for x in ["–∫—É–ø–∏—Ç—å", "—Ü–µ–Ω–∞", "–∑–∞–∫–∞–∑", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–ø—Ä–∞–π—Å", "–º–∞–≥–∞–∑–∏–Ω", "–∫–æ—Ä–∑–∏–Ω–∞"])
                    
                    # 2. –ê–∫—Ü–∏–æ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
                    is_promo = any(x in comm_context for x in ["–∞–∫—Ü–∏—è", "—Å–∫–∏–¥–∫", "—Ä–∞—Å–ø—Ä–æ–¥–∞–∂", "–≤—ã–≥–æ–¥–Ω"])
                    
                    # 3. –†–µ–π—Ç–∏–Ω–≥–æ–≤—ã–µ —Å–ª–æ–≤–∞
                    is_top = any(x in comm_context for x in ["—Ç–æ–ø", "–ª—É—á—à", "—Ä–µ–π—Ç–∏–Ω–≥", "–ø–æ–ø—É–ª—è—Ä–Ω"])

                    if is_promo:
                        target_header = "–°–ø–µ—Ü–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"
                    elif is_top:
                        target_header = "–õ–∏–¥–µ—Ä—ã —Å–ø—Ä–æ—Å–∞"
                    elif is_commercial:
                        # –ï—Å–ª–∏ —ç—Ç–æ —è–≤–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è, –ª—É—á—à–µ "–ü–æ–∫—É–ø–∞—é—Ç –≤–º–µ—Å—Ç–µ" –∏–ª–∏ "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º"
                        target_header = "–° —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º –ø–æ–∫—É–ø–∞—é—Ç"
                    
                    try: promo_smart_idx = promo_presets.index(target_header)
                    except: promo_smart_idx = 0

                    use_custom_header = st.checkbox("–í–≤–µ—Å—Ç–∏ —Å–≤–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫", key="cb_custom_header")
                    
                    if use_custom_header:
                        promo_title = st.text_input("–í–∞—à –∑–∞–≥–æ–ª–æ–≤–æ–∫", placeholder="–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ", key="pr_tit_vert")
                    else:
                        promo_title = st.selectbox(
                            "–í–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–≥–æ–ª–æ–≤–∫–∞", 
                            promo_presets, 
                            index=promo_smart_idx, # <--- –£–ú–ù–´–ô –í–´–ë–û–†
                            key="promo_header_select"
                        )

                    st.markdown("<br>", unsafe_allow_html=True)
                    u_img_man = st.checkbox("–°–≤–æ—è –±–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫", key="cb_img_vert")

                with col_p2:
                    default_img_db = "data/images_db.xlsx"
                    if not u_img_man and os.path.exists(default_img_db):
                        st.success("‚úÖ –ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ (`images_db.xlsx`)")
                        try: df_db_promo = pd.read_excel(default_img_db)
                        except: pass
                    elif u_img_man:
                        up_i = st.file_uploader("–§–∞–π–ª .xlsx", type=['xlsx'], key="up_img_vert", label_visibility="collapsed")
                        if up_i: df_db_promo = pd.read_excel(up_i)
                    else: st.error("‚ùå –ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")

        # --- SIDEBAR ---
        if use_sidebar:
            with st.container(border=True):
                st.markdown("#### üìë 5. –°–∞–π–¥–±–∞—Ä")
                kws_input_sidebar = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏", 
                    value=sidebar_default_text, 
                    height=100, 
                    key="kws_sidebar_auto"
                )
                global_sidebar_list = [x.strip() for x in kws_input_sidebar.split('\n') if x.strip()]
                if not global_sidebar_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                col_s1, col_s2 = st.columns([1, 2])
                with col_s1: u_sb_man = st.checkbox("–°–≤–æ–π —Ñ–∞–π–ª –º–µ–Ω—é (.txt)", key="cb_sb_vert")
                with col_s2:
                    def_menu = "data/menu_structure.txt"
                    if not u_sb_man and os.path.exists(def_menu):
                        st.success("‚úÖ –ú–µ–Ω—é —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (`menu_structure.txt`)")
                        with open(def_menu, "r", encoding="utf-8") as f: sidebar_content = f.read()
                    elif u_sb_man:
                        up_s = st.file_uploader("–§–∞–π–ª .txt", type=['txt'], key="up_sb_vert", label_visibility="collapsed")
                        if up_s: sidebar_content = up_s.getvalue().decode("utf-8")
                    else: st.error("‚ùå –§–∞–π–ª –º–µ–Ω—é –Ω–µ –Ω–∞–π–¥–µ–Ω!")

        # --- GEO BLOCK ---
        if use_geo:
            with st.container(border=True):
                st.markdown("#### üåç 6. –ì–µ–æ-–±–ª–æ–∫")
                kws_input_geo = st.text_area(
                    "–°–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤/—Ä–µ–≥–∏–æ–Ω–æ–≤ (–∏–∑ –≤–∫–ª–∞–¥–∫–∏ –ê–Ω–∞–ª–∏–∑) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=geo_context_default, 
                    height=100, 
                    key="kws_geo_auto"
                )
                global_geo_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_geo) if x.strip()]
                
                if not global_geo_list:
                    st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤ –ø—É—Å—Ç!")
                else:
                    st.info(f"–ë—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ç–µ–∫—Å—Ç –¥–æ—Å—Ç–∞–≤–∫–∏ –¥–ª—è –ø–æ–ª—è IP_PROP4819 —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º —ç—Ç–∏—Ö –≥–æ—Ä–æ–¥–æ–≤.")

    st.markdown("---")
    
# ==========================================
    # 4. –ó–ê–ü–£–°–ö –ì–ï–ù–ï–†–ê–¶–ò–ò (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê)
    # ==========================================
    
    ready_to_go = True
    if use_manual_html:
        if not manual_html_source: ready_to_go = False
    else:
        if not main_category_url: ready_to_go = False

    if (use_text or use_tables or use_geo) and not gemini_api_key: ready_to_go = False
    if use_promo and df_db_promo is None: ready_to_go = False

# ==========================================
    # üÜò –ë–õ–û–ö –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò (Gemini 2.0 Flash)
    # ==========================================
    st.markdown("---")
    with st.expander("üõ†Ô∏è –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê API (–ï—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∏)", expanded=True):
        if st.button("üì° –ü–†–û–í–ï–†–ò–¢–¨ GEMINI 2.0"):
            if not gemini_api_key:
                st.error("‚ùå –ö–ª—é—á API –Ω–µ –≤–≤–µ–¥–µ–Ω!")
            else:
                try:
                    from openai import OpenAI
                    client = OpenAI(api_key=gemini_api_key, base_url="https://litellm.tokengate.ru/v1")
                    response = client.chat.completions.create(
                        model="google/gemini-2.5-pro",
                        messages=[{"role": "user", "content": "Say OK"}]
                    )
                    st.success(f"‚úÖ –£–°–ü–ï–•! –û—Ç–≤–µ—Ç: {response.choices[0].message.content}")
                except Exception as e:
                    st.error(f"‚ùå –û–®–ò–ë–ö–ê: {str(e)}")
                    if "404" in str(e):
                        st.info("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω—É–∂–Ω–∞ –¥—Ä—É–≥–∞—è):")
                        try:
                            models = [m.name for m in genai.list_models()]
                            st.code("\n".join(models))
                        except: pass

    st.markdown("---")

# ==========================================
    # 4. –ó–ê–ü–£–°–ö –ì–ï–ù–ï–†–ê–¶–ò–ò (–ü–ê–ö–ï–¢–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê + LOGS)
    # ==========================================

    # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –æ–Ω–∞ –±—ã–ª–∞ —Ç–æ–ª—å–∫–æ –≤—ã—à–µ)
    if use_manual_html:
        if not manual_html_source: ready_to_go = False
    else:
        if not main_category_url: ready_to_go = False
    if (use_text or use_tables or use_geo) and not gemini_api_key: ready_to_go = False
    if use_promo and df_db_promo is None: ready_to_go = False

# ==========================================
    # 4. –£–ú–ù–´–ô –ó–ê–ü–£–°–ö (–°–ò–°–¢–ï–ú–ê STOP/RESUME + FULL GENERATION)
    # ==========================================
    st.markdown("### üöÄ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø—É—Å–∫–æ–º (–ê–≤—Ç–æ-—Ü–µ–ø–æ—á–∫–∞)")
    st.markdown("---")

    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π
    if 'auto_run_active' not in st.session_state: st.session_state.auto_run_active = False
    if 'auto_current_index' not in st.session_state: st.session_state.auto_current_index = 0
    if 'last_stopped_index' not in st.session_state: st.session_state.last_stopped_index = 0

    # 2. –ë–õ–û–ö –í–û–ó–û–ë–ù–û–í–õ–ï–ù–ò–Ø (–ü–æ—è–≤–ª—è–µ—Ç—Å—è, –µ—Å–ª–∏ –º—ã –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–ª–∏—Å—å)
    if not st.session_state.auto_run_active and st.session_state.last_stopped_index > 0:
        with st.container(border=True):
            st.warning(f"‚ö†Ô∏è **–ü—Ä–æ—Ü–µ—Å—Å –±—ã–ª –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.** –ü–æ—Å–ª–µ–¥–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å: {st.session_state.last_stopped_index}")
            
            col_res_btn, col_res_info = st.columns([1, 2])
            with col_res_btn:
                if st.button(f"‚èØÔ∏è –ü–†–û–î–û–õ–ñ–ò–¢–¨ —Å ‚Ññ {st.session_state.last_stopped_index}", type="primary", use_container_width=True):
                    st.session_state.auto_current_index = st.session_state.last_stopped_index
                    st.session_state.auto_run_active = True
                    st.rerun()
            with col_res_info:
                st.caption("–ù–∞–∂–º–∏—Ç–µ, —á—Ç–æ–±—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.")

# 3. –ù–ê–°–¢–†–û–ô–ö–ò –ó–ê–ü–£–°–ö–ê
    col_batch1, col_batch2, col_batch3 = st.columns([1, 1, 2])
    
    with col_batch1:
        # –ï—Å–ª–∏ –∑–∞–ø—É—â–µ–Ω–æ - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å (—Ä–∏–¥ –æ–Ω–ª–∏), –µ—Å–ª–∏ –Ω–µ—Ç - –ø–æ–ª–µ –≤–≤–æ–¥–∞
        if st.session_state.auto_run_active:
            st.text_input("üü¢ –í –ø—Ä–æ—Ü–µ—Å—Å–µ (–°—Ç–∞—Ä—Ç):", value=str(st.session_state.auto_current_index), disabled=True)
            start_index = st.session_state.auto_current_index
        else:
            # –ï—Å–ª–∏ —Ö–æ—Ç–∏–º –Ω–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ –∏–ª–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –º–µ—Å—Ç–∞ –≤—Ä—É—á–Ω—É—é
            start_index = st.number_input("–ù–∞—á–∞—Ç—å —Å —Ç–æ–≤–∞—Ä–∞ ‚Ññ (—Å 0)", min_value=0, value=st.session_state.last_stopped_index, step=1)

    with col_batch2:
        safe_batch_size = st.number_input("–†–∞–∑–º–µ—Ä –ø–∞—á–∫–∏ (—à—Ç)", min_value=1, value=5, help="–õ—É—á—à–µ 3-5 —à—Ç.")
        
    with col_batch3:
        st.write("")
        st.write("")
        enable_auto_chain = st.checkbox("üîÑ –ê–≤—Ç–æ-–ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–π –ø–∞—á–∫–µ", value=True, help="–ï—Å–ª–∏ –∞–∫—Ç–∏–≤–Ω–æ, —Å–∫—Ä–∏–ø—Ç –±—É–¥–µ—Ç —Å–∞–º –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–π–¥–µ—Ç –≤—Å–µ —Ç–æ–≤–∞—Ä—ã.")

# --- –ö–ù–û–ü–ö–ê –°–ë–†–û–°–ê –ö–≠–®–ê ---
    st.markdown("---")
    col_clear, _ = st.columns([2, 3])
    with col_clear:
        if st.button("üóëÔ∏è –û–ß–ò–°–¢–ò–¢–¨ –ö–≠–® –ì–ï–ù–ï–†–ê–¶–ò–ò (–°–±—Ä–æ—Å —Ç–∞–±–ª–∏—Ü—ã)", type="secondary", use_container_width=True, help="–£–¥–∞–ª—è–µ—Ç –≤—Å–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ü–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ –Ω–∞ —Ç–µ –∂–µ —Ç–æ–≤–∞—Ä—ã –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–∞ –¥—É–±–ª–µ–π."):
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
            st.session_state.gen_result_df = pd.DataFrame(columns=[
                'Page URL', 'Product Name', 'IP_PROP4839', 'IP_PROP4817', 'IP_PROP4818', 
                'IP_PROP4819', 'IP_PROP4820', 'IP_PROP4821', 'IP_PROP4822', 'IP_PROP4823', 
                'IP_PROP4824', 'IP_PROP4816', 'IP_PROP4825', 'IP_PROP4826', 'IP_PROP4834', 
                'IP_PROP4835', 'IP_PROP4836', 'IP_PROP4837', 'IP_PROP4838', 'IP_PROP4829', 'IP_PROP4831'
            ])
            st.session_state.unified_excel_data = None
            st.session_state.auto_current_index = 0
            st.session_state.last_stopped_index = 0
            
            # === –í–ê–ñ–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï ===
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã–∫–ª—é—á–∞–µ–º —Ñ–ª–∞–≥ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª –∞–≤—Ç–æ-–∑–∞–ø—É—Å–∫
            st.session_state.auto_run_active = False 
            # ==========================
            
            st.toast("‚úÖ –ö—ç—à –æ—á–∏—â–µ–Ω! –ü—Ä–æ—Ü–µ—Å—Å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.", icon="üóëÔ∏è")
            time.sleep(1)
            st.rerun()

    st.markdown("---")
    c_start, c_stop = st.columns([2, 1])
    with c_start:
        # –ö–Ω–æ–ø–∫–∞ –°–¢–ê–†–¢ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º—ã –ù–ï —Ä–∞–±–æ—Ç–∞–µ–º
        if not st.session_state.auto_run_active:
            if st.button("üöÄ –ó–ê–ü–£–°–¢–ò–¢–¨ –ù–û–í–´–ô –ü–†–û–¶–ï–°–°", type="primary", disabled=(not ready_to_go), use_container_width=True):
                st.session_state.auto_current_index = start_index
                st.session_state.auto_run_active = True
                st.session_state.last_stopped_index = start_index # –°–±—Ä–æ—Å –ø–∞–º—è—Ç–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–∏ –Ω–æ–≤–æ–º —Å—Ç–∞—Ä—Ç–µ
                st.rerun()
        else:
            st.info("‚è≥ –ü—Ä–æ—Ü–µ—Å—Å –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è...")
            
    # === –ò–ó–ú–ï–ù–ò–¢–¨ –í–û–¢ –≠–¢–û–¢ –ë–õ–û–ö ===
    with c_stop:
        if st.session_state.auto_run_active:
            st.button("‚õî –û–°–¢–ê–ù–û–í–ò–¢–¨ –ì–ï–ù–ï–†–ê–¶–ò–Æ", type="secondary", use_container_width=True, on_click=global_stop_callback)

    # =========================================================
    # –ì–õ–ê–í–ù–´–ô –ò–°–ü–û–õ–ù–Ø–Æ–©–ò–ô –ë–õ–û–ö
    # –ó–∞—Ö–æ–¥–∏–º —Å—é–¥–∞ –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –∞–∫—Ç–∏–≤–µ–Ω —Ñ–ª–∞–≥ auto_run_active
    # =========================================================

    if st.session_state.auto_run_active:
        
        # 0. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DataFrame –µ—Å–ª–∏ –Ω–µ—Ç
        if 'gen_result_df' not in st.session_state or st.session_state.gen_result_df is None:
             st.session_state.gen_result_df = pd.DataFrame(columns=[
                'Page URL', 'Product Name', 'IP_PROP4839', 'IP_PROP4817', 'IP_PROP4818', 
                'IP_PROP4819', 'IP_PROP4820', 'IP_PROP4821', 'IP_PROP4822', 'IP_PROP4823', 
                'IP_PROP4824', 'IP_PROP4816', 'IP_PROP4825', 'IP_PROP4826', 'IP_PROP4834', 
                'IP_PROP4835', 'IP_PROP4836', 'IP_PROP4837', 'IP_PROP4838', 'IP_PROP4829', 'IP_PROP4831'
            ])

        EXCEL_COLUMN_ORDER = st.session_state.gen_result_df.columns.tolist()
        TEXT_CONTAINERS = ['IP_PROP4839', 'IP_PROP4816', 'IP_PROP4838', 'IP_PROP4829', 'IP_PROP4831']

        # === 1. –ó–ê–ì–†–£–ó–ö–ê –ë–ê–ó (–ü–æ–≤—Ç–æ—Ä –ª–æ–≥–∏–∫–∏) ===
        all_tags_links = []
        if use_tags:
            if tags_file_content: 
                all_tags_links = [l.strip() for l in io.StringIO(tags_file_content).readlines() if l.strip()]
            elif os.path.exists("data/links_base.txt"):
                with open("data/links_base.txt", "r", encoding="utf-8") as f: 
                    all_tags_links = [l.strip() for l in f.readlines() if l.strip()]

        p_img_map = {}
        if use_promo and df_db_promo is not None:
            for _, row in df_db_promo.iterrows():
                u = str(row.iloc[0]).strip(); img = str(row.iloc[1]).strip()
                if u and u != 'nan' and img and img != 'nan': p_img_map[u.rstrip('/')] = img

        # === 2. –ü–û–î–ì–û–¢–û–í–ö–ê –°–ü–ò–°–ö–û–í –°–ï–ú–ê–ù–¢–ò–ö–ò ===
        raw_txt = st.session_state.get("ai_text_context_editable", "")
        list_text_initial = [x.strip() for x in re.split(r'[,\n]+', raw_txt) if x.strip()]
        
        raw_tags = st.session_state.get("kws_tags_auto", "")
        list_tags_initial = [x.strip() for x in re.split(r'[,\n]+', raw_tags) if x.strip()]
        
        raw_tables = st.session_state.get("table_context_editable", "")
        list_tables_final = [x.strip() for x in re.split(r'[,\n]+', raw_tables) if x.strip()] 
        str_tables_final = ", ".join(list_tables_final)

        raw_promo = st.session_state.get("kws_promo_auto", "")
        list_promo_initial = [x.strip() for x in re.split(r'[,\n]+', raw_promo) if x.strip()]

        raw_geo = st.session_state.get("kws_geo_auto", "")
        list_geo_final = [x.strip() for x in re.split(r'[,\n]+', raw_geo) if x.strip()]

        # –ü–æ–¥—Å—á–µ—Ç —Ü–µ–ª–µ–π SEO
        unique_seo_goals = set()
        if use_text: unique_seo_goals.update(list_text_initial)
        if use_tags: unique_seo_goals.update(list_tags_initial)
        if use_tables: unique_seo_goals.update(list_tables_final)
        if use_promo: unique_seo_goals.update(list_promo_initial)
        total_seo_goal = len(unique_seo_goals)

        # –ü–µ—Ä–µ–Ω–æ—Å —Å–ª–æ–≤
        final_tags_prepared = []
        final_text_seo_list = list(list_text_initial)
        
        if use_tags:
            for kw in list_tags_initial:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                matches = [u for u in all_tags_links if tr in u.lower()]
                if matches:
                    final_tags_prepared.append((kw, matches))
                else:
                    if kw not in final_text_seo_list: final_text_seo_list.append(kw)

        if use_promo and p_img_map:
            for kw in list_promo_initial:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                found_link = False
                for link in p_img_map.keys():
                    if tr in link.lower():
                        found_link = True; break
                if not found_link:
                    if kw not in final_text_seo_list: final_text_seo_list.append(kw)
        elif list_promo_initial: 
             for kw in list_promo_initial:
                 if kw not in final_text_seo_list: final_text_seo_list.append(kw)

        seo_keywords_string = ", ".join(final_text_seo_list)
        user_num_blocks = st.session_state.get("sb_num_blocks", 5)

        # –ü–õ–ï–ô–°–•–û–õ–î–ï–†–´ –î–õ–Ø –õ–û–ì–û–í
        live_download_placeholder = st.empty()
        live_table_placeholder = st.empty()
        log_container = st.status(f"üöÄ –í –†–ê–ë–û–¢–ï... –ü–∞—á–∫–∞ —Å {start_index}", expanded=True)

        # API CLIENT
        client = None
        if (use_text or use_tables or use_geo) and gemini_api_key:
            try:
                from openai import OpenAI
                client = OpenAI(api_key=gemini_api_key, base_url="https://litellm.tokengate.ru/v1")
            except Exception as e:
                log_container.error(f"–û—à–∏–±–∫–∞ API: {e}")
                st.session_state.auto_run_active = False
                st.stop()

        # Helper functions
        def resolve_real_names(urls_list, status_msg=""):
            if not urls_list: return {}
            results_map = {}
            if status_msg: log_container.write(status_msg)
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                future_to_url = {executor.submit(get_breadcrumb_only, u, st.session_state.settings_ua): u for u in urls_list}
                for future in concurrent.futures.as_completed(future_to_url):
                    url_key = future_to_url[future]
                    try:
                        extracted_name = future.result()
                        if extracted_name: results_map[url_key] = extracted_name
                    except: pass
            return results_map

# === –°–ë–û–† –°–¢–†–ê–ù–ò–¶ (–ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–ê–©–ò–¢–ê –û–¢ SSL –û–®–ò–ë–û–ö) ===
        log_container.write("üì• –°–±–æ—Ä —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü...")
        target_pages = []
        try:
            if use_manual_html:
                soup_main = BeautifulSoup(manual_html_source, 'html.parser')
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º curl_cffi –¥–ª—è –æ–±—Ö–æ–¥–∞ SSL –æ—à–∏–±–æ–∫
                try:
                    from curl_cffi import requests as cffi_requests
                    r = cffi_requests.get(
                        main_category_url, 
                        impersonate="chrome110", 
                        timeout=200,
                        headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'}
                    )
                    html_content = r.content
                except:
                    # Fallback
                    session = requests.Session()
                    r = session.get(main_category_url, timeout=200, verify=False)
                    html_content = r.text

                if r.status_code == 200: 
                    soup_main = BeautifulSoup(html_content, 'html.parser')
                else: 
                    log_container.error(f"–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {r.status_code}")
                    st.session_state.auto_run_active = False
                    st.stop()
            
            if soup_main:
                # –°–±–æ—Ä —Å—Å—ã–ª–æ–∫
                tags_container = soup_main.find(class_='popular-tags-inner')
                if tags_container:
                    for link in tags_container.find_all('a'):
                        href = link.get('href')
                        if href:
                            full_url = urljoin(main_category_url or "http://localhost", href)
                            target_pages.append({'url': full_url, 'name': link.get_text(strip=True)})
                
                # –ï—Å–ª–∏ —Ç–µ–≥–æ–≤ –Ω–µ—Ç, –∏—â–µ–º —Ö–æ—Ç—è –±—ã H1
                if not target_pages:
                    h1_found = soup_main.find('h1')
                    target_pages.append({'url': main_category_url or "local", 'name': h1_found.get_text(strip=True) if h1_found else "–¢–æ–≤–∞—Ä"})
                    
        except Exception as e:
            log_container.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–±–æ—Ä–∞: {e}")
            st.session_state.auto_run_active = False
            st.stop()

        total_found = len(target_pages)
        if start_index >= total_found:
             st.session_state.auto_run_active = False
             st.session_state.last_stopped_index = total_found
             st.success("üéâ –í—Å–µ —Ç–æ–≤–∞—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
             st.stop()

        end_index = min(start_index + safe_batch_size, total_found)
        target_pages_batch = target_pages[start_index:end_index]
        
        log_container.write(f"üìä –ü–ê–ß–ö–ê: {start_index+1} ‚Äî {end_index} –∏–∑ {total_found}")

# === –¶–ò–ö–õ –ü–û –ü–ê–ß–ö–ï (v9.0: –ò–ï–†–ê–†–•–ò–Ø –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–• + –õ–û–ì–ò–ß–ù–´–ï –ö–õ–Æ–ß–ò) ===
        for i, page in enumerate(target_pages_batch):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–µ–π
            current_urls_in_df = st.session_state.gen_result_df['Page URL'].values
            if page['url'] in current_urls_in_df:
                log_container.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –¥—É–±–ª—è: {page['name']}")
                continue 

            current_num = start_index + i + 1
            log_container.write(f"‚ñ∂Ô∏è **[{current_num}/{total_found}] {page['name']}**")
            
            # --- –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ---
            try:
                # 1. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π requests
                base_text_raw, _, real_header_h2, _ = get_page_data_for_gen(page['url'])
                header_for_ai = real_header_h2 if real_header_h2 else page['name']
                row_data = {col: "" for col in EXCEL_COLUMN_ORDER}
                row_data['Page URL'] = page['url']; row_data['Product Name'] = header_for_ai
                for k, v in STATIC_DATA_GEN.items():
                    if k in row_data: row_data[k] = v
                
                injections = []
                generated_full_text = "" 
                blocks = [""] * 5

                # =========================================================
                # –®–ê–ì 1. –ì–ï–ù–ï–†–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê
                # =========================================================
                if use_text and client:
                    log_container.write(f"   ‚Ü≥ ü§ñ –ü–∏—à–µ–º —Ç–µ–∫—Å—Ç...")
                    blocks_raw = generate_ai_content_blocks(
                        gemini_api_key, 
                        base_text_raw or "", 
                        page['name'], 
                        header_for_ai, 
                        user_num_blocks, 
                        final_text_seo_list
                    )
                    cleaned_blocks = [b.replace("```html", "").replace("```", "").strip() for b in blocks_raw]
                    for i_b in range(len(cleaned_blocks)):
                        if i_b < 5: blocks[i_b] = cleaned_blocks[i_b]
                    
                    generated_full_text = " ".join(blocks)

                # =========================================================
                # –®–ê–ì 2. –ì–ï–ù–ï–†–ê–¶–ò–Ø –¢–ê–ë–õ–ò–¶ (–°–¢–†–û–ì–ê–Ø –¢–ï–•–ù–ò–ß–ï–°–ö–ê–Ø –õ–û–ì–ò–ö–ê)
                # =========================================================
                if use_tables and client:
                    previous_tables_context = ""
                    keys_already_inserted = False 
                    
                    for t_topic in table_prompts:
                        context_snippet = generated_full_text[:3500] if generated_full_text else ""

                        # –õ–æ–≥–∏–∫–∞ –∫–ª—é—á–µ–π (1 —Ä–∞–∑, –Ω–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
                        if not keys_already_inserted and str_tables_final.strip():
                            curr_keys = str_tables_final
                            keys_instr = "–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –Ω–∞–π–¥–∏ –º–µ—Å—Ç–æ –¥–ª—è —ç—Ç–∏—Ö –∫–ª—é—á–µ–π. –ü–æ–¥–±–µ—Ä–∏ –¥–ª—è –Ω–∏—Ö –ª–æ–≥–∏—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ (–í–∏–¥, –¢–∏–ø, –ê–Ω–∞–ª–æ–≥–∏)."
                        else:
                            curr_keys = ""
                            keys_instr = ""

                        topic_guide = "–†–∞–∑–º–µ—Ä—ã, –¥–æ–ø—É—Å–∫–∏, –≤–µ—Å." if "–†–∞–∑–º–µ—Ä" in t_topic else ("–•–∏–º–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã." if "–•–∏–º" in t_topic else "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")

                        # === –ü–†–û–ú–¢ v9.0 (DATA QUALITY HIERARCHY) ===
                        prompt_tbl = f"""
    –¢–´ ‚Äî –°–¢–†–û–ì–ò–ô –¢–ï–•–ù–û–õ–û–ì. –ó–∞–¥–∞—á–∞: HTML-—Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è "{header_for_ai}".
    –¢–ï–ú–ê: {t_topic} ({topic_guide})
    
    –í–í–û–î–ù–´–ï:
    1. –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context_snippet} (–ò—â–∏ —Ñ–∞–∫—Ç—ã –∑–¥–µ—Å—å).
    2. –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–∏: [{curr_keys}] -> {keys_instr}
    3. –ò–≥–Ω–æ—Ä-–ª–∏—Å—Ç: {previous_tables_context}
    
    --- –ê–õ–ì–û–†–ò–¢–ú –ó–ê–ü–û–õ–ù–ï–ù–ò–Ø –Ø–ß–ï–ô–ö–ò (–ü–†–ò–û–†–ò–¢–ï–¢–´) ---
    1. üíé –ò–î–ï–ê–õ (–§–ê–ö–¢–´): –ü–∏—à–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, –¥–∏–∞–ø–∞–∑–æ–Ω—ã, –º–∞—Ä–∫–∏, –ì–û–°–¢—ã.
       - –ü—Ä–∏–º–µ—Ä: "HB 255", "–¥–æ 450 –ú–ü–∞", "–°—Ç3—Å–ø".
       
    2. üîß –ù–û–†–ú–ê (–¢–ï–†–ú–ò–ù–´): –ï—Å–ª–∏ —Ü–∏—Ñ—Ä –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ä–º–∏–Ω.
       - –í–º–µ—Å—Ç–æ "–•–æ—Ä–æ—à–∞—è —Å–≤–∞—Ä–∏–≤–∞–µ–º–æ—Å—Ç—å" -> –ø–∏—à–∏ "–ë–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π".
       - –í–º–µ—Å—Ç–æ "–¢–≤–µ—Ä–¥–∞—è —Å—Ç–∞–ª—å" -> –ø–∏—à–∏ "–í—ã—Å–æ–∫–æ–ø—Ä–æ—á–Ω–∞—è".
       
    3. ‚õî –ö–†–ê–ô–ù–ò–ô –°–õ–£–ß–ê–ô (–ü–†–û–ß–ï–†–ö): –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–æ–≤—Å–µ–º –Ω–µ—Ç.
       - –°—Ç–∞–≤—å "‚Äî".
       - –≠–¢–û –õ–£–ß–®–ï, —á–µ–º –ø–∏—Å–∞—Ç—å –≤–æ–¥—É ("–í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ", "–û—Ç–ª–∏—á–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞").
       
    --- –ü–†–ê–í–ò–õ–ê ---
    1. –ö–õ–Æ–ß–ò: –í—Å—Ç–∞–≤—å –∏—Ö –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ. –ï—Å–ª–∏ –∫–ª—é—á –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–Ω–∞–ø—Ä. –í–ì–ü –≤ –∫—Ä—É–≥–µ), —Å–æ–∑–¥–∞–π —Å—Ç—Ä–æ–∫—É "–°–º–µ–∂–Ω—ã–µ –≤–∏–¥—ã" –∏–ª–∏ "–¢–∞–∫–∂–µ –Ω–∞ —Å–∫–ª–∞–¥–µ".
    2. –û–§–û–†–ú–õ–ï–ù–ò–ï: 
       - –ú–∞—Ä–∫–∏/–ì–û–°–¢—ã ‚Äî –ó–ê–ì–õ–ê–í–ù–´–ú–ò.
       - –¢–æ–ª—å–∫–æ <table> —Å –∫–ª–∞—Å—Å–æ–º 'brand-accent-table' –∏ <thead>.
    """
                        try:
                            resp = client.chat.completions.create(model="google/gemini-2.5-pro", messages=[{"role": "user", "content": prompt_tbl}], temperature=0.25)
                            raw_table = resp.choices[0].message.content.replace("```html", "").replace("```", "").strip()
                            
                            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–∞ –ø–µ—Ä–µ–¥ —Ç–∞–±–ª–∏—Ü–µ–π
                            start_idx = raw_table.find("<table")
                            end_idx = raw_table.find("</table>")
                            
                            if start_idx != -1 and end_idx != -1:
                                clean_table_inner = raw_table[start_idx:end_idx+8]
                                if "brand-accent-table" not in clean_table_inner:
                                    clean_table_inner = clean_table_inner.replace("<table", "<table class='brand-accent-table'", 1)
                                
                                final_table_html = f'<div class="table-full-width-wrapper">{clean_table_inner}</div>'
                                injections.append(final_table_html)
                                
                                # –ó–∞–ø–∏—Å—å –≤ –ø–∞–º—è—Ç—å
                                content_stripped = re.sub(r'<[^>]+>', ' ', clean_table_inner)
                                previous_tables_context += f"\n[–¢–∞–±–ª–∏—Ü–∞ {t_topic}]: {content_stripped[:600]}..."
                                
                                # –ö–ª—é—á–∏ —Å—á–∏—Ç–∞–µ–º –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏
                                if curr_keys: keys_already_inserted = True

                        except Exception as e: 
                            log_container.write(f"–û—à–∏–±–∫–∞ —Ç–∞–±–ª–∏—Ü—ã: {e}")

                # =========================================================
                # –®–ê–ì 3. –û–°–¢–ê–õ–¨–ù–´–ï –ë–õ–û–ö–ò (–¢–ï–ì–ò, –ü–†–û–ú–û, –ì–ï–û)
                # =========================================================
                
                if use_tags and all_tags_links:
                    tags_cands_all = [u for u in all_tags_links if u.rstrip('/') != page['url'].rstrip('/')]
                    if tags_cands_all:
                        target_tag_urls = []
                        for kw in list_tags_initial:
                            tr_kw = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                            for url in tags_cands_all:
                                if tr_kw in url.lower() and url not in target_tag_urls:
                                    target_tag_urls.append(url); break 
                        needed_tags = 15
                        if len(target_tag_urls) < needed_tags:
                            pool_random = [u for u in tags_cands_all if u not in target_tag_urls]
                            if pool_random: target_tag_urls.extend(random.sample(pool_random, min(needed_tags - len(target_tag_urls), len(pool_random))))
                        if target_tag_urls:
                            tags_names_map = resolve_real_names(target_tag_urls)
                            html_t = []
                            for u in target_tag_urls:
                                name = tags_names_map.get(u, force_cyrillic_name_global(u.split("/")[-1]))
                                html_t.append(f'<a href="{u}" class="tag-item">{name}</a>')
                            injections.append(f'''<div class="popular-tags-text"><div class="popular-tags-inner-text"><div class="tag-items">{"\n".join(html_t)}</div></div></div>''')

# =========================================================
                # –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ë–õ–û–ö –ü–†–û–ú–û (–° –ß–ï–°–¢–ù–´–ú –†–ê–ù–î–û–ú–û–ú)
                # =========================================================
                if use_promo and p_img_map:
                    # 1. –ë–µ—Ä–µ–º –≤—Å–µ—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –∫—Ä–æ–º–µ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    p_cands_all = [u for u in p_img_map.keys() if u.rstrip('/') != page['url'].rstrip('/')]
                    
                    if p_cands_all:
                        target_urls = []
                        
                        # –®–ê–ì –ê: –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, —á—Ç–æ–±—ã –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–µ–º –º–µ–Ω—è–ª—Å—è
                        # (–ù–∞–ø—Ä–∏–º–µ—Ä, —á—Ç–æ–±—ã –Ω–µ –≤—Å–µ–≥–¥–∞ —Å–Ω–∞—á–∞–ª–∞ –∏—Å–∫–∞–ª–∞—Å—å "–¢—Ä—É–±–∞", –∞ –ø–æ—Ç–æ–º "–õ–∏—Å—Ç")
                        shuffled_keywords = list(list_promo_initial)
                        random.shuffle(shuffled_keywords)

                        # –®–ê–ì –ë: –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã —Å —Ä–∞–Ω–¥–æ–º–æ–º –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã
                        for kw in shuffled_keywords:
                            # –û–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—å, —á—Ç–æ–±—ã –Ω–µ –Ω–∞–±–∏—Ä–∞—Ç—å —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ, –µ—Å–ª–∏ –∫–ª—é—á–µ–π —Å–æ—Ç–Ω–∏
                            if len(target_urls) >= 10: break 

                            tr_kw = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                            
                            # –ù–∞—Ö–æ–¥–∏–º –í–°–ï –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å—Å—ã–ª–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —ç—Ç–æ—Ç –∫–ª—é—á (–∏ –∫–æ—Ç–æ—Ä—ã—Ö –µ—â–µ –Ω–µ—Ç –≤ —Å–ø–∏—Å–∫–µ)
                            all_matches_for_kw = [u for u in p_cands_all if tr_kw in u.lower() and u not in target_urls]
                            
                            if all_matches_for_kw:
                                # –í–ê–ñ–ù–û: –ë–µ—Ä–µ–º –°–õ–£–ß–ê–ô–ù–£–Æ —Å—Å—ã–ª–∫—É –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö, –∞ –Ω–µ –ø–µ—Ä–≤—É—é
                                target_urls.append(random.choice(all_matches_for_kw))
                        
                        # –®–ê–ì –í: –î–æ–±–∏–≤–∫–∞ –¥–æ –º–∏–Ω–∏–º—É–º–∞ (–æ–±—ã—á–Ω–æ 5 –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–π –≥–∞–ª–µ—Ä–µ–∏)
                        needed_total = 5
                        if len(target_urls) < needed_total:
                            pool_random = [u for u in p_cands_all if u not in target_urls]
                            if pool_random: 
                                count_to_add = min(needed_total - len(target_urls), len(pool_random))
                                target_urls.extend(random.sample(pool_random, count_to_add))
                        
                        # –®–ê–ì –ì: –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ –≤—ã–≤–æ–¥–æ–º, —á—Ç–æ–±—ã –ø–æ—Ä—è–¥–æ–∫ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –±—ã–ª —Ä–∞–∑–Ω—ã–º
                        random.shuffle(target_urls)

                        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∏–ª–µ–π)
                        if target_urls:
                            promo_names_map = resolve_real_names(target_urls)
                            gallery_items = []
                            for u in target_urls:
                                # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –ª–∏–±–æ –∏–∑ –ø–∞—Ä—Å–∏–Ω–≥–∞, –ª–∏–±–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑ URL
                                nm = promo_names_map.get(u, force_cyrillic_name_global(u.split("/")[-1]))
                                img_src = p_img_map[u]
                                gallery_items.append(f'''<div class="gallery-item"><h3><a href="{u}" target="_blank">{nm}</a></h3><figure><a href="{u}" target="_blank"><picture><img src="{img_src}" loading="lazy"></picture></a></figure></div>''')
                            
                            # –í—Å—Ç–∞–≤–ª—è–µ–º HTML
                            injections.append(f'''<style>.outer-full-width-section {{ padding: 25px 0; width: 100%; }}.gallery-content-wrapper {{ max-width: 1400px; margin: 0 auto; padding: 25px 15px; box-sizing: border-box; border-radius: 10px; overflow: hidden; background-color: #F6F7FC; }}h3.gallery-title {{ color: #3D4858; font-size: 1.8em; font-weight: normal; padding: 0; margin-top: 0; margin-bottom: 15px; text-align: left; }}.five-col-gallery {{ display: flex; justify-content: flex-start; align-items: flex-start; gap: 20px; margin-bottom: 0; padding: 0; list-style: none; flex-wrap: nowrap !important; overflow-x: auto !important; padding-bottom: 15px; }}.gallery-item {{ flex: 0 0 260px !important; box-sizing: border-box; text-align: center; scroll-snap-align: start; }}.gallery-item h3 {{ font-size: 1.1em; margin-bottom: 8px; font-weight: normal; text-align: center; line-height: 1.1em; display: block; min-height: 40px; }}.gallery-item h3 a {{ text-decoration: none; color: #333; display: block; height: 100%; display: flex; align-items: center; justify-content: center; transition: color 0.2s ease; }}.gallery-item h3 a:hover {{ color: #007bff; }}.gallery-item figure {{ width: 100%; margin: 0; float: none !important; height: 260px; overflow: hidden; margin-bottom: 5px; border-radius: 8px; }}.gallery-item figure a {{ display: block; height: 100%; text-decoration: none; }}.gallery-item img {{ width: 100%; height: 100%; display: block; margin: 0 auto; object-fit: cover; transition: transform 0.3s ease; border-radius: 8px; }}.gallery-item figure a:hover img {{ transform: scale(1.05); }}</style><div class="outer-full-width-section"><div class="gallery-content-wrapper"><h3 class="gallery-title">{promo_title}</h3><div class="five-col-gallery">{"".join(gallery_items)}</div></div></div>''')

                if use_geo and client:
                    log_container.write(f"   ‚Ü≥ üåç –ü–∏—à–µ–º –¥–æ—Å—Ç–∞–≤–∫—É...")
                    try:
                         cities = ", ".join(random.sample(list_geo_final, min(15, len(list_geo_final))))
                         prompt_geo = f"–ù–∞–ø–∏—à–∏ –æ–¥–∏–Ω HTML –ø–∞—Ä–∞–≥—Ä–∞—Ñ (<p>) –æ –¥–æ—Å—Ç–∞–≤–∫–µ —Ç–æ–≤–∞—Ä–∞ '{header_for_ai}' –≤ —Å–ª–µ–¥—É—é—â–∏–µ –≥–æ—Ä–æ–¥–∞: {cities}. –í–ø–∏—à–∏ –∫–ª—é—á–µ–≤–∏–∫–∏ {seo_keywords_string} (–≤—ã–¥–µ–ª–∏ <b>). –í—ã–¥–∞–π —Ç–æ–ª—å–∫–æ HTML."
                         resp = client.chat.completions.create(model="google/gemini-2.5-pro", messages=[{"role": "user", "content": prompt_geo}], temperature=0.5)
                         row_data['IP_PROP4819'] = resp.choices[0].message.content.replace("```html", "").replace("```", "").strip()
                    except: pass

                # =========================================================
                # –°–ë–û–†–ö–ê –ò –°–û–•–†–ê–ù–ï–ù–ò–ï
                # =========================================================
                effective_blocks_count = max(1, user_num_blocks)
                for i_inj, inj in enumerate(injections):
                    target_idx = i_inj % effective_blocks_count
                    blocks[target_idx] = blocks[target_idx] + "\n\n" + inj

                for i_c, c_name in enumerate(TEXT_CONTAINERS):
                    row_data[c_name] = blocks[i_c]

                new_row_df = pd.DataFrame([row_data])
                st.session_state.gen_result_df = pd.concat([st.session_state.gen_result_df, new_row_df], ignore_index=True)
                
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                    st.session_state.gen_result_df.to_excel(writer, index=False)
                st.session_state.unified_excel_data = buffer.getvalue()
                
                live_table_placeholder.dataframe(st.session_state.gen_result_df.tail(3), use_container_width=True)
                
                full_row_html = "".join([str(val) for val in row_data.values()])
                bolds_fact = full_row_html.count("<b>")
                
                # === –ù–û–í–´–ô –ö–û–î: –õ–û–í–ò–ú –°–õ–û–í–ê, –ö–û–¢–û–†–´–ï –ù–ò–ö–£–î–ê –ù–ï –†–ê–°–ü–†–ï–î–ï–õ–ò–õ–ò–°–¨ ===
                if 'global_unused_for_reviews' not in st.session_state:
                    st.session_state['global_unused_for_reviews'] = set()
                
                full_html_lower = full_row_html.lower()
                for kw in unique_seo_goals:
                    w_lower = str(kw).lower().strip()
                    if not w_lower: continue
                    # –û—Ç—Å–µ–∫–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–ª—è –≥–∏–±–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∫–æ—Ä–Ω—è
                    root = w_lower[:-2] if len(w_lower) > 5 else w_lower[:-1] if len(w_lower) > 3 else w_lower
                    
                    if root not in full_html_lower:
                        st.session_state['global_unused_for_reviews'].add(kw)
                # ================================================================

                with live_download_placeholder.container():
                    st.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {page['name']} (SEO-—Ç–µ–≥–æ–≤: {bolds_fact}/{total_seo_goal})")

            except Exception as e:
                log_container.error(f"–°–±–æ–π –Ω–∞ —Ç–æ–≤–∞—Ä–µ {page['name']}: {e}")

        log_container.update(label=f"‚úÖ –ü–∞—á–∫–∞ {start_index}-{end_index} –≥–æ—Ç–æ–≤–∞!", state="complete", expanded=False)
        
        # === –õ–û–ì–ò–ö–ê –ê–í–¢–û-–ü–ï–†–ï–ó–ê–ü–£–°–ö–ê (RELOAD) ===
        if enable_auto_chain:
            # –°–Ω–æ–≤–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞–∂–∞–ª –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –°–¢–û–ü –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–∞—á–∫–∏
            if st.session_state.auto_run_active:
                next_start = end_index
                if next_start < total_found:
                    st.session_state.auto_current_index = next_start
                    st.session_state.last_stopped_index = next_start # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è Resume
                    st.info(f"‚è≥ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ 1 —Å–µ–∫... –°–ª–µ–¥—É—é—â–∞—è –ø–∞—á–∫–∞ —Å {next_start}.")
                    time.sleep(1)
                    st.rerun() 
                else:
                    st.session_state.auto_run_active = False
                    st.session_state.last_stopped_index = total_found
                    st.balloons()
                    st.success("üèÅ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–û–õ–ù–û–°–¢–¨–Æ –ó–ê–í–ï–†–®–ï–ù–ê!")
            else:
                st.warning("‚õî –¶–µ–ø–æ—á–∫–∞ –±—ã–ª–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –≤—Ä—É—á–Ω—É—é.")

    # =========================================================
    # 5. –ë–õ–û–ö –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (–û–¢–û–ë–†–ê–ñ–ê–ï–¢–°–Ø –í–°–ï–ì–î–ê, –ï–°–õ–ò –ï–°–¢–¨ –î–ê–ù–ù–´–ï)
    # –≠—Ç–æ—Ç –∫–æ–¥ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç, –¥–∞–∂–µ –µ—Å–ª–∏ –≤—ã –Ω–∞–∂–∞–ª–∏ –°–¢–û–ü –∏ —Å–∫—Ä–∏–ø—Ç –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏–ª—Å—è
    # =========================================================

    has_data = (
        'gen_result_df' in st.session_state 
        and st.session_state.gen_result_df is not None 
        and not st.session_state.gen_result_df.empty
    )

    if has_data:
        st.markdown("---")
        st.success(f"üíæ **–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å:** –ì–æ—Ç–æ–≤–æ —Å—Ç—Ä–æ–∫: {len(st.session_state.gen_result_df)}")

        # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –±–∏–Ω–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ excel –ø–æ—Ç–µ—Ä—è–ª–∏—Å—å, –ø–µ—Ä–µ—Å–æ–∑–¥–∞–¥–∏–º –∏—Ö –∏–∑ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
        if st.session_state.get('unified_excel_data') is None:
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                st.session_state.gen_result_df.to_excel(writer, index=False)
            st.session_state.unified_excel_data = buffer.getvalue()

        col_dl_final, col_mon_final = st.columns([1, 1])

        with col_dl_final:
            st.download_button(
                label=f"üì• –°–ö–ê–ß–ê–¢–¨ –í–°–Å ({len(st.session_state.gen_result_df)} —à—Ç.)",
                data=st.session_state.unified_excel_data,
                file_name=f"wholesale_result_FULL_{int(time.time())}.xlsx",
                mime="application/vnd.ms-excel",
                key="btn_dl_persistent_v2",
                type="primary",
                use_container_width=True
            )

        with col_mon_final:
            if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å –≤ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", key="btn_add_mon_persistent", use_container_width=True):
                count_added = 0
                for idx, row in st.session_state.gen_result_df.iterrows():
                    u_val = str(row.get('Page URL', '')).strip()
                    kw_val = str(row.get('Product Name', '')).strip()
                    if u_val and kw_val and u_val != 'nan':
                        add_to_tracking(u_val, kw_val)
                        count_added += 1
                if count_added > 0:
                    st.toast(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {count_added} —Ç–æ–≤–∞—Ä–æ–≤!", icon="üìâ")

# === –ü–†–ï–î–ü–†–û–°–ú–û–¢–†–ê (–¢–û–ñ–ï –°–û–•–†–ê–ù–Ø–ï–¢–°–Ø) ===
        with st.expander("üëÄ –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Ç–æ–≥–æ, —á—Ç–æ —É–∂–µ –≥–æ—Ç–æ–≤–æ", expanded=False):
            # --- –í–°–¢–ê–í–õ–Ø–ï–ú –°–¢–ò–õ–ò CSS –î–õ–Ø –ö–†–ê–°–ò–í–´–• –¢–ê–ë–õ–ò–¶ ---
            st.markdown("""
            <style>
                /* –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞ */
                .preview-box {
                    border: 1px solid #e2e8f0;
                    background-color: #ffffff;
                    padding: 20px;
                    border-radius: 8px;
                    max-height: 600px;
                    overflow-y: auto;
                    box-shadow: inset 0 2px 4px 0 rgba(0, 0, 0, 0.06);
                }
                
                /* –í–ê–®–ò –°–¢–ò–õ–ò –î–õ–Ø –¢–ê–ë–õ–ò–¶ */
                .table-full-width-wrapper {
                    display: block !important;
                    width: 100% !important;
                    margin: 20px 0 !important;
                }
                .brand-accent-table {
                    width: 100% !important;
                    border-collapse: separate !important;
                    border-spacing: 0 !important;
                    background: white;
                    border-radius: 8px;
                    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
                    font-family: 'Inter', sans-serif;
                    border: 0 !important;
                }
                .brand-accent-table th {
                    background-color: #277EFF;
                    color: white;
                    text-align: left;
                    padding: 16px;
                    font-weight: 500;
                    font-size: 15px;
                    border: none;
                }
                .brand-accent-table th:first-child { border-top-left-radius: 8px; }
                .brand-accent-table th:last-child { border-top-right-radius: 8px; }
                .brand-accent-table td {
                    padding: 16px;
                    border-bottom: 1px solid #e5e7eb;
                    color: #4b5563;
                    font-size: 15px;
                    line-height: 1.4;
                }
                .brand-accent-table tr:last-child td { border-bottom: none; }
                .brand-accent-table tr:last-child td:first-child { border-bottom-left-radius: 8px; }
                .brand-accent-table tr:last-child td:last-child { border-bottom-right-radius: 8px; }
                .brand-accent-table tr:hover td { background-color: #f8faff; }
            </style>
            """, unsafe_allow_html=True)

            st.dataframe(st.session_state.gen_result_df, use_container_width=True)
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä –ø–æ –æ–¥–Ω–æ–º—É —Ç–æ–≤–∞—Ä—É
            df_p = st.session_state.gen_result_df
            if 'Product Name' in df_p.columns:
                all_products = df_p['Product Name'].tolist()
                # –í—ã–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                safe_index = len(all_products)-1 if len(all_products) > 0 else 0
                sel_p = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–æ–≤–∞—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–µ–∫—Å—Ç–∞:", all_products, index=safe_index, key="safe_preview_sel")
                
                if sel_p:
                    row_p = df_p[df_p['Product Name'] == sel_p].iloc[0]
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                    cols_to_show = ['IP_PROP4839', 'IP_PROP4816', 'IP_PROP4838', 'IP_PROP4829', 'IP_PROP4831', 'IP_PROP4819']
                    active_cols = [c for c in cols_to_show if str(row_p.get(c, "")).strip() != ""]
                    
                    if active_cols:
                        tabs = st.tabs([c.replace("IP_PROP", "") for c in active_cols])
                        for i, col in enumerate(active_cols):
                            with tabs[i]:
                                # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º HTML –≤–Ω—É—Ç—Ä–∏ —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                                st.markdown(f"<div class='preview-box'>{str(row_p[col])}</div>", unsafe_allow_html=True)
# ==========================================
# TAB 3: PROJECT MANAGER (SAVE/LOAD)
# ==========================================
with tab_projects:
    st.header("üìÅ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞–º–∏")
    st.markdown("–ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –≤ —Ñ–∞–π–ª –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–∞–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–µ–∫—Ç.")

    col_save, col_load = st.columns(2)

    # --- –§–£–ù–ö–¶–ò–Ø –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–Ø (CALLBACK) ---
    def restore_state_callback(data_to_restore):
        """
        –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –î–û –ø–µ—Ä–µ—Ä–∏—Å–æ–≤–∫–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.
        –ü–æ—ç—Ç–æ–º—É –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å session_state.
        """
        try:
            state_dict = data_to_restore["state"]
            restored_count = 0
            
            # 1. –û–±–Ω–æ–≤–ª—è–µ–º session_state
            for k, v in state_dict.items():
                st.session_state[k] = v
                restored_count += 1
            
            # 2. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–ª–∞–≥–∏
            st.session_state['analysis_done'] = True
            
            # 3. –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ (–ø–æ—è–≤–∏—Ç—Å—è –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏)
            st.toast(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {restored_count} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤!", icon="üéâ")
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –≤–Ω—É—Ç—Ä–∏ callback: {e}")

    # --- –ë–õ–û–ö –°–û–•–†–ê–ù–ï–ù–ò–Ø ---
    with col_save:
        with st.container(border=True):
            st.subheader("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–æ–µ–∫—Ç")
            
            if not st.session_state.get('analysis_done'):
                st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ–¥–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ (–í–∫–ª–∞–¥–∫–∞ SEO), —á—Ç–æ–±—ã –±—ã–ª–æ —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å.")
            else:
                st.info("–ë—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã, —Å–ø–∏—Å–∫–∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∏, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, —Å—Å—ã–ª–∫–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.")
                
                timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M")
                query_slug = transliterate_text(st.session_state.get('query_input', 'project'))[:20]
                default_filename = f"GAR_PRO_{query_slug}_{timestamp}.pkl"
                
                project_snapshot = {
                    "meta": {
                        "version": "2.6",
                        "date": str(datetime.datetime.now())
                    },
                    "state": {}
                }
                
                # –ö–ª—é—á–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                keys_to_save = [
                    'analysis_results', 'analysis_done', 'naming_table_df', 'ideal_h1_result',
                    'detected_anomalies', 'serp_trend_info', 'full_graph_data',
                    'categorized_products', 'categorized_services', 'categorized_commercial',
                    'categorized_dimensions', 'categorized_geo', 'categorized_general', 'categorized_sensitive',
                    'orig_products', 'orig_services', 'orig_commercial', 
                    'orig_dimensions', 'orig_geo', 'orig_general',
                    'sensitive_words_input_final', 'auto_tags_words', 'auto_promo_words',
                    'my_url_input', 'query_input', 'my_content_input', 'my_page_source_radio',
                    'competitor_source_radio', 'persistent_urls', 'excluded_urls_auto',
                    'settings_excludes', 'settings_stops', 'arsenkin_token', 'yandex_dict_key',
                    'settings_ua', 'settings_search_engine', 'settings_region', 'settings_top_n',
                    'settings_noindex', 'settings_alt', 'settings_numbers', 'settings_norm',
                    'gen_result_df', 'unified_excel_data'
                ]
                
                for k in keys_to_save:
                    if k in st.session_state:
                        project_snapshot["state"][k] = st.session_state[k]

                try:
                    pickle_data = pickle.dumps(project_snapshot)
                    st.download_button(
                        label="üì• –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª –ø—Ä–æ–µ–∫—Ç–∞ (.pkl)",
                        data=pickle_data,
                        file_name=default_filename,
                        mime="application/octet-stream",
                        type="primary",
                        use_container_width=True
                    )
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ø–∞–∫–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")

    # --- –ë–õ–û–ö –ó–ê–ì–†–£–ó–ö–ò ---
    with col_load:
        with st.container(border=True):
            st.subheader("üìÇ –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–µ–∫—Ç")
            
            uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª .pkl", type=["pkl"], key="project_loader")
            
            if uploaded_file is not None:
                try:
                    loaded_data = pickle.load(uploaded_file)
                    
                    if isinstance(loaded_data, dict) and "state" in loaded_data:
                        date_str = loaded_data['meta'].get('date', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        st.success(f"–ü—Ä–æ–µ–∫—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω! (–î–∞—Ç–∞: {date_str})")
                        
                        # –ò–°–ü–û–õ–¨–ó–£–ï–ú ON_CLICK –ò ARGS
                        # –≠—Ç–æ –≥–ª–∞–≤–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: —Ñ—É–Ω–∫—Ü–∏—è restore_state_callback –≤—ã–∑–æ–≤–µ—Ç—Å—è –î–û —Ç–æ–≥–æ,
                        # –∫–∞–∫ Streamlit –Ω–∞—á–Ω–µ—Ç –æ—Ç—Ä–∏—Å–æ–≤—ã–≤–∞—Ç—å –≤–∏–¥–∂–µ—Ç—ã –∑–∞–Ω–æ–≤–æ.
                        st.button(
                            "üöÄ –í–û–°–°–¢–ê–ù–û–í–ò–¢–¨ –°–û–°–¢–û–Ø–ù–ò–ï", 
                            type="primary", 
                            use_container_width=True,
                            on_click=restore_state_callback,
                            args=(loaded_data,)
                        )
                    else:
                        st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –ø—Ä–æ–µ–∫—Ç–∞.")
                except Exception as e:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")

# ==========================================
# –ú–û–ù–ò–¢–û–†–ò–ù–ì: –ß–ò–°–¢–ê–Ø –í–ï–†–°–ò–Ø (–ë–ï–ó –ú–£–°–û–†–ê)
# ==========================================
import os
import pandas as pd
import datetime
import time
import requests
import json
from urllib.parse import urlparse

TRACK_FILE = "monitoring.csv"

def add_to_tracking(url, keyword):
    if not os.path.exists(TRACK_FILE):
        with open(TRACK_FILE, "w", encoding="utf-8") as f:
            f.write("URL;Keyword;Date;Position\n")
    try:
        existing = pd.read_csv(TRACK_FILE, sep=";")
        if ((existing['URL'] == url) & (existing['Keyword'] == keyword)).any(): return
    except: pass
    with open(TRACK_FILE, "a", encoding="utf-8") as f:
        today = datetime.datetime.now().strftime("%Y-%m-%d")
        f.write(f"{url};{keyword};{today};0\n")

# –§—É–Ω–∫—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (—É–±–∏—Ä–∞–µ—Ç www –∏ http –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
def normalize_url(u):
    if not u: return ""
    u = str(u).lower().strip()
    u = u.replace("https://", "").replace("http://", "").replace("www.", "")
    if u.endswith("/"): u = u[:-1]
    return u

with tab_monitoring:
    st.header("üìâ –¢—Ä–µ–∫–µ—Ä –ø–æ–∑–∏—Ü–∏–π (DEBUG MODE)")

    # –í—ã–±–æ—Ä —Ä–µ–≥–∏–æ–Ω–∞
    default_reg_val = st.session_state.get('settings_region', '–ú–æ—Å–∫–≤–∞')
    try: def_index = list(REGION_MAP.keys()).index(default_reg_val)
    except: def_index = 0

    col_reg, col_btn, col_del = st.columns([2, 2, 1])
    
    with col_reg:
        selected_mon_region = st.selectbox("–†–µ–≥–∏–æ–Ω:", list(REGION_MAP.keys()), index=def_index, label_visibility="collapsed")

    # –§–æ—Ä–º–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
    with st.expander("‚ûï –î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –≤—Ä—É—á–Ω—É—é", expanded=False):
        with st.form("add_clean_manual"):
            col_u, col_k = st.columns(2)
            u_in = col_u.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã/—Å–∞–π—Ç–∞")
            k_in = col_k.text_input("–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ")
            if st.form_submit_button("–î–æ–±–∞–≤–∏—Ç—å –≤ —Å–ø–∏—Å–æ–∫"):
                if u_in and k_in:
                    add_to_tracking(u_in, k_in)
                    st.success(f"–î–æ–±–∞–≤–ª–µ–Ω–æ: {k_in}")
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –æ–±–∞ –ø–æ–ª—è")

    if not os.path.exists(TRACK_FILE):
        st.info("–°–ø–∏—Å–æ–∫ –ø—É—Å—Ç.")
    else:
        try: df_mon = pd.read_csv(TRACK_FILE, sep=";")
        except: df_mon = pd.DataFrame()

        if df_mon.empty:
            st.info("–§–∞–π–ª –±–∞–∑—ã –ø—É—Å—Ç.")
        else:
            with col_btn:
                if st.button("üöÄ –û–ë–ù–û–í–ò–¢–¨ –ü–û–ó–ò–¶–ò–ò", type="primary", use_container_width=True):
                    if not ARSENKIN_TOKEN:
                        st.error("‚ùå –û–®–ò–ë–ö–ê: –ù–µ—Ç —Ç–æ–∫–µ–Ω–∞!")
                    else:
                        status_container = st.status("üöÄ –ù–∞—á–∏–Ω–∞–µ–º...", expanded=True)
                        progress_bar = status_container.progress(0)
                        
                        reg_ids = REGION_MAP.get(selected_mon_region, {"ya": 213})
                        rid_int = int(reg_ids['ya'])
                        
                        total_rows = len(df_mon)

                        for i, row in df_mon.iterrows():
                            kw = str(row['Keyword']).strip()
                            target_url_raw = str(row['URL']).strip()
                            
                            # === 1. –í–´–î–ï–õ–Ø–ï–ú –ß–ò–°–¢–´–ô –î–û–ú–ï–ù –î–õ–Ø API ===
                            # API –≤ –ø–æ–ª–µ "url" —Ö–æ—á–µ—Ç "site.ru", –∞ –Ω–µ "site.ru/page"
                            parsed_url = urlparse(target_url_raw)
                            clean_domain = parsed_url.netloc.replace("www.", "")
                            if not clean_domain: clean_domain = target_url_raw.split('/')[0]

                            status_container.write(f"üì° –ó–∞–ø—Ä–æ—Å: **{kw}** (–î–æ–º–µ–Ω: {clean_domain})...")

                            payload = {
                                "tools_name": "positions",
                                "data": {
                                    "queries": [kw],
                                    "url": clean_domain, # <--- –û–¢–ü–†–ê–í–õ–Ø–ï–ú –¢–û–õ–¨–ö–û –î–û–ú–ï–ù
                                    "subdomain": True,
                                    "se": [{"type": 2, "region": rid_int}],
                                    "format": 0
                                }
                            }
                            
                            try:
                                # SET
                                r_set = requests.post("https://arsenkin.ru/api/tools/set", headers={"Authorization": f"Bearer {ARSENKIN_TOKEN}"}, json=payload, timeout=200)
                                if r_set.status_code != 200:
                                    st.error(f"HTTP Error: {r_set.status_code}")
                                    continue
                                
                                tid = r_set.json().get("task_id")
                                if not tid: 
                                    st.error(f"No Task ID: {r_set.json()}")
                                    continue

                                # CHECK
                                for _ in range(15):
                                    time.sleep(1.5)
                                    r_c = requests.post("https://arsenkin.ru/api/tools/check", headers={"Authorization": f"Bearer {ARSENKIN_TOKEN}"}, json={"task_id": tid})
                                    if r_c.json().get("status") == "finish": break
                                
                                # GET
                                r_get = requests.post("https://arsenkin.ru/api/tools/get", headers={"Authorization": f"Bearer {ARSENKIN_TOKEN}"}, json={"task_id": tid})
                                final_data = r_get.json()

                                # === üîç –û–¢–õ–ê–î–ö–ê: –í–´–í–û–î–ò–ú JSON –ù–ê –≠–ö–†–ê–ù ===
                                # –ï—Å–ª–∏ –∑–¥–µ—Å—å 0, –ø–æ—Å–º–æ—Ç—Ä–∏, —á—Ç–æ –≤–Ω—É—Ç—Ä–∏ JSON!
                                with status_container:
                                    st.write(f"üìù –û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è '{kw}':")
                                    st.json(final_data) 
                                
                                # –ü–ê–†–°–ò–ù–ì
                                res_data = final_data.get("result", [])
                                found_pos_val = 0
                                
                                if res_data and isinstance(res_data, list):
                                    item = res_data[0]
                                    
                                    # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –ø–æ –≤—Å–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–º –∫–ª—é—á–∞–º
                                    keys_to_check = ["position", "pos", str(rid_int)]
                                    
                                    for key in keys_to_check:
                                        val = item.get(key)
                                        if val is not None:
                                            # –ê—Ä—Å–µ–Ω–∫–∏–Ω –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å —á–∏—Å–ª–æ 11 –∏–ª–∏ —Å—Ç—Ä–æ–∫—É "11"
                                            if str(val).isdigit():
                                                found_pos_val = int(val)
                                                break
                                            # –ò–ª–∏ –≤–µ—Ä–Ω—É—Ç—å "-" –µ—Å–ª–∏ –Ω–µ –≤ —Ç–æ–ø–µ
                                            if str(val) in ["-", "0"]:
                                                found_pos_val = 0
                                                break
                                
                                df_mon.at[i, 'Position'] = found_pos_val
                                df_mon.at[i, 'Date'] = datetime.datetime.now().strftime("%Y-%m-%d")
                                df_mon.to_csv(TRACK_FILE, sep=";", index=False)
                                
                            except Exception as e:
                                st.error(f"Crash: {e}")
                            
                            progress_bar.progress((i + 1) / total_rows)

                        status_container.update(label="‚úÖ –ì–æ—Ç–æ–≤–æ!", state="complete", expanded=False)
                        st.rerun()

            # –¢–∞–±–ª–∏—Ü–∞
            def style_pos(v):
                try:
                    i = int(v)
                    if 0 < i <= 10: return 'color: #16a34a; font-weight: bold' 
                    if 10 < i <= 30: return 'color: #ca8a04' 
                    if i == 0: return 'color: #dc2626' 
                except: pass
                return ''

            st.dataframe(
                df_mon.style.map(style_pos, subset=['Position']),
                use_container_width=True,
                height=500,
                column_config={
                    "URL": st.column_config.LinkColumn("–°—Å—ã–ª–∫–∞"),
                    "Position": st.column_config.NumberColumn("–ü–æ–∑–∏—Ü–∏—è", format="%d"),
                    "Keyword": "–ö–ª—é—á",
                    "Date": "–î–∞—Ç–∞"
                }
            )
            
            with col_del:
                if st.button("üóëÔ∏è", help="–£–¥–∞–ª–∏—Ç—å –±–∞–∑—É"):
                    os.remove(TRACK_FILE); st.rerun()

# ==========================================
# TAB 5: LSI GENERATOR (FULL CYCLE + FIXES)
# ==========================================
with tab_lsi_gen:
    st.header("üè≠ –ú–∞—Å—Å–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è B2B (Full Technical Mode)")
    st.markdown("–ê–≤—Ç–æ-—Ü–∏–∫–ª: **H1 (–ú–∞—Ä–∫–µ—Ä) -> SEO –ê–Ω–∞–ª–∏–∑ (—Ñ–æ–Ω) -> LSI -> –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–æ–¥ H2**.")

    # --- –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ü–ï–†–ï–ú–ï–ù–ù–´–• ---
    if 'bg_tasks_queue' not in st.session_state: st.session_state.bg_tasks_queue = []
    if 'bg_results' not in st.session_state: st.session_state.bg_results = []
    if 'bg_is_running' not in st.session_state: st.session_state.bg_is_running = False
    if 'bg_batch_size' not in st.session_state: st.session_state.bg_batch_size = 3

# --- 1. –ù–ê–°–¢–†–û–ô–ö–ò (–ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –°–û–•–†–ê–ù–ï–ù–ò–ï –ö–õ–Æ–ß–ê) ---
    with st.expander("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API –∏ LSI", expanded=True):
        
# === –ñ–ï–õ–ï–ó–û–ë–ï–¢–û–ù–ù–û–ï –°–û–•–†–ê–ù–ï–ù–ò–ï –ö–õ–Æ–ß–ê ===
        # 1. –°–æ–∑–¥–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é, –∫–æ—Ç–æ—Ä–∞—è –ù–ï –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤–∏–¥–∂–µ—Ç–∞
        if 'FINAL_GEMINI_KEY' not in st.session_state:
            st.session_state.FINAL_GEMINI_KEY = ""
            
        # 2. –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–ª—é—á –≤ —Å–µ–∫—Ä–µ—Ç–∞—Ö –∏–ª–∏ –≤ —Å—Ç–∞—Ä—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        if not st.session_state.FINAL_GEMINI_KEY:
            try: st.session_state.FINAL_GEMINI_KEY = st.secrets["GEMINI_KEY"]
            except: pass
            
        # 3. –§—É–Ω–∫—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–∏ –≤–≤–æ–¥–µ
        def update_final_key():
            st.session_state.FINAL_GEMINI_KEY = st.session_state.bulk_api_key_v3

        default_lsi_text = "–≥–∞—Ä–∞–Ω—Ç–∏—è, –∑–≤–æ–Ω–∏—Ç–µ, –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è, –∫—É–ø–∏—Ç—å, –æ–ø–ª–∞—Ç–∞, –æ–ø—Ç–æ–º, –æ—Ç–≥—Ä—É–∑–∫–∞, –ø–æ–¥ –∑–∞–∫–∞–∑, –ø–æ—Å—Ç–∞–≤–∫–∞, –ø—Ä–∞–π—Å-–ª–∏—Å—Ç, —Ü–µ–Ω—ã"

        c1, c2 = st.columns([1, 2])
        with c1:
            # –ß–∏—Ç–∞–µ–º —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            current_val = st.session_state.SUPER_GLOBAL_KEY
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω–ø—É—Ç –ë–ï–ó –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ key (—ç—Ç–æ –≤–∞–∂–Ω–æ!)
            new_val = st.text_input(
                "Gemini API Key", 
                value=current_val, 
                type="password"
            )
            
            # –ï—Å–ª–∏ –≤–≤–µ–ª–∏ —á—Ç–æ-—Ç–æ –Ω–æ–≤–æ–µ ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –≥–ª–æ–±–∞–ª–∫—É
            if new_val != current_val:
                st.session_state.SUPER_GLOBAL_KEY = new_val
                st.rerun() # –û–±–Ω–æ–≤–ª—è–µ–º, —á—Ç–æ–±—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å

# –ï—Å–ª–∏ –≤–≤–µ–ª–∏ —á—Ç–æ-—Ç–æ –Ω–æ–≤–æ–µ ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –≥–ª–æ–±–∞–ª–∫—É
            if new_val != current_val:
                st.session_state.SUPER_GLOBAL_KEY = new_val
                st.rerun() # –û–±–Ω–æ–≤–ª—è–µ–º, —á—Ç–æ–±—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å

        with c2:
            # === –ü–û–õ–ï –î–õ–Ø –û–ë–©–ò–• LSI ===
            st.session_state['common_lsi_input'] = st.text_area(
                "–û–±—â–∏–µ LSI-—Å–ª–æ–≤–∞ (–¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –∫–æ –≤—Å–µ–º —Å—Ç–∞—Ç—å—è–º):", 
                value=st.session_state.get('common_lsi_input', default_lsi_text),
                help="–£–∫–∞–∂–∏—Ç–µ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é. –û–Ω–∏ –±—É–¥—É—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã —Å 15 —Å–ª–æ–≤–∞–º–∏ –∏–∑ –ø–∞—Ä—Å–∏–Ω–≥–∞."
            )

    # --- 2. –ó–ê–ì–†–£–ó–ö–ê –ó–ê–î–ê–ß ---
    st.subheader("1. –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–¥–∞—á")
    
    # –í—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞
    load_mode = st.radio(
        "–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö:", 
        ["üìù –í—Ä—É—á–Ω—É—é (–°–ø–∏—Å–∫–∏ H1 –∏ H2)", "üîó –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–ê–≤—Ç–æ-–ø–∞—Ä—Å–∏–Ω–≥)"], 
        horizontal=True
    )
    
    # 2.1 –†–£–ß–ù–û–ô –í–í–û–î
    if "–í—Ä—É—á–Ω—É—é" in load_mode:
        st.info("–í–≤–µ–¥–∏—Ç–µ —Å–ø–∏—Å–∫–∏. –°—Ç—Ä–æ–∫–∞ 1 –≤ –ª–µ–≤–æ–º –ø–æ–ª–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –°—Ç—Ä–æ–∫–µ 1 –≤ –ø—Ä–∞–≤–æ–º.")
        col_h1, col_h2 = st.columns(2)
        with col_h1:
            # –î–û–ë–ê–í–ò–õ KEY
            raw_h1_input = st.text_area(
                "–°–ø–∏—Å–æ–∫ H1 (–ú–ê–†–ö–ï–† –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê)", 
                height=200, 
                placeholder="–¢—Ä—É–±–∞ —Å—Ç–∞–ª—å–Ω–∞—è\n–õ–∏—Å—Ç –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω—ã–π",
                key="manual_h1_input" 
            )
        with col_h2:
            # –î–û–ë–ê–í–ò–õ KEY
            raw_h2_input = st.text_area(
                "–°–ø–∏—Å–æ–∫ H2 (–ó–ê–ì–û–õ–û–í–û–ö –°–¢–ê–¢–¨–ò)", 
                height=200, 
                placeholder="–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç—Ä—É–±—ã\n–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–∏—Å—Ç–∞",
                key="manual_h2_input"
            )
        raw_urls_input = None

    # 2.2 –ü–ê–†–°–ò–ù–ì –°–°–´–õ–û–ö
    else:
        st.info("–°–∫—Ä–∏–ø—Ç –∑–∞–π–¥–µ—Ç –Ω–∞ –∫–∞–∂–¥—É—é —Å—Å—ã–ª–∫—É, –Ω–∞–π–¥–µ—Ç —Ç–∞–º H1 (—Å—Ç–∞–Ω–µ—Ç –º–∞—Ä–∫–µ—Ä–æ–º) –∏ H2 (—Å—Ç–∞–Ω–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–º).")
        raw_urls_input = st.text_area("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", height=200, placeholder="https://site.ru/catalog/tovar1\nhttps://site.ru/catalog/tovar2", key="url_list_input")
        raw_h1_input = None; raw_h2_input = None

    # –ö–ù–û–ü–ö–ê –ó–ê–ì–†–£–ó–ö–ò –í –û–ß–ï–†–ï–î–¨
    if st.button("üì• –ó–∞–≥—Ä—É–∑–∏—Ç—å –∑–∞–¥–∞—á–∏ –≤ –æ—á–µ—Ä–µ–¥—å", use_container_width=True):
        st.session_state.bg_tasks_queue = [] # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—É—é –æ—á–µ—Ä–µ–¥—å –ø—Ä–∏ –Ω–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–µ
        st.session_state.bg_results = []
        st.session_state.bg_is_running = False
        
        # –õ–û–ì–ò–ö–ê –ó–ê–ì–†–£–ó–ö–ò (–†–£–ß–ù–ê–Ø)
        if "–í—Ä—É—á–Ω—É—é" in load_mode:
            lines_h1 = [l.strip() for l in raw_h1_input.split('\n') if l.strip()]
            lines_h2 = [l.strip() for l in raw_h2_input.split('\n') if l.strip()]
            
            if len(lines_h1) != len(lines_h2):
                st.error(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫! H1: {len(lines_h1)}, H2: {len(lines_h2)}")
            elif not lines_h1:
                st.error("‚ùå –°–ø–∏—Å–∫–∏ –ø—É—Å—Ç—ã!")
            else:
                for h1, h2 in zip(lines_h1, lines_h2):
                    st.session_state.bg_tasks_queue.append({
                        'h1': h1,
                        'h2': h2,
                        'source_url': 'Manual',
                        'lsi_added': []
                    })
                st.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–¥–∞—á –≤—Ä—É—á–Ω—É—é: {len(lines_h1)}")
                time.sleep(1)
                st.rerun()

        # –õ–û–ì–ò–ö–ê –ó–ê–ì–†–£–ó–ö–ò (–°–°–´–õ–ö–ò)
        else:
            urls_list = [u.strip() for u in raw_urls_input.split('\n') if u.strip()]
            if not urls_list:
                st.error("‚ùå –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –ø—É—Å—Ç!")
            else:
                progress_bar = st.progress(0)
                status_box = st.status("üîó –ü–∞—Ä—Å–∏–Ω–≥ —Å—Å—ã–ª–æ–∫...", expanded=True)
                valid_count = 0
                for i, url in enumerate(urls_list):
                    status_box.write(f"–°–∫–∞–Ω–∏—Ä—É—é: {url}...")
                    h1_found, h2_found, err = scrape_h1_h2_from_url(url)
                    if h1_found:
                        st.session_state.bg_tasks_queue.append({
                            'h1': h1_found,
                            'h2': h2_found,
                            'source_url': url,
                            'lsi_added': []
                        })
                        valid_count += 1
                    else:
                        status_box.warning(f"‚ö†Ô∏è –°–±–æ–π {url}: {err}")
                    progress_bar.progress((i + 1) / len(urls_list))
                
                status_box.update(label=f"‚úÖ –ì–æ—Ç–æ–≤–æ! –î–æ–±–∞–≤–ª–µ–Ω–æ: {valid_count}", state="complete")
                time.sleep(1)
                st.rerun()

    # --- 3. –£–ü–†–ê–í–õ–ï–ù–ò–ï –ü–†–û–¶–ï–°–°–û–ú ---
    
    total_q = len(st.session_state.bg_tasks_queue)
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–æ—Ç–æ–≤—ã–µ –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –ø–∞—Ä–µ
    finished_ids = set(f"{r['h1']}|{r['h2']}" for r in st.session_state.bg_results)
    
    pending_indices = []
    for i, t in enumerate(st.session_state.bg_tasks_queue):
        unique_id = f"{t['h1']}|{t['h2']}"
        if unique_id not in finished_ids:
            pending_indices.append(i)
            
    remaining_q = len(pending_indices)
    completed_q = total_q - remaining_q

    if total_q > 0:
        st.divider()
        st.subheader(f"2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (–ì–æ—Ç–æ–≤–æ: {completed_q} | –û—Å—Ç–∞–ª–æ—Å—å: {remaining_q})")
        
        # –ù–ê–°–¢–†–û–ô–ö–ò –ü–ê–ß–ö–ò (–í–ï–†–ù–£–õ –û–ë–†–ê–¢–ù–û)
        c_set1, c_set2 = st.columns([1, 3])
        with c_set1:
            st.session_state.bg_batch_size = st.number_input("–†–∞–∑–º–µ—Ä –ø–∞—á–∫–∏ (—à—Ç)", 1, 10, st.session_state.bg_batch_size)
        with c_set2:
            st.info("‚ö†Ô∏è –ë–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä –ø–∞—á–∫–∏ –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å —Ç–∞–π–º-–∞—É—Ç. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 2-3.")

# --- –§–£–ù–ö–¶–ò–Ø-–û–ë–†–ê–ë–û–¢–ß–ò–ö (CALLBACK) ---
        # –û–Ω–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç—Å—è –î–û –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –ø–æ—ç—Ç–æ–º—É –æ—à–∏–±–∫–∏ –Ω–µ –±—É–¥–µ—Ç
        def start_automode_callback(indices_list):
            st.session_state.lsi_automode_active = True
            if indices_list:
                idx = indices_list[0]
                task = st.session_state.bg_tasks_queue[idx]
                
                # –î–ª—è LSI —Ç–µ–∫—Å—Ç–æ–≤ –≤—Å–µ–≥–¥–∞ —Ä–µ–∂–∏–º "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
                st.session_state['pending_widget_updates'] = {
                    'query_input': task['h1'],
                    'my_page_source_radio': "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã",
                    'my_url_input': "",
                    'competitor_source_radio': "–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)"
                }
                
                st.session_state.lsi_processing_task_id = idx
                st.session_state.start_analysis_flag = True
                st.session_state.pop('analysis_results', None)
                st.session_state.pop('analysis_done', None)
        # -------------------------------------
        c_act1, c_act2, c_act3 = st.columns([1, 1, 1])
        with c_act1:
            if not st.session_state.get('lsi_automode_active'):
                btn_label = "‚ñ∂Ô∏è –°–¢–ê–†–¢ –ß–ï–†–ï–ó –í–ö–õ–ê–î–ö–£ 1" if remaining_q > 0 else "‚úÖ –í–°–ï –ì–û–¢–û–í–û"
                lsi_api_key = st.session_state.get('SUPER_GLOBAL_KEY')
                keys_valid = bool(lsi_api_key and ARSENKIN_TOKEN)
                
                if st.button(btn_label, type="primary", disabled=(remaining_q == 0), 
                             use_container_width=True,
                             on_click=start_automode_callback if keys_valid else None,
                             args=(pending_indices,) if keys_valid else None):
                    
                    if not keys_valid:
                        if not lsi_api_key: st.error("–í–≤–µ–¥–∏—Ç–µ API –∫–ª—é—á Gemini!")
                        if not ARSENKIN_TOKEN: st.error("–ù—É–∂–µ–Ω —Ç–æ–∫–µ–Ω Arsenkin!")
                    else:
                        st.toast("üöÄ –ó–∞–ø—É—Å–∫... –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –í–∫–ª–∞–¥–∫—É 1")
            else:
                # === –ò–ó–ú–ï–ù–ò–¢–¨ –í–û–¢ –≠–¢–û–¢ –ë–õ–û–ö ===
                st.button("‚õî –û–°–¢–ê–ù–û–í–ò–¢–¨ –ì–ï–ù–ï–†–ê–¶–ò–Æ", type="secondary", use_container_width=True, on_click=global_stop_callback)

        with c_act3:
            # –ö–Ω–æ–ø–∫–∞ —Å–±—Ä–æ—Å–∞
            if st.button("üóëÔ∏è –°–±—Ä–æ—Å –æ—á–µ—Ä–µ–¥–∏", disabled=st.session_state.get('lsi_automode_active', False), use_container_width=True):
                st.session_state.bg_tasks_queue = []
                st.session_state.bg_results = []
                st.session_state.lsi_automode_active = False
                keys_to_del = ["manual_h1_input", "manual_h2_input", "url_list_input"]
                for k in keys_to_del:
                    if k in st.session_state: del st.session_state[k]
                st.rerun()

        # ==================================================================
        # üî• HOOK –î–õ–Ø LSI –ì–ï–ù–ï–†–ê–¢–û–†–ê (–í–ö–õ–ê–î–ö–ê 5) - –ß–ò–°–¢–´–ô –ë–õ–û–ö
        # ==================================================================
        if st.session_state.get('lsi_automode_active'):
            current_idx = st.session_state.get('lsi_processing_task_id')
            
            if 'bg_tasks_queue' not in st.session_state or current_idx is None or current_idx >= len(st.session_state.bg_tasks_queue):
                st.session_state.lsi_automode_active = False
                st.success("–û—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞ –∏–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
                st.stop()

            task = st.session_state.bg_tasks_queue[current_idx]
            
            lsi_words = []
            results_data = st.session_state.get('analysis_results')
            if results_data and results_data.get('hybrid') is not None and not results_data['hybrid'].empty:
                lsi_words = results_data['hybrid'].head(15)['–°–ª–æ–≤–æ'].tolist()
            
# –ß–∏—Ç–∞–µ–º –æ–±—â–∏–µ LSI –∏–∑ –ø–æ–ª—è –≤–≤–æ–¥–∞ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º
            raw_common = st.session_state.get('common_lsi_input', "–≥–∞—Ä–∞–Ω—Ç–∏—è, –∑–≤–æ–Ω–∏—Ç–µ, –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è, –∫—É–ø–∏—Ç—å, –æ–ø–ª–∞—Ç–∞, –æ–ø—Ç–æ–º, –æ—Ç–≥—Ä—É–∑–∫–∞, –ø–æ–¥ –∑–∞–∫–∞–∑, –ø–æ—Å—Ç–∞–≤–∫–∞, –ø—Ä–∞–π—Å-–ª–∏—Å—Ç, –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º, —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å, —Ü–µ–Ω—ã")
            common_lsi = [w.strip() for w in raw_common.split(",") if w.strip()]
            combined_lsi = list(set(common_lsi + lsi_words))

            # 4. –ì–ï–ù–ï–†–ò–†–£–ï–ú –°–¢–ê–¢–¨–Æ
            api_key_gen = st.session_state.get('SUPER_GLOBAL_KEY')
            html_out = ""
            status_code = "Error"
            
            if not api_key_gen:
                html_out = "–û–®–ò–ë–ö–ê: –ö–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á –Ω–∞ –í–∫–ª–∞–¥–∫–µ 5!"
                st.error(html_out)
            else:
                try:
                    html_out = generate_full_article_v2(api_key_gen, task['h1'], task['h2'], combined_lsi)
                    status_code = "OK"
                except Exception as e:
                    html_out = f"Error generating: {e}"
                    status_code = "Gen Error"

            # 5. –°–û–•–†–ê–ù–Ø–ï–ú –†–ï–ó–£–õ–¨–¢–ê–¢
            if 'bg_results' not in st.session_state:
                st.session_state.bg_results = []
                
            found_existing = False
            for existing_res in st.session_state.bg_results:
                if existing_res['h1'] == task['h1'] and existing_res['h2'] == task['h2']:
                    existing_res['content'] = html_out
                    existing_res['lsi_added'] = lsi_words
                    existing_res['status'] = status_code
                    found_existing = True
                    break
            
            if not found_existing:
                st.session_state.bg_results.append({
                    "h1": task['h1'],
                    "h2": task['h2'],
                    "source_url": task.get('source_url', '-'),
                    "lsi_added": lsi_words,
                    "content": html_out,
                    "status": status_code
                })

            # 6. –ü–ï–†–ï–•–û–î –ö –°–õ–ï–î–£–Æ–©–ï–ô –ó–ê–î–ê–ß–ï
            next_task_idx = current_idx + 1
            
            if next_task_idx < len(st.session_state.bg_tasks_queue):
                next_task = st.session_state.bg_tasks_queue[next_task_idx]
                st.toast(f"‚úÖ –ì–æ—Ç–æ–≤–æ: {task['h1']}. –î–∞–ª—å—à–µ: {next_task['h1']}")
                
                # === –¢–û–ß–ï–ß–ù–ê–Ø –û–ß–ò–°–¢–ö–ê –°–¢–ê–†–´–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
                keys_to_clear = [
                    'analysis_results', 'analysis_done', 'naming_table_df', 
                    'ideal_h1_result', 'raw_comp_data', 'full_graph_data',
                    'detected_anomalies', 'serp_trend_info', 'excluded_urls_auto'
                ]
                for k in keys_to_clear:
                    st.session_state.pop(k, None)
                
# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–∏–¥–∂–µ—Ç–æ–≤ –≤ –±—É—Ñ–µ—Ä, —á—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω–∏—Ç—å –î–û –∏—Ö –æ—Ç—Ä–∏—Å–æ–≤–∫–∏
                st.session_state['pending_widget_updates'] = {
                    'query_input': next_task['h1'],
                    'competitor_source_radio': "–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)",
                    'my_page_source_radio': "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã",
                    'my_url_input': ""
                }
                st.session_state['lsi_processing_task_id'] = next_task_idx
                st.session_state['start_analysis_flag'] = True 
                st.session_state['analysis_done'] = False
                
                time.sleep(0.5)
                st.rerun()
                
            else:
                st.session_state.lsi_automode_active = False
                st.balloons()
                st.success("üèÅ –í–°–ï –ó–ê–î–ê–ß–ò –í –û–ß–ï–†–ï–î–ò –í–´–ü–û–õ–ù–ï–ù–´!")

    # --- 4. –≠–ö–°–ü–û–†–¢ –ò –ü–†–û–°–ú–û–¢–† ---
    if st.session_state.bg_results:
        st.divider()
        st.subheader("3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        
        df_res = pd.DataFrame(st.session_state.bg_results)
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–æ–∫ LSI –≤ —Å—Ç—Ä–æ–∫—É
        df_res['lsi_added'] = df_res['lsi_added'].apply(lambda x: ", ".join(x) if isinstance(x, list) else str(x))
        
        buf = io.BytesIO()
        with pd.ExcelWriter(buf, engine='xlsxwriter') as writer: 
            df_res.to_excel(writer, index=False)
            
        st.download_button("üì• –°–∫–∞—á–∞—Ç—å Excel (Full Report)", data=buf.getvalue(), file_name="SEO_Content_Result.xlsx", mime="application/vnd.ms-excel", type="primary")
        
        st.markdown("---")
        st.markdown("#### üëÅÔ∏è –ü—Ä–æ—Å–º–æ—Ç—Ä —Å—Ç–∞—Ç—å–∏")
        
        # –°—Ç–∏–ª–∏ –¥–ª—è –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞
        table_css = """
        <style>
            .brand-accent-table { width: 100%; border-collapse: collapse; margin: 20px 0; font-family: 'Inter', sans-serif; font-size: 14px; }
            .brand-accent-table th { background-color: #277EFF; color: white; padding: 12px; text-align: left; }
            .brand-accent-table td { border: 1px solid #e5e7eb; padding: 10px; color: #374151; }
            .brand-accent-table tr:nth-child(even) { background-color: #f9fafb; }
            ul { margin-bottom: 15px; }
            li { margin-bottom: 5px; }
        </style>
        """
        
        opts = [f"{i+1}. {r['h2']} (H1: {r['h1']})" for i, r in enumerate(st.session_state.bg_results)]
        sel = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–∞—Ç—å—é:", opts)
        
        if sel:
            idx = int(sel.split(".")[0]) - 1
            rec = st.session_state.bg_results[idx]
            
            lsi_str = ", ".join(rec['lsi_added']) if rec['lsi_added'] else "–ù–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö LSI"
            st.info(f"üìä **–°–æ–±—Ä–∞–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ (—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ):** {lsi_str}")
            
            with st.container(border=True):
                # –í—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç–∏–ª–∏ + –∫–æ–Ω—Ç–µ–Ω—Ç
                st.markdown(table_css + rec['content'], unsafe_allow_html=True)
            
            with st.expander("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ HTML"):
                st.code(rec['content'], language='html')

# ==================================================================
# ‚ùì –í–ö–õ–ê–î–ö–ê 6: FAQ –ì–ï–ù–ï–†–ê–¢–û–†
# ==================================================================
with tab_faq_gen:
    st.markdown("### ‚ùì –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ–ª–æ–≤–µ—á–Ω—ã—Ö FAQ –ø–æ TF-IDF")
    
    c_faq1, c_faq2 = st.columns([1, 2])
    with c_faq1:
        faq_source = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è FAQ:", ["–í—Ä—É—á–Ω—É—é (–°–ø–∏—Å–∫–∏ H1)", "–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–ê–≤—Ç–æ-–ø–∞—Ä—Å–∏–Ω–≥ H1)"])
        # –ü–æ–ª–∑—É–Ω–æ–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
        # –†—É—á–Ω–æ–π –≤–≤–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
        st.session_state['faq_questions_count'] = st.number_input(
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ (–æ—Ç 2 –¥–æ 100):", 
            min_value=2, max_value=100, value=st.session_state.get('faq_questions_count', 10), step=1
        )
        
    with c_faq2:
        st.info("–°–∫—Ä–∏–ø—Ç –ø–æ –æ—á–µ—Ä–µ–¥–∏ –ø—Ä–æ–≤–µ–¥–µ—Ç SEO-–∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞/—Å—Å—ã–ª–∫–∏, –≤–æ–∑—å–º–µ—Ç 15 —Ç–æ–ø–æ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç JSON-–º–∞—Å—Å–∏–≤ —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –∏ –æ—Ç–≤–µ—Ç–∞–º–∏.")
        
        # === –ü–û–õ–ï –î–õ–Ø API –ö–õ–Æ–ß–ê –ù–ê 6 –í–ö–õ–ê–î–ö–ï ===
        faq_api_val = st.text_input(
            "üîë Gemini API Key:", 
            value=st.session_state.get('SUPER_GLOBAL_KEY', ''), 
            type="password", 
            key="faq_api_key_input_unique"
        )
        # –ï—Å–ª–∏ –∫–ª—é—á –≤–≤–µ–ª–∏ –∏–ª–∏ –∏–∑–º–µ–Ω–∏–ª–∏ ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–∞–º—è—Ç—å
        if faq_api_val != st.session_state.get('SUPER_GLOBAL_KEY', ''):
            st.session_state['SUPER_GLOBAL_KEY'] = faq_api_val
            st.rerun()
        
    faq_input = st.text_area("–í–≤–µ–¥–∏—Ç–µ H1 –∏–ª–∏ URL (–∫–∞–∂–¥—ã–π —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏):", height=150)
    
    # 1. –ó–ê–ì–†–£–ó–ö–ê –ó–ê–î–ê–ß
    if st.button("üì• –ó–∞–≥—Ä—É–∑–∏—Ç—å –∑–∞–¥–∞—á–∏ (FAQ)", use_container_width=True):
        tasks = []
        lines = [line.strip() for line in faq_input.split('\n') if line.strip()]
        
        if faq_source == "–í—Ä—É—á–Ω—É—é (–°–ø–∏—Å–∫–∏ H1)":
            for line in lines:
                tasks.append({"h1": line, "url": "-"})
        else:
            with st.spinner("üïµÔ∏è –ü–∞—Ä—Å–∏–º H1 —Å —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤..."):
                import requests
                from bs4 import BeautifulSoup
                for url in lines:
                    try:
                        res = requests.get(url, timeout=5)
                        soup = BeautifulSoup(res.text, 'html.parser')
                        h1_tag = soup.find('h1')
                        h1_text = h1_tag.text.strip() if h1_tag else f"–ë–µ–∑ H1 ({url})"
                        tasks.append({"h1": h1_text, "url": url})
                    except:
                        tasks.append({"h1": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞", "url": url})
                        
        st.session_state.faq_tasks_queue = tasks
        st.session_state.faq_results = []
        st.success(f"‚úÖ –í –æ—á–µ—Ä–µ–¥—å –¥–æ–±–∞–≤–ª–µ–Ω–æ –∑–∞–¥–∞—á: {len(tasks)}")

    st.markdown("---")
    
    # 2. –ò–ù–§–û –û –ó–ê–î–ê–ß–ê–• –ò –ö–ù–û–ü–ö–ê –°–¢–ê–†–¢–ê
    faq_queue = st.session_state.get('faq_tasks_queue', [])
    faq_q_count = len(faq_queue)
    
    c_fstart1, c_fstart2 = st.columns([1, 1])
    with c_fstart1:
        st.markdown(f"**–í –æ—á–µ—Ä–µ–¥–∏:** {faq_q_count} —à—Ç. | **–ì–æ—Ç–æ–≤–æ:** {len(st.session_state.get('faq_results', []))} —à—Ç.")
        
        if not st.session_state.get('faq_automode_active'):
            btn_lbl = "‚ñ∂Ô∏è –°–¢–ê–†–¢ –ì–ï–ù–ï–†–ê–¶–ò–ò FAQ" if faq_q_count > 0 else "‚úÖ –í–°–ï FAQ –ì–û–¢–û–í–´"
            if st.button(btn_lbl, type="primary", disabled=(faq_q_count == 0), use_container_width=True, key="faq_start_btn_unique"):
                api_key_check = st.session_state.get('SUPER_GLOBAL_KEY')
                if not api_key_check:
                    st.error("–í–≤–µ–¥–∏—Ç–µ API –∫–ª—é—á Gemini (–Ω–∞ –í–∫–ª–∞–¥–∫–µ 5)!")
                else:
                    st.session_state.faq_automode_active = True
                    st.session_state.faq_processing_task_id = 0
                    first_t = st.session_state.faq_tasks_queue[0]
                    
                    st.session_state['pending_widget_updates'] = {
                        'query_input': first_t['h1'],
                        'competitor_source_radio': "–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)",
                        'my_page_source_radio': "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã",
                        'my_url_input': ""
                    }
                    st.session_state['start_analysis_flag'] = True 
                    st.session_state['analysis_done'] = False
                    st.toast("üöÄ –ó–∞–ø—É—Å–∫ FAQ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞... –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –í–∫–ª–∞–¥–∫—É 1")
                    st.rerun()
        else:
            # === –ò–ó–ú–ï–ù–ò–¢–¨ –í–û–¢ –≠–¢–û–¢ –ë–õ–û–ö ===
            st.button("‚õî –û–°–¢–ê–ù–û–í–ò–¢–¨ –ì–ï–ù–ï–†–ê–¶–ò–Æ", type="secondary", use_container_width=True, on_click=global_stop_callback)

    with c_fstart2:
        if st.button("üóëÔ∏è –°–±—Ä–æ—Å–∏—Ç—å –æ—á–µ—Ä–µ–¥—å FAQ", disabled=st.session_state.get('faq_automode_active', False), use_container_width=True):
            st.session_state.faq_tasks_queue = []
            st.session_state.faq_results = []
            st.session_state.faq_automode_active = False
            st.rerun()

# ==================================================================
    # üî• HOOK –î–õ–Ø FAQ –ì–ï–ù–ï–†–ê–¢–û–†–ê (–°–†–ê–ë–ê–¢–´–í–ê–ï–¢ –ü–û–°–õ–ï –ü–ï–†–í–û–ô –í–ö–õ–ê–î–ö–ò)
    # ==================================================================
    if st.session_state.get('faq_automode_active'):
        curr_idx = st.session_state.get('faq_processing_task_id')
        if 'faq_tasks_queue' not in st.session_state or curr_idx is None or curr_idx >= len(st.session_state.faq_tasks_queue):
            st.session_state.faq_automode_active = False
            st.stop()

        task = st.session_state.faq_tasks_queue[curr_idx]
        target_q_count = st.session_state.get('faq_questions_count', 10)
        
        lsi_words = []
        res_data = st.session_state.get('analysis_results')
        if res_data and res_data.get('hybrid') is not None and not res_data['hybrid'].empty:
            # –ñ–µ—Å—Ç–∫–æ –±–µ—Ä–µ–º –¢–û–ü-150 —Å–ª–æ–≤, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å –º—É—Å–æ—Ä–æ–º
            lsi_words = res_data['hybrid'].head(150)['–°–ª–æ–≤–æ'].tolist()
        
        # –ì–ï–ù–ï–†–ê–¶–ò–Ø
        api_key_gen = str(st.session_state.get('SUPER_GLOBAL_KEY', '')).strip()
        faq_json_result = generate_faq_gemini(api_key_gen, task['h1'], lsi_words, target_q_count)
        
        if 'faq_results' not in st.session_state: st.session_state.faq_results = []

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—Ç–∑—ã–≤–æ–≤
        if 'reviews_results' not in st.session_state: st.session_state.reviews_results = []
        if 'reviews_queue' not in st.session_state: st.session_state.reviews_queue = []
        if 'reviews_automode_active' not in st.session_state: st.session_state.reviews_automode_active = False
        if 'reviews_current_index' not in st.session_state: st.session_state.reviews_current_index = 0
        if 'reviews_per_query' not in st.session_state: st.session_state.reviews_per_query = 3
        
        st.session_state.faq_results.append({
            "h1": task['h1'],
            "url": task['url'],
            "lsi": lsi_words,
            "faq_data": faq_json_result
        })

        # –ü–ï–†–ï–•–û–î –î–ê–õ–¨–®–ï
        next_idx = curr_idx + 1
        if next_idx < len(st.session_state.faq_tasks_queue):
            next_t = st.session_state.faq_tasks_queue[next_idx]
            st.toast(f"‚úÖ FAQ –≥–æ—Ç–æ–≤: {task['h1']}")
            
            # –û—á–∏—Å—Ç–∫–∞ –º—É—Å–æ—Ä–∞
            keys_to_clear = ['analysis_results', 'analysis_done', 'naming_table_df', 'ideal_h1_result', 'raw_comp_data', 'full_graph_data', 'detected_anomalies', 'serp_trend_info', 'excluded_urls_auto']
            for k in keys_to_clear: st.session_state.pop(k, None)
            
            # –ë—É—Ñ–µ—Ä –≤–∏–¥–∂–µ—Ç–æ–≤
            st.session_state['pending_widget_updates'] = {
                'query_input': next_t['h1'],
                'competitor_source_radio': "–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)",
                'my_page_source_radio': "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã",
                'my_url_input': ""
            }
            st.session_state['faq_processing_task_id'] = next_idx
            st.session_state['start_analysis_flag'] = True 
            st.session_state['analysis_done'] = False
            import time
            time.sleep(0.5)
            st.rerun()
        else:
            st.session_state.faq_automode_active = False
            st.balloons()
            st.success("üèÅ –í–°–ï FAQ –°–ì–ï–ù–ï–†–ò–†–û–í–ê–ù–´!")

# 3. –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò –≠–ö–°–ü–û–†–¢ –í EXCEL
    if st.session_state.get('faq_results'):
        st.markdown("### üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
        
        # --- –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø EXCEL ---
        all_faq_rows = []
        for res in st.session_state.faq_results:
            h1_val = res['h1']
            url_val = res['url']
            lsi_val = ", ".join(res['lsi'])
            
            faq_items = res['faq_data']
            if isinstance(faq_items, list):
                for item in faq_items:
                    if isinstance(item, dict):
                        all_faq_rows.append({
                            "H1 / –ó–∞–≥–æ–ª–æ–≤–æ–∫": h1_val,
                            "URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞": url_val,
                            "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ LSI": lsi_val,
                            "–í–æ–ø—Ä–æ—Å": item.get("–í–æ–ø—Ä–æ—Å", ""),
                            "–û—Ç–≤–µ—Ç": item.get("–û—Ç–≤–µ—Ç", "")
                        })
        
        # --- –ö–ù–û–ü–ö–ê –°–ö–ê–ß–ò–í–ê–ù–ò–Ø EXCEL ---
        if all_faq_rows:
            import pandas as pd
            import io
            
            df_export = pd.DataFrame(all_faq_rows)
            
            # –°–æ–∑–¥–∞–µ–º Excel-—Ñ–∞–π–ª –≤ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                df_export.to_excel(writer, index=False, sheet_name='FAQ_–†–µ–∑—É–ª—å—Ç–∞—Ç—ã')
                
                # –î–µ–ª–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ —à–∏—Ä–µ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —á—Ç–µ–Ω–∏—è
                worksheet = writer.sheets['FAQ_–†–µ–∑—É–ª—å—Ç–∞—Ç—ã']
                worksheet.set_column('A:B', 30)
                worksheet.set_column('C:C', 40)
                worksheet.set_column('D:E', 70)

            excel_data = output.getvalue()
            
            st.download_button(
                label="üíæ –°–ö–ê–ß–ê–¢–¨ –í–°–ï FAQ –í EXCEL",
                data=excel_data,
                file_name="–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ_FAQ.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                type="primary",
                use_container_width=True
            )
            
            st.markdown("---")
            
        # --- –ü–†–ï–î–ü–†–û–°–ú–û–¢–† –ù–ê –≠–ö–†–ê–ù–ï ---
        for res in st.session_state.faq_results:
            with st.expander(f"üìå {res['h1']} ({res['url']})"):
                st.caption(f"**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —Å–ª–æ–≤–∞:** {', '.join(res['lsi'])}")
                faq_items = res['faq_data']
                if isinstance(faq_items, list) and len(faq_items) > 0 and isinstance(faq_items[0], dict):
                    import pandas as pd
                    st.table(pd.DataFrame(faq_items))
                else:
                    st.error("–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:")
                    st.write(faq_items)

# ==========================================
# TAB 7: –ì–ï–ù–ï–†–ê–¢–û–† –û–¢–ó–´–í–û–í
# ==========================================
with tab_reviews_gen:
    st.header("üí¨ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç–∑—ã–≤–æ–≤ (–ê–≤—Ç–æ–º–∞—Ç)")
    
    rev_mode = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤:", ["–°–ø–∏—Å–æ–∫ H1", "–°–ø–∏—Å–æ–∫ URL"], horizontal=True, key="rev_mode_radio")
    rev_input = st.text_area("–í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É):", height=150, key="rev_data_input")
    rev_count_input = st.number_input("–°–∫–æ–ª—å–∫–æ –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ –æ–¥–∏–Ω —Ç–æ–≤–∞—Ä?", 1, 10, 3, key="rev_count_val")

    col_r1, col_r2 = st.columns(2)
    with col_r1:
        if st.button("üöÄ –ó–ê–ü–£–°–¢–ò–¢–¨ –ì–ï–ù–ï–†–ê–¶–ò–Æ", type="primary", use_container_width=True):
            lines =[l.strip() for l in rev_input.split('\n') if l.strip()]
            if lines:
                queue =[]
                if rev_mode == "–°–ø–∏—Å–æ–∫ URL":
                    for u in lines:
                        h1_text = get_h1_from_url(u) 
                        if not h1_text:
                            h1_text = u.split('/')[-1].replace('-', ' ').capitalize()
                        queue.append({'q': h1_text, 'url': u})
                else:
                    for q in lines: 
                        queue.append({'q': q, 'url': 'manual'})
                
                st.session_state.reviews_queue = queue
                st.session_state.reviews_results =[]
                st.session_state.reviews_current_index = 0
                st.session_state.reviews_per_query = rev_count_input
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º –¥—Ä—É–≥–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
                st.session_state.lsi_automode_active = False
                st.session_state.faq_automode_active = False
                st.session_state.reviews_automode_active = True

                updates = {
                    'query_input': queue[0]['q'],
                    'competitor_source_radio': "–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)"
                }
                
                if rev_mode == "–°–ø–∏—Å–æ–∫ URL":
                    updates['my_page_source_radio'] = "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ"
                    updates['my_url_input'] = queue[0]['url']
                else:
                    updates['my_page_source_radio'] = "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
                    updates['my_url_input'] = ""
                
                st.session_state['pending_widget_updates'] = updates
                st.session_state.start_analysis_flag = True
                
                # === –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–ë–†–û–° –°–¢–ê–†–û–ì–û –ê–ù–ê–õ–ò–ó–ê –ü–†–ò –°–¢–ê–†–¢–ï ===
                st.session_state.pop('analysis_done', None)
                st.session_state.pop('analysis_results', None)
                # =================================================================
                
                st.rerun()

    # --- –û–¢–†–†–ò–°–û–í–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ---
    if 'reviews_results' in st.session_state and st.session_state.reviews_results:
        st.markdown("---")
        st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        
        df_display = pd.DataFrame(st.session_state.reviews_results)
        st.dataframe(df_display, use_container_width=True)
        
        # –ö–Ω–æ–ø–∫–∞ Excel —Ç–æ–∂–µ —Ç—É—Ç
        import io
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            df_display.to_excel(writer, index=False)
        
        st.download_button(
            label="üì• –°–ö–ê–ß–ê–¢–¨ –í EXCEL",
            data=buffer.getvalue(),
            file_name="reviews.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )




