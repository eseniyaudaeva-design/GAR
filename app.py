import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
from collections import Counter
import math
import inspect
import concurrent.futures
from urllib.parse import urlparse

# ==========================================
# 1. –°–¢–ò–õ–ò–ó–ê–¶–ò–Ø (–°–≤–µ—Ç–ª–∞—è —Ç–µ–º–∞ + UI –∫–∞–∫ –Ω–∞ —Å–∫—Ä–∏–Ω–µ)
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO: SEO Analysis", page_icon="üìà")

st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
        
        /* –û–±—â–∏–π —Ñ–æ–Ω –∏ —à—Ä–∏—Ñ—Ç */
        .stApp {
            background-color: #F3F4F6;
            font-family: 'Inter', sans-serif;
            color: #1F2937;
        }
        
        /* –ë–ª–æ–∫–∏ –≤–≤–æ–¥–∞ (–ö–∞—Ä—Ç–æ—á–∫–∏) */
        .input-card {
            background-color: #FFFFFF;
            padding: 24px;
            border-radius: 12px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
            margin-bottom: 20px;
            border: 1px solid #E5E7EB;
        }
        
        /* –ó–∞–≥–æ–ª–æ–≤–∫–∏ */
        h1, h2, h3 {
            color: #111827;
            font-weight: 700;
        }
        
        /* –ü–æ–ª—è –≤–≤–æ–¥–∞ */
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] {
            background-color: #F9FAFB;
            border: 1px solid #D1D5DB;
            border-radius: 6px;
            color: #111827;
        }
        
        /* –ö–Ω–æ–ø–∫–∞ (–°–∏–Ω—è—è, –∫–∞–∫ –Ω–∞ —Å–∫—Ä–∏–Ω–µ) */
        div.stButton > button {
            background-color: #1D4ED8; /* –Ø—Ä–∫–æ-—Å–∏–Ω–∏–π */
            color: white;
            font-weight: 600;
            border-radius: 8px;
            border: none;
            padding: 12px 24px;
            width: 100%;
            font-size: 16px;
            transition: all 0.2s;
        }
        div.stButton > button:hover {
            background-color: #1E40AF;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        }
        
        /* –¢–∞–±–ª–∏—Ü—ã */
        div[data-testid="stDataFrame"] {
            background-color: white;
            border-radius: 8px;
            padding: 10px;
            border: 1px solid #E5E7EB;
        }
        
        /* –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –æ—Ç—Å—Ç—É–ø—ã */
        .block-container {
            padding-top: 2rem;
            padding-bottom: 5rem;
        }
        
        /* Expander (–ù–∞—Å—Ç—Ä–æ–π–∫–∏) */
        .streamlit-expanderHeader {
            background-color: #FFFFFF;
            border-radius: 8px;
            border: 1px solid #E5E7EB;
        }
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 2. –Ø–î–†–û (–ë–≠–ö–ï–ù–î)
# ==========================================

# --- –ü–∞—Ç—á NLP ---
try:
    if not hasattr(inspect, 'getargspec'):
        def getargspec(func):
            spec = inspect.getfullargspec(func)
            return spec.args, spec.varargs, spec.varkw, spec.defaults
        inspect.getargspec = getargspec
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except:
    morph = None
    USE_NLP = False

# --- –ü–æ–∏—Å–∫ ---
try:
    from googlesearch import search
    USE_SEARCH = True
except:
    USE_SEARCH = False

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
DEFAULT_EXCLUDE = ["yandex.ru", "avito.ru", "ozon.ru", "wildberries.ru", "youtube.com", "dzen.ru", "hh.ru", "t.me", "tiu.ru", "pulscen.ru", "satu.kz"]
DEFAULT_STOPS = ["—Ä—É–±–ª–µ–π", "—Ä—É–±", "–∫—É–ø–∏—Ç—å", "—Ü–µ–Ω–∞", "—à—Ç", "—Å–º", "–º–º", "–∫–≥", "–∫–≤", "–º2", "—Å—Ç—Ä", "—É–ª", "–¥–æ—Å—Ç–∞–≤–∫–∞", "–∑–≤–æ–Ω–∏—Ç–µ", "–∑–∞–∫–∞–∑–∞—Ç—å"]

# --- –§—É–Ω–∫—Ü–∏–∏ –ü–∞—Ä—Å–∏–Ω–≥–∞ –∏ NLP ---

def process_text(text, settings, n_gram=1):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ª–µ–º–º –∏–ª–∏ n-–≥—Ä–∞–º–º"""
    # 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    pattern = r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+' if settings['numbers'] else r'[–∞-—è–ê-–Ø—ë–Åa-zA-Z]+'
    words = re.findall(pattern, text.lower())
    
    # 2. –°—Ç–æ–ø-—Å–ª–æ–≤–∞
    stops = set(w.lower() for w in settings['custom_stops'])
    clean_words = []
    
    for w in words:
        if len(w) < 2 or w in stops: continue
        
        lemma = w
        if USE_NLP and n_gram == 1: # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è —É–Ω–∏–≥—Ä–∞–º
            p = morph.parse(w)[0]
            # –§–∏–ª—å—Ç—Ä —á–∞—Å—Ç–µ–π —Ä–µ—á–∏
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag:
                continue
            lemma = p.normal_form
        
        clean_words.append(lemma)
    
    # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è N-–≥—Ä–∞–º–º (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    if n_gram > 1:
        ngrams = []
        for i in range(len(clean_words) - n_gram + 1):
            phrase = " ".join(clean_words[i:i+n_gram])
            ngrams.append(phrase)
        return ngrams
    
    return clean_words

def parse_page(url, settings):
    headers = {'User-Agent': settings['ua']}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code != 200: return None
        
        soup = BeautifulSoup(r.text, 'html.parser')
        
        # –ú–µ—Ç–∞-—Ç–µ–≥–∏
        title = soup.title.string.strip() if soup.title and soup.title.string else ""
        desc = ""
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc: desc = meta_desc.get("content", "").strip()
        h1 = soup.find("h1").get_text(strip=True) if soup.find("h1") else ""
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–∞ (noindex –∏ —Å–∫—Ä–∏–ø—Ç—ã)
        if settings['noindex']:
            for t in soup.find_all(['noindex', 'script', 'style', 'head', 'footer', 'nav']): t.decompose()
        else:
            for t in soup(['script', 'style', 'head']): t.decompose()
            
        # –ê–Ω–∫–æ—Ä—ã (—Ç–µ–∫—Å—Ç —Å—Å—ã–ª–æ–∫)
        anchors_list = []
        for a in soup.find_all('a'):
            txt = a.get_text(strip=True)
            if txt: anchors_list.append(txt)
        anchor_text = " ".join(anchors_list)
        
        # –¢–µ–∫—Å—Ç (Body) - –¥–æ–±–∞–≤–ª—è–µ–º alt –∏ title
        extra_text = []
        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])
            
        body_text = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        
        return {
            'url': url,
            'domain': urlparse(url).netloc,
            'title': title,
            'desc': desc,
            'h1': h1,
            'body_text': body_text,
            'anchor_text': anchor_text,
            'full_text': body_text + " " + anchor_text # –î–ª—è –æ–±—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        }
    except:
        return None

# --- –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ (TF-IDF, BM25) ---
def calculate_advanced_metrics(corpus_pages, my_page, settings):
    
    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–£–Ω–∏–≥—Ä–∞–º–º—ã)
    my_lemmas = process_text(my_page['body_text'], settings)
    my_anchors = process_text(my_page['anchor_text'], settings)
    
    comp_docs = []
    for p in corpus_pages:
        body = process_text(p['body_text'], settings)
        anchor = process_text(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor, 'full': body + anchor})
        
    # –ù–æ—Ä–º–∏—Ä–æ–≤–∫–∞
    avg_len = np.mean([len(d['body']) for d in comp_docs])
    my_len = len(my_lemmas)
    norm_k = (my_len / avg_len) if (settings['norm'] and avg_len > 0) else 1.0
    
    # –°–ª–æ–≤–∞—Ä—å
    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    
    # --- –ë–õ–û–ö 1: –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (BM25, IDF) ---
    N = len(comp_docs)
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
        
    k1, b = 1.2, 0.75
    
    # --- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¢–ê–ë–õ–ò–¶ ---
    
    # –¢–∞–±–ª–∏—Ü–∞ 1: –ì–ª—É–±–∏–Ω–∞ (Actionable)
    table_depth = []
    # –¢–∞–±–ª–∏—Ü–∞ 2: –ì–∏–±—Ä–∏–¥–Ω—ã–π –¢–û–ü (–ê–Ω–∞–ª–∏—Ç–∏–∫–∞)
    table_hybrid = []
    
    for word in vocab:
        df = doc_freqs[word]
        if df < 2 and word not in my_lemmas: continue # –û—Ç—Å–µ–∫–∞–µ–º —Ä–µ–¥–∫–∏–π —à—É–º
        
        # –°—á–µ—Ç—á–∏–∫–∏
        my_tf = my_lemmas.count(word)
        my_anch_tf = my_anchors.count(word)
        
        comp_tfs = [d['body'].count(word) for d in comp_docs]
        comp_anch_tfs = [d['anchor'].count(word) for d in comp_docs]
        
        med_tf = np.median(comp_tfs)
        mean_tf = np.mean(comp_tfs)
        max_tf = np.max(comp_tfs)
        med_anch = np.median(comp_anch_tfs)
        
        # IDF
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        
        # BM25 –¥–ª—è –¢–æ–ø–∞ (–º–µ–¥–∏–∞–Ω–∞)
        bm25_scores = []
        for i, d in enumerate(comp_docs):
            tf = comp_tfs[i]
            dl = len(d['body'])
            score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (dl / avg_len)))
            bm25_scores.append(score)
        bm25_top = np.median(bm25_scores)
        
        # BM25 My
        bm25_my = idf * (my_tf * (k1 + 1)) / (my_tf + k1 * (1 - b + b * (my_len / avg_len)))
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        target_body = int(med_tf * 1.3 * norm_k)
        diff_body = target_body - my_tf
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
        if med_tf > 0.5 or my_tf > 0:
            # 1. –ì–ª—É–±–∏–Ω–∞
            table_depth.append({
                "–°–ª–æ–≤–æ": word,
                "–ü–æ–≤—Ç–æ—Ä—ã —É –≤–∞—Å": my_tf,
                "–û–±—â–µ–µ –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": diff_body,
                "–¢–µ–≥ A —É –≤–∞—Å": my_anch_tf,
                "–¢–µ–≥ A —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏": int(med_anch * norm_k),
                "–¢–µ–≥ A –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": int(med_anch * norm_k) - my_anch_tf,
                "–¢–µ–∫—Å—Ç —É –≤–∞—Å": my_tf,
                "–¢–µ–∫—Å—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏": target_body,
                "–¢–µ–∫—Å—Ç –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": diff_body,
                "–ü–µ—Ä–µ—Å–ø–∞–º": int(max_tf * norm_k),
                "–ü–µ—Ä–µ—Å–ø–∞–º*IDF": round(max_tf * norm_k * idf, 1),
                "diff_abs": abs(diff_body)
            })
            
            # 2. –ì–∏–±—Ä–∏–¥–Ω—ã–π –¢–û–ü
            table_hybrid.append({
                "–°–ª–æ–≤–æ": word,
                "TF-IDF –¢–û–ü": round(med_tf * idf, 2),
                "TF-IDF –≤–∞—à —Å–∞–π—Ç": round(my_tf * idf, 2),
                "BM25 –¢–û–ü": round(bm25_top, 2),
                "BM25 –≤–∞—à —Å–∞–π—Ç": round(bm25_my, 2),
                "IDF": round(idf, 2),
                "–ö–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤": df,
                "–ú–µ–¥–∏–∞–Ω–∞": round(med_tf, 1),
                "–ü–µ—Ä–µ—Å–ø–∞–º": max_tf,
                "–°—Ä–µ–¥–Ω–µ–µ –ø–æ –¢–û–ü—É": round(mean_tf, 1),
                "–í–∞—à —Å–∞–π—Ç": my_tf
            })

    # –¢–∞–±–ª–∏—Ü–∞ 3: N-–≥—Ä–∞–º–º—ã (–ë–∏–≥—Ä–∞–º–º—ã)
    my_bigrams = process_text(my_page['body_text'], settings, n_gram=2)
    comp_bigrams_list = [process_text(p['body_text'], settings, n_gram=2) for p in corpus_pages]
    
    all_bigrams = set(my_bigrams)
    for cb in comp_bigrams_list: all_bigrams.update(cb)
    
    # –°—á–∏—Ç–∞–µ–º DF –¥–ª—è –±–∏–≥—Ä–∞–º–º
    bg_freqs = Counter()
    for cb in comp_bigrams_list:
        for bg in set(cb): bg_freqs[bg] += 1
        
    table_ngrams = []
    for bg in all_bigrams:
        df = bg_freqs[bg]
        if df < 2 and bg not in my_bigrams: continue
        
        my_cnt = my_bigrams.count(bg)
        comp_cnts = [cb.count(bg) for cb in comp_bigrams_list]
        med_cnt = np.median(comp_cnts)
        
        if med_cnt > 0 or my_cnt > 0:
            table_ngrams.append({
                "N-–≥—Ä–∞–º–º–∞": bg,
                "–ö–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤": df,
                "–ú–µ–¥–∏–∞–Ω–∞": med_cnt,
                "–°—Ä–µ–¥–Ω–µ–µ": round(np.mean(comp_cnts), 1),
                "–ù–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ": my_cnt,
                "TF-IDF": round(my_cnt * math.log(N/df if df>0 else 1), 3)
            })

    # –¢–∞–±–ª–∏—Ü–∞ 4: –¢–û–ü –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–°–≤–æ–¥–Ω–∞—è –ø–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º)
    table_relevance = []
    for i, p in enumerate(corpus_pages):
        p_lemmas = process_text(p['body_text'], settings)
        # –®–∏—Ä–∏–Ω–∞ (—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –∏–∑ –æ–±—â–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è –µ—Å—Ç—å –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ)
        common_words = set(p_lemmas).intersection(vocab)
        width = len(common_words)
        depth = len(p_lemmas)
        
        table_relevance.append({
            "–î–æ–º–µ–Ω": p['domain'],
            "–ü–æ–∑–∏—Ü–∏—è": i+1,
            "–®–∏—Ä–∏–Ω–∞ (–°–ª–æ–≤ –∏–∑ —è–¥—Ä–∞)": width,
            "–ì–ª—É–±–∏–Ω–∞ (–í—Å–µ–≥–æ —Å–ª–æ–≤)": depth,
            "–û–±—â–∞—è": width + (depth / 100) # –£—Å–ª–æ–≤–Ω—ã–π —Å–∫–æ—Ä
        })
        
    # –û—Ü–µ–Ω–∫–∞ –º–æ–µ–≥–æ —Å–∞–π—Ç–∞
    my_width = len(set(my_lemmas).intersection(vocab))
    my_depth = len(my_lemmas)
    
    return {
        "depth": pd.DataFrame(table_depth),
        "hybrid": pd.DataFrame(table_hybrid),
        "ngrams": pd.DataFrame(table_ngrams),
        "relevance_top": pd.DataFrame(table_relevance),
        "my_score": {"width": my_width, "depth": my_depth}
    }

# ==========================================
# 3. –ò–ù–¢–ï–†–§–ï–ô–° (FRONTEND)
# ==========================================

st.markdown("<h1 style='text-align: center; color: #1E3A8A;'>SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏</h1>", unsafe_allow_html=True)

# --- –í–ï–†–•–ù–ò–ô –ë–õ–û–ö (–í–°–ï–ì–î–ê –í–ò–î–ï–ù) ---
with st.container():
    st.markdown('<div class="input-card">', unsafe_allow_html=True)
    c1, c2 = st.columns(2)
    with c1:
        my_url = st.text_input("–í–∞—à URL (–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)", placeholder="https://site.ru/catalog")
    with c2:
        query = st.text_input("–ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞ —Ü–µ–Ω–∞")
    st.markdown('</div>', unsafe_allow_html=True)

# --- –ò–°–¢–û–ß–ù–ò–ö –ò –ù–ê–°–¢–†–û–ô–ö–ò ---
col_L, col_R = st.columns([2, 1])

with col_L:
    st.markdown("### üïµÔ∏è –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö")
    source_type = st.radio("–¢–∏–ø —Å–±–æ—Ä–∞:", ["Google –ü–æ–∏—Å–∫ (–ê–≤—Ç–æ)", "–°–ø–∏—Å–æ–∫ URL –≤—Ä—É—á–Ω—É—é"], horizontal=True, label_visibility="collapsed")
    
    if source_type == "Google –ü–æ–∏—Å–∫ (–ê–≤—Ç–æ)":
        cl1, cl2 = st.columns(2)
        with cl1:
            top_n = st.selectbox("–ì–ª—É–±–∏–Ω–∞ –¢–û–ü–∞:", [5, 10, 20], index=1)
        with cl2:
            excludes = st.text_input("–ò—Å–∫–ª—é—á–∏—Ç—å –¥–æ–º–µ–Ω—ã:", " ".join(DEFAULT_EXCLUDE))
        st.caption("–ü–æ–∏—Å–∫ —ç–º—É–ª–∏—Ä—É–µ—Ç—Å—è. –î–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫.")
    else:
        manual_urls = st.text_area("URLs –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (—Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏):", height=120)

with col_R:
    st.markdown("### ‚öôÔ∏è –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
    with st.container():
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–≥–≥–ª—ã –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–∞
        s_noindex = st.toggle("–ò—Å–∫–ª—é—á–∞—Ç—å noindex", True)
        s_alt = st.toggle("–í–∫–ª—é—á–∞—Ç—å alt –∏ title", False)
        s_num = st.toggle("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ü–∏—Ñ—Ä—ã", False)
        s_norm = st.toggle("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True)
        s_agg = st.toggle("–ò—Å–∫–ª—é—á–∞—Ç—å –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã", True)
    
    with st.expander("–°—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ User-Agent"):
        ua = st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0", "Googlebot/2.1"])
        c_stops = st.text_area("–î–æ–ø. —Å—Ç–æ–ø-—Å–ª–æ–≤–∞:", "\n".join(DEFAULT_STOPS), height=80)

# --- –ö–ù–û–ü–ö–ê –ó–ê–ü–£–°–ö–ê ---
st.markdown("<br>", unsafe_allow_html=True)
if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑ üöÄ"):
    
    if not my_url:
        st.error("–£–∫–∞–∂–∏—Ç–µ URL –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞!")
        st.stop()
        
    # –°–±–æ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–∫
    settings = {
        'noindex': s_noindex, 'alt_title': s_alt, 'numbers': s_num,
        'norm': s_norm, 'ua': ua, 'custom_stops': c_stops.split(),
        'std_stops': True # –í—Å–µ–≥–¥–∞ –≤–∫–ª
    }
    
    # 1. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ URL
    target_urls = []
    if source_type == "Google –ü–æ–∏—Å–∫ (–ê–≤—Ç–æ)":
        if not query:
            st.error("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å!")
            st.stop()
        try:
            excl = excludes.split()
            if s_agg: excl.extend(["avito", "ozon", "wildberries", "market", "tiu"])
            
            with st.spinner("–°–±–æ—Ä –¢–û–ü–∞..."):
                found = search(query, num_results=top_n*2, lang="ru")
                cnt = 0
                for u in found:
                    if my_url in u: continue
                    if any(x in u for x in excl): continue
                    target_urls.append(u)
                    cnt += 1
                    if cnt >= top_n: break
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            st.stop()
    else:
        target_urls = [u.strip() for u in manual_urls.split('\n') if u.strip()]
        
    if not target_urls:
        st.error("–ù–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
        st.stop()
        
    # 2. –ü–∞—Ä—Å–∏–Ω–≥
    progress_bar = st.progress(0)
    status_txt = st.empty()
    
    # –ú–æ–π —Å–∞–π—Ç
    status_txt.text(f"–°–∫–∞—á–∏–≤–∞–µ–º –≤–∞—à —Å–∞–π—Ç: {my_url}...")
    my_page_data = parse_page(my_url, settings)
    
    if not my_page_data:
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –≤–∞—à —Å–∞–π—Ç.")
        st.stop()
        
    # –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã
    comp_pages_data = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(parse_page, u, settings): u for u in target_urls}
        done = 0
        for f in concurrent.futures.as_completed(futures):
            res = f.result()
            if res: comp_pages_data.append(res)
            done += 1
            progress_bar.progress(done / len(target_urls))
            status_txt.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {done}/{len(target_urls)}...")
            
    progress_bar.empty()
    status_txt.empty()
    
    if len(comp_pages_data) < 2:
        st.error("–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.")
        st.stop()
        
    # 3. –†–∞—Å—á–µ—Ç—ã
    results = calculate_advanced_metrics(comp_pages_data, my_page_data, settings)
    
    # 4. –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
    st.success("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    
    # –ú–µ—Ç—Ä–∏–∫–∏ —Å–∞–π—Ç–∞
    st.markdown("### üèÜ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞")
    m1, m2, m3 = st.columns(3)
    m1.metric("–®–∏—Ä–∏–Ω–∞ (–û—Ö–≤–∞—Ç —Å–ª–æ–≤)", results['my_score']['width'])
    m2.metric("–ì–ª—É–±–∏–Ω–∞ (–í—Å–µ–≥–æ —Å–ª–æ–≤)", results['my_score']['depth'])
    m3.metric("–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤ –∞–Ω–∞–ª–∏–∑–µ", len(comp_pages_data))
    
    st.divider()
    
    # –¢–ê–ë–õ–ò–¶–ê 1: –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò (–ì–õ–£–ë–ò–ù–ê)
    st.subheader("1. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≥–ª—É–±–∏–Ω–µ (LSI)")
    df_depth = results['depth']
    if not df_depth.empty:
        df_depth = df_depth.sort_values(by="diff_abs", ascending=False)
        
        def color_table(val):
            if isinstance(val, (int, float)):
                if val > 0: return 'background-color: #dcfce7; color: #166534'
                if val < 0: return 'background-color: #fee2e2; color: #991b1b'
            return ''
            
        st.dataframe(
            df_depth.style.map(color_table, subset=['–û–±—â–µ–µ –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å', '–¢–µ–≥ A –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å', '–¢–µ–∫—Å—Ç –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å']),
            column_config={"diff_abs": None},
            use_container_width=True,
            height=500
        )
        # CSV Download
        st.download_button("–°–∫–∞—á–∞—Ç—å (CSV)", df_depth.to_csv().encode('utf-8'), "depth_recommendations.csv")
    else:
        st.info("–ù–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.")

    # –¢–ê–ë–õ–ò–¶–ê 2: –ì–ò–ë–†–ò–î–ù–´–ô –¢–û–ü
    with st.expander("2. –ì–∏–±—Ä–∏–¥–Ω—ã–π –¢–û–ü —É–Ω–∏–≥—Ä–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤", expanded=False):
        st.dataframe(results['hybrid'].sort_values(by="TF-IDF –¢–û–ü", ascending=False), use_container_width=True)

    # –¢–ê–ë–õ–ò–¶–ê 3: N-–ì–†–ê–ú–ú–´
    with st.expander("3. N-–≥—Ä–∞–º–º—ã (–ë–∏–≥—Ä–∞–º–º—ã)", expanded=False):
        st.dataframe(results['ngrams'].sort_values(by="TF-IDF", ascending=False), use_container_width=True)
        
    # –¢–ê–ë–õ–ò–¶–ê 4: –¢–û–ü –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò
    with st.expander("4. –¢–û–ü —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–°–≤–æ–¥–Ω–∞—è)", expanded=False):
        st.dataframe(results['relevance_top'], use_container_width=True)
