import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin, unquote
import inspect
import time
import json
import io
import os
import random
import streamlit.components.v1 as components
import copy
import plotly.graph_objects as go

# ==========================================
# FIX FOR PYTHON 3.11+
# ==========================================
if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except ImportError:
    morph = None
    USE_NLP = False

try:
    import openai
except ImportError:
    openai = None

# ==========================================
# 0. –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ==========================================

def transliterate_text(text):
    mapping = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }
    result = []
    for char in text.lower():
        if char in mapping:
            result.append(mapping[char])
        elif char.isalnum() or char == '-':
            result.append(char)
    return "".join(result)

def force_cyrillic_name_global(slug_text):
    raw = unquote(slug_text).lower()
    raw = raw.replace('.html', '').replace('.php', '')
    if re.search(r'[–∞-—è]', raw):
        return raw.replace('-', ' ').replace('_', ' ').capitalize()

    words = re.split(r'[-_]', raw)
    rus_words = []
    
    exact_map = {
        'nikel': '–Ω–∏–∫–µ–ª—å', 'stal': '—Å—Ç–∞–ª—å', 'med': '–º–µ–¥—å', 'latun': '–ª–∞—Ç—É–Ω—å',
        'bronza': '–±—Ä–æ–Ω–∑–∞', 'svinec': '—Å–≤–∏–Ω–µ—Ü', 'titan': '—Ç–∏—Ç–∞–Ω', 'tsink': '—Ü–∏–Ω–∫',
        'dural': '–¥—é—Ä–∞–ª—å', 'dyural': '–¥—é—Ä–∞–ª—å', 'chugun': '—á—É–≥—É–Ω',
        'alyuminiy': '–∞–ª—é–º–∏–Ω–∏–π', 'al': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è', 'alyuminievaya': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è',
        'nerzhaveyushchiy': '–Ω–µ—Ä–∂–∞–≤–µ—é—â–∏–π', 'nerzhaveyka': '–Ω–µ—Ä–∂–∞–≤–µ–π–∫–∞',
        'profil': '–ø—Ä–æ—Ñ–∏–ª—å', 'shveller': '—à–≤–µ–ª–ª–µ—Ä', 'ugolok': '—É–≥–æ–ª–æ–∫',
        'polosa': '–ø–æ–ª–æ—Å–∞', 'krug': '–∫—Ä—É–≥', 'kvadrat': '–∫–≤–∞–¥—Ä–∞—Ç',
        'list': '–ª–∏—Å—Ç', 'truba': '—Ç—Ä—É–±–∞', 'setka': '—Å–µ—Ç–∫–∞',
        'provoloka': '–ø—Ä–æ–≤–æ–ª–æ–∫–∞', 'armatura': '–∞—Ä–º–∞—Ç—É—Ä–∞', 'balka': '–±–∞–ª–∫–∞',
        'katanka': '–∫–∞—Ç–∞–Ω–∫–∞', 'otvod': '–æ—Ç–≤–æ–¥', 'perehod': '–ø–µ—Ä–µ—Ö–æ–¥',
        'flanec': '—Ñ–ª–∞–Ω–µ—Ü', 'zaglushka': '–∑–∞–≥–ª—É—à–∫–∞', 'metiz': '–º–µ—Ç–∏–∑—ã',
        'profnastil': '–ø—Ä–æ—Ñ–Ω–∞—Å—Ç–∏–ª', 'shtrips': '—à—Ç—Ä–∏–ø—Å', 'lenta': '–ª–µ–Ω—Ç–∞',
        'shina': '—à–∏–Ω–∞', 'prutok': '–ø—Ä—É—Ç–æ–∫', 'shestigrannik': '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫',
        'vtulka': '–≤—Ç—É–ª–∫–∞', 'kabel': '–∫–∞–±–µ–ª—å', 'panel': '–ø–∞–Ω–µ–ª—å',
        'detal': '–¥–µ—Ç–∞–ª—å', 'set': '—Å–µ—Ç—å', 'cep': '—Ü–µ–ø—å', 'svyaz': '—Å–≤—è–∑—å',
        'rezba': '—Ä–µ–∑—å–±–∞', 'gost': '–ì–û–°–¢',
        'polipropilenovye': '–ø–æ–ª–∏–ø—Ä–æ–ø–∏–ª–µ–Ω–æ–≤—ã–µ', 'truby': '—Ç—Ä—É–±—ã',
        'ocinkovannaya': '–æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω–∞—è', 'riflenyy': '—Ä–∏—Ñ–ª–µ–Ω—ã–π'
    }

    for w in words:
        if not w: continue
        if w in exact_map:
            rus_words.append(exact_map[w])
            continue
        
        processed_w = w
        if processed_w.endswith('yy'): processed_w = processed_w[:-2] + '—ã–π'
        elif processed_w.endswith('iy'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('ij'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('yi'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('aya'): processed_w = processed_w[:-3] + '–∞—è'
        elif processed_w.endswith('oye'): processed_w = processed_w[:-3] + '–æ–µ'
        elif processed_w.endswith('ye'): processed_w = processed_w[:-2] + '—ã–µ'

        replacements = [
            ('shch', '—â'), ('sch', '—â'), ('yo', '—ë'), ('zh', '–∂'), ('ch', '—á'), ('sh', '—à'), 
            ('yu', '—é'), ('ya', '—è'), ('kh', '—Ö'), ('ts', '—Ü'), ('ph', '—Ñ'),
            ('a', '–∞'), ('b', '–±'), ('v', '–≤'), ('g', '–≥'), ('d', '–¥'), ('e', '–µ'), 
            ('z', '–∑'), ('i', '–∏'), ('j', '–π'), ('k', '–∫'), ('l', '–ª'), ('m', '–º'), 
            ('n', '–Ω'), ('o', '–æ'), ('p', '–ø'), ('r', '—Ä'), ('s', '—Å'), ('t', '—Ç'), 
            ('u', '—É'), ('f', '—Ñ'), ('h', '—Ö'), ('c', '–∫'), ('w', '–≤'), ('y', '—ã'), ('x', '–∫—Å')
        ]
        
        temp_res = processed_w
        for eng, rus in replacements:
            temp_res = temp_res.replace(eng, rus)
        
        rus_words.append(temp_res)

    draft_phrase = " ".join(rus_words)
    draft_phrase = draft_phrase.replace('–ø—Ä–æ—Ñ–∏–ª', '–ø—Ä–æ—Ñ–∏–ª—å').replace('–ø—Ä–æ—Ñ–∏–ª—å–Ω', '–ø—Ä–æ—Ñ–∏–ª—å–Ω')
    draft_phrase = draft_phrase.replace('–µ–ª–Ω—ã–π', '–µ–ª—å–Ω—ã–π').replace('–∞–ª–Ω—ã–π', '–∞–ª—å–Ω—ã–π')
    draft_phrase = draft_phrase.replace('–µ–ª–Ω–∞—è', '–µ–ª—å–Ω–∞—è').replace('–∞–ª–Ω–∞—è', '–∞–ª—å–Ω–∞—è')
    draft_phrase = draft_phrase.replace('—Å—Ç–∞–ª–Ω', '—Å—Ç–∞–ª—å–Ω').replace('–º–µ–¥—å–Ω', '–º–µ–¥–Ω')
    draft_phrase = draft_phrase.replace('–π–∞', '—è').replace('–π–æ', '—ë')

    return draft_phrase.capitalize()

def get_breadcrumb_only(url, ua_settings="Mozilla/5.0"):
    try:
        session = requests.Session()
        retry = Retry(connect=3, read=3, redirect=3, backoff_factor=0.5)
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        
        headers = {'User-Agent': ua_settings}
        r = session.get(url, headers=headers, timeout=25)
        if r.status_code != 200: 
            return None
        
        soup = BeautifulSoup(r.text, 'html.parser')
        
        breadcrumbs = soup.find(class_=re.compile(r'breadcrumb|breadcrumbs|nav-path|nav-chain|bx-breadcrumb', re.I))
        if not breadcrumbs:
            breadcrumbs = soup.find(id=re.compile(r'breadcrumb|breadcrumbs|nav-path', re.I))

        if breadcrumbs:
            full_text = breadcrumbs.get_text(separator='|||', strip=True)
            parts = [p.strip() for p in full_text.split('|||') if p.strip()]
            clean_parts = [p for p in parts if p not in ['/', '\\', '>', '¬ª', '‚Ä¢', '-', '|']]
            
            if clean_parts:
                last_item = clean_parts[-1]
                if len(last_item) > 2 and last_item.lower() != "–≥–ª–∞–≤–Ω–∞—è":
                    return last_item
    except:
        return None
    return None

def render_clean_block(title, icon, words_list):
    unique_words = sorted(list(set(words_list))) if words_list else []
    count = len(unique_words)
    
    if count > 0:
        content_html = ", ".join(unique_words)
        html_code = f"""
        <details class="custom-card">
            <summary class="custom-card-header">
                <div class="header-left">
                    <span class="card-arrow"></span>
                    <span class="card-icon">{icon}</span>
                    <span class="card-title">{title}</span>
                </div>
                <div class="header-right">
                    <span class="card-count">{count}</span>
                </div>
            </summary>
            <div class="custom-card-content">
                {content_html}
            </div>
        </details>
        """
    else:
        html_code = f"""
        <div class="custom-card disabled">
            <div class="custom-card-header">
                <div class="header-left">
                    <span class="card-arrow" style="opacity:0"></span>
                    <span class="card-icon">{icon}</span>
                    <span class="card-title" style="color:#9ca3af">{title}</span>
                </div>
                <div class="header-right">
                    <span class="card-count">0</span>
                </div>
            </div>
        </div>
        """
    st.markdown(html_code, unsafe_allow_html=True)

def render_relevance_chart(df_rel, unique_key="default"):
    if df_rel.empty:
        return

    # 1. –ñ–ï–°–¢–ö–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ > 0
    # –í–∞—à —Å–∞–π—Ç (–ø–æ–∑–∏—Ü–∏—è 0) —É–¥–∞–ª—è–µ—Ç—Å—è –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
    df = df_rel[df_rel['–ü–æ–∑–∏—Ü–∏—è'] > 0].copy()
    
    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø—É—Å—Ç–∞ - –≤—ã—Ö–æ–¥–∏–º
    if df.empty:
        return

    df = df.sort_values(by='–ü–æ–∑–∏—Ü–∏—è')
    x_indices = np.arange(len(df))
    
    tick_links = []
    
    for _, row in df.iterrows():
        # –ß–∏—Å—Ç–∏–º –∏–º—è –¥–æ–º–µ–Ω–∞
        raw_name = row['–î–æ–º–µ–Ω'].replace(' (–í—ã)', '').strip()
        clean_domain = raw_name.replace('www.', '').split('/')[0]
        
        # –§–æ—Ä–º–∞—Ç: "1. site.ru" (–±–µ–∑ #)
        label_text = f"{row['–ü–æ–∑–∏—Ü–∏—è']}. {clean_domain}"
        if len(label_text) > 20: label_text = label_text[:18] + ".."
        
        url_target = row.get('URL', f"https://{raw_name}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º CSS-–∫–ª–∞—Å—Å .chart-link –≤–º–µ—Å—Ç–æ style="..." –¥–ª—è —Ä–∞–±–æ—Ç—ã hover
        link_html = f"<a href='{url_target}' target='_blank' class='chart-link'>{label_text}</a>"
        tick_links.append(link_html)

    # –ú–µ—Ç—Ä–∏–∫–∏
    df['Total_Rel'] = (df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] + df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)']) / 2
    
    # –¢—Ä–µ–Ω–¥
    z = np.polyfit(x_indices, df['Total_Rel'], 1)
    p = np.poly1d(z)
    df['Trend'] = p(x_indices)

    # 2. –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
    fig = go.Figure()

    # --- –ü–ê–õ–ò–¢–†–ê (Premium) ---
    COLOR_MAIN = '#4F46E5'  # –ò–Ω–¥–∏–≥–æ
    COLOR_WIDTH = '#0EA5E9' # –ì–æ–ª—É–±–æ–π
    COLOR_DEPTH = '#E11D48' # –ú–∞–ª–∏–Ω–æ–≤—ã–π
    COLOR_TREND = '#15803d' # –ó–µ–ª–µ–Ω—ã–π (Forest Green)

    COMMON_CONFIG = dict(
        mode='lines+markers',
        line=dict(width=3, shape='spline'), 
        marker=dict(size=8, line=dict(width=2, color='white'), symbol='circle')
    )

    # 1. –û–ë–©–ê–Ø
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['Total_Rel'],
        name='–û–±—â–∞—è',
        line=dict(color=COLOR_MAIN, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_MAIN, **COMMON_CONFIG['marker']),
        mode='lines+markers'
    ))

    # 2. –®–ò–†–ò–ù–ê
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'],
        name='–®–∏—Ä–∏–Ω–∞',
        line=dict(color=COLOR_WIDTH, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_WIDTH, **COMMON_CONFIG['marker']),
        mode='lines+markers'
    ))

    # 3. –ì–õ–£–ë–ò–ù–ê
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'],
        name='–ì–ª—É–±–∏–Ω–∞',
        line=dict(color=COLOR_DEPTH, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_DEPTH, **COMMON_CONFIG['marker']),
        mode='lines+markers'
    ))

    # 4. –¢–†–ï–ù–î
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['Trend'],
        name='–¢—Ä–µ–Ω–¥',
        line=dict(color=COLOR_TREND, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_TREND, **COMMON_CONFIG['marker']),
        mode='lines+markers',
        opacity=0.8
    ))

    # 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Layout
    fig.update_layout(
        template="plotly_white",
        legend=dict(
            orientation="h",
            yanchor="bottom", y=1.05,
            xanchor="center", x=0.5,
            font=dict(size=12, color="#111827", family="Inter, sans-serif")
        ),
        xaxis=dict(
            showgrid=False, 
            linecolor='#E5E7EB',
            tickmode='array',
            tickvals=x_indices,
            ticktext=tick_links, 
            tickfont=dict(size=12),
            fixedrange=True,
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–∞–µ–º –±–æ–ª—å—à–µ –º–µ—Å—Ç–∞ —Å–ª–µ–≤–∞ –∏ —Å–ø—Ä–∞–≤–∞ (-0.5 ... +0.5), —á—Ç–æ–±—ã –∫—Ä–∞–π–Ω–∏–µ —Ç–æ—á–∫–∏ –Ω–µ —Ä–µ–∑–∞–ª–∏—Å—å
            range=[-0.5, len(df) - 0.5],
            automargin=True # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—Å—Ç—É–ø, —á—Ç–æ–±—ã —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–µ–∑–∂–∞–ª
        ),
        yaxis=dict(
            range=[0, 115], 
            showgrid=True, 
            gridcolor='#F3F4F6', 
            gridwidth=1,
            zeroline=False,
            fixedrange=True
        ),
        margin=dict(l=10, r=10, t=50, b=40),
        hovermode="x unified",
        height=380
    )
    
    st.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False}, key=f"rel_chart_{unique_key}")

def analyze_serp_anomalies(df_rel):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–í–µ—Ä—Å–∏—è v5 - Robust).
    –ü–æ—Ä–æ–≥: 75% –æ—Ç –ª–∏–¥–µ—Ä–∞. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∏–ø–∏–∑–∞—Ü–∏—è.
    """
    if df_rel.empty:
        return [], [], {"type": "none", "msg": ""}

    # –ò—Å–∫–ª—é—á–∞–µ–º "–í–∞—à —Å–∞–π—Ç" –∏–∑ —Ä–∞—Å—á–µ—Ç–æ–≤ —ç—Ç–∞–ª–æ–Ω–∞
    df = df_rel[~df_rel['–î–æ–º–µ–Ω'].str.contains("\(–í—ã\)", na=False)].copy()
    
    if df.empty:
        return [], [], {"type": "none", "msg": ""}

    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –¥–µ–ª–∞–µ–º —á–∏—Å–ª–∞–º–∏ (–∑–∞—â–∏—Ç–∞ –æ—Ç —Å–±–æ–µ–≤)
    df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] = pd.to_numeric(df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'], errors='coerce').fillna(0)
    df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'] = pd.to_numeric(df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'], errors='coerce').fillna(0)

    # –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª
    df['Total'] = (df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] + df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)']) / 2
    
    # 1. –ò–©–ï–ú –õ–ò–î–ï–†–ê
    max_score = df['Total'].max()
    if max_score < 1: max_score = 1 # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
    
    # 2. –ñ–ï–°–¢–ö–ò–ô –ü–û–†–û–ì: 75% –æ—Ç –ª–∏–¥–µ—Ä–∞.
    # –ï—Å–ª–∏ –õ–∏–¥–µ—Ä=100, –ø–æ—Ä–æ–≥=75. –í—Å–µ —á—Ç–æ < 75 - —É–¥–∞–ª—è–µ–º.
    threshold = max(max_score * 0.75, 40) 
    
    anomalies = []
    normal_urls = []
    
    debug_counts = 0
    
    for _, row in df.iterrows():
        # –î–æ—Å—Ç–∞–µ–º —Å—Å—ã–ª–∫—É. –ó–∞—â–∏—Ç–∞ –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤.
        current_url = str(row.get('URL', '')).strip()
        if not current_url or current_url.lower() == 'nan':
             current_url = f"https://{row['–î–æ–º–µ–Ω']}" 

        score = row['Total']
        
        # –ê–ù–ê–õ–ò–ó
        if score < threshold:
            reason = f"–°–∫–æ—Ä {int(score)} < {int(threshold)} (–õ–∏–¥–µ—Ä {int(max_score)})"
            anomalies.append({'url': current_url, 'reason': reason, 'score': score})
            debug_counts += 1
        else:
            normal_urls.append(current_url)

    # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—è–º–∏
    if anomalies:
        st.toast(f"üóëÔ∏è –§–∏–ª—å—Ç—Ä (–õ–∏–¥–µ—Ä {int(max_score)} / –ü–æ—Ä–æ–≥ {int(threshold)}). –ò—Å–∫–ª—é—á–µ–Ω–æ: {len(anomalies)}", icon="‚ö†Ô∏è")
    else:
        # –ï—Å–ª–∏ –Ω–∏–∫–æ–≥–æ –Ω–µ –∏—Å–∫–ª—é—á–∏–ª–∏, –ø–∏—à–µ–º –ø–æ—á–µ–º—É
        st.toast(f"‚úÖ –í—Å–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –æ–∫. (–õ–∏–¥–µ—Ä {int(max_score)} / –ü–æ—Ä–æ–≥ {int(threshold)}). –ú–∏–Ω. –±–∞–ª–ª: {int(df['Total'].min())}", icon="‚ÑπÔ∏è")
    
    # –¢—Ä–µ–Ω–¥
    x = np.arange(len(df)); y = df['Total'].values
    slope = np.polyfit(x, y, 1)[0] if len(x) > 1 else 0
    trend_msg = "üìâ –ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–æ–ø" if slope < -1 else ("üìà –ü–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç—ã–π —Ç–æ–ø" if slope > 1 else "‚û°Ô∏è –†–æ–≤–Ω—ã–π —Ç–æ–ø")

    return normal_urls, anomalies, {"type": "info", "msg": trend_msg}

@st.cache_data
def load_lemmatized_dictionaries():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_path = os.path.join(script_dir, "data")
    
    # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞
    sets = {
        "products": set(),
        "commercial": set(),
        "specs": set(),
        "geo": set(),
        "services": set(),
        "sensitive": set()
    }

    # –ö–∞—Ä—Ç–∞ —Ñ–∞–π–ª–æ–≤
    files_map = {
        "metal_products.json": "products",
        "commercial_triggers.json": "commercial",
        "geo_locations.json": "geo",
        "services_triggers.json": "services",
        "tech_specs.json": "specs",
        "SENSITIVE_STOPLIST.json": "sensitive"
    }

    for filename, set_key in files_map.items():
        full_path = os.path.join(base_path, filename)
        if not os.path.exists(full_path):
            continue
        
        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                data = json.load(f) 
                
                words_bucket = []
                if isinstance(data, dict):
                    for cat_list in data.values():
                        words_bucket.extend(cat_list)
                elif isinstance(data, list):
                    words_bucket = data
                
                for phrase in words_bucket:
                    w_clean = str(phrase).lower().strip().replace('—ë', '–µ')
                    if not w_clean: continue
                    sets[set_key].add(w_clean)
                    if morph:
                        normal_form = morph.parse(w_clean)[0].normal_form.replace('—ë', '–µ')
                        sets[set_key].add(normal_form)
                    if ' ' in w_clean:
                        parts = w_clean.split()
                        for p in parts:
                            sets[set_key].add(p)
                            if morph: 
                                sets[set_key].add(morph.parse(p)[0].normal_form.replace('—ë', '–µ'))
        except: pass

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 6 –Ω–∞–±–æ—Ä–æ–≤
    return sets["products"], sets["commercial"], sets["specs"], sets["geo"], sets["services"], sets["sensitive"]

def classify_semantics_with_api(words_list, yandex_key):
    # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º 6 —Å–ª–æ–≤–∞—Ä–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Ä–Ω—É–ª–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
    PRODUCTS_SET, COMM_SET, SPECS_SET, GEO_SET, SERVICES_SET, SENS_SET = load_lemmatized_dictionaries()
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ —Ñ–∞–π–ª–∞ –∏ –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –≤ –∫–æ–¥–µ
    FULL_SENSITIVE = SENS_SET.union(SENSITIVE_STOPLIST)

    if 'debug_geo_count' not in st.session_state:
        st.session_state.debug_geo_count = len(GEO_SET)
    
    st.sidebar.info(f"–°–ª–æ–≤–∞—Ä–∏ (–∏–∑ —Ñ–∞–π–ª–æ–≤):\nüì¶ –¢–æ–≤–∞—Ä—ã: {len(PRODUCTS_SET)}\nüí∞ –ö–æ–º–º–µ—Ä—Ü–∏—è: {len(COMM_SET)}\nüõ†Ô∏è –£—Å–ª—É–≥–∏: {len(SERVICES_SET)}\nüåç –ì–æ—Ä–æ–¥–∞: {len(GEO_SET)}")

    dim_pattern = re.compile(r'\d+(?:[\.\,]\d+)?\s?[—Öx\*√ó]\s?\d+', re.IGNORECASE)
    grade_pattern = re.compile(r'^([–∞-—èa-z]{1,4}\-?\d+[–∞-—èa-z0-9]*)$', re.IGNORECASE)
    
    categories = {'products': set(), 'services': set(), 'commercial': set(), 
                  'dimensions': set(), 'geo': set(), 'general': set(), 'sensitive': set()}
    
    for word in words_list:
        word_lower = word.lower()
        
        # 1. –°–¢–û–ü-–°–õ–û–í–ê
        is_sensitive = False
        if word_lower in FULL_SENSITIVE: is_sensitive = True
        else:
            for stop_w in FULL_SENSITIVE:
                if len(stop_w) > 3 and stop_w in word_lower: is_sensitive = True; break
        if is_sensitive: categories['sensitive'].add(word_lower); continue
        
        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
        lemma = word_lower
        if morph:
            p = morph.parse(word_lower)[0]
            lemma = p.normal_form

        # 2. –†–ê–ó–ú–ï–†–´ / –ì–û–°–¢
        if word_lower in SPECS_SET or lemma in SPECS_SET:
            categories['dimensions'].add(word_lower); continue
        if dim_pattern.search(word_lower) or grade_pattern.match(word_lower) or word_lower.isdigit():
            categories['dimensions'].add(word_lower); continue

        # 3. –¢–û–í–ê–†–´ (–£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞)
        if word_lower in PRODUCTS_SET or lemma in PRODUCTS_SET:
            categories['products'].add(word_lower); continue
        
        is_product_root = False
        for prod in PRODUCTS_SET:
            check_root = prod[:-1] if len(prod) > 4 else prod
            if len(check_root) > 3 and check_root in word_lower:
                categories['products'].add(word_lower)
                is_product_root = True
                break
        if is_product_root: continue

        # 4. –ì–ï–û
        if lemma in GEO_SET or word_lower in GEO_SET:
            categories['geo'].add(word_lower); continue
        
        # 5. –£–°–õ–£–ì–ò
        if lemma in SERVICES_SET or word_lower in SERVICES_SET:
             categories['services'].add(word_lower); continue
        if lemma.endswith('–æ–±—Ä–∞–±–æ—Ç–∫–∞') or lemma.endswith('–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ') or lemma == "—Ä–µ–∑–∫–∞":
            categories['services'].add(word_lower); continue

        # 6. –ö–û–ú–ú–ï–†–¶–ò–Ø
        if lemma in COMM_SET or word_lower in COMM_SET:
            categories['commercial'].add(word_lower); continue
            
        # 7. –û–ë–©–ò–ï
        categories['general'].add(word_lower)

    return {k: sorted(list(v)) for k, v in categories.items()}

# ==========================================
# STATE INIT
# ==========================================
if 'sidebar_gen_df' not in st.session_state: st.session_state.sidebar_gen_df = None
if 'sidebar_excel_bytes' not in st.session_state: st.session_state.sidebar_excel_bytes = None
if 'analysis_results' not in st.session_state: st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state: st.session_state.analysis_done = False
if 'ai_generated_df' not in st.session_state: st.session_state.ai_generated_df = None
if 'ai_excel_bytes' not in st.session_state: st.session_state.ai_excel_bytes = None
if 'tags_html_result' not in st.session_state: st.session_state.tags_html_result = None
if 'table_html_result' not in st.session_state: st.session_state.table_html_result = None
if 'tags_generated_df' not in st.session_state: st.session_state.tags_generated_df = None
if 'tags_excel_data' not in st.session_state: st.session_state.tags_excel_data = None

# Current lists
if 'categorized_products' not in st.session_state: st.session_state.categorized_products = []
if 'categorized_services' not in st.session_state: st.session_state.categorized_services = []
if 'categorized_commercial' not in st.session_state: st.session_state.categorized_commercial = []
if 'categorized_dimensions' not in st.session_state: st.session_state.categorized_dimensions = []
if 'categorized_geo' not in st.session_state: st.session_state.categorized_geo = []
if 'categorized_general' not in st.session_state: st.session_state.categorized_general = []
if 'categorized_sensitive' not in st.session_state: st.session_state.categorized_sensitive = []

# Original lists (for restoration)
if 'orig_products' not in st.session_state: st.session_state.orig_products = []
if 'orig_services' not in st.session_state: st.session_state.orig_services = []
if 'orig_commercial' not in st.session_state: st.session_state.orig_commercial = []
if 'orig_dimensions' not in st.session_state: st.session_state.orig_dimensions = []
if 'orig_geo' not in st.session_state: st.session_state.orig_geo = []
if 'orig_general' not in st.session_state: st.session_state.orig_general = []

if 'auto_tags_words' not in st.session_state: st.session_state.auto_tags_words = []
if 'auto_promo_words' not in st.session_state: st.session_state.auto_promo_words = []
if 'persistent_urls' not in st.session_state: st.session_state['persistent_urls'] = ""

st.set_page_config(layout="wide", page_title="GAR PRO v2.6 (Mass Promo)", page_icon="üìä")

GARBAGE_LATIN_STOPLIST = {
    'whatsapp', 'viber', 'telegram', 'skype', 'vk', 'instagram', 'facebook', 'youtube', 'twitter',
    'cookie', 'cookies', 'policy', 'privacy', 'agreement', 'terms',
    'click', 'submit', 'send', 'zakaz', 'basket', 'cart', 'order', 'call', 'back', 'callback',
    'login', 'logout', 'sign', 'register', 'auth', 'account', 'profile',
    'search', 'menu', 'nav', 'navigation', 'footer', 'header', 'sidebar',
    'img', 'jpg', 'png', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'svg',
    'ok', 'error', 'undefined', 'null', 'true', 'false', 'var', 'let', 'const', 'function', 'return',
    'ru', 'en', 'com', 'net', 'org', 'biz', 'shop', 'store',
    'phone', 'email', 'tel', 'fax', 'mob', 'address', 'copyright', 'all', 'rights', 'reserved',
    'div', 'span', 'class', 'id', 'style', 'script', 'body', 'html', 'head', 'meta', 'link'
}

SENSITIVE_STOPLIST_RAW = {
    "—É–∫—Ä–∞–∏–Ω–∞", "ukraine", "ua", "–≤—Å—É", "–∑—Å—É", "–∞—Ç–æ",
    "–∫–∏–µ–≤", "–ª—å–≤–æ–≤", "—Ö–∞—Ä—å–∫–æ–≤", "–æ–¥–µ—Å—Å–∞", "–¥–Ω–µ–ø—Ä", "–º–∞—Ä–∏—É–ø–æ–ª—å",
    "–¥–æ–Ω–µ—Ü–∫", "–ª—É–≥–∞–Ω—Å–∫", "–¥–Ω—Ä", "–ª–Ω—Ä", "–¥–æ–Ω–±–∞—Å—Å", 
    "–º–µ–ª–∏—Ç–æ–ø–æ–ª—å", "–±–µ—Ä–¥—è–Ω—Å–∫", "–±–∞—Ö–º—É—Ç", "–∑–∞–ø–æ—Ä–æ–∂—å–µ", "—Ö–µ—Ä—Å–æ–Ω",
    "–∫—Ä—ã–º", "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å", "—Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å"
}
SENSITIVE_STOPLIST = {w.lower() for w in SENSITIVE_STOPLIST_RAW}

def check_password():
    if st.session_state.get("authenticated"):
        return True
    st.markdown("""<style>.main { display: flex; flex-direction: column; justify-content: center; align-items: center; } .auth-logo-box { text-align: center; margin-bottom: 1rem; padding-top: 0; }</style>""", unsafe_allow_html=True)
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="auth-logo-box"><h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3></div>', unsafe_allow_html=True)
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "ZVC01w4_pIquj0bMiaAu":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

if "arsenkin_token" in st.session_state:
    ARSENKIN_TOKEN = st.session_state.arsenkin_token
else:
    try: ARSENKIN_TOKEN = st.secrets["api"]["arsenkin_token"]
    except (FileNotFoundError, KeyError): ARSENKIN_TOKEN = None

if "yandex_dict_key" in st.session_state:
    YANDEX_DICT_KEY = st.session_state.yandex_dict_key
else:
    try: YANDEX_DICT_KEY = st.secrets["api"]["yandex_dict_key"]
    except (FileNotFoundError, KeyError): YANDEX_DICT_KEY = None

REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956},
    "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931},
    "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–ö–∏–µ–≤ (UA)": {"ya": 143, "go": 1012852},
    "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493},
    "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601}
}

DEFAULT_EXCLUDE_DOMAINS = {
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "aliexpress.ru", 
    "ebay.com", "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", 
    "pandao.ru", "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", 
    "banki.ru", "regmarkets.ru", "zoon.ru", "pulscen.ru", "prodoctorov.ru", 
    "blizko.ru", "domclick.ru", "satom.ru", "quto.ru", "edadeal.ru", 
    "cataloxy.ru", "irr.ru", "onliner.by", "shop.by", "deal.by", "yell.ru", 
    "profi.ru", "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by", 
    "market.yandex.ru", "youtube.com", "www.youtube.com", "gosuslugi.ru", 
    "www.gosuslugi.ru", "dzen.ru", "2gis.by", "wildberries.ru", "rutube.ru", 
    "vk.com", "facebook.com", "chipdip.ru"
    }

DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n—Å—Ç—Ä\n—É–ª\n—à—Ç\n—Å–º\n–º–º\n–º–ª\n–∫–≥\n–∫–≤\n–º¬≤\n—Å–º¬≤\n–º2\n—Å–º2"

PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE"

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif; color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important; }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important; color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] div[role="gridcell"] {{
            background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; border-bottom: 1px solid {ROW_BORDER_COLOR} !important;
        }}
        .legend-box {{ padding: 10px; background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px; }}
        .text-red {{ color: #D32F2F; font-weight: bold; }}
        .text-green {{ color: #2E7D32; font-weight: bold; }}
        .text-bold {{ font-weight: 600; }}
        .sort-container {{ background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 8px; margin-bottom: 10px; border: 1px solid {BORDER_COLOR}; }}
        
        .stApp > header {{ background-color: transparent !important; }}
        .stTextInput input:disabled, .stTextArea textarea:disabled, .stSelectbox div[aria-disabled="true"] {{
            opacity: 1 !important; background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; cursor: text !important; -webkit-text-fill-color: {TEXT_COLOR} !important; border-color: {BORDER_COLOR} !important;
        }}
        .stButton button:disabled {{ opacity: 1 !important; background-color: {PRIMARY_COLOR} !important; color: white !important; cursor: progress !important; }}
        div[data-testid="stAppViewContainer"] {{ filter: none !important; opacity: 1 !important; transition: none !important; }}
        /* –°—Ç–∏–ª–∏ –¥–ª—è —Å—Å—ã–ª–æ–∫ –≤–Ω—É—Ç—Ä–∏ –≥—Ä–∞—Ñ–∏–∫–∞ Plotly */
        .chart-link {{
            color: #277EFF !important;
            font-weight: 600 !important;
            text-decoration: none !important;
            border-bottom: 4px solid #CBD5E1 !important; 
            display: inline-block !important;
            transition: border-color 0.2s ease !important;
        }}
        .chart-link:hover {{
            border-bottom-color: #277EFF !important;
            cursor: pointer !important;
        }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# PARSING & METRICS
# ==========================================
# ... (–û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ —Ñ—É–Ω–∫—Ü–∏–π –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)

def get_yandex_dict_info(text, api_key):
    if not api_key: return {'lemma': text, 'pos': 'unknown'}
    url = "https://dictionary.yandex.net/api/v1/dicservice.json/lookup"
    params = {'key': api_key, 'lang': 'ru-ru', 'text': text, 'ui': 'ru'}
    try:
        r = requests.get(url, params=params, timeout=2)
        if r.status_code == 200:
            data = r.json()
            if data.get('def'):
                first_def = data['def'][0]
                return {'lemma': first_def.get('text', text), 'pos': first_def.get('pos', 'unknown')}
    except: pass
    return {'lemma': text, 'pos': 'unknown'}

def get_arsenkin_urls(query, engine_type, region_name, api_token, depth_val=10):
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check"
    url_get = "https://arsenkin.ru/api/tools/get"
    headers = {"Authorization": f"Bearer {api_token}", "Content-type": "application/json"}
    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []
    if "–Ø–Ω–¥–µ–∫—Å" in engine_type: se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type: se_params.append({"type": 11, "region": reg_ids['go']})

    payload = {"tools_name": "check-top", "data": {"queries": [query], "is_snippet": False, "noreask": True, "se": se_params, "depth": depth_val}}
    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        resp_json = r.json()
        if "error" in resp_json or "task_id" not in resp_json: st.error(f"‚ùå –û—à–∏–±–∫–∞ API: {resp_json}"); return []
        task_id = resp_json["task_id"]
        st.toast(f"–ó–∞–¥–∞—á–∞ ID {task_id} –∑–∞–ø—É—â–µ–Ω–∞")
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}"); return []

    status = "process"
    attempts = 0
    # Timeout increased to 10 minutes (120 * 5s)
    while status == "process" and attempts < 120:
        time.sleep(5); attempts += 1
        try:
            r_check = requests.post(url_check, headers=headers, json={"task_id": task_id})
            res_check_data = r_check.json()
            if res_check_data.get("status") == "finish": status = "done"; break
        except: pass

    if status != "done": st.error(f"‚è≥ –¢–∞–π–º-–∞—É—Ç API"); return []

    try:
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id}, timeout=30)
        res_data = r_final.json()
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}"); return []

    results_list = []
    try:
        collect = res_data.get('result', {}).get('result', {}).get('collect')
        if not collect: return []
        final_url_list = []
        if isinstance(collect, list) and len(collect) > 0 and isinstance(collect[0], list): final_url_list = collect[0][0]
        else:
             unique_urls = set()
             for engine_data in collect:
                 if isinstance(engine_data, dict):
                     for _, serps in engine_data.items():
                         for item in serps:
                             if item.get('url') and item.get('url') not in unique_urls:
                                 results_list.append({'url': item['url'], 'pos': item['pos']})
                                 unique_urls.add(item['url'])
             return results_list

        if final_url_list:
            for index, url in enumerate(final_url_list): results_list.append({'url': url, 'pos': index + 1})
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}"); return []
    return results_list

def process_text_detailed(text, settings, n_gram=1):
    text = text.lower().replace('—ë', '–µ')
    words = re.findall(r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+', text)
    stops = set(w.lower().replace('—ë', '–µ') for w in settings['custom_stops'])
    lemmas = []
    forms_map = defaultdict(set)
    for w in words:
        if len(w) < 2: continue
        if not settings['numbers'] and w.isdigit(): continue
        if w in stops: continue
        lemma = w
        if USE_NLP and n_gram == 1:
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            lemma = p.normal_form.replace('—ë', '–µ')
        lemmas.append(lemma)
        forms_map[lemma].add(w)
    return lemmas, forms_map

def parse_page(url, settings, query_context=""):
    import streamlit as st
    try:
        from curl_cffi import requests as cffi_requests
        headers = {
            'User-Agent': settings['ua'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        }
        r = cffi_requests.get(url, headers=headers, timeout=20, impersonate="chrome110")
        if r.status_code == 403: raise Exception("CURL_CFFI –ø–æ–ª—É—á–∏–ª 403 Forbidden")
        if r.status_code != 200: return None
        content = r.content
        encoding = r.encoding if r.encoding else 'utf-8'
    except Exception:
        try:
            import requests
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            session = requests.Session()
            headers = {'User-Agent': settings['ua']}
            r = session.get(url, headers=headers, timeout=20, verify=False)
            if r.status_code != 200: return None
            content = r.content
            encoding = r.apparent_encoding
        except Exception: return None

    try:
        # 1. –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Soup (–ü–æ–ª–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞)
        soup = BeautifulSoup(content, 'html.parser', from_encoding=encoding)
        
        # === –õ–û–ì–ò–ö–ê –¢–ê–ë–õ–ò–¶–´ 2 (–ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ URL/–°—Å—ã–ª–∫–µ) ===
        product_titles = []
        search_roots = set()
        if query_context:
            clean_q = query_context.lower().replace('–∫—É–ø–∏—Ç—å', '').replace('—Ü–µ–Ω–∞', '').replace(' –≤ ', ' ')
            words = re.findall(r'[–∞-—èa-z]+', clean_q)
            for w in words:
                if len(w) > 3: search_roots.add(w[:-1])
                else: search_roots.add(w)
        
        parsed_current = urlparse(url)
        current_path_clean = parsed_current.path.rstrip('/')
        seen_titles = set()
        
        for a in soup.find_all('a', href=True):
            txt = a.get_text(strip=True)
            raw_href = a['href']
            if len(txt) < 5 or len(txt) > 300: continue
            if raw_href.startswith('#') or raw_href.startswith('javascript'): continue
            
            abs_href = urljoin(url, raw_href)
            parsed_href = urlparse(abs_href)
            href_path_clean = parsed_href.path.rstrip('/')
            
            is_child_path = href_path_clean.startswith(current_path_clean)
            is_deeper = len(href_path_clean) > len(current_path_clean)
            is_not_query_param_only = (href_path_clean != current_path_clean)

            if is_child_path and is_deeper and is_not_query_param_only:
                txt_lower = txt.lower()
                href_lower = abs_href.lower()
                has_keywords = False
                if search_roots:
                    for root in search_roots:
                        if root in txt_lower or root in href_lower:
                            has_keywords = True; break
                else:
                    if re.search(r'\d', txt): has_keywords = True

                is_buy_button = txt_lower in {'–∫—É–ø–∏—Ç—å', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ', '–≤ –∫–æ—Ä–∑–∏–Ω—É', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Ü–µ–Ω–∞'}
                if has_keywords and not is_buy_button:
                    if txt not in seen_titles:
                        product_titles.append(txt)
                        seen_titles.add(txt)
        # ========================================================
        
        h1_tag = soup.find('h1')
        h1_text = h1_tag.get_text(strip=True) if h1_tag else ""

        # 2. –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –¢–∞–±–ª–∏—Ü—ã 2 (–£–¥–∞–ª—è–µ–º –±–ª–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤)
        soup_no_grid = BeautifulSoup(content, 'html.parser', from_encoding=encoding)
        grid_div = soup_no_grid.find('div', class_='an-container-fluid an-container-xl')
        if grid_div: grid_div.decompose()
        
        # === [–í–ê–ñ–ù–û] –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ö–û–ù–¢–ï–ù–¢–ê –ü–û –ì–ê–õ–û–ß–ö–ê–ú ===
        
        tags_to_remove = []
        # –ï—Å–ª–∏ –≥–∞–ª–æ—á–∫–∞ "–ò—Å–∫–ª—é—á–∞—Ç—å noindex" –í–ö–õ–Æ–ß–ï–ù–ê - –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥ –≤ —Å–ø–∏—Å–æ–∫ –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ
        if settings['noindex']: tags_to_remove.append('noindex')
        
        for s in [soup, soup_no_grid]:
            # –£–¥–∞–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (–≤—Å–µ–≥–¥–∞)
            for c in s.find_all(string=lambda text: isinstance(text, Comment)): c.extract()
            
            # –£–¥–∞–ª—è–µ–º —Ç–µ–≥–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ (noindex, –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω)
            if tags_to_remove:
                for t in s.find_all(tags_to_remove): t.decompose()
            
            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏ (–≤—Å–µ–≥–¥–∞)
            for script in s(["script", "style", "svg", "path", "noscript"]): script.decompose()

        # –¢–µ–∫—Å—Ç —Å—Å—ã–ª–æ–∫ (–∞–Ω–∫–æ—Ä—ã)
        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)
        
        # –°–±–æ—Ä –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ì–û —Ç–µ–∫—Å—Ç–∞ (Description, Alt, Title)
        extra_text = []
        
        # Description –±–µ—Ä–µ–º –≤—Å–µ–≥–¥–∞
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'): extra_text.append(meta_desc['content'])

        # === [–í–ê–ñ–ù–û] –£–ß–ï–¢ ALT / TITLE ===
        # –ï—Å–ª–∏ –≥–∞–ª–æ—á–∫–∞ –í–ö–õ–Æ–ß–ï–ù–ê - —Å–æ–±–∏—Ä–∞–µ–º, –µ—Å–ª–∏ –ù–ï–¢ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])
        # ================================

        # –°–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç: –í–∏–¥–∏–º—ã–π —Ç–µ–∫—Å—Ç + –°–∫—Ä—ã—Ç—ã–π (Alt/Title/Desc)
        body_text_raw = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text = re.sub(r'\s+', ' ', body_text_raw).strip()

        # –¢–æ –∂–µ —Å–∞–º–æ–µ –¥–ª—è –≤–µ—Ä—Å–∏–∏ "–±–µ–∑ —Ç–æ–≤–∞—Ä–æ–≤"
        body_text_no_grid_raw = soup_no_grid.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text_no_grid = re.sub(r'\s+', ' ', body_text_no_grid_raw).strip()

        if not body_text: return None
            
        return {
            'url': url, 
            'domain': urlparse(url).netloc, 
            'body_text': body_text, 
            'body_text_no_grid': body_text_no_grid,
            'anchor_text': anchor_text,
            'h1': h1_text,
            'product_titles': product_titles
        }
    except Exception:
        return None
        
def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    # –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –æ–∫—Ä—É–≥–ª–µ–Ω–∏—è
    def math_round(number):
        return int(number + 0.5)

    all_forms_map = defaultdict(set)
    global_forms_counter = defaultdict(Counter) 
    
    # 1. –û–ë–†–ê–ë–û–¢–ö–ê –ú–û–ï–ì–û –°–ê–ô–¢–ê
    if not my_data or not my_data.get('body_text'): 
        my_lemmas, my_forms, my_anchors, my_len = [], {}, [], 0
        my_clean_domain = "local"
    else:
        my_lemmas, my_forms = process_text_detailed(my_data['body_text'], settings)
        my_anchors, _ = process_text_detailed(my_data['anchor_text'], settings)
        my_len = len(my_lemmas)
        for k, v in my_forms.items(): all_forms_map[k].update(v)
        my_clean_domain = my_data['domain'].lower().replace('www.', '').split(':')[0]

    # 2. –û–ë–†–ê–ë–û–¢–ö–ê –ö–û–ù–ö–£–†–ï–ù–¢–û–í
    comp_docs = []
    for p in comp_data_full:
        if not p.get('body_text'): continue
        
        # –ó–ê–©–ò–¢–ê: –ò—Å–∫–ª—é—á–∞–µ–º —Å–≤–æ–π —Å–∞–π—Ç –∏–∑ —Å–ø–∏—Å–∫–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤, –µ—Å–ª–∏ –æ–Ω —Ç—É–¥–∞ –ø–æ–ø–∞–ª
        p_domain = p['domain'].lower().replace('www.', '').split(':')[0]
        if my_clean_domain != "local" and p_domain == my_clean_domain:
            continue

        body, c_forms = process_text_detailed(p['body_text'], settings)
        
        # –°–±–æ—Ä —Ä–µ–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        raw_words_for_stats = re.findall(r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+', p['body_text'].lower())
        for rw in raw_words_for_stats:
            if len(rw) < 2: continue
            if morph:
                parsed = morph.parse(rw)[0]
                if 'PREP' not in parsed.tag and 'CONJ' not in parsed.tag:
                    rw_lemma = parsed.normal_form.replace('—ë', '–µ')
                    global_forms_counter[rw_lemma][rw] += 1
        
        anchor, _ = process_text_detailed(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor, 'url': p['url'], 'domain': p['domain']})
        for k, v in c_forms.items(): all_forms_map[k].update(v)

    if not comp_docs:
        return { "depth": pd.DataFrame(), "hybrid": pd.DataFrame(), "relevance_top": pd.DataFrame(), "my_score": {"width": 0, "depth": 0}, "missing_semantics_high": [], "missing_semantics_low": [] }

    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–æ—Ä–ø—É—Å–∞
    c_lens = [len(d['body']) for d in comp_docs]
    avg_dl = np.mean(c_lens) if c_lens else 1
    N = len(comp_docs)
    
    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
    
    word_counts_per_doc = [Counter(d['body']) for d in comp_docs]

    # IDF
    word_idf_map = {}
    for lemma in vocab:
        df = doc_freqs[lemma]
        if df == 0: continue
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        word_idf_map[lemma] = max(idf, 0.01)

    # =========================================================================
    # 3. –†–ê–°–ß–ï–¢ –ú–ï–î–ò–ê–ù (–ñ–ï–°–¢–ö–û–ï –†–ê–ó–î–ï–õ–ï–ù–ò–ï)
    # =========================================================================
    
    table_depth = []
    table_hybrid = []
    missing_semantics_high = []
    missing_semantics_low = []
    
    words_with_median_gt_0 = set() 
    my_found_words_from_median = set() 
    
    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        
        # 1. –°–û–ë–ò–†–ê–ï–ú –°–´–†–´–ï –î–ê–ù–ù–´–ï (–ë–ï–ó –ù–û–†–ú–ò–†–û–í–ö–ò)
        # –≠—Ç–æ —Ä–æ–≤–Ω–æ —Ç–æ, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –Ω–∞ —Å–∞–π—Ç–∞—Ö.
        raw_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        sorted_raw = sorted(raw_counts)
        
        # –°—á–∏—Ç–∞–µ–º –ê–ë–°–û–õ–Æ–¢–ù–£–Æ –º–µ–¥–∏–∞–Ω—É (–¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç–∞–±–ª–∏—Ü–µ)
        if sorted_raw:
            rec_median_absolute = math_round(np.median(sorted_raw))
            obs_min = math_round(sorted_raw[0])
            obs_max = math_round(sorted_raw[-1])
        else:
            rec_median_absolute = 0; obs_min = 0; obs_max = 0

        # 2. –°–ß–ò–¢–ê–ï–ú –¶–ï–õ–ï–í–£–Æ –ú–ï–î–ò–ê–ù–£ (–î–õ–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô)
        # –ó–¥–µ—Å—å –º—ã —É—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω–∏—Ü—É –≤ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–æ–≤, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –≥–∞–ª–æ—á–∫–∞
        if settings['norm'] and my_len > 0:
            norm_counts = []
            for i in range(N):
                raw_cnt = raw_counts[i]
                comp_len = c_lens[i]
                if comp_len > 0:
                    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –≤–∞—à–µ–π –¥–ª–∏–Ω–µ
                    normalized_val = raw_cnt * (my_len / comp_len)
                    norm_counts.append(normalized_val)
                else:
                    norm_counts.append(0)
            
            if norm_counts:
                rec_median_target = math_round(np.median(sorted(norm_counts)))
            else:
                rec_median_target = 0
        else:
            # –ï—Å–ª–∏ –≥–∞–ª–æ—á–∫–∞ —Å–Ω—è—Ç–∞, —Ü–µ–ª–µ–≤–∞—è —Ä–∞–≤–Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π
            rec_median_target = rec_median_absolute

        # --- –ú–û–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ---
        my_tf_count = my_lemmas.count(lemma)
        
        if obs_max == 0 and my_tf_count == 0: continue

        # --- –®–ò–†–ò–ù–ê (–°—á–∏—Ç–∞–µ–º –ø–æ —Ü–µ–ª–µ–≤–æ–π, —á—Ç–æ–±—ã –Ω–µ —É–ø—É—Å–∫–∞—Ç—å –≤–∞–∂–Ω–æ–µ) ---
        if rec_median_target >= 1:
            words_with_median_gt_0.add(lemma)
            if my_tf_count > 0:
                my_found_words_from_median.add(lemma)

        # --- –£–ü–£–©–ï–ù–ù–ê–Ø –°–ï–ú–ê–ù–¢–ò–ö–ê ---
        display_word = lemma
        if global_forms_counter[lemma]:
            display_word = global_forms_counter[lemma].most_common(1)[0][0]

        if my_tf_count == 0:
            weight = word_idf_map.get(lemma, 0) * (rec_median_target if rec_median_target > 0 else 1)
            item = {'word': display_word, 'weight': weight}
            if rec_median_target >= 1: missing_semantics_high.append(item)
            else: missing_semantics_low.append(item)

        # --- –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò (–í–ê–ñ–ù–û: —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å TARGET) ---
        diff = rec_median_target - my_tf_count
        
        if diff == 0: status = "–ù–æ—Ä–º–∞"; action_text = "‚úÖ"; sort_val = 0
        elif diff > 0: status = "–ù–µ–¥–æ—Å–ø–∞–º"; action_text = f"+{diff}"; sort_val = diff
        else: status = "–ü–µ—Ä–µ—Å–ø–∞–º"; action_text = f"{diff}"; sort_val = abs(diff)

        forms_str = ", ".join(sorted(list(all_forms_map.get(lemma, set())))) if all_forms_map.get(lemma) else lemma

        table_depth.append({
            "–°–ª–æ–≤–æ": display_word,
            "–°–ª–æ–≤–æ—Ñ–æ—Ä–º—ã": forms_str,
            "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_tf_count,
            
            # !!!!!!!! –ó–î–ï–°–¨ –¢–ï–ü–ï–†–¨ –ê–ë–°–û–õ–Æ–¢–ù–ê–Ø –ú–ï–î–ò–ê–ù–ê (2, –∞ –Ω–µ 17) !!!!!!!!
            "–ú–µ–¥–∏–∞–Ω–∞": rec_median_absolute, 
            
            "–ú–∏–Ω–∏–º—É–º (–∫–æ–Ω–∫—É—Ä.)": obs_min,
            "–ú–∞–∫—Å–∏–º—É–º (–∫–æ–Ω–∫—É—Ä.)": obs_max,
            "–°—Ç–∞—Ç—É—Å": status,
            "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": action_text, # –ê –∑–¥–µ—Å—å —É—á—Ç–µ–Ω–∞ –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞
            "is_missing": (my_tf_count == 0),
            "sort_val": sort_val
        })
        
        table_hybrid.append({
            "–°–ª–æ–≤–æ": display_word,
            "TF-IDF –¢–û–ü": round(word_idf_map.get(lemma, 0) * (rec_median_absolute / avg_dl if avg_dl > 0 else 0), 4),
            "TF-IDF —É –≤–∞—Å": round(word_idf_map.get(lemma, 0) * (my_tf_count / my_len if my_len > 0 else 0), 4),
            "–°–∞–π—Ç–æ–≤": doc_freqs[lemma],
            "–ü–µ—Ä–µ—Å–ø–∞–º": obs_max
        })

    # =========================================================================
    # 4. –§–ò–ù–ê–õ–ò–ó–ê–¶–ò–Ø
    # =========================================================================
    
    total_needed = len(words_with_median_gt_0)
    total_found = len(my_found_words_from_median)
    
    if total_needed > 0:
        ratio = total_found / total_needed
        my_width_score_final = int(min(100, ratio * 120))
    else:
        my_width_score_final = 0

    S_WIDTH_CORE = words_with_median_gt_0 
    
    def calculate_raw_power(doc_tokens, doc_len):
        if avg_dl == 0 or doc_len == 0: return 0
        score = 0
        counts = Counter(doc_tokens)
        k1 = 1.2; b = 0.75
        for word in S_WIDTH_CORE:
            if word not in counts: continue
            tf = counts[word]
            idf = word_idf_map.get(word, 0)
            term_weight = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avg_dl)))
            score += term_weight
        return score

    comp_raw_scores = []
    competitor_scores_map = {}
    
    for i, doc in enumerate(comp_docs):
        c_found = len(set(doc['body']).intersection(S_WIDTH_CORE))
        if total_needed > 0:
            c_width_val = int(min(100, (c_found / total_needed) * 120))
        else:
            c_width_val = 0
            
        raw_val = calculate_raw_power(doc['body'], c_lens[i])
        comp_raw_scores.append(raw_val)
        competitor_scores_map[doc['url']] = {'width_final': c_width_val, 'raw_depth': raw_val}

    if comp_raw_scores:
        median_raw = np.median(comp_raw_scores)
        ref_val = median_raw if median_raw > 0.1 else 1.0
    else:
        ref_val = 1.0
    
    k_norm = 80.0 / ref_val
    my_raw_bm25 = calculate_raw_power(my_lemmas, my_len)
    my_depth_score_final = int(round(min(100, my_raw_bm25 * k_norm)))

    for url, data in competitor_scores_map.items():
        data['depth_final'] = int(round(min(100, data['raw_depth'] * k_norm)))

    # –°–±–æ—Ä–∫–∞ —Ç–∞–±–ª–∏—Ü—ã
    missing_semantics_high.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low = missing_semantics_low[:500]

    table_rel = []
    my_site_found_in_selection = False
    for item in original_results:
        url = item['url']
        if url not in competitor_scores_map: continue
        row_domain = urlparse(url).netloc.lower().replace('www.', '')
        is_my_site = False
        if my_clean_domain and my_clean_domain != "local" and my_clean_domain in row_domain:
            is_my_site = True
            my_site_found_in_selection = True
            display_name = f"{urlparse(url).netloc} (–í—ã)"
        else:
            display_name = urlparse(url).netloc
        scores = competitor_scores_map[url]
        table_rel.append({ 
            "–î–æ–º–µ–Ω": display_name, 
            "URL": url,
            "–ü–æ–∑–∏—Ü–∏—è": item['pos'], 
            "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": scores['width_final'], 
            "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": scores['depth_final'] 
        })
        
    if not my_site_found_in_selection:
        pos_to_show = my_serp_pos if my_serp_pos > 0 else 0
        my_label = f"{my_data['domain']} (–í—ã)" if (my_data and my_data.get('domain')) else "–í–∞—à —Å–∞–π—Ç"
        my_full_url = my_data['url'] if (my_data and 'url' in my_data) else "#"
        table_rel.append({ "–î–æ–º–µ–Ω": my_label, "URL": my_full_url, "–ü–æ–∑–∏—Ü–∏—è": pos_to_show, "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": my_width_score_final, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": my_depth_score_final })

    df_rel_for_analysis = pd.DataFrame(table_rel)
    good_urls, bad_urls_dicts, trend_info = analyze_serp_anomalies(df_rel_for_analysis)
    
    st.session_state['detected_anomalies'] = bad_urls_dicts
    st.session_state['serp_trend_info'] = trend_info
    
    if bad_urls_dicts:
        st.session_state['persistent_urls'] = "\n".join(good_urls)
        st.session_state['excluded_urls_auto'] = "\n".join([item['url'] for item in bad_urls_dicts])
    else:
        st.session_state['persistent_urls'] = "\n".join([r.get('URL', r['–î–æ–º–µ–Ω']) for r in table_rel if "(–í—ã)" not in r['–î–æ–º–µ–Ω']])
        st.session_state['excluded_urls_auto'] = ""

    return { 
        "depth": pd.DataFrame(table_depth), 
        "hybrid": pd.DataFrame(table_hybrid), 
        "relevance_top": pd.DataFrame(table_rel).sort_values(by='–ü–æ–∑–∏—Ü–∏—è', ascending=True).reset_index(drop=True), 
        "my_score": {"width": my_width_score_final, "depth": my_depth_score_final}, 
        "missing_semantics_high": missing_semantics_high, 
        "missing_semantics_low": missing_semantics_low 
    }
    
def get_hybrid_word_type(word, main_marker_root, specs_dict=None):
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä 3.1 (–§–∏–∫—Å –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤).
    """
    w = word.lower()
    specs_dict = specs_dict or set()
    
    # 1. –ú–ê–†–ö–ï–†
    if w == main_marker_root: return "1. üíé –ú–∞—Ä–∫–µ—Ä (–¢–æ–≤–∞—Ä)"
    if morph:
        norm = morph.parse(w)[0].normal_form
        if norm == main_marker_root: return "1. üíé –ú–∞—Ä–∫–µ—Ä (–¢–æ–≤–∞—Ä)"

    # 2. –°–¢–ê–ù–î–ê–†–¢–´
    if re.search(r'(gost|din|iso|en|tu|astm|aisi|–≥–æ—Å—Ç|–æ—Å—Ç|—Ç—É|–¥–∏–Ω)', w):
        return "6. üìú –°—Ç–∞–Ω–¥–∞—Ä—Ç"

    # 3. –†–ê–ó–ú–ï–†–´ / –¢–ï–•. –ü–ê–†–ê–ú–ï–¢–†–´
    # –ê. –ì–æ–ª—ã–µ —Ü–∏—Ñ—Ä—ã (10, 50.5)
    if re.fullmatch(r'\d+([.,]\d+)?', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    # –ë. –†–∞–∑–º–µ—Ä—ã —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ (10—Ö20, 10*20, 10-20, 10/20) <--- –î–û–ë–ê–í–ò–õ –¢–ò–†–ï –ò –°–õ–ï–®
    if re.search(r'^\d+[x—Ö*\-/]\d+', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    # –í. –ï–¥–∏–Ω–∏—Ü—ã (–º–º, –∫–≥)
    if re.search(r'\d+(–º–º|mm|–º|m|kg|–∫–≥|bar|–±–∞—Ä|–∞—Ç–º)$', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    # –ì. –ü—Ä–µ—Ñ–∏–∫—Å—ã (–î—É, –†—É, SDR)
    if re.match(r'^(d|dn|pn|sn|sdr|–¥—É|—Ä—É|√∏)\d+', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"

    # 4. –ú–ê–†–ö–ò / –°–ü–õ–ê–í–´
    if w in specs_dict: return "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –º–∞—Ä–æ–∫ (–ë—É–∫–≤—ã+–¶–∏—Ñ—Ä—ã)
    if re.search(r'\d', w): return "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"

    # 5. –õ–ê–¢–ò–ù–ò–¶–ê (–ë—Ä–µ–Ω–¥—ã)
    if re.search(r'^[a-z\-]+$', w): return "7. üî† –õ–∞—Ç–∏–Ω–∏—Ü–∞/–ë—Ä–µ–Ω–¥"

    # 6. –¢–ï–ö–°–¢
    if morph:
        p = morph.parse(w)[0]
        tag = p.tag
        if {'PREP'} in tag or {'CONJ'} in tag: return "SKIP"
        if {'ADJF'} in tag or {'PRTF'} in tag or {'ADJS'} in tag: return "2. üé® –°–≤–æ–π—Å—Ç–≤–∞"
        if {'NOUN'} in tag: return "4. üîó –î–æ–ø–æ–ª–Ω–µ–Ω–∏—è"

    if w.endswith(('–∏–π', '—ã–π', '–∞—è', '–æ–µ', '—ã–µ', '–∞—è')): return "2. üé® –°–≤–æ–π—Å—Ç–≤–∞"
    return "4. üîó –î–æ–ø–æ–ª–Ω–µ–Ω–∏—è"
    
def calculate_naming_metrics(comp_data_full, my_data, settings):
    """
    –¢–∞–±–ª–∏—Ü–∞ 2. –ë–µ–∑ "–æ–±—Ä–µ–∑–∞–Ω–∏—è" —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤.
    """
    # –ü–æ–¥–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è
    SPECS_SET = st.session_state.get('categorized_dimensions', set())
    if not SPECS_SET: _, _, SPECS_SET, _, _, _ = load_lemmatized_dictionaries()

    # 1. –ú–æ–π —Å–∞–π—Ç
    my_tokens = []
    if my_data and my_data.get('body_text_no_grid'):
        # –°–≤–æ—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –î—É50
        raw_w = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', my_data['body_text_no_grid'].lower())
        for w in raw_w:
            # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–ª–æ–≤–∞
            if not re.search(r'\d', w) and morph:
                my_tokens.append(morph.parse(w)[0].normal_form)
            else:
                my_tokens.append(w)

    # 2. –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã
    all_words_flat = []
    site_vocab_map = []
    
    for p in comp_data_full:
        titles = p.get('product_titles', [])
        valid_titles = [t for t in titles if 5 < len(t) < 150]
        
        if not valid_titles:
            site_vocab_map.append(set())
            continue
            
        curr_site_tokens = set()
        for t in valid_titles:
            words = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower())
            for w in words:
                if len(w) < 2: continue
                
                # –õ–û–ì–ò–ö–ê –°–û–•–†–ê–ù–ï–ù–ò–Ø –§–û–†–ú–´:
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Ü–∏—Ñ—Ä–∞ -> —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å (d50 -> d50)
                if re.search(r'\d', w):
                    token = w
                elif re.search(r'^[a-z]+$', w): # –õ–∞—Ç–∏–Ω–∏—Ü–∞ -> –∫–∞–∫ –µ—Å—Ç—å
                    token = w
                elif morph: # –†—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ -> –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º (—Å—Ç–∞–ª—å–Ω–∞—è -> —Å—Ç–∞–ª—å–Ω–æ–π)
                    token = morph.parse(w)[0].normal_form
                else:
                    token = w
                
                all_words_flat.append(token)
                curr_site_tokens.add(token)
                
        site_vocab_map.append(curr_site_tokens)

    if not all_words_flat: return pd.DataFrame()
    N_sites = len(site_vocab_map)

    # 3. –ú–∞—Ä–∫–µ—Ä (–°–∞–º–æ–µ —á–∞—Å—Ç–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–ª–æ–≤–æ)
    counts = Counter([w for w in all_words_flat if not re.search(r'\d', w)])
    main_marker_root = ""
    # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
    for w, c in counts.most_common(10):
        if morph and 'NOUN' in morph.parse(w)[0].tag: main_marker_root = w; break
    if not main_marker_root and counts: main_marker_root = counts.most_common(1)[0][0]

    # 4. –°–±–æ—Ä —Ç–∞–±–ª–∏—Ü—ã
    vocab = sorted(list(set(all_words_flat)))
    table_rows = []
    
    for token in vocab:
        if token in GARBAGE_LATIN_STOPLIST: continue
        
        # –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å
        sites_with_word = sum(1 for s_set in site_vocab_map if token in s_set)
        freq_percent = int((sites_with_word / N_sites) * 100)
        
        # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø
        cat = get_hybrid_word_type(token, main_marker_root, SPECS_SET)
        
        if cat == "SKIP": continue
        
        # –§–∏–ª—å—Ç—Ä—ã –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        # –ú–∞—Ä–∫–∏ –∏ –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç 5%
        is_spec = "–ú–∞—Ä–∫–∞" in cat or "–°—Ç–∞–Ω–¥–∞—Ä—Ç" in cat
        if is_spec and freq_percent < 5: continue
        
        # –û–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞ –æ—Ç 15%
        if not is_spec and "–†–∞–∑–º–µ—Ä—ã" not in cat and freq_percent < 15: continue
        
        # –†–∞–∑–º–µ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ —Ä–µ–∞–ª—å–Ω–æ —á–∞—Å—Ç—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ö–æ–¥–æ–≤–æ–π –¥–∏–∞–º–µ—Ç—Ä)
        # –ò–Ω–∞—á–µ —Ç–∞–±–ª–∏—Ü–∞ –±—É–¥–µ—Ç –∑–∞–±–∏—Ç–∞ —Ü–∏—Ñ—Ä–∞–º–∏ 10, 11, 12...
        if "–†–∞–∑–º–µ—Ä—ã" in cat and freq_percent < 15: continue

        rec_median = 1 if freq_percent > 30 else 0
        my_tf = my_tokens.count(token)
        diff = rec_median - my_tf
        action_text = f"+{diff}" if diff > 0 else ("‚úÖ" if diff == 0 else f"{diff}")
        
        table_rows.append({
            "–¢–∏–ø —Ö–∞—Ä-–∫–∏": cat[3:],
            "–°–ª–æ–≤–æ": token, # –í—ã–≤–æ–¥–∏–º —Ç–æ–∫–µ–Ω –∫–∞–∫ –µ—Å—Ç—å (—Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏ –±—É–∫–≤–∞–º–∏)
            "–ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å (%)": f"{freq_percent}%",
            "–£ –í–∞—Å": my_tf,
            "–ú–µ–¥–∏–∞–Ω–∞": rec_median,
            "–î–æ–±–∞–≤–∏—Ç—å": action_text,
            "raw_freq": freq_percent,
            "cat_sort": int(cat[0])
        })
        
    df = pd.DataFrame(table_rows)
    if not df.empty:
        df = df.sort_values(by=["cat_sort", "raw_freq"], ascending=[True, False])
        
    return df

def analyze_ideal_name(comp_data_full):
    """
    –°—Ç—Ä–æ–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å —É—á–µ—Ç–æ–º –ú–∞—Ä–æ–∫ –∏ –ì–û–°–¢–æ–≤.
    """
    # –ü–æ–¥–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
    SPECS_SET = st.session_state.get('categorized_dimensions', set())
    if not SPECS_SET: _, _, SPECS_SET, _, _, _ = load_lemmatized_dictionaries()

    titles = []
    for d in comp_data_full:
        ts = d.get('product_titles', [])
        titles.extend([t for t in ts if 5 < len(t) < 150])
    
    if not titles: return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö", []

    # –ú–∞—Ä–∫–µ—Ä
    all_w = []
    for t in titles: all_w.extend(re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower()))
    c = Counter(all_w)
    main_marker_root = ""
    for w, _ in c.most_common(5):
        if not re.search(r'\d', w):
             if morph and 'NOUN' in morph.parse(w)[0].tag: main_marker_root = w; break
             elif not morph: main_marker_root = w; break
    if not main_marker_root and c: main_marker_root = c.most_common(1)[0][0]

    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    structure_counter = Counter()
    vocab_by_type = defaultdict(Counter)
    
    sample = titles[:500]
    
    for t in sample:
        words = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower())
        pattern = []
        
        for w in words:
            if len(w) < 2: continue
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º —Å–ª–æ–≤–∞—Ä—è
            cat_full = get_hybrid_word_type(w, main_marker_root, SPECS_SET)
            if cat_full == "SKIP": continue
            
            # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∏–º—è —Ç–∏–ø–∞ ("–°–≤–æ–π—Å—Ç–≤–∞", "–ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤", "–°—Ç–∞–Ω–¥–∞—Ä—Ç")
            # "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤" -> "–ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"
            try:
                cat_short = cat_full.split('.', 1)[1].strip().split(' ', 1)[1] # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –∏–∫–æ–Ω–∫–∏
            except:
                cat_short = cat_full # Fallback
            
            vocab_by_type[cat_short][w] += 1
            
            if not pattern or pattern[-1] != cat_short:
                pattern.append(cat_short)
        
        if pattern:
            structure_str = " + ".join(pattern)
            structure_counter[structure_str] += 1
            
    # –°–±–æ—Ä–∫–∞
    if not structure_counter: return "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞", []
    
    best_struct_str, _ = structure_counter.most_common(1)[0]
    best_struct_list = best_struct_str.split(" + ")
    
    final_parts = []
    used_words = set()
    
    for block in best_struct_list:
        # –î–ª—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç–∞–≤–∏–º –∑–∞–≥–ª—É—à–∫—É
        if "–†–∞–∑–º–µ—Ä—ã" in block or "–°—Ç–∞–Ω–¥–∞—Ä—Ç" in block or "–ú–∞—Ä–∫–∞" in block:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –æ–Ω –æ—á–µ–Ω—å –ø–æ–ø—É–ª—è—Ä–µ–Ω
            top_cand = vocab_by_type[block].most_common(1)
            if top_cand and top_cand[0][1] > (len(sample) * 0.3): # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —É 30%
                 final_parts.append(top_cand[0][0])
            else:
                 final_parts.append(f"[{block.upper()}]")
            continue
            
        # –î–ª—è —Å–ª–æ–≤ (–ú–∞—Ä–∫–µ—Ä, –°–≤–æ–π—Å—Ç–≤–∞) –±–µ—Ä–µ–º –¢–û–ü-1
        candidates = vocab_by_type[block].most_common(3)
        for w, cnt in candidates:
            if w not in used_words:
                if "–ú–∞—Ä–∫–µ—Ä" in block: w = w.capitalize()
                final_parts.append(w)
                used_words.add(w)
                break
                
    ideal_name = " ".join(final_parts)
    
    # –û—Ç—á–µ—Ç
    report = []
    report.append(f"**–°—Ö–µ–º–∞:** {best_struct_str}")
    report.append("")
    report.append("**–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**")
    for block in best_struct_list:
        if "–†–∞–∑–º–µ—Ä—ã" in block: continue
        top = [f"{w}" for w, c in vocab_by_type[block].most_common(3)]
        report.append(f"- **{block}**: {', '.join(top)}")
            
    return ideal_name, report

def render_paginated_table(df, title_text, key_prefix, default_sort_col=None, use_abs_sort_default=False):
    if df.empty: st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö."); return
    col_t1, col_t2 = st.columns([7, 3])
    with col_t1: st.markdown(f"### {title_text}")
    if f'{key_prefix}_sort_col' not in st.session_state: st.session_state[f'{key_prefix}_sort_col'] = default_sort_col if (default_sort_col and default_sort_col in df.columns) else df.columns[0]
    if f'{key_prefix}_sort_order' not in st.session_state: st.session_state[f'{key_prefix}_sort_order'] = "–£–±—ã–≤–∞–Ω–∏–µ"

    search_query = st.text_input(f"üîç –ü–æ–∏—Å–∫ ({title_text})", key=f"{key_prefix}_search")
    if search_query:
        mask = df.astype(str).apply(lambda x: x.str.contains(search_query, case=False, na=False)).any(axis=1)
        df_filtered = df[mask].copy()
    else: df_filtered = df.copy()

    if df_filtered.empty: st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."); return

    with st.container():
        st.markdown("<div class='sort-container'>", unsafe_allow_html=True)
        col_s1, col_s2, col_sp = st.columns([2, 2, 4])
        with col_s1:
            current_sort = st.session_state[f'{key_prefix}_sort_col']
            if current_sort not in df_filtered.columns: current_sort = df_filtered.columns[0]
            sort_col = st.selectbox("üóÇ –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ:", df_filtered.columns, key=f"{key_prefix}_sort_box", index=list(df_filtered.columns).index(current_sort))
            st.session_state[f'{key_prefix}_sort_col'] = sort_col
        with col_s2:
            sort_order = st.radio("–ü–æ—Ä—è–¥–æ–∫:", ["–£–±—ã–≤–∞–Ω–∏–µ", "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ"], horizontal=True, key=f"{key_prefix}_order_box", index=0 if st.session_state[f'{key_prefix}_sort_order'] == "–£–±—ã–≤–∞–Ω–∏–µ" else 1)
            st.session_state[f'{key_prefix}_sort_order'] = sort_order
        st.markdown("</div>", unsafe_allow_html=True)

    ascending = (sort_order == "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ")
    if use_abs_sort_default and sort_col == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è" and "sort_val" in df_filtered.columns: df_filtered = df_filtered.sort_values(by="sort_val", ascending=ascending)
    elif ("–î–æ–±–∞–≤–∏—Ç—å" in sort_col or "+/-" in sort_col) and df_filtered[sort_col].dtype == object:
        try:
            df_filtered['_temp_sort'] = df_filtered[sort_col].astype(str).str.replace(r'[^\d]', '', regex=True)
            df_filtered['_temp_sort'] = pd.to_numeric(df_filtered['_temp_sort'], errors='coerce').fillna(0)
            df_filtered = df_filtered.sort_values(by='_temp_sort', ascending=ascending).drop(columns=['_temp_sort'])
        except: df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)
    else: df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)

    df_filtered = df_filtered.reset_index(drop=True); df_filtered.index = df_filtered.index + 1
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        export_df = df_filtered.copy()
        if "is_missing" in export_df.columns: del export_df["is_missing"]
        if "sort_val" in export_df.columns: del export_df["sort_val"]
        export_df.to_excel(writer, index=False, sheet_name='Data')
    excel_data = buffer.getvalue()
    with col_t2: st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel", data=excel_data, file_name=f"{key_prefix}_export.xlsx", mime="application/vnd.ms-excel", key=f"{key_prefix}_down")

    ROWS_PER_PAGE = 20
    if f'{key_prefix}_page' not in st.session_state: st.session_state[f'{key_prefix}_page'] = 1
    total_rows = len(df_filtered); total_pages = math.ceil(total_rows / ROWS_PER_PAGE)
    if total_pages == 0: total_pages = 1
    current_page = st.session_state[f'{key_prefix}_page']
    if current_page > total_pages: current_page = total_pages
    if current_page < 1: current_page = 1
    st.session_state[f'{key_prefix}_page'] = current_page
    start_idx = (current_page - 1) * ROWS_PER_PAGE
    end_idx = start_idx + ROWS_PER_PAGE
    df_view = df_filtered.iloc[start_idx:end_idx]

    def highlight_rows(row):
        base_style = 'background-color: #FFFFFF; color: #3D4858; border-bottom: 1px solid #DBEAFE;'
        styles = []
        status = row.get("–°—Ç–∞—Ç—É—Å", "")
        for col_name in row.index:
            cell_style = base_style
            if col_name == "–°—Ç–∞—Ç—É—Å":
                if status == "–ù–µ–¥–æ—Å–ø–∞–º": cell_style += "color: #D32F2F; font-weight: bold;"
                elif status == "–ü–µ—Ä–µ—Å–ø–∞–º": cell_style += "color: #E65100; font-weight: bold;"
                elif status == "–ù–æ—Ä–º–∞": cell_style += "color: #2E7D32; font-weight: bold;"
            styles.append(cell_style)
        return styles

    cols_to_hide = [c for c in ["is_missing", "sort_val"] if c in df_view.columns]
    try: styled_df = df_view.style.apply(highlight_rows, axis=1)
    except: styled_df = df_view
    st.dataframe(styled_df, use_container_width=True, height=(len(df_view) * 35) + 40, column_config={c: None for c in cols_to_hide})
    c_spacer, c_btn_prev, c_info, c_btn_next = st.columns([6, 1, 1, 1])
    with c_btn_prev:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_prev", disabled=(current_page <= 1), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] -= 1
            st.rerun()
    with c_info: st.markdown(f"<div style='text-align: center; margin-top: 10px;'><b>{current_page}</b> / {total_pages}</div>", unsafe_allow_html=True)
    with c_btn_next:
        if st.button("‚û°Ô∏è", key=f"{key_prefix}_next", disabled=(current_page >= total_pages), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] += 1
            st.rerun()
    st.markdown("---")

# ==========================================
# PERPLEXITY GEN
# ==========================================
STATIC_DATA_GEN = {
    'IP_PROP4817': "–£—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏",
    'IP_PROP4818': "–û–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –≤ —Ä–µ–≥–∏–æ–Ω—ã —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4819': """<p>–ù–∞–¥–µ–∂–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫–∞–∑–∞ –≤ –ª—é–±—É—é —Ç–æ—á–∫—É —Å—Ç—Ä–∞–Ω—ã: "–°—Ç–∞–ª—å–º–µ—Ç—É—Ä–∞–ª" –æ—Ç–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–≤–∞—Ä 24 —á–∞—Å–∞ –≤ —Å—É—Ç–∫–∏, 7 –¥–Ω–µ–π –≤ –Ω–µ–¥–µ–ª—é. –ë–æ–ª–µ–µ 4 000 –æ—Ç–≥—Ä—É–∑–æ–∫ –≤ –≥–æ–¥. –ü—Ä–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–∏ –∑–∞–∫–∞–∑–∞ –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –≤–∞–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ä—à—Ä—É—Ç.</p>""",
    'IP_PROP4820': """<p>–ù–∞—à–∏ –∏–∑–¥–µ–ª–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è—Ö –£—Ä–∞–ª–∞, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞, –ü–æ–≤–æ–ª–∂—å—è, –°–∏–±–∏—Ä–∏. –ü–∞—Ä—Ç–Ω–µ—Ä—ã –ø–æ –ª–æ–≥–∏—Å—Ç–∏–∫–µ –ø—Ä–µ–¥–ª–æ–∂–∞—Ç –¥–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–∫–∞–∑ —Å–∞–º—ã–º —É–¥–æ–±–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º ‚Äì –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–º, –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω—ã–º, –¥–∞–∂–µ –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º. –î–ª—è –≤–∞—Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—É—é —Å—Ö–µ–º—É –ø–æ–¥ —É–¥–æ–±–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è. –ü–æ–≥—Ä—É–∑–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ç–µ—Ö–Ω–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.</p><div class="h4"><h4>–°–∞–º–æ–≤—ã–≤–æ–∑</h4></div><p>–ï—Å–ª–∏ –æ–±—ã—á–Ω–æ —Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∑–∞–±—Ä–∞—Ç—å —Ç–æ–≤–∞—Ä –∏–ª–∏ –¥–∞–µ—Ç–µ —ç—Ç–æ –ø—Ä–∞–≤–æ —É–ø–æ–ª–Ω–æ–º–æ—á–µ–Ω–Ω—ã–º, –∞–¥—Ä–µ—Å –∏ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã —Å–∫–ª–∞–¥–∞ –≤ —Å–≤–æ–µ–º –≥–æ—Ä–æ–¥–µ —É—Ç–æ—á–Ω—è–π—Ç–µ —É –º–µ–Ω–µ–¥–∂–µ—Ä–∞.</p><div class="h4"><h4>–ì—Ä—É–∑–æ–≤–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏</h4></div><p>–û—Ç–ø—Ä–∞–≤–∏–º –ø—Ä–æ–∫–∞—Ç –Ω–∞ –≤–∞—à –æ–±—ä–µ–∫—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –∞–≤—Ç–æ–ø–∞—Ä–∫–æ–º. –ü–æ–ª—É—á–µ–Ω–∏–µ –≤ —É–ø–∞–∫–æ–≤–∫–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏, –∞ –∏–º–µ–Ω–Ω–æ –Ω–∞ –¥–µ—Ä–µ–≤—è–Ω–Ω–æ–º –ø–æ–¥–¥–æ–Ω–µ.</p><div class="h4"><h4>–°–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å –¢–ö</h4></div><p>–î–æ—Å—Ç–∞–≤–∫–∞ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –†–æ—Å—Å–∏–∏ –∏ –°–ù–ì. –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å—Å—è, —Ç–∞–∫ –∫–∞–∫ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç, –æ–¥–Ω–∞–∫–æ, —Å—Ä–∞–≤–Ω–∏–º —Å—Ç–æ–∏–º–æ—Å—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª—É–∂–± –∏ –≤—ã–±–µ—Ä–µ–º –ª—É—á—à—É—é.</p>""",
    'IP_PROP4821': "–û–ø–ª–∞—Ç–∞ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤:",
    'IP_PROP4822': """<p>–ù–∞—à–∞ –∫–æ–º–ø–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞ –ø—Ä–∏–Ω—è—Ç—å –ª—é–±—ã–µ –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–µ –≤–∏–¥—ã –æ–ø–ª–∞—Ç—ã –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏—Ü: –ø–æ —Å—á–µ—Ç—É, –Ω–∞–ª–∏—á–Ω–∞—è –∏ –±–µ–∑–Ω–∞–ª–∏—á–Ω–∞—è, –Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–π –ø–ª–∞—Ç–µ–∂, —Ç–∞–∫–∂–µ –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞ –∏ –æ—Ç—Å—Ä–æ—á–∫–∞ –ø–ª–∞—Ç–µ–∂–∞.</p>""",
    'IP_PROP4823': """<div class="h4"><h3>–ü—Ä–∏–º–µ—Ä—ã –≤–æ–∑–º–æ–∂–Ω–æ–π –æ–ø–ª–∞—Ç—ã</h3></div><div class="an-col-12"><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–° –ø–æ–º–æ—â—å—é –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤ —Ü–µ–Ω—Ç—Ä–∞—Ö –ø—Ä–æ–¥–∞–∂</span></p></li></ul><p>–í–∞–∂–Ω–æ! –¶–µ–Ω–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—É–±–ª–∏—á–Ω–æ–π –æ—Ñ–µ—Ä—Ç–æ–π. –ü—Ä–∏—Ö–æ–¥–∏—Ç–µ –≤ –Ω–∞—à –æ—Ñ–∏—Å, —á—Ç–æ–±—ã —É—Ç–æ—á–Ω–∏—Ç—å –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ, –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ—á—Ç–∏ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å, —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –≤–æ–∑–≤—Ä–∞—Ç, —Å—á–µ—Ç, —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ª–æ–≥–∏—Å—Ç–∏–∫—É.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞ —Ä–∞—Å—á–µ—Ç–Ω—ã–π —Å—á–µ—Ç</span></p></li></ul><p>–ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É —Å—á–µ—Ç—É –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏ –±–∞–Ω–∫–∞ –∏–ª–∏ –ø—É—Ç–µ–º –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥—Å—Ç–≤ —á–µ—Ä–µ–∑ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç (—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∑–∞—â–∏—â–µ–Ω—ã, —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –æ—Ç–¥–µ–ª–µ–Ω–∏—è). –î–ª—è –ø—Ä–∞–≤–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å —Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –ø–ª–∞—Ç–µ–∂–Ω–æ–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ —Å –æ—Ç–º–µ—Ç–∫–æ–π –±–∞–Ω–∫–∞.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞–ª–∏—á–Ω—ã–º–∏ –∏–ª–∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∫–∞—Ä—Ç–æ–π –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏</span></p></li></ul><p><span style="font-weight: 400;">–ü–æ–º–æ–∂–µ–º —Å –æ–ø–ª–∞—Ç–æ–π: –æ–±—ä–µ–º –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ. –ö—Ä—É–ø–Ω—ã–º –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º ‚Äì –¥–µ–Ω—å–≥–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–µ–º–∫–∏ —Ç–æ–≤–∞—Ä–∞.</span></p><p>–ú–µ–Ω–µ–¥–∂–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.</p><p>–ó–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ —á–µ—Ä–µ–∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç:</p><p><a class="btn btn-blue" href="/catalog/">–ö–∞—Ç–∞–ª–æ–≥ (–º–∞–≥–∞–∑–∏–Ω-–º–µ–Ω—é):</a></p></div></div><br>""",
    'IP_PROP4824': "–û–ø–∏—Å–∞–Ω–∏–µ, —Å—Ç–∞—Ç—å–∏, –ø–æ–∏—Å–∫, –æ—Ç–∑—ã–≤—ã, –Ω–æ–≤–æ—Å—Ç–∏, –∞–∫—Ü–∏–∏, –∂—É—Ä–Ω–∞–ª, info:",
    'IP_PROP4825': "–ú–æ–∂–µ–º –º–µ—Ç–∞–ª–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –æ—Ü–∏–Ω–∫–æ–≤–∞—Ç—å, –Ω–∏–∫–µ–ª–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–æ–≤–æ–ª–æ—á—å",
    'IP_PROP4826': "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥",
    'IP_PROP4834': "–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –±–µ–∑ –ø—Ä–∏–º–µ—Å–µ–π",
    'IP_PROP4835': "–ü–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫",
    'IP_PROP4836': "–ö–∞—á–µ—Å—Ç–≤–æ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä",
    'IP_PROP4837': "–ü–æ—Ä—è–¥–æ–∫ –≤ –ì–û–°–¢–∞—Ö"
}

def get_page_data_for_gen(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.encoding = 'utf-8'
    except Exception as e: return None, None, None, f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}"
    
    if response.status_code != 200: return None, None, None, f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—É—Å–∞: {response.status_code}"
    
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # 1. –ó–ê–ì–û–õ–û–í–û–ö: –ò—â–µ–º —Å—Ç—Ä–æ–≥–æ H2 (–∫–∞–∫ –≤—ã –ø—Ä–æ—Å–∏–ª–∏)
    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–π —á–∞—Å—Ç–∏ (—á–∞—Å—Ç–æ –±—ã–≤–∞–µ—Ç h2 –≤ –º–µ–Ω—é, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–º –Ω–µ –Ω—É–∂–µ–Ω)
    # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–ª–∞—Å—Å description-container, –∏—â–µ–º –≤–Ω—É—Ç—Ä–∏ –Ω–µ–≥–æ, –µ—Å–ª–∏ –Ω–µ—Ç - –ø–µ—Ä–≤—ã–π H2 –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    description_div = soup.find('div', class_='description-container')
    
    target_h2 = None
    if description_div:
        target_h2 = description_div.find('h2')
    
    if not target_h2:
        target_h2 = soup.find('h2')
        
    page_header = target_h2.get_text(strip=True) if target_h2 else "–û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞" # –î–µ—Ñ–æ–ª—Ç, –µ—Å–ª–∏ H2 –Ω–µ—Ç —Å–æ–≤—Å–µ–º

    # 2. –§–∞–∫—Ç—É—Ä–∞ (—Ç–µ–∫—Å—Ç)
    base_text = description_div.get_text(separator="\n", strip=True) if description_div else soup.body.get_text(separator="\n", strip=True)[:5000]
    
    # 3. –¢–µ–≥–∏
    tags_container = soup.find(class_='popular-tags-inner')
    tags_data = []
    if tags_container:
        links = tags_container.find_all('a')
        for link in links:
            tag_url = urljoin(url, link.get('href')) if link.get('href') else None
            if tag_url: tags_data.append({'name': link.get_text(strip=True), 'url': tag_url})
            
    return base_text, tags_data, page_header, None

def generate_ai_content_blocks(client, base_text, tag_name, forced_header, num_blocks=5, seo_words=None):
    if not base_text: return ["Error: No base text"] * num_blocks
    
    seo_words = seo_words or []
    seo_instruction_block = ""
    if seo_words:
        seo_list_str = ", ".join(seo_words)
        seo_instruction_block = f"""
--- –í–ê–ñ–ù–ê–Ø –ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û SEO-–°–õ–û–í–ê–ú ---
–¢–µ–±–µ –Ω—É–∂–Ω–æ –≤–Ω–µ–¥—Ä–∏—Ç—å –≤ —Ç–µ–∫—Å—Ç —Å–ª–µ–¥—É—é—â–∏–µ —Å–ª–æ–≤–∞ –≤ –ª—é–±–æ–π –ø–æ–¥—Ö–æ–¥—è—â–µ–π –ø–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ª–µ–º–º–µ: {seo_list_str}
–ü–†–ê–í–ò–õ–ê:
1. –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï: –†–∞—Å–∫–∏–¥–∞–π —Å–ª–æ–≤–∞ –ø–æ –≤—Å–µ–º {num_blocks} –±–ª–æ–∫–∞–º.
2. –í–´–î–ï–õ–ï–ù–ò–ï: –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–¥–µ–ª–∏ –≤–Ω–µ–¥—Ä–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ —Ç–µ–≥–æ–º <b>.
3. –ï–°–¢–ï–°–¢–í–ï–ù–ù–û–°–¢–¨: –ú–µ–Ω—è–π —Å–ª–æ–≤–æ—Ñ–æ—Ä–º—ã –ø–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç.
-------------------------------------------
"""
    system_instruction = (
        "–¢—ã ‚Äî —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä. –í—ã–¥–∞–µ—à—å –¢–û–õ–¨–ö–û HTML. "
        "–°—Ç–∏–ª—å: –î–µ–ª–æ–≤–æ–π, —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π. –ë–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ [1]."
    )
    user_prompt = f"""
    –ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:
    –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: "{tag_name}"
    –ë–∞–∑–æ–≤—ã–π —Ç–µ–∫—Å—Ç: \"\"\"{base_text[:4000]}\"\"\"
    {seo_instruction_block}
    
    –ó–ê–î–ê–ß–ê:
    –ù–∞–ø–∏—à–∏ {num_blocks} HTML-–±–ª–æ–∫–æ–≤, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º: |||BLOCK_SEP|||
    
    –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
    1. –û–ë–™–ï–ú: –ë–ª–æ–∫ ~600 —Å–∏–º–≤–æ–ª–æ–≤.
    2. –°–¢–†–£–ö–¢–£–†–ê –ë–õ–û–ö–ê:
       - –ó–∞–≥–æ–ª–æ–≤–æ–∫ (<h2> –¥–ª—è 1-–≥–æ, <h3> –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö).
       - –¢–µ–∫—Å—Ç (<p>).
       - –°–ø–∏—Å–æ–∫ (<ul><li>).
       - –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (<p>).
    3. –¢–ï–ú–´:
       - –ë–õ–û–ö 1: <h2>{forced_header}</h2>.
       - –ë–õ–û–ö–ò 2-{num_blocks}: <h3> –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏ —Ç.–¥.
    """
    
    try:
        response = client.chat.completions.create(
            model="sonar-pro",
            messages=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.75
        )
        content = response.choices[0].message.content
        content = re.sub(r'\[\d+\]', '', content)
        content = content.replace("```html", "").replace("```", "").strip()
        blocks = [b.strip() for b in content.split("|||BLOCK_SEP|||") if b.strip()]
        while len(blocks) < num_blocks: blocks.append("")
        return blocks[:num_blocks]
    except Exception as e:
        return [f"API Error: {str(e)}"] * num_blocks

# ==========================================
# 7. UI TABS RESTRUCTURED
# ==========================================
tab_seo_main, tab_wholesale_main = st.tabs(["üìä SEO –ê–Ω–∞–ª–∏–∑", "üè≠ –û–ø—Ç–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä"])

# ------------------------------------------
# TAB 1: SEO ANALYSIS (KEPT AS IS)
# ------------------------------------------
with tab_seo_main:
    col_main, col_sidebar = st.columns([65, 35])
    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
        
        # –°–±—Ä–æ—Å –∫—ç—à–∞ –¥–ª—è —Å–ª–æ–≤–∞—Ä–µ–π
        if st.button("üßπ –û–±–Ω–æ–≤–∏—Ç—å —Å–ª–æ–≤–∞—Ä–∏ (–ö—ç—à)", key="clear_cache_btn"):
            st.cache_data.clear()
            st.rerun()

        my_input_type = st.radio("–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], horizontal=True, label_visibility="collapsed", key="my_page_source_radio")
        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            st.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã", placeholder="https://site.ru/catalog/tovar", label_visibility="collapsed", key="my_url_input")
        elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏–ª–∏ —Ç–µ–∫—Å—Ç", height=200, label_visibility="collapsed", placeholder="–í—Å—Ç–∞–≤—å—Ç–µ HTML", key="my_content_input")

        st.markdown("### –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å")
        st.text_input("–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞", label_visibility="collapsed", key="query_input")
        
        st.markdown("### –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        
        # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ-–ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è ---
        if st.session_state.get('force_radio_switch'):
            # –ú–µ–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –î–û –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ –≤–∏–¥–∂–µ—Ç–∞
            st.session_state["competitor_source_radio"] = "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–∫–ª—é—á–∞–ª–æ –≤–µ—á–Ω–æ
            st.session_state['force_radio_switch'] = False
        # -----------------------------------------------

        source_type_new = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫", ["–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)", "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"], horizontal=True, label_visibility="collapsed", key="competitor_source_radio")
        source_type = "API" if "API" in source_type_new else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"
        
        if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
            # 1. –ö–ù–û–ü–ö–ê –°–ë–†–û–°–ê (–ü–æ—è–≤–ª—è–µ—Ç—Å—è, –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)
            if st.session_state.get('analysis_done'):
                col_reset, _ = st.columns([1, 4])
                with col_reset:
                    if st.button("üîÑ –ù–æ–≤—ã–π –ø–æ–∏—Å–∫ (–°–±—Ä–æ—Å)", type="secondary", help="–°–±—Ä–æ—Å–∏—Ç—å –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤–≤–µ—Å—Ç–∏ –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫"):
                        # –ß–∏—Å—Ç–∏–º –≤–æ–æ–±—â–µ –≤—Å—ë, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å —Å —á–∏—Å—Ç–æ–≥–æ –ª–∏—Å—Ç–∞
                        keys_to_clear = [
                            'analysis_done', 'analysis_results', 'excluded_urls_auto', 
                            'detected_anomalies', 'serp_trend_info', 'persistent_urls',
                            'naming_table_df', 'ideal_h1_result'
                        ]
                        for k in keys_to_clear:
                            if k in st.session_state: del st.session_state[k]
                        st.rerun()

            # 2. –õ–û–ì–ò–ö–ê –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø (1 –∏–ª–∏ 2 –∫–æ–ª–æ–Ω–∫–∏)
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º 2 –∫–æ–ª–æ–Ω–∫–∏
            has_exclusions = st.session_state.get('excluded_urls_auto') and len(st.session_state.get('excluded_urls_auto')) > 5
            
            if has_exclusions:
                c_url_1, c_url_2 = st.columns(2)
                with c_url_1:
                    manual_val = st.text_area(
                        "‚úÖ –ê–∫—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã (–î–ª—è –∞–Ω–∞–ª–∏–∑–∞)", 
                        height=200, 
                        key="manual_urls_widget", 
                        value=st.session_state.get('persistent_urls', "")
                    )
                    st.session_state['persistent_urls'] = manual_val
                with c_url_2:
                    st.text_area(
                        "üö´ –ê–≤—Ç–æ-–∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ (–í—ã –º–æ–∂–µ—Ç–µ –≤–µ—Ä–Ω—É—Ç—å –∏—Ö –≤–ª–µ–≤–æ)", 
                        height=200, 
                        key="excluded_urls_widget_display", 
                        value=st.session_state.get('excluded_urls_auto', ""),
                        help="–°—é–¥–∞ –ø–æ–ø–∞–ª–∏ —Å–ª–∞–±—ã–µ —Å–∞–π—Ç—ã. –ï—Å–ª–∏ —Å—á–∏—Ç–∞–µ—Ç–µ, —á—Ç–æ —Å–∞–π—Ç –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π - —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –µ–≥–æ –∏ –≤—Å—Ç–∞–≤—å—Ç–µ –æ–±—Ä–∞—Ç–Ω–æ –≤ –ª–µ–≤–æ–µ –æ–∫–Ω–æ."
                    )
            else:
                # –û–±—ã—á–Ω—ã–π –≤–∏–¥ (1 –∫–æ–ª–æ–Ω–∫–∞)
                manual_val = st.text_area(
                    "–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", 
                    height=200, 
                    key="manual_urls_widget", 
                    value=st.session_state.get('persistent_urls', "")
                )
                st.session_state['persistent_urls'] = manual_val

    # ================= –ù–û–í–û–ï –ú–ï–°–¢–û –î–õ–Ø –ì–†–ê–§–ò–ö–ê =================
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        if st.session_state.get('analysis_done') and st.session_state.get('analysis_results'):
            results = st.session_state.analysis_results
            if 'relevance_top' in results and not results['relevance_top'].empty:
                st.markdown("<br>", unsafe_allow_html=True)
                with st.expander("üìä –ì—Ä–∞—Ñ–∏–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–ù–∞–∂–º–∏—Ç–µ, —á—Ç–æ–±—ã —Ä–∞—Å–∫—Ä—ã—Ç—å)", expanded=False):
                  graph_data = st.session_state.get('full_graph_data', results['relevance_top'])
                  render_relevance_chart(graph_data, unique_key="main")
                st.markdown("<br>", unsafe_allow_html=True)
    # ===========================================================

        st.markdown("### –°–ø–∏—Å–∫–∏ (Stop / Exclude)")
        st.text_area("–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–æ–º–µ–Ω—ã", DEFAULT_EXCLUDE, height=100, key="settings_excludes")
        st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞", DEFAULT_STOPS, height=100, key="settings_stops")
        if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", type="primary", use_container_width=True, key="start_analysis_btn"):
            # === –û–ß–ò–°–¢–ö–ê –í–°–ï–• –°–¢–ê–†–´–• –î–ê–ù–ù–´–• ===
            st.session_state.analysis_results = None
            st.session_state.analysis_done = False
            st.session_state.naming_table_df = None
            st.session_state.ideal_h1_result = None
            st.session_state.gen_result_df = None
            st.session_state.unified_excel_data = None

            if 'excluded_urls_auto' in st.session_state: del st.session_state['excluded_urls_auto']
            if 'detected_anomalies' in st.session_state: del st.session_state['detected_anomalies']
            if 'serp_trend_info' in st.session_state: del st.session_state['serp_trend_info']
            # ---------------------------------------------------------------
            
            # –°–±—Ä–æ—Å –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü
            for key in list(st.session_state.keys()):
                if key.endswith('_page'): st.session_state[key] = 1
            
            # –ó–∞–ø—É—Å–∫ —Ñ–ª–∞–≥–∞ –∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, —á—Ç–æ–±—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –æ—á–∏—Å—Ç–∏–ª—Å—è
            st.session_state.start_analysis_flag = True
            st.rerun()

    with col_sidebar:
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API")
        if not ARSENKIN_TOKEN:
             new_arsenkin = st.text_input("Arsenkin Token", type="password", key="input_arsenkin")
             if new_arsenkin: st.session_state.arsenkin_token = new_arsenkin; ARSENKIN_TOKEN = new_arsenkin 
        if not YANDEX_DICT_KEY:
             new_yandex = st.text_input("Yandex Dict Key", type="password", key="input_yandex")
             if new_yandex: st.session_state.yandex_dict_key = new_yandex; YANDEX_DICT_KEY = new_yandex
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
        st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
        st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
        st.selectbox("–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞", list(REGION_MAP.keys()), key="settings_region")
        
        # –ò–ó–ú–ï–ù–ï–ù–ò–ï: –£–±—Ä–∞–ª–∏ 30, –æ—Å—Ç–∞–≤–∏–ª–∏ —Ç–æ–ª—å–∫–æ 10 –∏ 20
        st.selectbox("–ö–æ–ª-–≤–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", [10, 20], index=0, key="settings_top_n")
        
        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å <noindex>", True, key="settings_noindex")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", False, key="settings_alt")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", False, key="settings_numbers")
        st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True, key="settings_norm")

# ==========================================
    # –ë–õ–û–ö 1: –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (–¢–ï–ü–ï–†–¨ –ü–ï–†–í–´–ô)
    # ==========================================
    if st.session_state.analysis_done and st.session_state.analysis_results:
        results = st.session_state.analysis_results
        
        d_score = results['my_score']['depth']
        w_score = results['my_score']['width']
        
        # –®–∏—Ä–∏–Ω–∞
        w_color = "#2E7D32" if w_score >= 80 else ("#E65100" if w_score >= 50 else "#D32F2F")
        
        # –ì–ª—É–±–∏–Ω–∞ (–¶–µ–ª—å = 80)
        if 75 <= d_score <= 88:
            d_color = "#2E7D32" # –ó–µ–ª–µ–Ω—ã–π (–û—Ç–ª–∏—á–Ω–æ)
            d_status = "–ò–î–ï–ê–õ (–¢–æ–ø)"
        elif 88 < d_score <= 100:
            d_color = "#D32F2F" # –ö—Ä–∞—Å–Ω—ã–π (–†–∏—Å–∫ –ø–µ—Ä–µ—Å–ø–∞–º–∞)
            d_status = "–ü–ï–†–ï–°–ü–ê–ú (–†–∏—Å–∫)"
        elif 55 <= d_score < 75:
            d_color = "#F9A825" # –ñ–µ–ª—Ç—ã–π
            d_status = "–°—Ä–µ–¥–Ω—è—è"
        else:
            d_color = "#D32F2F" # –ö—Ä–∞—Å–Ω—ã–π
            d_status = "–ù–∏–∑–∫–∞—è"

        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        
        st.markdown("""
        <style>
            details > summary { list-style: none; }
            details > summary::-webkit-details-marker { display: none; }
            .details-card {
                background-color: #f8f9fa; border: 1px solid #e9ecef;
                border-radius: 8px; margin-bottom: 10px;
                overflow: hidden; transition: all 0.2s ease;
            }
            .details-card:hover { box-shadow: 0 2px 5px rgba(0,0,0,0.05); border-color: #d1d5db; }
            .card-summary {
                padding: 12px 15px; cursor: pointer; font-weight: 700;
                font-size: 15px; color: #111827; display: flex;
                justify-content: space-between; align-items: center;
                background-color: #ffffff;
            }
            .card-summary:hover { background-color: #f3f4f6; }
            .card-content {
                padding: 15px; border-top: 1px solid #e9ecef;
                font-size: 14px; color: #374151; line-height: 1.6;
                background-color: #fcfcfc;
            }
            .count-tag { 
                background: #e5e7eb; color: #374151; padding: 2px 8px; 
                border-radius: 10px; font-size: 12px; font-weight: 600;
                min-width: 25px; text-align: center;
            }
            .arrow-icon {
                font-size: 10px; margin-right: 8px; color: #9ca3af;
                transition: transform 0.2s;
            }
            details[open] .arrow-icon { transform: rotate(90deg); color: #277EFF; }
        </style>
        """, unsafe_allow_html=True)

# –ù–∞–π—Ç–∏ –≤ –∫–æ–¥–µ –º–µ—Å—Ç–æ, –≥–¥–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø—Ä–∏–º–µ—Ä–Ω–æ –ø–æ—Å–ª–µ d_status = "–ù–∏–∑–∫–∞—è")
        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        
        # --- –°–¢–ò–õ–ò (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –° –≠–¢–ò–ú–ò –û–¢–°–¢–£–ü–ê–ú–ò) ---
        st.markdown("""
        <style>
            details.custom-card summary::-webkit-details-marker { display: none !important; }
            details.custom-card summary { list-style: none !important; display: block !important; }
            .custom-card {
                background: #ffffff !important;
                border: 1px solid #e2e8f0 !important;
                border-radius: 10px !important;
                margin-bottom: 12px !important;
                overflow: hidden !important;
            }
            .custom-card.disabled { background: #f8fafc !important; }
            .custom-card-header {
                padding: 12px 16px !important;
                display: flex !important;
                justify-content: space-between !important;
                align-items: center !important;
                cursor: pointer !important;
            }
            .header-left { display: flex !important; align-items: center !important; gap: 10px !important; }
            .card-arrow {
                width: 0; height: 0; 
                border-top: 5px solid transparent;
                border-bottom: 5px solid transparent;
                border-left: 8px solid #94a3b8;
                transition: transform 0.2s ease;
                display: inline-block !important;
            }
            details[open] .card-arrow { transform: rotate(90deg); border-left-color: #277EFF; }
            .card-title { font-weight: 600 !important; color: #1e293b !important; font-size: 14px !important; }
            .card-count {
                background: #f1f5f9 !important;
                color: #475569 !important;
                padding: 2px 10px !important;
                border-radius: 20px !important;
                font-size: 12px !important;
                font-weight: 700 !important;
                border: 1px solid #e2e8f0 !important;
            }
            .custom-card-content {
                padding: 12px 16px !important;
                font-size: 13px !important;
                line-height: 1.6 !important;
                color: #475569 !important;
                border-top: 1px solid #f1f5f9 !important;
                background: #fafafa !important;
            }
        </style>
        """, unsafe_allow_html=True)

        st.markdown(f"""
        <div style='display: flex; gap: 20px; flex-wrap: wrap;'>
            <div style='flex: 1; background:{LIGHT_BG_MAIN}; padding:15px; border-radius:8px; border-left: 5px solid {w_color};'>
                <div style='font-size: 12px; color: #666;'>–®–ò–†–ò–ù–ê (–û—Ö–≤–∞—Ç —Ç–µ–º)</div>
                <div style='font-size: 24px; font-weight: bold; color: {w_color};'>{w_score}/100</div>
            </div>
            <div style='flex: 1; background:{LIGHT_BG_MAIN}; padding:15px; border-radius:8px; border-left: 5px solid {d_color};'>
                <div style='font-size: 12px; color: #666;'>–ì–õ–£–ë–ò–ù–ê (–¶–µ–ª—å: ~80)</div>
                <div style='font-size: 24px; font-weight: bold; color: {d_color};'>{d_score}/100 <span style='font-size:14px; font-weight:normal;'>({d_status})</span></div>
            </div>
        </div>
        <br>
        """, unsafe_allow_html=True)

        with st.expander("üõí –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ –∏ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è", expanded=True):
            if not st.session_state.get('orig_products'):
                st.info("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑.")
            else:
                c1, c2, c3 = st.columns(3)
                with c1: render_clean_block("–¢–æ–≤–∞—Ä—ã", "üß±", st.session_state.categorized_products)
                with c2: render_clean_block("–ì–µ–æ", "üåç", st.session_state.categorized_geo)
                with c3: render_clean_block("–ö–æ–º–º–µ—Ä—Ü–∏—è", "üí∞", st.session_state.categorized_commercial)
                
                c4, c5, c6 = st.columns(3)
                with c4: render_clean_block("–£—Å–ª—É–≥–∏", "üõ†Ô∏è", st.session_state.categorized_services)
                with c5: render_clean_block("–†–∞–∑–º–µ—Ä—ã/–ì–û–°–¢", "üìè", st.session_state.categorized_dimensions)
                with c6: render_clean_block("–û–±—â–∏–µ", "üìÇ", st.session_state.categorized_general)

                st.markdown("<hr style='margin: 15px 0;'>", unsafe_allow_html=True)

                # –ë–ª–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
                cs1, cs2 = st.columns([1, 3])
                
                if 'sensitive_words_input_final' not in st.session_state:
                    current_list = st.session_state.get('categorized_sensitive', [])
                    st.session_state['sensitive_words_input_final'] = "\n".join(current_list)
                
                current_text_value = st.session_state['sensitive_words_input_final']
                
                with cs1:
                    count_excluded = len([x for x in current_text_value.split('\n') if x.strip()])
                    st.markdown(f"**‚õî –°—Ç–æ–ø-—Å–ª–æ–≤–∞**")
                    st.markdown(f"–ò—Å–∫–ª—é—á–µ–Ω–æ: **{count_excluded}**")
                    st.caption("–≠—Ç–∏ —Å–ª–æ–≤–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª–µ–Ω—ã.")
                
                with cs2:
                    new_sens_str = st.text_area(
                        "hidden_label", height=100,
                        key="sensitive_words_input_final",
                        label_visibility="collapsed",
                        placeholder="–°–ª–æ–≤–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è..."
                    )

                    if st.button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä", type="primary", use_container_width=True):
                        raw_input = st.session_state.get("sensitive_words_input_final", "")
                        new_stop_set = set([w.strip().lower() for w in raw_input.split('\n') if w.strip()])
                        
                        st.session_state.categorized_sensitive = sorted(list(new_stop_set))
                        
                        def apply_filter(orig_list_key, stop_set):
                            original = st.session_state.get(orig_list_key, [])
                            return [w for w in original if w.lower() not in stop_set]

                        st.session_state.categorized_products = apply_filter('orig_products', new_stop_set)
                        st.session_state.categorized_services = apply_filter('orig_services', new_stop_set)
                        st.session_state.categorized_commercial = apply_filter('orig_commercial', new_stop_set)
                        st.session_state.categorized_geo = apply_filter('orig_geo', new_stop_set)
                        st.session_state.categorized_dimensions = apply_filter('orig_dimensions', new_stop_set)
                        st.session_state.categorized_general = apply_filter('orig_general', new_stop_set)

                        # –û–±–Ω–æ–≤–ª—è–µ–º –≤–∫–ª–∞–¥–∫—É –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
                        all_prods = st.session_state.categorized_products
                        count_prods = len(all_prods)
                        if count_prods < 20:
                            st.session_state.auto_tags_words = all_prods
                            st.session_state.auto_promo_words = []
                        else:
                            half = int(math.ceil(count_prods / 2))
                            st.session_state.auto_tags_words = all_prods[:half]
                            st.session_state.auto_promo_words = all_prods[half:]

                        st.session_state['kws_tags_auto'] = "\n".join(st.session_state.auto_tags_words)
                        st.session_state['kws_promo_auto'] = "\n".join(st.session_state.auto_promo_words)

                        st.toast("–§–∏–ª—å—Ç—Ä –æ–±–Ω–æ–≤–ª–µ–Ω!", icon="‚úÖ")
                        time.sleep(0.5)
                        st.rerun()

        # --- –£–ü–£–©–ï–ù–ù–ê–Ø –°–ï–ú–ê–ù–¢–ò–ö–ê ---
        high = results.get('missing_semantics_high', [])
        low = results.get('missing_semantics_low', [])
        if high or low:
            with st.expander(f"üß© –£–ø—É—â–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ ({len(high)+len(low)})", expanded=False):
                if high: st.markdown(f"<div style='background:#EBF5FF;padding:10px;border-radius:5px;'><b>–í–∞–∂–Ω—ã–µ:</b> {', '.join([x['word'] for x in high])}</div>", unsafe_allow_html=True)
                if low: st.markdown(f"<div style='background:#F7FAFC;padding:10px;border-radius:5px;margin-top:5px;'><b>–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞:</b> {', '.join([x['word'] for x in low])}</div>", unsafe_allow_html=True)

        render_paginated_table(results['depth'], "1. –ì–ª—É–±–∏–Ω–∞", "tbl_depth_1", default_sort_col="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è", use_abs_sort_default=True)
        
        # === –¢–ê–ë–õ–ò–¶–ê ‚Ññ2 (–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é) ===
        if 'naming_table_df' in st.session_state and st.session_state.naming_table_df is not None:
            df_naming = st.session_state.naming_table_df
            
            st.markdown("### 2. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ç–æ–≤–∞—Ä–æ–≤")
            
            # –§–û–†–ú–£–õ–ê
            if 'ideal_h1_result' in st.session_state:
                res_ideal = st.session_state.ideal_h1_result
                if isinstance(res_ideal, (tuple, list)) and len(res_ideal) >= 2:
                    example_name = res_ideal[0]
                    report_list = res_ideal[1]
                    formula_str = "–§–æ—Ä–º—É–ª–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
                    for line in report_list:
                        if "—Å—Ç—Ä—É–∫—Ç—É—Ä–∞" in line or "–°—Ö–µ–º–∞" in line:
                            formula_str = line.replace("**–°–∞–º–∞—è —á–∞—Å—Ç–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**", "").replace("**–°—Ö–µ–º–∞:**", "").strip()
                            break
                    with st.container(border=True):
                        st.markdown("#### üß™ –ò–¥–µ–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ –Ω–∞–∑–≤–∞–Ω–∏—è")
                        st.info(f"**{formula_str}**", icon="üß©")
                        st.markdown(f"**–ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:** _{example_name}_")
                else:
                    st.warning("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ —É—Å—Ç–∞—Ä–µ–ª–∏.")

            # –¢–ê–ë–õ–ò–¶–ê
            st.markdown("##### –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫")
            if not df_naming.empty:
                col_ctrl1, col_ctrl2 = st.columns([1, 3])
                with col_ctrl1:
                    show_tech = st.toggle("–ü–æ–∫–∞–∑–∞—Ç—å —Ä–∞–∑–º–µ—Ä—ã –∏ —Ü–∏—Ñ—Ä—ã", value=False, key="toggle_show_tech_specs_unique")
                
                df_display = df_naming.copy()
                if not show_tech:
                    df_display = df_display[~df_display['–¢–∏–ø —Ö–∞—Ä-–∫–∏'].str.contains("–†–∞–∑–º–µ—Ä—ã", na=False)]

                if 'cat_sort' in df_display.columns:
                    df_display = df_display.sort_values(by=["cat_sort", "raw_freq"], ascending=[True, False])
                
                cols_to_show = ["–¢–∏–ø —Ö–∞—Ä-–∫–∏", "–°–ª–æ–≤–æ", "–ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å (%)", "–£ –í–∞—Å", "–ú–µ–¥–∏–∞–Ω–∞", "–î–æ–±–∞–≤–∏—Ç—å"]
                existing_cols = [c for c in cols_to_show if c in df_display.columns]
                df_display = df_display[existing_cols]

                def style_rows(row):
                    val = str(row.get('–î–æ–±–∞–≤–∏—Ç—å', ''))
                    if "+" in val: return ['background-color: #fff1f2; color: #9f1239'] * len(row)
                    if "‚úÖ" in val: return ['background-color: #f0fdf4; color: #166534'] * len(row)
                    return [''] * len(row)

                st.dataframe(
                    df_display.style.apply(style_rows, axis=1),
                    use_container_width=True,
                    hide_index=True,
                    height=(len(df_display) * 35) + 38 if len(df_display) < 15 else 500
                )
            else:
                st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
                
            render_paginated_table(results['hybrid'], "3. TF-IDF", "tbl_hybrid", default_sort_col="TF-IDF –¢–û–ü")
            render_paginated_table(results['relevance_top'], "4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", "tbl_rel", default_sort_col="–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)")

# ==========================================
    # –ë–õ–û–ö 2: –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ï –ò –†–ê–°–ß–ï–¢
    # ==========================================
    if st.session_state.get('start_analysis_flag'):
        st.session_state.start_analysis_flag = False
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        settings = {
            'noindex': st.session_state.settings_noindex, 
            'alt_title': st.session_state.settings_alt, 
            'numbers': st.session_state.settings_numbers, 
            'norm': st.session_state.settings_norm, 
            'ua': st.session_state.settings_ua, 
            'custom_stops': st.session_state.settings_stops.split()
        }
        
        my_data, my_domain, my_serp_pos = None, "", 0
        current_input_type = st.session_state.get("my_page_source_radio")
        
        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –í–ê–®–ï–ô —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if current_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
                my_data = parse_page(st.session_state.my_url_input, settings, st.session_state.query_input)
                if not my_data: st.error("–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."); st.stop()
                my_domain = urlparse(st.session_state.my_url_input).netloc
        elif current_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            my_data = {'url': 'Local', 'domain': 'local', 'body_text': st.session_state.my_content_input, 'anchor_text': ''}
            
        # 2. –°–±–æ—Ä –ö–ê–ù–î–ò–î–ê–¢–û–í
        candidates_pool = []
        current_source_val = st.session_state.get("competitor_source_radio")
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–µ—Ä–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (10 –∏–ª–∏ 20) –¥–ª—è –§–ò–ù–ê–õ–ê
        user_target_top_n = st.session_state.settings_top_n
        # –ê —Å–∫–∞—á–∏–≤–∞–µ–º –≤—Å–µ–≥–¥–∞ –ú–ê–ö–°–ò–ú–£–ú (30), —á—Ç–æ–±—ã –±—ã–ª–æ –∏–∑ —á–µ–≥–æ –≤—ã–±–∏—Ä–∞—Ç—å
        download_limit = 30 
        
        if "API" in current_source_val:
            if not ARSENKIN_TOKEN: st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç API —Ç–æ–∫–µ–Ω Arsenkin."); st.stop()
            with st.spinner(f"API Arsenkin (–ó–∞–ø—Ä–æ—Å –¢–æ–ø-30)..."):
                raw_top = get_arsenkin_urls(st.session_state.query_input, st.session_state.settings_search_engine, st.session_state.settings_region, ARSENKIN_TOKEN, depth_val=30)
                
                if not raw_top: st.stop()
                
                excl = [d.strip() for d in st.session_state.settings_excludes.split('\n') if d.strip()]
                agg_list = [
                    "avito", "ozon", "wildberries", "market.yandex", "tiu", "youtube", "vk.com", "yandex",
                    "leroymerlin", "petrovich", "satom", "pulscen", "blizko", "deal.by", "satu.kz", "prom.ua",
                    "wikipedia", "dzen", "rutube", "kino", "otzovik", "irecommend", "profi.ru", "zoon", "2gis",
                    "megamarket.ru", "lamoda.ru", "utkonos.ru", "vprok.ru", "allbiz.ru", "all-companies.ru",
                    "orgpage.ru", "list-org.com", "rusprofile.ru", "e-katalog.ru", "kufar.by", "wildberries.kz",
                    "ozon.kz", "kaspi.kz", "pulscen.kz", "allbiz.kz", "wildberries.uz", "olx.uz", "pulscen.uz",
                    "allbiz.uz", "wildberries.kg", "pulscen.kg", "allbiz.kg", "all.biz", "b2b-center.ru"
                ]
                excl.extend(agg_list)
                for res in raw_top:
                    dom = urlparse(res['url']).netloc.lower()
                    if my_domain and (my_domain in dom or dom in my_domain):
                        if my_serp_pos == 0 or res['pos'] < my_serp_pos: 
                            my_serp_pos = res['pos']
                    is_garbage = False
                    for x in excl:
                        if x.lower() in dom:
                            is_garbage = True
                            break
                    if is_garbage: continue
                    candidates_pool.append(res)
        else:
            raw_input_urls = st.session_state.get("persistent_urls", "")
            candidates_pool = [{'url': u.strip(), 'pos': i+1} for i, u in enumerate(raw_input_urls.split('\n')) if u.strip()]

        if not candidates_pool: st.error("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤."); st.stop()
        
        # 3. –°–ö–ê–ß–ò–í–ê–ù–ò–ï (–í—Å–µ—Ö 30)
        comp_data_valid = []
        with st.status(f"üïµÔ∏è –ì–ª—É–±–æ–∫–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (–í—Å–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidates_pool)})...", expanded=True) as status:
            with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
                futures = {
                    executor.submit(parse_page, item['url'], settings, st.session_state.query_input): item 
                    for item in candidates_pool
                }
                done_count = 0
                for f in concurrent.futures.as_completed(futures):
                    original_item = futures[f]
                    try:
                        res = f.result()
                        if res:
                            res['pos'] = original_item['pos']
                            comp_data_valid.append(res)
                    except: pass
                    done_count += 1
                    status.update(label=f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {done_count}/{len(candidates_pool)} | –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ: {len(comp_data_valid)}")

            comp_data_valid.sort(key=lambda x: x['pos'])
            # –°–Ω–∞—á–∞–ª–∞ –±–µ—Ä–µ–º –í–°–ï–•, –∫—Ç–æ —Å–∫–∞—á–∞–ª—Å—è (–¥–æ 30), –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
            data_for_graph = comp_data_valid[:download_limit]
            targets_for_graph = [{'url': d['url'], 'pos': d['pos']} for d in data_for_graph]

        # 5. –†–ê–°–ß–ï–¢ –ú–ï–¢–†–ò–ö (–î–í–û–ô–ù–û–ô –ü–†–û–ì–û–ù)
        with st.spinner("–ê–Ω–∞–ª–∏–∑ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è..."):
            
            # --- –≠–¢–ê–ü 1: –ß–µ—Ä–Ω–æ–≤–æ–π –ø—Ä–æ–≥–æ–Ω (–ø–æ –≤—Å–µ–º 30 —Å–∞–π—Ç–∞–º) ---
            # –≠—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ –∏ –Ω–∞–π—Ç–∏ –∞–Ω–æ–º–∞–ª–∏–∏
            results_full = calculate_metrics(data_for_graph, my_data, settings, my_serp_pos, targets_for_graph)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ü–û–õ–ù–´–ï –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ (—á—Ç–æ–±—ã –Ω–∞ –Ω–µ–º –±—ã–ª–∏ –≤—Å–µ)
            st.session_state['full_graph_data'] = results_full['relevance_top']
            
            # –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π –ø–æ –ø–æ–ª–Ω–æ–º—É —Å–ø–∏—Å–∫—É
            df_rel_check = results_full['relevance_top']
            good_urls, bad_urls_dicts, trend = analyze_serp_anomalies(df_rel_check)
            st.session_state['serp_trend_info'] = trend
            
            # --- –≠–¢–ê–ü 2: –û—Ç–±–æ—Ä —á–∏—Å—Ç–æ–≤—ã—Ö (–¢–æ–ø-10/20 –±–µ–∑ –º—É—Å–æ—Ä–∞) ---
            
            # 1. –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–µ—Ö —Å–∞–π—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –≤ —Å–ø–∏—Å–∫–µ –ø–ª–æ—Ö–∏—Ö
            bad_urls_set = set(item['url'] for item in bad_urls_dicts)
            
            # === –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –§–ò–õ–¨–¢–†–ê–¶–ò–ò ===
            # –ï—Å–ª–∏ —ç—Ç–æ API - –º—ã —Ñ–∏–ª—å—Ç—Ä—É–µ–º –∏ —Ä–µ–∂–µ–º —Ç–æ–ø.
            # –ï—Å–ª–∏ —ç—Ç–æ –†–£–ß–ù–û–ô —Ä–µ–∂–∏–º - –º—ã –ù–ï —Ñ–∏–ª—å—Ç—Ä—É–µ–º (–¥–æ–≤–µ—Ä—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é).
            if "API" in current_source_val:
                clean_data_pool = [d for d in data_for_graph if d['url'] not in bad_urls_set]
                final_clean_data = clean_data_pool[:user_target_top_n]
            else:
                # –í —Ä—É—á–Ω–æ–º —Ä–µ–∂–∏–º–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï–• —Å–∫–∞—á–∞–Ω–Ω—ã—Ö, –Ω–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º "—Å–ª–∞–±—ã—Ö"
                final_clean_data = data_for_graph 
            
            final_clean_targets = [{'url': d['url'], 'pos': d['pos']} for d in final_clean_data]
            
            # 3. –§–ò–ù–ê–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ (–¢–æ–ª—å–∫–æ –ø–æ —ç–ª–∏—Ç–µ)
            # –ú–µ–¥–∏–∞–Ω—ã –∏ TF-IDF –ø–µ—Ä–µ—Å—á–∏—Ç–∞—é—Ç—Å—è —Å—Ç—Ä–æ–≥–æ –ø–æ —ç—Ç–æ–º—É —Å–ø–∏—Å–∫—É
            results_final = calculate_metrics(final_clean_data, my_data, settings, my_serp_pos, final_clean_targets)
            st.session_state.analysis_results = results_final
            
            # --- –û—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ (–Ω–µ–π–º–∏–Ω–≥, —Å–µ–º–∞–Ω—Ç–∏–∫–∞) ---
            naming_df = calculate_naming_metrics(final_clean_data, my_data, settings)
            st.session_state.naming_table_df = naming_df 
            st.session_state.ideal_h1_result = analyze_ideal_name(final_clean_data)
            st.session_state.analysis_done = True
            
# === –£–ú–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø (Smart Filter Logic) ===
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ï—Å–ª–∏ —ç—Ç–æ API-–ø—Ä–æ–≥–æ–Ω, –±–µ—Ä–µ–º –ü–û–õ–ù–´–ï –¥–∞–Ω–Ω—ã–µ (30 —Å–∞–π—Ç–æ–≤) –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π.
            # –ò–Ω–∞—á–µ –±–µ—Ä–µ–º —Ç–µ–∫—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–¥–ª—è —Ä—É—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞).
            if "API" in current_source_val and 'full_graph_data' in st.session_state:
                df_rel_check = st.session_state['full_graph_data']
            else:
                df_rel_check = st.session_state.analysis_results['relevance_top']
            
            # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–∏–º–µ–Ω—è—Ç—å –∞–≤—Ç–æ-—Ñ–∏–ª—å—Ç—Ä
            # –§–∏–ª—å—Ç—Ä—É–µ–º, –µ—Å–ª–∏:
            # –ê) –ò—Å—Ç–æ—á–Ω–∏–∫ API (–≤—Å–µ–≥–¥–∞ —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ)
            # –ë) –ò—Å—Ç–æ—á–Ω–∏–∫ –†—É—á–Ω–æ–π, –ù–û —ç—Ç–æ –ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ (–Ω–µ—Ç —Å–ø–∏—Å–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö)
            should_auto_filter = True
            
            is_manual_mode = "–†—É—á–Ω–æ–π" in current_source_val
            has_previous_exclusions = 'excluded_urls_auto' in st.session_state and len(st.session_state.get('excluded_urls_auto', '')) > 5
            
            if is_manual_mode and has_previous_exclusions:
                should_auto_filter = False # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É—Ç–æ—á–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫, –Ω–µ –º–µ—à–∞–µ–º –µ–º—É
            
            # 2. –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ (–æ–Ω –Ω—É–∂–µ–Ω –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ –∏ —Ç—Ä–µ–Ω–¥–æ–≤ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ)
            good_urls, bad_urls_dicts, trend = analyze_serp_anomalies(df_rel_check)
            st.session_state['serp_trend_info'] = trend
            
            # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ—à–µ–Ω–∏–µ
            if should_auto_filter and bad_urls_dicts:
                # –†–ï–ñ–ò–ú 1: –ü–ï–†–í–´–ô –ü–†–û–ì–û–ù -> –ñ–µ—Å—Ç–∫–æ —Ä–∞–∑–¥–µ–ª—è–µ–º
                st.session_state['detected_anomalies'] = bad_urls_dicts
                st.session_state['persistent_urls'] = "\n".join(good_urls)
                
                excluded_list = [item['url'] for item in bad_urls_dicts]
                st.session_state['excluded_urls_auto'] = "\n".join(excluded_list)
                
                st.toast(f"üßπ –ê–≤—Ç–æ-—Ñ–∏–ª—å—Ç—Ä: –ò—Å–∫–ª—é—á–µ–Ω–æ {len(bad_urls_dicts)} —Å–ª–∞–±—ã—Ö —Å–∞–π—Ç–æ–≤.", icon="üóëÔ∏è")
            
            elif not should_auto_filter and bad_urls_dicts:
                # –†–ï–ñ–ò–ú 2: –£–¢–û–ß–ù–ï–ù–ò–ï –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø -> –ù–µ —Ç—Ä–æ–≥–∞–µ–º —Å–ø–∏—Å–∫–∏
                # –ú—ã –≤–∏–¥–∏–º, —á—Ç–æ –µ—Å—Ç—å —Å–ª–∞–±—ã–µ —Å–∞–π—Ç—ã, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –∏—Ö, —Ç–∞–∫ –∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏—Ö –≤–µ—Ä–Ω—É–ª
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —Å–ø–∏—Å–æ–∫ –∫–∞–∫ –µ—Å—Ç—å (–≤—Å–µ, —á—Ç–æ –ø—Ä–∏—à–ª–æ –Ω–∞ –≤—Ö–æ–¥)
                # –ë–µ—Ä–µ–º URL –∏–∑ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (final_clean_data), —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫ –∏ —Å–æ—Å—Ç–∞–≤
                all_current_urls = [d['url'] for d in final_clean_data]
                st.session_state['persistent_urls'] = "\n".join(all_current_urls)
                
                st.toast(f"üõ°Ô∏è –†—É—á–Ω–æ–π —Ä–µ–∂–∏–º: –°–ª–∞–±—ã–µ —Å–∞–π—Ç—ã ({len(bad_urls_dicts)} —à—Ç.) –æ—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –∞–Ω–∞–ª–∏–∑–µ.", icon="üîì")
                
            else:
                # –†–ï–ñ–ò–ú 3: –ê–Ω–æ–º–∞–ª–∏–π –Ω–µ—Ç -> –ü—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫
                st.session_state['persistent_urls'] = "\n".join(good_urls)
                if should_auto_filter: # –ß–∏—Å—Ç–∏–º —Ö–≤–æ—Å—Ç—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º—ã –≤ —Ä–µ–∂–∏–º–µ –∞–≤—Ç–æ
                    if 'excluded_urls_auto' in st.session_state: del st.session_state['excluded_urls_auto']
                    if 'detected_anomalies' in st.session_state: del st.session_state['detected_anomalies']
            # ==============================================================

            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ (–ø–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º)
            res = st.session_state.analysis_results
            words_to_check = [x['word'] for x in res.get('missing_semantics_high', [])]
            if not words_to_check:
                st.session_state.categorized_products = []; st.session_state.categorized_services = []
                st.session_state.categorized_commercial = []; st.session_state.categorized_dimensions = []
            else:
                with st.spinner("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏..."):
                    categorized = classify_semantics_with_api(words_to_check, YANDEX_DICT_KEY)
                
                st.session_state.categorized_products = categorized['products']
                st.session_state.categorized_services = categorized['services']
                st.session_state.categorized_commercial = categorized['commercial']
                st.session_state.categorized_geo = categorized['geo']
                st.session_state.categorized_dimensions = categorized['dimensions']
                st.session_state.categorized_general = categorized['general']
                st.session_state.categorized_sensitive = categorized['sensitive']

                st.session_state.orig_products = categorized['products'] + categorized['sensitive']
                st.session_state.orig_services = categorized['services'] + categorized['sensitive']
                st.session_state.orig_commercial = categorized['commercial'] + categorized['sensitive']
                st.session_state.orig_geo = categorized['geo'] + categorized['sensitive']
                st.session_state.orig_dimensions = categorized['dimensions'] + categorized['sensitive']
                st.session_state.orig_general = categorized['general'] + categorized['sensitive']
                
                st.session_state['sensitive_words_input_final'] = "\n".join(categorized['sensitive'])

            all_found_products = st.session_state.categorized_products
            count_prods = len(all_found_products)
            if count_prods < 20:
                st.session_state.auto_tags_words = all_found_products
                st.session_state.auto_promo_words = []
            else:
                half_count = int(math.ceil(count_prods / 2))
                st.session_state.auto_tags_words = all_found_products[:half_count]
                st.session_state.auto_promo_words = all_found_products[half_count:]
            
            st.session_state['tags_products_edit_final'] = "\n".join(st.session_state.auto_tags_words)
            st.session_state['promo_keywords_area_final'] = "\n".join(st.session_state.auto_promo_words)

            # === –§–ò–ù–ê–õ–¨–ù–´–ô –®–¢–†–ò–•: –ê–í–¢–û-–ü–ï–†–ï–ö–õ–Æ–ß–ï–ù–ò–ï ===
            # 1. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—è–µ–º —Ä–∞–¥–∏–æ-–∫–Ω–æ–ø–∫—É –Ω–∞ "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"
            st.session_state['force_radio_switch'] = True
            
            # 2. –ï—Å–ª–∏ –º—ã –∑–∞–ø—É—Å–∫–∞–ª–∏ —á–µ—Ä–µ–∑ API, —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤ –ø–æ–ª–µ "–†—É—á–Ω–æ–π –≤–≤–æ–¥" –ø–æ–ø–∞–ª–∏ —Å—Å—ã–ª–∫–∏
            # (good_urls –º—ã –ø–æ–ª—É—á–∏–ª–∏ —á—É—Ç—å –≤—ã—à–µ –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞)
            if "API" in current_source_val:
                 st.session_state['persistent_urls'] = "\n".join(good_urls)
                 # –ï—Å–ª–∏ –±—ã–ª–∏ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ, –æ–Ω–∏ —É–∂–µ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ 'excluded_urls_auto' –≤—ã—à–µ
            
            st.rerun()

# ------------------------------------------
# TAB 2: WHOLESALE GENERATOR (COMBINED)
# ------------------------------------------
with tab_wholesale_main:
    st.header("üè≠ –ï–¥–∏–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    
    # ==========================================
    # 0. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• (–ò–ó –¢–ï–ö–£–©–ï–ì–û –°–û–°–¢–û–Ø–ù–ò–Ø)
    # ==========================================
    cat_products = st.session_state.get('categorized_products', [])
    cat_services = st.session_state.get('categorized_services', [])
    
    # 1. –î–ª—è –¢–µ–≥–æ–≤ –∏ –ü—Ä–æ–º–æ
    structure_keywords = cat_products + cat_services
    count_struct = len(structure_keywords)

    if 'auto_tags_words' in st.session_state and st.session_state.auto_tags_words:
         tags_list_source = st.session_state.auto_tags_words
         promo_list_source = st.session_state.auto_promo_words
    else:
         if count_struct > 0:
            if count_struct < 10:
                tags_list_source = structure_keywords
                promo_list_source = []
            elif count_struct < 30:
                mid = math.ceil(count_struct / 2)
                tags_list_source = structure_keywords[:mid]
                promo_list_source = structure_keywords[mid:]
            else:
                part = math.ceil(count_struct / 3)
                tags_list_source = structure_keywords[:part]
                promo_list_source = structure_keywords[part:part*2]
         else:
             tags_list_source = []
             promo_list_source = []
    
    # –î–µ—Ñ–æ–ª—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å–∞–π–¥–±–∞—Ä–∞
    sidebar_default_text = ""
    if count_struct >= 30 and 'auto_tags_words' not in st.session_state:
         part = math.ceil(count_struct / 3)
         sidebar_default_text = "\n".join(structure_keywords[part*2:])

    tags_default_text = ", ".join(tags_list_source)
    promo_default_text = ", ".join(promo_list_source)

    # 2. –î–ª—è –¢–∞–±–ª–∏—Ü (–†–∞–∑–º–µ—Ä—ã/–ì–û–°–¢)
    cat_dimensions = st.session_state.get('categorized_dimensions', [])
    tech_context_default = ", ".join(cat_dimensions) if cat_dimensions else ""

    # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ö–æ–º–º–µ—Ä—Ü–∏–∏/–û–±—â–∏—Ö –∏ –ì–ï–û
    cat_commercial = st.session_state.get('categorized_commercial', [])
    cat_general = st.session_state.get('categorized_general', [])
    cat_geo = st.session_state.get('categorized_geo', [])
    
    # –ò–°–ö–õ–Æ–ß–ê–ï–ú –ì–ï–û –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    text_context_list_raw = cat_commercial + cat_general
    text_context_default = ", ".join(text_context_list_raw)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ñ–æ–ª—Ç –¥–ª—è –ì–ï–û –±–ª–æ–∫–∞
    geo_context_default = ", ".join(cat_geo)

    # --- –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ê–ö–¢–ò–í–ù–û–°–¢–ò –ú–û–î–£–õ–ï–ô ---
    # –ï—Å–ª–∏ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –Ω–µ –ø—É—Å—Ç -> —Å—Ç–∞–≤–∏–º –≥–∞–ª–æ—á–∫—É True, –∏–Ω–∞—á–µ False
    auto_check_text = bool(text_context_list_raw)
    auto_check_tags = bool(tags_list_source)
    auto_check_tables = bool(cat_dimensions)
    auto_check_promo = bool(promo_list_source)
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–∞–π–¥–±–∞—Ä –≤–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–ª—è –Ω–µ–≥–æ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç
    auto_check_sidebar = bool(sidebar_default_text.strip())
    
    auto_check_geo = bool(cat_geo)

    # ==========================================
    # 1. –í–í–û–î–ù–´–ï –î–ê–ù–ù–´–ï
    # ==========================================
    with st.container(border=True):
        st.subheader("1. –ò—Å—Ç–æ—á–Ω–∏–∫ –∏ –î–æ—Å—Ç—É–ø—ã")
        
        col_source, col_key = st.columns([3, 1])
        
        use_manual_html = st.checkbox("üìù –í—Å—Ç–∞–≤–∏—Ç—å HTML –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", key="cb_manual_html_mode", value=False)
        
        with col_source:
            if use_manual_html:
                manual_html_source = st.text_area(
                    "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (HTML)", 
                    height=200, 
                    placeholder="<html>...</html>", 
                    help="–°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Å—é–¥–∞ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."
                )
                main_category_url = None
            else:
                main_category_url = st.text_input(
                    "URL –ö–∞—Ç–µ–≥–æ—Ä–∏–∏", 
                    placeholder="https://site.ru/catalog/...", 
                    help="–°–∫—Ä–∏–ø—Ç —Å–æ–±–µ—Ä–µ—Ç —Ç–æ–≤–∞—Ä—ã —Å —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
                )
                manual_html_source = None

        with col_key:
            default_key = st.session_state.get('pplx_key_cache', "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")
            pplx_api_key = st.text_input("Perplexity API Key", value=default_key, type="password")
            if pplx_api_key: st.session_state.pplx_key_cache = pplx_api_key

    # ==========================================
    # 2. –í–´–ë–û–† –ú–û–î–£–õ–ï–ô
    # ==========================================
    st.subheader("2. –ö–∞–∫–∏–µ –±–ª–æ–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º?")
    st.info("‚ÑπÔ∏è **–ê–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞:** –ì–∞–ª–æ—á–∫–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ç–∞–º, –≥–¥–µ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—à–ª–∏—Å—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–ª–æ–≤–∞. –í—ã –º–æ–∂–µ—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –≤—ã–±–æ—Ä –≤—Ä—É—á–Ω—É—é.")
    col_ch1, col_ch2, col_ch3, col_ch4, col_ch5, col_ch6 = st.columns(6)
    
    # –í—Å—Ç–∞–≤–ª—è–µ–º –∞–≤—Ç–æ-–∑–Ω–∞—á–µ–Ω–∏—è –≤ value=...
    with col_ch1: use_text = st.checkbox("ü§ñ AI –¢–µ–∫—Å—Ç—ã", value=auto_check_text)
    with col_ch2: use_tags = st.checkbox("üè∑Ô∏è –¢–µ–≥–∏", value=auto_check_tags)
    with col_ch3: use_tables = st.checkbox("üß© –¢–∞–±–ª–∏—Ü—ã", value=auto_check_tables)
    with col_ch4: use_promo = st.checkbox("üî• –ü—Ä–æ–º–æ", value=auto_check_promo)
    with col_ch5: use_sidebar = st.checkbox("üìë –°–∞–π–¥–±–∞—Ä", value=auto_check_sidebar)
    with col_ch6: use_geo = st.checkbox("üåç –ì–µ–æ-–±–ª–æ–∫", value=auto_check_geo)

    # ==========================================
    # 3. –ù–ê–°–¢–†–û–ô–ö–ò –ú–û–î–£–õ–ï–ô
    # ==========================================
    global_tags_list = []
    global_promo_list = []
    global_sidebar_list = []
    global_geo_list = []
    tags_file_content = ""
    table_prompts = []
    df_db_promo = None
    promo_title = "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º"
    sidebar_content = ""
    text_context_final_list = []
    tech_context_final_str = ""
    
    # –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–ª–æ–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞ (–ø–æ –¥–µ—Ñ–æ–ª—Ç—É 5)
    num_text_blocks_val = 5 

    if any([use_text, use_tags, use_tables, use_promo, use_sidebar, use_geo]):
        st.subheader("3. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥—É–ª–µ–π")

        # --- AI TEXT ---
        if use_text:
            with st.container(border=True):
                st.markdown("#### ü§ñ 1. AI –¢–µ–∫—Å—Ç—ã")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–±–æ—Ä –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–ª–æ–∫–æ–≤
                col_txt1, col_txt2 = st.columns([1, 4])
                with col_txt1:
                    num_text_blocks_val = st.selectbox("–ö–æ–ª-–≤–æ –±–ª–æ–∫–æ–≤", [1, 2, 3, 4, 5], index=4, key="sb_num_blocks")
                
                with col_txt2:
                    ai_words_input = st.text_area(
                        "–°–ª–æ–≤–∞ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è (–ö–æ–º–º–µ—Ä—Ü–∏—è + –û–±—â–∏–µ)", 
                        value=text_context_default, 
                        height=100, 
                        key="ai_text_context_editable",
                        help="–≠—Ç–∏ —Å–ª–æ–≤–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –ø–æ—Å—Ç–∞—Ä–∞–µ—Ç—Å—è –≤–Ω–µ–¥—Ä–∏—Ç—å –≤ —Ç–µ–∫—Å—Ç."
                    )
                
                text_context_final_list = [x.strip() for x in re.split(r'[,\n]+', ai_words_input) if x.strip()]

        # --- TAGS ---
        if use_tags:
            with st.container(border=True):
                st.markdown("#### üè∑Ô∏è 2. –¢–µ–≥–∏")
                kws_input_tags = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=tags_default_text, 
                    height=100, 
                    key="kws_tags_auto"
                )
                global_tags_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_tags) if x.strip()]
                if not global_tags_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                col_t1, col_t2 = st.columns([1, 2])
                with col_t1: u_manual = st.checkbox("–°–≤–æ—è –±–∞–∑–∞ —Å—Å—ã–ª–æ–∫ (.txt)", key="cb_tags_vert")
                with col_t2:
                    default_tags_path = "data/links_base.txt"
                    if not u_manual and os.path.exists(default_tags_path):
                        st.success(f"‚úÖ –ë–∞–∑–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (`links_base.txt`)")
                        with open(default_tags_path, "r", encoding="utf-8") as f: tags_file_content = f.read()
                    elif u_manual:
                        up_t = st.file_uploader("–§–∞–π–ª .txt", type=["txt"], key="up_tags_vert", label_visibility="collapsed")
                        if up_t: tags_file_content = up_t.getvalue().decode("utf-8")
                    else: st.error("‚ùå –§–∞–π–ª –±–∞–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω!")

# --- –§–£–ù–ö–¶–ò–Ø –ì–õ–£–ë–û–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê –ö–û–ù–¢–ï–ö–°–¢–ê –î–õ–Ø –¢–ê–ë–õ–ò–¶ ---
        def generate_context_aware_headers(count, query, dimensions_list, general_list):
            """
            –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ò –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (—Ä–∞–∑–º–µ—Ä—ã, –æ–±—â–∏–µ), 
            —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —Ç–∏–ø—ã —Ç–∞–±–ª–∏—Ü –Ω—É–∂–Ω—ã.
            """
            query_lower = query.lower()
            
            # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–∫–∏ —Å–ª–æ–≤ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            dims_str = " ".join(dimensions_list).lower()
            gen_str = " ".join(general_list).lower()
            full_context = f"{dims_str} {gen_str} {query_lower}"
            
            # --- 1. –î–ï–¢–ï–ö–¢–û–†–´ –°–ò–ì–ù–ê–õ–û–í (–ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Å–µ–º–∞–Ω—Ç–∏–∫–µ) ---
            
            # –ü—Ä–∏–∑–Ω–∞–∫ —Ä–∞–∑–º–µ—Ä–æ–≤: –µ—Å—Ç—å —Ü–∏—Ñ—Ä—ã —Å '—Ö' (10—Ö20), —Å–ª–æ–≤–∞ –º–º, –∫–≥, —Ç–æ–Ω–Ω–∞, —Ä–∞–∑–º–µ—Ä
            has_sizes_signal = (
                len(dimensions_list) > 0 or 
                bool(re.search(r'\d+[x—Ö*]\d+', full_context)) or 
                any(x in full_context for x in ['—Ä–∞–∑–º–µ—Ä', '–≥–∞–±–∞—Ä–∏—Ç', '—Ç–æ–ª—â–∏–Ω', '–¥–∏–∞–º–µ—Ç—Ä', '—Ä–∞—Å–∫—Ä–æ–π', '–≤–µ—Å', '–º–∞—Å—Å'])
            )
            
            # –ü—Ä–∏–∑–Ω–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤: –ì–û–°–¢, –û–°–¢, –¢–£, DIN, AISI
            has_gost_signal = any(x in full_context for x in ['–≥–æ—Å—Ç', 'din', 'aisi', 'astm', '—Ç—É ', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç'])
            
            # –ü—Ä–∏–∑–Ω–∞–∫ –º–∞—Ä–æ–∫/–º–∞—Ç–µ—Ä–∏–∞–ª–∞: —Å—Ç–∞–ª—å, —Å–ø–ª–∞–≤, –º–∞—Ä–∫–∞, —Å—Ç.3, 09–≥2—Å
            has_grade_signal = any(x in full_context for x in ['–º–∞—Ä–∫', '—Å–ø–ª–∞–≤', '—Å—Ç–∞–ª—å', '—Å—Ç.', '–º–∞—Ç–µ—Ä–∏–∞–ª', '—Ö–∏–º–∏—á', '—Å–æ—Å—Ç–∞–≤'])
            
            # –ü—Ä–∏–∑–Ω–∞–∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è: –¥–ª—è —á–µ–≥–æ, —Å—Ñ–µ—Ä–∞, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            has_usage_signal = any(x in full_context for x in ['–ø—Ä–∏–º–µ–Ω–µ–Ω', '—Å—Ñ–µ—Ä', '–Ω–∞–∑–Ω–∞—á–µ–Ω', '–∏—Å–ø–æ–ª—å–∑'])

            # --- 2. –°–ë–û–†–ö–ê –û–ß–ï–†–ï–î–ò (PRIORITY QUEUE) ---
            # –ú—ã —Ä–∞—Å—Å—Ç–∞–≤–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—ã –≤ –ø–æ—Ä—è–¥–∫–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
            priority_stack = []
            
            # –ï—Å–ª–∏ —ç—Ç–æ –º–µ—Ç–∞–ª–ª–æ–ø—Ä–æ–∫–∞—Ç (–µ—Å—Ç—å –º–∞—Ä–∫–∏/—Å–ø–ª–∞–≤—ã), –æ–±—ã—á–Ω–æ —Å–Ω–∞—á–∞–ª–∞ —Å—Ç–∞–≤—è—Ç –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–ª–∏ –ú–∞—Ä–∫–∏
            if has_grade_signal:
                priority_stack.append("–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã")
                
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã - —ç—Ç–æ —Å—É–ø–µ—Ä –≤–∞–∂–Ω–æ, —Å—Ç–∞–≤–∏–º –≤ –Ω–∞—á–∞–ª–æ
            if has_sizes_signal:
                # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –ú–∞—Ä–∫–∏, —Ç–æ –†–∞–∑–º–µ—Ä—ã –≤—Ç–æ—Ä—ã–º–∏. –ï—Å–ª–∏ –Ω–µ—Ç - –ø–µ—Ä–≤—ã–º–∏.
                priority_stack.append("–¢–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤")
                
            # –ï—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ì–û–°–¢–æ–≤
            if has_gost_signal:
                priority_stack.append("–ì–û–°–¢—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã")
                
            # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—Ä–æ —Ö–∏–º —Å–æ—Å—Ç–∞–≤ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ)
            if "—Ö–∏–º" in full_context and "—Å–æ—Å—Ç–∞–≤" in full_context:
                 # –í—Å—Ç–∞–≤–ª—è–µ–º "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤" –≤–º–µ—Å—Ç–æ "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã" –∏–ª–∏ —Ä—è–¥–æ–º
                 if "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã" in priority_stack:
                     idx = priority_stack.index("–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã")
                     priority_stack.insert(idx+1, "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤")
                 else:
                     priority_stack.append("–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤")

            # --- 3. –ó–ê–ü–û–õ–ù–ï–ù–ò–ï –ü–£–°–¢–û–¢ (DEFAULTS) ---
            # –ï—Å–ª–∏ –º—ã –≤—ã–±—Ä–∞–ª–∏ 5 —Ç–∞–±–ª–∏—Ü, –∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞—à–ª–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ 2, –Ω—É–∂–Ω–æ –¥–æ–±–∏—Ç—å –æ—Å—Ç–∞–ª—å–Ω–æ–µ
            defaults = [
                "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞
                "–°–≤–æ–π—Å—Ç–≤–∞",
                "–°—Ñ–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è",
                "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–¥–µ–ª–∏—è",
                "–ê–Ω–∞–ª–æ–≥–∏"
            ]
            
            final_headers = []
            # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ, —á—Ç–æ –Ω–∞—à–ª–∏ —É–º–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
            for p in priority_stack:
                if p not in final_headers: final_headers.append(p)
            
            # –î–æ–±–∏–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏
            for d in defaults:
                if d not in final_headers: final_headers.append(d)
                
            # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –≤—Å—ë —Ä–∞–≤–Ω–æ –º–∞–ª–æ (—Ä–µ–¥–∫–∏–π —Å–ª—É—á–∞–π)
            while len(final_headers) < count:
                final_headers.append("–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏")
                
            return final_headers[:count]

        # --- –ë–õ–û–ö –ò–ù–¢–ï–†–§–ï–ô–°–ê TABLES ---
        if use_tables:
            with st.container(border=True):
                st.markdown("#### üß© 3. –¢–∞–±–ª–∏—Ü—ã")
                
                # –î–ê–ù–ù–´–ï –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê (–ë–µ—Ä–µ–º –∏–∑ session_state, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞)
                raw_query = st.session_state.get('query_input', '')
                found_dims = st.session_state.get('categorized_dimensions', []) # –°–ª–æ–≤–∞—Ä—å —Ä–∞–∑–º–µ—Ä–æ–≤
                found_general = st.session_state.get('categorized_general', []) # –°–ª–æ–≤–∞—Ä—å –æ–±—â–∏—Ö —Å–ª–æ–≤
                
                col_ctx, col_cnt = st.columns([3, 1]) 
                
                with col_ctx:
                    tech_context_final_str = st.text_area(
                        "–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ç–∞–±–ª–∏—Ü (–ú–∞—Ä–∫–∏, –ì–û–°–¢, –†–∞–∑–º–µ—Ä—ã)", 
                        value=tech_context_default, # –ó–¥–µ—Å—å –ª–µ–∂–∞—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
                        height=68, 
                        key="table_context_editable",
                        help="–≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–º–æ–≥—É—Ç AI —Å–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É."
                    )
                
                with col_cnt:
                    cnt_options = [1, 2, 3, 4, 5]
                    cnt = st.selectbox("–ö–æ–ª-–≤–æ —Ç–∞–±–ª–∏—Ü", cnt_options, index=1, key="num_tbl_vert_select")

                # --- –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê ---
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢–û–ì–û, –ß–¢–û –ù–ê–®–õ–ò –í –°–ï–ú–ê–ù–¢–ò–ö–ï
                smart_headers_list = generate_context_aware_headers(cnt, raw_query, found_dims, found_general)

                table_presets = [
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–°–≤–æ–π—Å—Ç–≤–∞", "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–¥–µ–ª–∏—è",
                    "–û–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ", "–¢–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤", "–°–æ—Ä—Ç–∞–º–µ–Ω—Ç",
                    "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤", "–§–∏–∑–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞", "–ú–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞",
                    "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã", "–°–æ—Å—Ç–∞–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞", "–ì–û–°–¢—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã",
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –ì–û–°–¢", "–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ",
                    "–°—Ñ–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", "–£—Å–ª–æ–≤–∏—è —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏", "–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è",
                    "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", "–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏", "–ê–Ω–∞–ª–æ–≥–∏",
                    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π", "–†–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç–∏"
                ]
                
                table_prompts = []
                st.write("") 
                
                cols = st.columns(cnt)
                
                for i, col in enumerate(cols):
                    with col:
                        st.caption(f"**–¢–∞–±–ª–∏—Ü–∞ {i+1}**")
                        
                        # –ê–≤—Ç–æ-–≤—ã–±–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
                        suggested_topic = smart_headers_list[i]
                        
                        try: default_idx = table_presets.index(suggested_topic)
                        except: default_idx = 0
                        
                        is_manual = st.checkbox("–°–≤–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫", key=f"cb_tbl_manual_{i}")
                        
                        if is_manual:
                            selected_topic = st.text_input(
                                f"–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª. {i+1}", value="", 
                                key=f"tbl_topic_custom_{i}", label_visibility="collapsed"
                            )
                            if not selected_topic.strip(): selected_topic = "–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏" 
                        else:
                            selected_topic = st.selectbox(
                                f"–¢–µ–º–∞ —Ç–∞–±–ª. {i+1}", 
                                table_presets, 
                                index=default_idx, # <--- –£–ú–ù–´–ô –ò–ù–î–ï–ö–°
                                key=f"tbl_topic_select_{i}",
                                label_visibility="collapsed"
                            )
                        
                        table_prompts.append(selected_topic)

# --- PROMO (–° –ê–ù–ê–õ–ò–ó–û–ú –ö–û–ú–ú–ï–†–ß–ï–°–ö–ò–• –§–ê–ö–¢–û–†–û–í) ---
        if use_promo:
            with st.container(border=True):
                st.markdown("#### üî• 4. –ü—Ä–æ–º–æ-–±–ª–æ–∫")
                
                kws_input_promo = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=promo_default_text, 
                    height=100, 
                    key="kws_promo_auto"
                )
                global_promo_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_promo) if x.strip()]
                if not global_promo_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                
                col_p1, col_p2 = st.columns([1, 2])
                with col_p1:
                    promo_presets = [
                        "–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ", "–ü–æ—Ö–æ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã", "–í–∞—Å –º–æ–∂–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å",
                        "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º", "–î—Ä—É–≥–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", "–í–∞–º –º–æ–∂–µ—Ç –ø—Ä–∏–≥–æ–¥–∏—Ç—å—Å—è",
                        "–¢–∞–∫–∂–µ –≤ —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ", "–° —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º –ø–æ–∫—É–ø–∞—é—Ç", "–ß–∞—Å—Ç–æ –ø–æ–∫—É–ø–∞—é—Ç –≤–º–µ—Å—Ç–µ",
                        "–°–æ–ø—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–≤–∞—Ä—ã", "–•–∏—Ç—ã –ø—Ä–æ–¥–∞–∂", "–í—ã–±–æ—Ä –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π",
                        "–õ–∏–¥–µ—Ä—ã —Å–ø—Ä–æ—Å–∞", "–ü–æ–ø—É–ª—è—Ä–Ω–æ–µ —Å–µ–π—á–∞—Å", "–¢–æ–ø —Ç–æ–≤–∞—Ä–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏",
                        "–õ—É—á—à–∞—è —Ü–µ–Ω–∞", "–°–ø–µ—Ü–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", "–£—Å–ø–µ–π—Ç–µ –∑–∞–∫–∞–∑–∞—Ç—å",
                        "–ù–µ –∑–∞–±—É–¥—å—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å", "–í—ã –Ω–µ–¥–∞–≤–Ω–æ —Å–º–æ—Ç—Ä–µ–ª–∏"
                    ]

                    # --- –õ–û–ì–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê –ö–û–ú–ú–ï–†–¶–ò–ò ---
                    # –ë–µ—Ä–µ–º –∑–∞–ø—Ä–æ—Å + —Å–ø–∏—Å–æ–∫ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ (Tab 1)
                    raw_query = st.session_state.get('query_input', '').lower()
                    comm_words = st.session_state.get('categorized_commercial', [])
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                    comm_context = f"{raw_query} {' '.join(comm_words)}".lower()
                    
                    target_header = "–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ" # –î–µ—Ñ–æ–ª—Ç (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π)

                    # 1. –Ø–≤–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è (–µ—Å—Ç—å —Å–ª–æ–≤–∞ '—Ü–µ–Ω–∞', '–∫—É–ø–∏—Ç—å' –≤ —Å–µ–º–∞–Ω—Ç–∏–∫–µ –∏–ª–∏ –∑–∞–ø—Ä–æ—Å–µ)
                    is_commercial = any(x in comm_context for x in ["–∫—É–ø–∏—Ç—å", "—Ü–µ–Ω–∞", "–∑–∞–∫–∞–∑", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–ø—Ä–∞–π—Å", "–º–∞–≥–∞–∑–∏–Ω", "–∫–æ—Ä–∑–∏–Ω–∞"])
                    
                    # 2. –ê–∫—Ü–∏–æ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
                    is_promo = any(x in comm_context for x in ["–∞–∫—Ü–∏—è", "—Å–∫–∏–¥–∫", "—Ä–∞—Å–ø—Ä–æ–¥–∞–∂", "–≤—ã–≥–æ–¥–Ω"])
                    
                    # 3. –†–µ–π—Ç–∏–Ω–≥–æ–≤—ã–µ —Å–ª–æ–≤–∞
                    is_top = any(x in comm_context for x in ["—Ç–æ–ø", "–ª—É—á—à", "—Ä–µ–π—Ç–∏–Ω–≥", "–ø–æ–ø—É–ª—è—Ä–Ω"])

                    if is_promo:
                        target_header = "–°–ø–µ—Ü–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"
                    elif is_top:
                        target_header = "–õ–∏–¥–µ—Ä—ã —Å–ø—Ä–æ—Å–∞"
                    elif is_commercial:
                        # –ï—Å–ª–∏ —ç—Ç–æ —è–≤–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è, –ª—É—á—à–µ "–ü–æ–∫—É–ø–∞—é—Ç –≤–º–µ—Å—Ç–µ" –∏–ª–∏ "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º"
                        target_header = "–° —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º –ø–æ–∫—É–ø–∞—é—Ç"
                    
                    try: promo_smart_idx = promo_presets.index(target_header)
                    except: promo_smart_idx = 0

                    use_custom_header = st.checkbox("–í–≤–µ—Å—Ç–∏ —Å–≤–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫", key="cb_custom_header")
                    
                    if use_custom_header:
                        promo_title = st.text_input("–í–∞—à –∑–∞–≥–æ–ª–æ–≤–æ–∫", placeholder="–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ", key="pr_tit_vert")
                    else:
                        promo_title = st.selectbox(
                            "–í–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–≥–æ–ª–æ–≤–∫–∞", 
                            promo_presets, 
                            index=promo_smart_idx, # <--- –£–ú–ù–´–ô –í–´–ë–û–†
                            key="promo_header_select"
                        )

                    st.markdown("<br>", unsafe_allow_html=True)
                    u_img_man = st.checkbox("–°–≤–æ—è –±–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫", key="cb_img_vert")

                with col_p2:
                    default_img_db = "data/images_db.xlsx"
                    if not u_img_man and os.path.exists(default_img_db):
                        st.success("‚úÖ –ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ (`images_db.xlsx`)")
                        try: df_db_promo = pd.read_excel(default_img_db)
                        except: pass
                    elif u_img_man:
                        up_i = st.file_uploader("–§–∞–π–ª .xlsx", type=['xlsx'], key="up_img_vert", label_visibility="collapsed")
                        if up_i: df_db_promo = pd.read_excel(up_i)
                    else: st.error("‚ùå –ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")

        # --- SIDEBAR ---
        if use_sidebar:
            with st.container(border=True):
                st.markdown("#### üìë 5. –°–∞–π–¥–±–∞—Ä")
                kws_input_sidebar = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏", 
                    value=sidebar_default_text, 
                    height=100, 
                    key="kws_sidebar_auto"
                )
                global_sidebar_list = [x.strip() for x in kws_input_sidebar.split('\n') if x.strip()]
                if not global_sidebar_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                col_s1, col_s2 = st.columns([1, 2])
                with col_s1: u_sb_man = st.checkbox("–°–≤–æ–π —Ñ–∞–π–ª –º–µ–Ω—é (.txt)", key="cb_sb_vert")
                with col_s2:
                    def_menu = "data/menu_structure.txt"
                    if not u_sb_man and os.path.exists(def_menu):
                        st.success("‚úÖ –ú–µ–Ω—é —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (`menu_structure.txt`)")
                        with open(def_menu, "r", encoding="utf-8") as f: sidebar_content = f.read()
                    elif u_sb_man:
                        up_s = st.file_uploader("–§–∞–π–ª .txt", type=['txt'], key="up_sb_vert", label_visibility="collapsed")
                        if up_s: sidebar_content = up_s.getvalue().decode("utf-8")
                    else: st.error("‚ùå –§–∞–π–ª –º–µ–Ω—é –Ω–µ –Ω–∞–π–¥–µ–Ω!")

        # --- GEO BLOCK ---
        if use_geo:
            with st.container(border=True):
                st.markdown("#### üåç 6. –ì–µ–æ-–±–ª–æ–∫")
                kws_input_geo = st.text_area(
                    "–°–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤/—Ä–µ–≥–∏–æ–Ω–æ–≤ (–∏–∑ –≤–∫–ª–∞–¥–∫–∏ –ê–Ω–∞–ª–∏–∑) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=geo_context_default, 
                    height=100, 
                    key="kws_geo_auto"
                )
                global_geo_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_geo) if x.strip()]
                
                if not global_geo_list:
                    st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤ –ø—É—Å—Ç!")
                else:
                    st.info(f"–ë—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ç–µ–∫—Å—Ç –¥–æ—Å—Ç–∞–≤–∫–∏ –¥–ª—è –ø–æ–ª—è IP_PROP4819 —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º —ç—Ç–∏—Ö –≥–æ—Ä–æ–¥–æ–≤.")

    st.markdown("---")
    
# ==========================================
    # 4. –ó–ê–ü–£–°–ö –ì–ï–ù–ï–†–ê–¶–ò–ò (–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–û–ò–°–ö –°–°–´–õ–û–ö)
    # ==========================================
    
    ready_to_go = True
    
    if use_manual_html:
        if not manual_html_source: ready_to_go = False
    else:
        if not main_category_url: ready_to_go = False

    if (use_text or use_tables) and not pplx_api_key: ready_to_go = False
    # –£–±–∏—Ä–∞–µ–º –∂–µ—Å—Ç–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∑–¥–µ—Å—å, —Ç–∞–∫ –∫–∞–∫ –ø–æ–¥–≥—Ä—É–∑–∏–º —Ñ–∞–π–ª—ã –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –Ω–∏–∂–µ
    # if use_tags and not tags_file_content: ready_to_go = False 
    if use_promo and df_db_promo is None: ready_to_go = False
    if use_geo and not pplx_api_key: ready_to_go = False
    
    if st.button("üöÄ –ó–ê–ü–£–°–¢–ò–¢–¨ –ì–ï–ù–ï–†–ê–¶–ò–Æ", type="primary", disabled=not ready_to_go, use_container_width=True):
        # === –û–ß–ò–°–¢–ö–ê –ü–†–ï–î–´–î–£–©–ò–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
        st.session_state.gen_result_df = None
        st.session_state.unified_excel_data = None
        # ======================================
        
        status_box = st.status("üõ†Ô∏è –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...", expanded=True)
        final_data = [] 
        
        # 1. –°–ë–û–† –ò–°–•–û–î–ù–´–• –°–°–´–õ–û–ö –ò –ë–ê–ó (–ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê)
        
        # --- –ë–∞–∑–∞ –¢–µ–≥–æ–≤ (links_base.txt) ---
        tags_map = {}
        all_tags_links = []
        if use_tags:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –∏–∑ UI (–µ—Å–ª–∏ –∑–∞–≥—Ä—É–∂–∞–ª–∏ –≤—Ä—É—á–Ω—É—é)
            if tags_file_content:
                s_io = io.StringIO(tags_file_content)
                all_tags_links = [l.strip() for l in s_io.readlines() if l.strip()]
            # –ï—Å–ª–∏ –ø—É—Å—Ç–æ, –ø—Ä–æ–±—É–µ–º —á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª —Å –¥–∏—Å–∫–∞ –Ω–∞–ø—Ä—è–º—É—é
            elif os.path.exists("data/links_base.txt"):
                with open("data/links_base.txt", "r", encoding="utf-8") as f:
                    all_tags_links = [l.strip() for l in f.readlines() if l.strip()]
            
            # --- –£–ú–ù–´–ô –ü–û–ò–°–ö (Smart Matching) ---
            for kw in global_tags_list:
                # 1. –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                
                # 2. –í—ã–¥–µ–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω—è (–æ–±—Ä–µ–∑–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ 'gibkiy' –≤ 'gibkaya')
                search_roots = {tr} # –ò—Å—Ö–æ–¥–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
                if len(tr) > 5: 
                    search_roots.add(tr[:-1]) # –º–∏–Ω—É—Å 1 –±—É–∫–≤–∞
                    search_roots.add(tr[:-2]) # –º–∏–Ω—É—Å 2 –±—É–∫–≤—ã (iy, yy, ay)
                elif len(tr) > 4:
                    search_roots.add(tr[:-1])

                # 3. –ò—â–µ–º –ª—é–±–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω—è –≤ —Å—Å—ã–ª–∫–∞—Ö
                matches = []
                for u in all_tags_links:
                    u_lower = u.lower()
                    for root in search_roots:
                        if root in u_lower:
                            matches.append(u)
                            break # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –ø–æ –æ–¥–Ω–æ–º—É –∫–æ—Ä–Ω—é, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥. —Å—Å—ã–ª–∫–µ
                
                if matches: tags_map[kw] = matches

# --- –ë–∞–∑–∞ –ü—Ä–æ–º–æ (images_db.xlsx) ---
        p_img_map = {}
        promo_matched_pool = []
        urls_to_fetch_names = set()
        if use_promo and df_db_promo is not None:
            # 1. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø–æ–ª–Ω—è–µ–º –æ–±—â–∏–π —Å–ª–æ–≤–∞—Ä—å (—á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –æ—à–∏–±–∫–∏ NameError –Ω–∏–∂–µ)
            for _, row in df_db_promo.iterrows():
                u = str(row.iloc[0]).strip().lower()
                img = str(row.iloc[1]).strip()
                if u and u != 'nan' and img and img != 'nan':
                    p_img_map[u.rstrip('/')] = img
            
            # 2. –¢–µ–ø–µ—Ä—å —Ñ–∏–ª—å—Ç—Ä—É–µ–º –°–¢–†–û–ì–û –ø–æ —Å–ø–∏—Å–∫—É —Å–ª–æ–≤ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
            for kw in global_promo_list:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                roots = [tr]
                if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                
                matches_for_kw = []
                for u, img in p_img_map.items():
                    if any(root in u for root in roots):
                        matches_for_kw.append({'url': u, 'img': img})
                
                if matches_for_kw:
                    random.shuffle(matches_for_kw)
                    # –ë–µ—Ä–µ–º –ø–æ 2 —Ç–æ–≤–∞—Ä–∞ –Ω–∞ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ
                    for m in matches_for_kw[:2]:
                        if m['url'] not in [p['url'] for p in promo_matched_pool]:
                            promo_matched_pool.append(m)
                            urls_to_fetch_names.add(m['url'])

        # --- –ë–∞–∑–∞ –°–∞–π–¥–±–∞—Ä–∞ (menu_structure.txt) ---
        all_menu_urls = []
        if use_sidebar:
            if sidebar_content:
                s_io = io.StringIO(sidebar_content)
                all_menu_urls = [l.strip() for l in s_io.readlines() if l.strip()]
            elif os.path.exists("data/menu_structure.txt"):
                with open("data/menu_structure.txt", "r", encoding="utf-8") as f:
                    all_menu_urls = [l.strip() for l in f.readlines() if l.strip()]

        # =========================================================
        # üî• –õ–û–ì–ò–ö–ê –ü–û–ò–°–ö–ê –ü–û–¢–ï–†–Ø–ù–ù–´–• –°–õ–û–í (–û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø)
        # =========================================================
        missing_words_log = set()
        if use_tags:
            for kw in global_tags_list:
                if kw not in tags_map: missing_words_log.add(kw)
        
        if use_promo:
            for kw in global_promo_list:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                roots = [tr]
                if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                has_match = False
                # –ó–¥–µ—Å—å p_img_map —Ç–µ–ø–µ—Ä—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ—à–∏–±–∫–∏ –Ω–µ –±—É–¥–µ—Ç
                for u in p_img_map.keys():
                    if any(r in u for r in roots):
                        has_match = True
                        break
                if not has_match: missing_words_log.add(kw)
        
        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ü–†–û–ú–û (–¢–æ–∂–µ —É–º–Ω—ã–π –ø–æ–∏—Å–∫)
        if use_promo:
            for kw in global_promo_list:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                roots = [tr]
                if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                
                has_match = False
                # –ó–ê–ú–ï–ù–ê –¢–£–¢: –≤–º–µ—Å—Ç–æ p_img_map.keys() –∏—Å–ø–æ–ª—å–∑—É–µ–º img_list
                for entry in img_list: 
                    if any(r in entry['url'] for r in roots):
                        has_match = True
                        break
                
                if not has_match:
                    missing_words_log.add(kw)

        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –°–ê–ô–î–ë–ê–† (–¢–æ–∂–µ —É–º–Ω—ã–π –ø–æ–∏—Å–∫)
        if use_sidebar and global_sidebar_list:
            for kw in global_sidebar_list:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                roots = [tr]
                if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                
                has_match = False
                for u in all_menu_urls:
                    if any(r in u for r in roots):
                        has_match = True
                        break
                
                if not has_match:
                    missing_words_log.add(kw)

        # 4. –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –ü–ï–†–ï–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï
        if missing_words_log:
            missing_list = sorted(list(missing_words_log))
            
            # –ê. –î–æ–±–∞–≤–ª—è–µ–º –≤ –¢–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            for w in missing_list:
                if w not in text_context_final_list:
                    text_context_final_list.append(w)
            
            # –ë. –î–æ–±–∞–≤–ª—è–µ–º –≤ –¢–∞–±–ª–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            tech_additions = []
            for w in missing_list:
                # –ï—Å–ª–∏ —Ü–∏—Ñ—Ä–∞, –ì–û–°–¢ –∏–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞
                if any(char.isdigit() for char in w) or any(x in w.lower() for x in ['–≥–æ—Å—Ç', '—Ç–∏–ø', '—Ñ–æ—Ä–º–∞', '–º–º', '–∫–≥']):
                    tech_additions.append(w)
            
            if tech_additions:
                tech_context_final_str += "\n" + ", ".join(tech_additions)

            # –í. –ü–õ–ê–®–ö–ê
            status_box.markdown(f"""
                <div style="background-color: #FFF4E5; border-left: 5px solid #FF9800; padding: 15px; border-radius: 4px; margin-bottom: 15px; color: #663C00;">
                    <strong>‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –ß–∞—Å—Ç—å —Å—Å—ã–ª–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞</strong><br>
                    <span style="font-size: 0.9em;">
                    –ú—ã –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–ª—è: <b>{', '.join(missing_list)}</b>.<br>
                    ‚úÖ <u>–û–Ω–∏ –±—ã–ª–∏ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã –≤ –¢–ó –¥–ª—è –ù–µ–π—Ä–æ—Å–µ—Ç–∏ (–±—É–¥—É—Ç –≤ —Ç–µ–∫—Å—Ç–µ/—Ç–∞–±–ª–∏—Ü–∞—Ö).</u>
                    </span>
                </div>
            """, unsafe_allow_html=True)
            time.sleep(2)

        # =========================================================
        # –î–ê–õ–ï–ï –°–¢–ê–ù–î–ê–†–¢–ù–ê–Ø –õ–û–ì–ò–ö–ê (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô –í –°–¢–†–£–ö–¢–£–†–ï)
        # =========================================================

        target_pages = []
        soup = None
        current_base_url = main_category_url if main_category_url else "http://localhost"

        try:
            if use_manual_html:
                status_box.write("üìÇ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º HTML –∫–æ–¥...")
                soup = BeautifulSoup(manual_html_source, 'html.parser')
            else:
                status_box.write(f"üïµÔ∏è –°–∫–∞–Ω–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {main_category_url}")
                session = requests.Session()
                retry = Retry(connect=3, read=3, redirect=3, backoff_factor=0.5)
                adapter = HTTPAdapter(max_retries=retry)
                session.mount('http://', adapter)
                session.mount('https://', adapter)
                
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
                r = session.get(main_category_url, headers=headers, timeout=30, verify=False)
                
                if r.status_code == 200:
                    soup = BeautifulSoup(r.text, 'html.parser')
                else: 
                    status_box.error(f"–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞: {r.status_code}")
                    st.stop()
            
            if soup:
                tags_container = soup.find(class_='popular-tags-inner')
                if tags_container:
                    for link in tags_container.find_all('a'):
                        href = link.get('href')
                        name = link.get_text(strip=True)
                        if href and name:
                            full_url = urljoin(current_base_url, href)
                            target_pages.append({'url': full_url, 'name': name})
                
                if not target_pages:
                    status_box.warning("–¢–µ–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–∫–ª–∞—Å—Å .popular-tags-inner). –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.")
                    h1 = soup.find('h1')
                    name = h1.get_text(strip=True) if h1 else "–¢–æ–≤–∞—Ä"
                    target_pages.append({'url': current_base_url, 'name': name})
                    
        except Exception as e: 
            status_box.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            st.stop()
            
        # –°–±–æ—Ä –∏–º–µ–Ω –¥–ª—è —Å—Å—ã–ª–æ–∫
        urls_to_fetch_names = set()
        promo_items_pool = []  # <--- –î–û–ë–ê–í–õ–ï–ù–ê –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø
        
        if use_tags:
            for kw, matches in tags_map.items():
                urls_to_fetch_names.update(matches)

        if use_promo:
            used_urls = set()
            for kw in global_promo_list:
                if kw in missing_words_log: continue
                
                # –ü–æ–≤—Ç–æ—Ä—è–µ–º —É–º–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                roots = [tr]
                if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                
                matches = []
                # –ò—â–µ–º –≤ keys() –∫–∞—Ä—Ç—ã –∫–∞—Ä—Ç–∏–Ω–æ–∫
                for u in p_img_map.keys():
                    if any(r in u for r in roots): matches.append(u)

                for m in matches:
                    if m not in used_urls:
                        urls_to_fetch_names.add(m)
                        # –¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ—à–∏–±–∫–∏ –Ω–µ –±—É–¥–µ—Ç
                        promo_items_pool.append({'url': m, 'img': p_img_map[m]})
                        used_urls.add(m)

        sidebar_matched_urls = []
        if use_sidebar:
            if global_sidebar_list:
                for kw in global_sidebar_list:
                    if kw in missing_words_log: continue
                    
                    tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                    roots = [tr]
                    if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                    
                    found = []
                    for u in all_menu_urls:
                        if any(r in u for r in roots): found.append(u)
                    
                    sidebar_matched_urls.extend(found)
                sidebar_matched_urls = list(set(sidebar_matched_urls))
            else:
                sidebar_matched_urls = all_menu_urls
            
            urls_to_fetch_names.update(sidebar_matched_urls)

        # --- –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –ò–ú–ï–ù ---
        url_name_cache = {}
        if urls_to_fetch_names:
            status_box.write(f"üåç –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è {len(urls_to_fetch_names)} —Å—Å—ã–ª–æ–∫...")
            
            def fetch_name_worker(u): 
                return u, get_breadcrumb_only(u) 
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
                future_to_url = {executor.submit(fetch_name_worker, u): u for u in urls_to_fetch_names}
                done_cnt = 0
                prog_fetch = status_box.progress(0)
                for future in concurrent.futures.as_completed(future_to_url):
                    u_res, name_res = future.result()
                    norm_key = u_res.rstrip('/')
                    
                    if name_res:
                        url_name_cache[norm_key] = name_res
                    else:
                        slug = norm_key.split('/')[-1]
                        url_name_cache[norm_key] = force_cyrillic_name_global(slug)
                    
                    done_cnt += 1
                    prog_fetch.progress(done_cnt / len(urls_to_fetch_names))
            
            status_box.write("‚úÖ –ù–∞–∑–≤–∞–Ω–∏—è —Å–æ–±—Ä–∞–Ω—ã!")

        # ------------------------------------------------------------------
        # –°–ë–û–†–ö–ê –ö–û–ù–¢–ï–ù–¢–ê
        # ------------------------------------------------------------------
        
        full_sidebar_code = ""
        if use_sidebar:
            status_box.write("üî® –°–±–æ—Ä–∫–∞ –º–µ–Ω—é...")
            tree = {}
            for url in sidebar_matched_urls:
                path = urlparse(url).path.strip('/')
                parts = [p for p in path.split('/') if p]
                idx_start = 0
                if 'catalog' in parts: idx_start = parts.index('catalog') + 1
                rel_parts = parts[idx_start:] if parts[idx_start:] else parts
                
                curr = tree
                for i, part in enumerate(rel_parts):
                    if part not in curr: curr[part] = {}
                    if i == len(rel_parts) - 1:
                        curr[part]['__url__'] = url
                        cache_key = url.rstrip('/')
                        curr[part]['__name__'] = url_name_cache.get(cache_key, force_cyrillic_name_global(part))
                    curr = curr[part]
            
            def render_tree_internal(node, level=1):
                html = ""
                keys = sorted([k for k in node.keys() if not k.startswith('__')])
                for key in keys:
                    child = node[key]
                    name = child.get('__name__', force_cyrillic_name_global(key))
                    url = child.get('__url__')
                    has_children = any(k for k in child.keys() if not k.startswith('__'))
                    
                    if level == 1:
                        html += '<li class="level-1-header">\n'
                        if has_children:
                            html += f'    <span class="dropdown-toggle">{name}</span>\n'
                            html += '    <ul class="collapse-menu list-unstyled">\n'
                            html += render_tree_internal(child, level=2)
                            html += '    </ul>\n'
                        else:
                            target = url if url else "#"
                            html += f'    <a href="{target}">{name}</a>\n'
                        html += '</li>\n'
                    elif level == 2:
                        if has_children:
                            html += '<li class="level-2-header">\n'
                            html += f'    <span class="dropdown-toggle">{name}</span>\n'
                            html += '    <ul class="collapse-menu list-unstyled">\n'
                            html += render_tree_internal(child, level=3)
                            html += '    </ul>\n'
                        else:
                            target = url if url else "#"
                            html += f'<li class="level-2-link-special"><a href="{target}">{name}</a></li>\n'
                    elif level >= 3:
                        target = url if url else "#"
                        html += f'<li class="level-3-link"><a href="{target}">{name}</a></li>\n'
                return html

            inner_html = render_tree_internal(tree, level=1)
            full_sidebar_code = f"""<div class="page-content-with-sidebar"><button id="mobile-menu-toggle" class="menu-toggle-button">‚ò∞</button><div class="sidebar-wrapper"><nav id="sidebar-menu"><ul class="list-unstyled components">{inner_html}</ul></nav></div></div>"""

        # === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ö–õ–ò–ï–ù–¢–ê PERPLEXITY ===
        client = None
        if openai and (use_text or use_tables or use_geo) and pplx_api_key:
            client = openai.OpenAI(api_key=pplx_api_key, base_url="https://api.perplexity.ai")

        progress_bar = status_box.progress(0)
        total_steps = len(target_pages)
        
        for idx, page in enumerate(target_pages):
            base_text_raw, tags_on_page, real_header_h2, err = get_page_data_for_gen(page['url'])
            header_for_ai = real_header_h2 if real_header_h2 else page['name']
            
            row_data = {'Page URL': page['url'], 'Product Name': header_for_ai}
            for k, v in STATIC_DATA_GEN.items(): row_data[k] = v
            current_page_seo_words = list(text_context_final_list)
            
# --- –í–ò–ó–£–ê–õ–¨–ù–´–ï –ë–õ–û–ö–ò (TAGS / PROMO) ---
            row_data['Tags HTML'] = "" 
            row_data['Promo HTML'] = ""
            
            if use_tags:
                html_collector = []
                # –ë–µ—Ä–µ–º –¢–û–õ–¨–ö–û —Ç–µ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–≤–µ–ª –≤ –ø–æ–ª–µ "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏)"
                for kw in global_tags_list:
                    # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –±–∞–∑–µ —Å—Å—ã–ª–æ–∫ –∏–º–µ–Ω–Ω–æ –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
                    if kw in tags_map:
                        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É, —á—Ç–æ–±—ã –Ω–µ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ —Å–∞–º–æ–≥–æ —Å–µ–±—è
                        valid = [u for u in tags_map[kw] if u.rstrip('/') != page['url'].rstrip('/')]
                        if valid:
                            sel = random.choice(valid)
                            nm = url_name_cache.get(sel.rstrip('/'), kw)
                            html_collector.append(f'<a href="{sel}" class="tag-link">{nm}</a>')
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Ç–µ–≥–æ–≤
                if html_collector:
                    row_data['Tags HTML'] = '<div class="popular-tags">' + "\n".join(html_collector) + '</div>'

            if use_promo:
                # –ë–µ—Ä–µ–º —Ç–æ–≤–∞—Ä—ã –∏–∑ –Ω–∞—à–µ–≥–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—É–ª–∞ (–∏—Å–∫–ª—é—á–∞—è —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É)
                cands = [p for p in promo_matched_pool if p['url'] != page['url'].rstrip('/')]
                random.shuffle(cands)
                if cands:
                    p_html = f'<div class="promo-section"><h3 style="margin-bottom:15px; font-size:1.1rem; font-weight:700;">{promo_title}</h3>'
                    # flex-wrap: nowrap –∏ overflow-x: auto –¥–µ–ª–∞—é—Ç —Å–∫—Ä–æ–ª–ª –≤ –æ–¥–Ω—É –ª–∏–Ω–∏—é
                    p_html += '<div class="promo-grid-scroll" style="display:flex; gap:15px; overflow-x:auto; flex-wrap:nowrap; padding-bottom:15px; -webkit-overflow-scrolling:touch;">'
                    for item in cands[:12]:
                        p_name = url_name_cache.get(item['url'], "–¢–æ–≤–∞—Ä")
                        p_html += f'''
                        <div class="promo-card-item" style="flex: 0 0 220px; min-width:220px; border:1px solid #eee; border-radius:8px; padding:10px; text-align:center; background:#fff;">
                            <a href="{item["url"]}" style="text-decoration:none; color:#333;">
                                <div style="height:120px; display:flex; align-items:center; justify-content:center; margin-bottom:10px;">
                                    <img src="{item["img"]}" style="max-height:100px; max-width:100%; object-fit:contain;">
                                </div>
                                <div style="font-size:13px; font-weight:600; line-height:1.2; height:3em; overflow:hidden;">{p_name}</div>
                            </a>
                        </div>'''
                    p_html += '</div></div>'
                    p_html += '<style>.promo-grid-scroll::-webkit-scrollbar {height:6px;} .promo-grid-scroll::-webkit-scrollbar-thumb {background:#ccc; border-radius:10px;}</style>'
                    row_data['Promo HTML'] = p_html

            # --- –ù–ï–ô–†–û–°–ï–¢–¨ (PERPLEXITY) ---

            # 1. –¢–ï–ö–°–¢
            if use_text and client:
                # –ü–µ—Ä–µ–¥–∞–µ–º client, –∞ –Ω–µ api_key
                blocks = generate_ai_content_blocks(client, base_text_raw if base_text_raw else "", page['name'], header_for_ai, num_text_blocks_val, current_page_seo_words)
                for i, b in enumerate(blocks): row_data[f'Text_Block_{i+1}'] = b

            # 2. –¢–ê–ë–õ–ò–¶–´
            if use_tables and client:
                for t_i, t_topic in enumerate(table_prompts):
                    ctx = f"–î–∞–Ω–Ω—ã–µ: {tech_context_final_str}" if tech_context_final_str else ""
                    prompt = f"Create HTML table for '{header_for_ai}'. Topic: {t_topic}. {ctx}. Requirements: Real data, raw HTML <table>, no Markdown."
                    try:
                        resp = client.chat.completions.create(model="sonar-pro", messages=[{"role": "user", "content": prompt}])
                        html = resp.choices[0].message.content.replace("```html", "").replace("```", "").strip()
                        if "<table" not in html: html = f"<table>{html}</table>"
                        row_data[f'Table_{t_i+1}_HTML'] = html
                    except Exception as e: row_data[f'Table_{t_i+1}_HTML'] = f"Error: {e}"

            # 3. GEO
            if use_geo and client and global_geo_list:
                cities = ", ".join(random.sample(global_geo_list, min(20, len(global_geo_list))))
                prompt = f"Write HTML <p> regarding delivery of '{header_for_ai}' to: {cities}. No Markdown."
                try:
                    resp = client.chat.completions.create(model="sonar-pro", messages=[{"role": "user", "content": prompt}])
                    row_data['IP_PROP4819'] = resp.choices[0].message.content.replace("```html", "").replace("```", "").strip()
                except Exception as e: row_data['IP_PROP4819'] = f"Error: {e}"

            if use_sidebar: row_data['Sidebar HTML'] = full_sidebar_code
            final_data.append(row_data)
            progress_bar.progress((idx + 1) / total_steps)

        # --- –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ---
        df_result = pd.DataFrame(final_data)
        st.session_state.gen_result_df = df_result 
        
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            df_result.to_excel(writer, index=False)
        
        st.session_state.unified_excel_data = buffer.getvalue()
        
        status_box.update(label="‚úÖ –ö–æ–Ω–≤–µ–π–µ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω! –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã.", state="complete", expanded=False)

    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ—Å—Ç–æ –µ—Å—Ç—å –≤ –∫–ª—é—á–∞—Ö, –∞ –Ω–µ —Ä–∞–≤–Ω—ã None
    if st.session_state.get('unified_excel_data') is not None:
        st.success("–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω!")
        st.download_button(
            label="üì• –°–ö–ê–ß–ê–¢–¨ –ï–î–ò–ù–´–ô EXCEL",
            data=st.session_state.unified_excel_data,
            file_name="unified_content_gen.xlsx",
            mime="application/vnd.ms-excel",
            key="btn_dl_unified"
        )
# ==========================================
# 5. –ë–õ–û–ö –ü–†–ï–î–ü–†–û–°–ú–û–¢–†–ê (PREVIEW) - –§–ò–ù–ê–õ–¨–ù–´–ô
# ==========================================
with tab_wholesale_main: 
    if 'gen_result_df' in st.session_state and st.session_state.gen_result_df is not None:
        st.markdown("---")
        st.header("üëÄ –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
        
        df = st.session_state.gen_result_df
        
        # 1. –í—ã–±–æ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        page_options = df['Product Name'].tolist()
        selected_page_name = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞:", page_options, key="preview_selector")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–æ–∫—É –¥–∞–Ω–Ω—ã—Ö
        row = df[df['Product Name'] == selected_page_name].iloc[0]
        
        # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
        has_text = any(
            (f'Text_Block_{i}' in row and pd.notna(row[f'Text_Block_{i}']) and str(row[f'Text_Block_{i}']).strip())
            for i in range(1, 6)
        )
        
        table_cols = [c for c in df.columns if 'Table_' in c and '_HTML' in c and pd.notna(row[c]) and str(row[c]).strip()]
        has_tables = len(table_cols) > 0
        
        has_tags = 'Tags HTML' in row and pd.notna(row['Tags HTML']) and str(row['Tags HTML']).strip()
        has_sidebar = 'Sidebar HTML' in row and pd.notna(row['Sidebar HTML']) and str(row['Sidebar HTML']).strip()
        has_geo = 'IP_PROP4819' in row and pd.notna(row['IP_PROP4819']) and str(row['IP_PROP4819']).strip()
        
        # --- –ü–†–û–í–ï–†–ö–ê –ü–†–û–ú–û ---
        has_promo = 'Promo HTML' in row and pd.notna(row['Promo HTML']) and str(row['Promo HTML']).strip()
        
        has_visual = has_tags or has_sidebar or has_geo or has_promo # <-- –î–æ–±–∞–≤–∏–ª–∏ –ø—Ä–æ–º–æ –≤ —É—Å–ª–æ–≤–∏–µ

        # 3. –ê–∫—Ç–∏–≤–Ω—ã–µ –≤–∫–ª–∞–¥–∫–∏
        active_tabs = []
        if has_text: active_tabs.append("üìù –¢–µ–∫—Å—Ç")
        if has_tables: active_tabs.append("üß© –¢–∞–±–ª–∏—Ü—ã")
        if has_visual: active_tabs.append("üé® –í–∏–∑—É–∞–ª")

        st.markdown("""
        <style>
            .preview-box { border: 1px solid #e0e0e0; padding: 20px; border-radius: 8px; margin-bottom: 20px; }
            .preview-label { font-size: 12px; font-weight: bold; color: #888; text-transform: uppercase; margin-bottom: 5px; }
            
            /* –°—Ç–∏–ª–∏ —Ç–µ–≥–æ–≤ */
            .popular-tags { display: flex; flex-wrap: wrap; gap: 8px; }
            .tag-link { background: #f0f2f5; color: #277EFF; padding: 6px 12px; border-radius: 4px; text-decoration: none; font-size: 13px; font-weight: 500; border: 1px solid #e2e8f0; }
            .tag-link:hover { background: #e2e8f0; }
            
            /* –°—Ç–∏–ª–∏ –¢–∞–±–ª–∏—Ü */
            table { width: 100%; border-collapse: collapse; font-size: 14px; margin-bottom: 15px; }
            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
            th { background-color: #f8fafc; font-weight: bold; }
            
        /* –í–∞–∂–Ω–æ –¥–ª—è —Å–∫—Ä–æ–ª–ª–∞ –≤ –ø—Ä–µ–≤—å—é Streamlit */
        .promo-grid-scroll { 
            display: flex !important; 
            flex-wrap: nowrap !important; 
            overflow-x: auto !important; 
            gap: 15px; 
            -webkit-overflow-scrolling: touch;
        }
        .promo-card-item { 
            flex: 0 0 220px !important; 
            width: 220px !important; 
            border: 1px solid #eee; 
            border-radius: 8px; 
            padding: 10px; 
            background: #fff;
        }
            /* –°—Ç–∏–ª–∏ —Å–∫—Ä–æ–ª–ª–±–∞—Ä–∞ –¥–ª—è Chrome/Safari */
            .promo-grid::-webkit-scrollbar { height: 6px; }
            .promo-grid::-webkit-scrollbar-track { background: #f1f1f1; border-radius: 10px; }
            .promo-grid::-webkit-scrollbar-thumb { background: #ccc; border-radius: 10px; }
            .promo-grid::-webkit-scrollbar-thumb:hover { background: #277EFF; }
            
            .promo-card { 
                flex: 0 0 220px !important; /* –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —à–∏—Ä–∏–Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ */
                background: white;
                transition: transform 0.2s;
            }
            .promo-card:hover { transform: translateY(-5px); }
            
            /* –°–∞–π–¥–±–∞—Ä */
            .sidebar-wrapper ul { list-style-type: none; padding-left: 10px; }
            .level-1-header { font-weight: bold; margin-top: 10px; color: #277EFF; }
        </style>
        """, unsafe_allow_html=True)

        if not active_tabs:
            st.warning("‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç –ø—É—Å—Ç.")
        else:
            tabs_objects = st.tabs(active_tabs)
            tabs_map = dict(zip(active_tabs, tabs_objects))
            
            # --- –¢–ï–ö–°–¢ ---
            if "üìù –¢–µ–∫—Å—Ç" in tabs_map:
                with tabs_map["üìù –¢–µ–∫—Å—Ç"]:
                    st.subheader(row['Product Name'])
                    for i in range(1, 6):
                        col_key = f'Text_Block_{i}'
                        if col_key in row and pd.notna(row[col_key]):
                            content = str(row[col_key]).strip()
                            if content:
                                with st.container():
                                    st.caption(f"–ë–ª–æ–∫ {i}")
                                    st.markdown(f"<div class='preview-box'>{content}</div>", unsafe_allow_html=True)

            # --- –¢–ê–ë–õ–ò–¶–´ ---
            if "üß© –¢–∞–±–ª–∏—Ü—ã" in tabs_map:
                with tabs_map["üß© –¢–∞–±–ª–∏—Ü—ã"]:
                    for t_col in table_cols:
                        content = row[t_col]
                        clean_title = t_col.replace('_HTML', '').replace('_', ' ')
                        st.caption(clean_title)
                        st.markdown(content, unsafe_allow_html=True)

            # --- –í–ò–ó–£–ê–õ ---
            if "üé® –í–∏–∑—É–∞–ª" in tabs_map:
                with tabs_map["üé® –í–∏–∑—É–∞–ª"]:
                    # –í—ã–≤–æ–¥ –ü—Ä–æ–º–æ
                    if has_promo:
                         st.markdown('<div class="preview-label">–ü—Ä–æ–º–æ-–±–ª–æ–∫ (–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏)</div>', unsafe_allow_html=True)
                         st.markdown(f"<div class='preview-box'>{row['Promo HTML']}</div>", unsafe_allow_html=True)
                    
                    c1, c2 = st.columns(2)
                    with c1:
                        if has_tags:
                            st.markdown('<div class="preview-label">–¢–µ–≥–∏</div>', unsafe_allow_html=True)
                            st.markdown(f"<div class='preview-box'>{row['Tags HTML']}</div>", unsafe_allow_html=True)
                        if has_geo:
                            st.markdown('<div class="preview-label">–ì–µ–æ-–±–ª–æ–∫</div>', unsafe_allow_html=True)
                            st.markdown(f"<div class='preview-box'>{row['IP_PROP4819']}</div>", unsafe_allow_html=True)
                    with c2:
                        if has_sidebar:
                            st.markdown('<div class="preview-label">–°–∞–π–¥–±–∞—Ä</div>', unsafe_allow_html=True)
                            st.markdown(f"<div class='preview-box' style='max-height: 400px; overflow-y: auto;'>{row['Sidebar HTML']}</div>", unsafe_allow_html=True)









