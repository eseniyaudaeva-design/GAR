import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
from collections import Counter
import math
import inspect
import concurrent.futures
from urllib.parse import urlparse

# ==========================================
# 0. –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø
# ==========================================
def check_password():
    if st.session_state.get("authenticated"):
        return True
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown("""
            <style>
            .auth-container {
                display: flex; flex-direction: column; align-items: center;
                justify-content: center; padding: 2rem; background-color: white;
                border-radius: 10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); margin-top: 5rem;
            }
            </style>
            <div class="auth-container">
                <h3>üìä GAR PRO</h3>
                <h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3>
            </div>
        """, unsafe_allow_html=True)
        
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "jfV6Xel-Q7vp-_s2UYPO":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò –°–¢–ò–õ–ò (–ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –ü–ï–†–ï–ö–†–ê–°–ö–ê)
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO", page_icon="üìä")

DEFAULT_EXCLUDE_DOMAINS = [
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "ebay.com",
    "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", "pandao.ru",
    "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", "banki.ru", 
    "regmarkets.ru", "zoon.ru", "pulscen.ru", "prodoctorov.ru", "blizko.ru", 
    "domclick.ru", "satom.ru", "quto.ru", "edadeal.ru", "cataloxy.ru", 
    "irr.ru", "onliner.by", "shop.by", "deal.by", "yell.ru", "profi.ru", 
    "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by", "market.yandex.ru", 
    "youtube.com", "gosuslugi.ru", "dzen.ru", "2gis.by"
]
DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n–∫—É–ø–∏—Ç—å\n—Ü–µ–Ω–∞\n—à—Ç\n—Å–º\n–º–º\n–∫–≥\n–∫–≤\n–º2\n—Å—Ç—Ä\n—É–ª"
REGIONS = ["–ú–æ—Å–∫–≤–∞", "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫", "–ö–∞–∑–∞–Ω—å", "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥", "–°–∞–º–∞—Ä–∞", "–ß–µ–ª—è–±–∏–Ω—Å–∫", "–û–º—Å–∫", "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä", "–ö–∏–µ–≤ (UA)", "–ú–∏–Ω—Å–∫ (BY)", "–ê–ª–º–∞—Ç—ã (KZ)"]

# –¶–≤–µ—Ç–∞
PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF" 

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        
        /* 1. –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ñ–æ–Ω */
        .stApp {{
            background-color: #FFFFFF !important;
            color: {TEXT_COLOR} !important;
        }}
        
        html, body, p, li, h1, h2, h3, h4 {{
            font-family: 'Inter', sans-serif;
            color: {TEXT_COLOR} !important;
        }}

        /* 2. –≠–ª–µ–º–µ–Ω—Ç—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è */
        .stButton button {{
            background-color: {PRIMARY_COLOR} !important;
            color: white !important;
            border: none;
            border-radius: 6px;
        }}
        .stButton button:hover {{
            background-color: {PRIMARY_DARK} !important;
        }}
        
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important;
            color: {TEXT_COLOR} !important;
            border: 1px solid {BORDER_COLOR} !important;
        }}

        /* 3. –ê–ì–†–ï–°–°–ò–í–ù–´–ô –•–ê–ö –î–õ–Ø –¢–ê–ë–õ–ò–¶ (st.dataframe) */
        /* –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∫—Ä–∞—Å–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü—ã */
        [data-testid="stDataFrame"] th {{
            background-color: {HEADER_BG} !important;
            color: {PRIMARY_COLOR} !important;
            font-weight: bold !important;
            border-bottom: 2px solid {PRIMARY_COLOR} !important;
            text-align: center !important;
        }}
        
        /* –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∫—Ä–∞—Å–∏–º —è—á–µ–π–∫–∏ —Ç–∞–±–ª–∏—Ü—ã */
        [data-testid="stDataFrame"] td {{
            background-color: #FFFFFF !important;
            color: {TEXT_COLOR} !important;
            border-bottom: 1px solid {BORDER_COLOR} !important;
        }}
        
        /* –ò–Ω–¥–µ–∫—Å (–ª–µ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü) */
        [data-testid="stDataFrame"] th[role="rowheader"] {{
            color: {PRIMARY_COLOR} !important;
            background-color: {HEADER_BG} !important;
        }}
        
        /* –ü—Ä–∏ –Ω–∞–≤–µ–¥–µ–Ω–∏–∏ –Ω–∞ —Å—Ç—Ä–æ–∫—É */
        [data-testid="stDataFrame"] tr:hover td {{
            background-color: {LIGHT_BG_MAIN} !important;
        }}
        
        /* –£–±–∏—Ä–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–µ–º–Ω—ã–µ —Ä–∞–º–∫–∏ Streamlit */
        div[data-testid="stDataFrame"] {{
            border: 1px solid {BORDER_COLOR} !important;
        }}

        section[data-testid="stSidebar"] {{
            background-color: #FFFFFF;
            border-left: 1px solid {BORDER_COLOR};
        }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 2. –õ–û–ì–ò–ö–ê (–ë–≠–ö–ï–ù–î)
# ==========================================

if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False

try:
    if not hasattr(inspect, 'getargspec'):
        def getargspec(func):
            spec = inspect.getfullargspec(func)
            return spec.args, spec.varargs, spec.varkw, spec.defaults
        inspect.getargspec = getargspec
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except:
    morph = None
    USE_NLP = False

try:
    from googlesearch import search
    USE_SEARCH = True
except:
    USE_SEARCH = False

def process_text(text, settings, n_gram=1):
    pattern = r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+' if settings['numbers'] else r'[–∞-—è–ê-–Ø—ë–Åa-zA-Z]+'
    words = re.findall(pattern, text.lower())
    stops = set(w.lower() for w in settings['custom_stops'])
    clean_words = []
    
    for w in words:
        if len(w) < 2 or w in stops: continue
        lemma = w
        if USE_NLP and n_gram == 1: 
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            lemma = p.normal_form
        clean_words.append(lemma)
    
    if n_gram > 1:
        ngrams = []
        for i in range(len(clean_words) - n_gram + 1):
            phrase = " ".join(clean_words[i:i+n_gram])
            ngrams.append(phrase)
        return ngrams
    return clean_words

def parse_page(url, settings):
    headers = {'User-Agent': settings['ua']}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code != 200: return None
        soup = BeautifulSoup(r.text, 'html.parser')
        
        if settings['noindex']:
            for t in soup.find_all(['noindex', 'script', 'style', 'head', 'footer', 'nav']): t.decompose()
        else:
            for t in soup(['script', 'style', 'head']): t.decompose()
            
        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)
        
        extra_text = []
        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])
        body_text = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        
        return {
            'url': url, 'domain': urlparse(url).netloc, 
            'body_text': body_text, 'anchor_text': anchor_text
        }
    except: return None

def calculate_metrics(comp_data, my_data, settings):
    if not my_data or not my_data['body_text']:
        my_lemmas = []
        my_anchors = []
        my_len = 0
    else:
        my_lemmas = process_text(my_data['body_text'], settings)
        my_anchors = process_text(my_data['anchor_text'], settings)
        my_len = len(my_lemmas)
    
    comp_docs = []
    for p in comp_data:
        body = process_text(p['body_text'], settings)
        anchor = process_text(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor})
        
    if not comp_docs:
        return {"depth": pd.DataFrame(), "hybrid": pd.DataFrame(), "ngrams": pd.DataFrame(), "relevance_top": pd.DataFrame(), "my_score": {"width": 0, "depth": 0}}

    avg_len = np.mean([len(d['body']) for d in comp_docs])
    norm_k = (my_len / avg_len) if (settings['norm'] and avg_len > 0) else 1.0
    
    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    
    N = len(comp_docs)
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
        
    k1, b = 1.2, 0.75
    table_depth, table_hybrid = [], []
    
    for word in vocab:
        df = doc_freqs[word]
        if df < 2 and word not in my_lemmas: continue 
        
        my_tf = my_lemmas.count(word)
        my_anch_tf = my_anchors.count(word)
        
        c_body_tfs = [d['body'].count(word) for d in comp_docs]
        c_anch_tfs = [d['anchor'].count(word) for d in comp_docs]
        
        med_tf = np.median(c_body_tfs)
        med_anch = np.median(c_anch_tfs)
        max_tf = np.max(c_body_tfs)
        
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        
        target_body = int(med_tf * 1.3 * norm_k)
        diff_body = target_body - my_tf
        target_anch = int(med_anch * norm_k)
        diff_anch = target_anch - my_anch_tf
        
        if med_tf > 0.5 or my_tf > 0:
            table_depth.append({
                "–°–ª–æ–≤–æ": word,
                "–£ –≤–∞—Å (TF)": my_tf, 
                "–ú–µ–¥–∏–∞–Ω–∞": round(med_tf, 1),
                "–ú–∞–∫—Å–∏–º—É–º": int(max_tf * norm_k),
                "–î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": diff_body, 
                "diff_abs": abs(diff_body),
                "–¢–µ–≥ A —É –≤–∞—Å": my_anch_tf,
                "–¢–µ–≥ A —Ä–µ–∫–æ–º.": target_anch
            })
            table_hybrid.append({
                "–°–ª–æ–≤–æ": word, 
                "TF-IDF –¢–û–ü": round(med_tf * idf, 2), 
                "TF-IDF —É –≤–∞—Å": round(my_tf * idf, 2),
                "–°–∞–π—Ç–æ–≤": df, 
                "–ü–µ—Ä–µ—Å–ø–∞–º": max_tf
            })

    table_ngrams = []
    if comp_docs and my_data and 'body_text' in my_data:
        try:
            my_bi = process_text(my_data['body_text'], settings, 2)
            comp_bi = [process_text(p['body_text'], settings, 2) for p in comp_data if p and 'body_text' in p]
            
            all_bi = set(my_bi)
            for c in comp_bi:
                if c: all_bi.update(c)
            bi_freqs = Counter()
            for c in comp_bi:
                if c:
                    for b_ in set(c): bi_freqs[b_] += 1

            for bg in all_bi:
                df = bi_freqs[bg]
                if df < 2 and bg not in my_bi: continue
                my_c = my_bi.count(bg)
                comp_c = [c['body'].count(bg) for c in comp_docs if 'body' in c]
                med_c = np.median(comp_c) if comp_c else 0
                
                if med_c > 0 or my_c > 0:
                    table_ngrams.append({
                        "N-–≥—Ä–∞–º–º–∞": bg, 
                        "–°–∞–π—Ç–æ–≤": df, 
                        "–ú–µ–¥–∏–∞–Ω–∞": med_c,
                        "–ù–∞ —Å–∞–π—Ç–µ": my_c,
                        "TF-IDF": round(my_c * math.log(N/df if df>0 else 1), 3)
                    })
        except Exception as e:
            st.error(f"Error n-grams: {e}")

    table_rel = []
    for i, p in enumerate(comp_data):
        p_lemmas = process_text(p['body_text'], settings)
        w = len(set(p_lemmas).intersection(vocab))
        table_rel.append({
            "–î–æ–º–µ–Ω": p['domain'], "–ü–æ–∑–∏—Ü–∏—è": i+1, "URL": p['url'],
            "–®–∏—Ä–∏–Ω–∞": w, "–ì–ª—É–±–∏–Ω–∞": len(p_lemmas)
        })
        
    return {
        "depth": pd.DataFrame(table_depth), 
        "hybrid": pd.DataFrame(table_hybrid),
        "ngrams": pd.DataFrame(table_ngrams), 
        "relevance_top": pd.DataFrame(table_rel),
        "my_score": {"width": len(set(my_lemmas).intersection(vocab)), "depth": len(my_lemmas)}
    }

# ==========================================
# 3. –§–£–ù–ö–¶–ò–Ø –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø (DATAFRAME + SORT)
# ==========================================

def render_paginated_table(df, title_text, key_prefix, sort_by_col=None, use_abs_sort=False):
    """
    –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ —á–µ—Ä–µ–∑ st.dataframe (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É –≤ —à–∞–ø–∫–µ),
    –Ω–æ —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º CSS –∏ Pandas Styler –¥–ª—è –±–µ–ª–æ–≥–æ —Ñ–æ–Ω–∞.
    """
    if df.empty:
        st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")
        return

    # 1. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ (–ù–∞—á–∞–ª—å–Ω–∞—è)
    if sort_by_col and sort_by_col in df.columns:
        if use_abs_sort:
            df['_abs_sort'] = df[sort_by_col].abs()
            df = df.sort_values(by='_abs_sort', ascending=False).drop(columns=['_abs_sort'])
        else:
            df = df.sort_values(by=sort_by_col, ascending=False)
            
    # 2. –ò–Ω–¥–µ–∫—Å —Å 1
    df = df.reset_index(drop=True)
    df.index = df.index + 1
    
    # 3. –ü–∞–≥–∏–Ω–∞—Ü–∏—è (10 —Å—Ç—Ä–æ–∫)
    ROWS_PER_PAGE = 10 
    if f'{key_prefix}_page' not in st.session_state:
        st.session_state[f'{key_prefix}_page'] = 1
        
    total_rows = len(df)
    total_pages = math.ceil(total_rows / ROWS_PER_PAGE)
    current_page = st.session_state[f'{key_prefix}_page']
    
    if current_page > total_pages: current_page = total_pages
    if current_page < 1: current_page = 1
    
    start_idx = (current_page - 1) * ROWS_PER_PAGE
    end_idx = start_idx + ROWS_PER_PAGE
    
    df_view = df.iloc[start_idx:end_idx]

    # 4. –ü–û–ö–†–ê–°–ö–ê –Ø–ß–ï–ï–ö (Pandas Styler - fallback)
    # –≠—Ç–æ –∫—Ä–∞—Å–∏—Ç —Å–∞–º—É —Ç–∞–±–ª–∏—Ü—É –∏–∑–Ω—É—Ç—Ä–∏ –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ CSS –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç
    def style_dataframe(d):
        return d.style.set_properties(**{
            'background-color': '#FFFFFF',
            'color': '#3D4858',
        })

    st.markdown(f"### {title_text}")
    
    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    st.dataframe(
        style_dataframe(df_view),
        use_container_width=True,
        column_config={"diff_abs": None}
    )
    
    # 5. –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    c_spacer, c_btn_prev, c_info, c_btn_next = st.columns([6, 1, 1, 1])
    
    with c_btn_prev:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_prev", disabled=(current_page <= 1), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] -= 1
            st.rerun()
            
    with c_info:
        st.markdown(f"<div style='text-align: center; margin-top: 10px; color:{TEXT_COLOR}'><b>{current_page}</b> / {total_pages}</div>", unsafe_allow_html=True)
        
    with c_btn_next:
        if st.button("‚û°Ô∏è", key=f"{key_prefix}_next", disabled=(current_page >= total_pages), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] += 1
            st.rerun()
            
    st.markdown("---")

# ==========================================
# 4. –ò–ù–¢–ï–†–§–ï–ô–°
# ==========================================

col_main, col_sidebar = st.columns([65, 35]) 

with col_main:
    st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")

    st.markdown("### URL –∏–ª–∏ –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –í–∞—à–µ–≥–æ —Å–∞–π—Ç–∞")
    my_input_type = st.radio(
        "–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", 
        ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], 
        horizontal=True, label_visibility="collapsed", key="my_page_source_radio"
    )

    my_url = ""
    my_page_content = ""
    if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
        my_url = st.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã", placeholder="https://site.ru/catalog/tovar", label_visibility="collapsed", key="my_url_input")
    elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
        my_page_content = st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏–ª–∏ —Ç–µ–∫—Å—Ç", height=200, label_visibility="collapsed", placeholder="–í—Å—Ç–∞–≤—å—Ç–µ HTML", key="my_content_input")

    st.markdown("### –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å")
    query = st.text_input("–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞", label_visibility="collapsed", key="query_input")

    st.markdown("### –ü–æ–∏—Å–∫ –∏–ª–∏ URL —Å—Ç—Ä–∞–Ω–∏—Ü –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
    source_type_new = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤", ["–ü–æ–∏—Å–∫", "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"], horizontal=True, label_visibility="collapsed", key="competitor_source_radio")
    source_type = "Google (–ê–≤—Ç–æ)" if source_type_new == "–ü–æ–∏—Å–∫" else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫" 

    if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
        st.markdown("### –í–≤–µ–¥–∏—Ç–µ —Å–ø–∏—Å–æ–∫ URL")
        st.text_area("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫–∏ –∑–¥–µ—Å—å (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", height=200, key="manual_urls_ui")

    st.markdown("### –†–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º—ã–µ —Å–ø–∏—Å–∫–∏")
    excludes = st.text_area("–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–æ–º–µ–Ω—ã", DEFAULT_EXCLUDE, height=200, key="settings_excludes")
    c_stops = st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞", DEFAULT_STOPS, height=200, key="settings_stops")

    st.markdown("---")
    
    if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", type="primary", use_container_width=True, key="start_analysis_btn"):
        for key in list(st.session_state.keys()):
            if key.endswith('_page'):
                st.session_state[key] = 1
        st.session_state.start_analysis_flag = True

with col_sidebar:
    st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    ua = st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
    search_engine = st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["Google", "–Ø–Ω–¥–µ–∫—Å", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
    region = st.selectbox("–Ø–Ω–¥–µ–∫—Å / –†–µ–≥–∏–æ–Ω", REGIONS, key="settings_region")
    device = st.selectbox("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", ["Desktop", "Mobile"], key="settings_device")
    top_n = st.selectbox("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¢–û–ü", [10, 20, 30], index=1, key="settings_top_n")
    st.selectbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ url", ["–í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", "–ì–ª–∞–≤–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", "–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], key="settings_url_type")
    st.selectbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —Ç–∏–ø", ["–í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", "–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ", "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ"], key="settings_content_type")
    
    col_c1, col_c2 = st.columns(2)
    with col_c1:
        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å noindex/script", True, key="settings_noindex")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", False, key="settings_alt")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", False, key="settings_numbers")
    with col_c2:
        st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True, key="settings_norm")
        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã", True, key="settings_agg")

# ==========================================
# 5. –û–ë–†–ê–ë–û–¢–ö–ê –ò –í–´–í–û–î (Session State Logic)
# ==========================================

if st.session_state.get('start_analysis_flag'):
    st.session_state.start_analysis_flag = False

    if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ" and not st.session_state.get('my_url_input'):
        st.error("–í–≤–µ–¥–∏—Ç–µ URL!")
        st.stop()
        
    if my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç" and not st.session_state.get('my_content_input', '').strip():
        st.error("–í–≤–µ–¥–∏—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥!")
        st.stop()

    settings = {
        'noindex': st.session_state.settings_noindex, 
        'alt_title': st.session_state.settings_alt, 
        'numbers': st.session_state.settings_numbers,
        'norm': st.session_state.settings_norm, 
        'ua': st.session_state.settings_ua, 
        'custom_stops': st.session_state.settings_stops.split()
    }
    
    target_urls = []
    if source_type == "Google (–ê–≤—Ç–æ)":
        excl = [d.strip() for d in st.session_state.settings_excludes.split('\n') if d.strip()]
        if st.session_state.settings_agg: excl.extend(["avito", "ozon", "wildberries", "market", "tiu", "youtube"])
        
        try:
            with st.spinner(f"–°–±–æ—Ä –¢–û–ü–∞..."):
                if not USE_SEARCH:
                    st.error("–ù–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ googlesearch")
                    st.stop()
                found = search(st.session_state.query_input, num_results=st.session_state.settings_top_n * 2, lang="ru")
                cnt = 0
                for u in found:
                    if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ" and st.session_state.my_url_input in u: continue
                    if any(x in urlparse(u).netloc for x in excl): continue
                    target_urls.append(u)
                    cnt += 1
                    if cnt >= st.session_state.settings_top_n: break
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            st.stop()
    else:
        raw_urls = st.session_state.get("manual_urls_ui", "")
        target_urls = [u.strip() for u in raw_urls.split('\n') if u.strip()]

    if not target_urls:
        st.error("–ù–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.")
        st.stop()
        
    my_data = None
    if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
        with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
            my_data = parse_page(st.session_state.my_url_input, settings)
    elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
        my_data = {'url': 'Local', 'domain': 'local', 'body_text': st.session_state.my_content_input, 'anchor_text': ''}

    comp_data = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(parse_page, u, settings): u for u in target_urls}
        done = 0
        total = len(target_urls)
        prog = st.progress(0)
        stat = st.empty()
        for f in concurrent.futures.as_completed(futures):
            res = f.result()
            if res: comp_data.append(res)
            done += 1
            prog.progress(done / total)
            stat.text(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: {done}/{total}")
    prog.empty()
    stat.empty()

    with st.spinner("–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö..."):
        st.session_state.analysis_results = calculate_metrics(comp_data, my_data, settings)
        st.session_state.analysis_done = True
        st.rerun()

if st.session_state.analysis_done and st.session_state.analysis_results:
    results = st.session_state.analysis_results
    
    st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
    
    st.markdown(f"""
        <div style='background-color: {LIGHT_BG_MAIN}; padding: 15px; border-radius: 8px; border: 1px solid {BORDER_COLOR}; margin-bottom: 20px;'>
            <h4 style='margin:0; color: {PRIMARY_COLOR};'>–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞</h4>
            <p style='margin:5px 0 0 0;'>–®–∏—Ä–∏–Ω–∞ (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞): <b>{results['my_score']['width']}</b> | –ì–ª—É–±–∏–Ω–∞ (–≤—Å–µ–≥–æ —Å–ª–æ–≤): <b>{results['my_score']['depth']}</b></p>
        </div>
    """, unsafe_allow_html=True)

    render_paginated_table(
        results['depth'], 
        "1. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≥–ª—É–±–∏–Ω–µ (–î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å —Å–ª–æ–≤–∞)", 
        "tbl_depth_1", 
        sort_by_col="–î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å", 
        use_abs_sort=True
    )

    render_paginated_table(
        results['hybrid'], 
        "3. –ì–∏–±—Ä–∏–¥–Ω—ã–π –¢–û–ü (TF-IDF)", 
        "tbl_hybrid", 
        sort_by_col="TF-IDF –¢–û–ü", 
        use_abs_sort=False
    )

    render_paginated_table(
        results['ngrams'], 
        "4. N-–≥—Ä–∞–º–º—ã (–§—Ä–∞–∑—ã)", 
        "tbl_ngrams", 
        sort_by_col="TF-IDF", 
        use_abs_sort=False
    )

    render_paginated_table(
        results['relevance_top'], 
        "5. –¢–û–ü —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤", 
        "tbl_rel", 
        sort_by_col="–®–∏—Ä–∏–Ω–∞", 
        use_abs_sort=False
    )
