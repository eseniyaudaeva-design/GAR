import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin
import inspect
import time
import json
import io
import os

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ openai, –µ—Å–ª–∏ –Ω–µ—Ç - –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
try:
    import openai
except ImportError:
    openai = None

# ==========================================
# 0. –ü–ê–¢–ß –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò
# ==========================================
if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO + AI", page_icon="üìä")

# ==========================================
# 2. –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø
# ==========================================
def check_password():
    if st.session_state.get("authenticated"):
        return True
    
    st.markdown("""
        <style>
        .main { display: flex; flex-direction: column; justify-content: center; align-items: center; }
        .auth-logo-box { text-align: center; margin-bottom: 1rem; }
        .login-box h3 { margin-top: 0; text-align: center; }
        </style>
    """, unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="login-box"><h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3>', unsafe_allow_html=True)
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "jfV6Xel-Q7vp-_s2UYPO":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
        st.markdown('</div>', unsafe_allow_html=True)
    return False

if not check_password():
    st.stop()

# ==========================================
# 3. –°–¢–ò–õ–ò –ò –ù–ê–°–¢–†–û–ô–ö–ò
# ==========================================
ARSENKIN_TOKEN = "43acbbb60cb7989c05914ff21be45379"

REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω—ã
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956},
    "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931},
    "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–ö–∏–µ–≤ (UA)": {"ya": 143, "go": 1012852},
    "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493},
    "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601}
}

DEFAULT_EXCLUDE_DOMAINS = ["yandex.ru", "avito.ru", "ozon.ru", "wildberries.ru", "youtube.com", "vk.com"]
DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n–∫—É–ø–∏—Ç—å\n—Ü–µ–Ω–∞\n—à—Ç\n—Å–º\n–º–º\n–∫–≥\n–∫–≤\n–º2\n—Å—Ç—Ä\n—É–ª"

PRIMARY_COLOR = "#277EFF"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"

st.markdown(f"""
    <style>
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
    </style>
""", unsafe_allow_html=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP
try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except Exception:
    morph = None
    USE_NLP = False

# ==========================================
# 4. –§–£–ù–ö–¶–ò–ò (–û–ë–©–ò–ï –ò SEO)
# ==========================================

def get_arsenkin_urls(query, engine_type, region_name, depth_val=10):
    # (–ö–æ–¥ —Ñ—É–Ω–∫—Ü–∏–∏ get_arsenkin_urls –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏–∑ –≤–∞—à–µ–≥–æ —Ñ–∞–π–ª–∞)
    # ... –î–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏ –æ—Å—Ç–∞–≤–ª—è—é –ª–æ–≥–∏–∫—É —Ç–∞–∫–æ–π –∂–µ ...
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check" 
    url_get = "https://arsenkin.ru/api/tools/get"
    
    headers = {"Authorization": f"Bearer {ARSENKIN_TOKEN}", "Content-type": "application/json"}
    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []
    if "–Ø–Ω–¥–µ–∫—Å" in engine_type: se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type: se_params.append({"type": 11, "region": reg_ids['go']})
        
    payload = {
        "tools_name": "check-top",
        "data": {"queries": [query], "is_snippet": False, "noreask": True, "se": se_params, "depth": depth_val}
    }
    
    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        resp_json = r.json()
        if "task_id" not in resp_json: return []
        task_id = resp_json["task_id"]
    except: return []
    
    status = "process"
    attempts = 0
    while status == "process" and attempts < 40:
        time.sleep(3)
        attempts += 1
        try:
            r_check = requests.post(url_check, headers=headers, json={"task_id": task_id})
            if r_check.json().get("status") == "finish": status = "done"
        except: pass
            
    if status != "done": return []
    try:
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id})
        collect = r_final.json().get('result', {}).get('result', {}).get('collect', [])
        results_list = []
        if collect and isinstance(collect, list) and len(collect) > 0:
            if isinstance(collect[0], list) and len(collect[0]) > 0 and isinstance(collect[0][0], list):
                 # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
                final_url_list = collect[0][0]
                for idx, u in enumerate(final_url_list):
                    results_list.append({'url': u, 'pos': idx + 1})
            else:
                # –°—Ç–∞—Ä—ã–π/—Å–º–µ—à–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                unique_urls = set()
                for engine_data in collect:
                    if isinstance(engine_data, dict):
                        for _, serps in engine_data.items():
                            for item in serps:
                                u = item.get('url')
                                if u and u not in unique_urls:
                                    results_list.append({'url': u, 'pos': item.get('pos')})
                                    unique_urls.add(u)
        return results_list
    except: return []

def process_text_detailed(text, settings, n_gram=1):
    if settings['numbers']: pattern = r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+' 
    else: pattern = r'[–∞-—è–ê-–Ø—ë–Åa-zA-Z]+'
    words = re.findall(pattern, text.lower())
    stops = set(w.lower() for w in settings['custom_stops'])
    lemmas = []
    forms_map = defaultdict(set)
    for w in words:
        if len(w) < 2 or w in stops: continue
        lemma = w
        if USE_NLP and n_gram == 1: 
            p = morph.parse(w)[0]
            if any(t in p.tag for t in ['PREP', 'CONJ', 'PRCL', 'NPRO']): continue
            lemma = p.normal_form
        lemmas.append(lemma)
        forms_map[lemma].add(w)
    return lemmas, forms_map

def parse_page(url, settings):
    headers = {'User-Agent': settings['ua']}
    try:
        r = requests.get(url, headers=headers, timeout=10)
        if r.status_code != 200: return None
        soup = BeautifulSoup(r.text, 'html.parser')
        for t in soup.find_all(['script', 'style', 'head', 'noindex', 'nav', 'footer']): t.decompose()
        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)
        extra_text = []
        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
        body_text = re.sub(r'\s+', ' ', soup.get_text(separator=' ') + " " + " ".join(extra_text)).strip()
        if not body_text: return None 
        return {'url': url, 'domain': urlparse(url).netloc, 'body_text': body_text, 'anchor_text': anchor_text}
    except: return None

def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞, –ø–æ–ª–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞
    # ... (–ª–æ–≥–∏–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ TF-IDF, —à–∏—Ä–∏–Ω—ã/–≥–ª—É–±–∏–Ω—ã) ...
    # –ó–¥–µ—Å—å –ø—Ä–æ—Å—Ç–æ –∑–∞–≥–ª—É—à–∫–∞, –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —á—Ç–æ–±—ã –Ω–µ –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å 300 —Å—Ç—Ä–æ–∫
    # –í —Ä–µ–∞–ª—å–Ω–æ–º —Ñ–∞–π–ª–µ –æ—Å—Ç–∞–≤—å—Ç–µ –≤–∞—à—É —Ñ—É–Ω–∫—Ü–∏—é calculate_metrics –∫–∞–∫ –µ—Å—Ç—å.
    
    # --- –í–ê–ñ–ù–û: –í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ –ø–æ–ª–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é calculate_metrics –∏–∑ –≤–∞—à–µ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ ---
    # –î–ª—è —Ä–∞–±–æ—Ç—ã —Å–∫—Ä–∏–ø—Ç–∞ —è –≤–µ—Ä–Ω—É –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –∫–∞–∫ –±—É–¥—Ç–æ —Ä–∞—Å—á–µ—Ç –ø—Ä–æ—à–µ–ª
    
    return {
        "depth": pd.DataFrame(), "hybrid": pd.DataFrame(), 
        "relevance_top": pd.DataFrame(), "my_score": {"width": 0, "depth": 0},
        "missing_semantics_high": [], "missing_semantics_low": []
    }

# ==========================================
# 5. –ù–û–í–´–ô –ú–û–î–£–õ–¨: PERPLEXITY GENERATION
# ==========================================

STATIC_DATA_GEN = {
    'IP_PROP4817': "–£—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏",
    'IP_PROP4818': "–û–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –≤ —Ä–µ–≥–∏–æ–Ω—ã —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4819': """<p>–ù–∞–¥–µ–∂–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫–∞–∑–∞...</p>""",
    'IP_PROP4820': """<p>–ù–∞—à–∏ –∏–∑–¥–µ–ª–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è...</p>""",
    'IP_PROP4821': "–û–ø–ª–∞—Ç–∞ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤:",
    'IP_PROP4822': """<p>–ù–∞—à–∞ –∫–æ–º–ø–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞ –ø—Ä–∏–Ω—è—Ç—å –ª—é–±—ã–µ –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–µ –≤–∏–¥—ã –æ–ø–ª–∞—Ç—ã...</p>""",
    'IP_PROP4823': """<div class="h4"><h3>–ü—Ä–∏–º–µ—Ä—ã –≤–æ–∑–º–æ–∂–Ω–æ–π –æ–ø–ª–∞—Ç—ã</h3></div>...""",
    'IP_PROP4824': "–û–ø–∏—Å–∞–Ω–∏–µ, —Å—Ç–∞—Ç—å–∏, –ø–æ–∏—Å–∫, –æ—Ç–∑—ã–≤—ã, –Ω–æ–≤–æ—Å—Ç–∏, –∞–∫—Ü–∏–∏, –∂—É—Ä–Ω–∞–ª, info:",
    'IP_PROP4825': "–ú–æ–∂–µ–º –º–µ—Ç–∞–ª–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –æ—Ü–∏–Ω–∫–æ–≤–∞—Ç—å, –Ω–∏–∫–µ–ª–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–æ–≤–æ–ª–æ—á—å",
    'IP_PROP4826': "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥",
    'IP_PROP4834': "–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –±–µ–∑ –ø—Ä–∏–º–µ—Å–µ–π",
    'IP_PROP4835': "–ü–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫",
    'IP_PROP4836': "–ö–∞—á–µ—Å—Ç–≤–æ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä",
    'IP_PROP4837': "–ü–æ—Ä—è–¥–æ–∫ –≤ –ì–û–°–¢–∞—Ö"
}

def get_page_data_for_gen(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
    except Exception as e:
        return None, None, f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}"
    
    if response.status_code != 200:
        return None, None, f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—É—Å–∞: {response.status_code}"

    soup = BeautifulSoup(response.text, 'html.parser')
    
    # –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è
    description_div = soup.find('div', class_='description-container')
    base_text = description_div.get_text(separator="\n", strip=True) if description_div else ""
    if not base_text:
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫, –µ—Å–ª–∏ –∫–ª–∞—Å—Å –¥—Ä—É–≥–æ–π
        base_text = soup.body.get_text(separator="\n", strip=True)[:5000]

    # –ü–æ–∏—Å–∫ —Ç–µ–≥–æ–≤
    tags_container = soup.find(class_='popular-tags-inner')
    tags_data = []
    if tags_container:
        links = tags_container.find_all('a')
        for link in links:
            tag_name = link.get_text(strip=True)
            tag_url = link.get('href')
            if tag_url:
                tag_url = urljoin(url, tag_url)
            tags_data.append({'name': tag_name, 'url': tag_url})
    
    return base_text, tags_data, None

def generate_five_blocks(client, base_text, tag_name):
    if not base_text: return ["Error: No base text"] * 5

    system_instruction = """
    –¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä.
    –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –Ω–∞–ø–∏—Å–∞—Ç—å 5 –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ HTML.
    –í–ê–ñ–ù–û: –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π markdown –æ–±–µ—Ä—Ç–∫–∏ (```html). –ü–∏—à–∏ —Å—Ä–∞–∑—É —á–∏—Å—Ç—ã–π –∫–æ–¥.
    """

    user_prompt = f"""
    –í–í–û–î–ù–´–ï:
    –¢–æ–≤–∞—Ä: "{tag_name}".
    –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: \"\"\"{base_text[:3000]}\"\"\"

    –ó–ê–î–ê–ß–ê:
    –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Ä–æ–≤–Ω–æ 5 —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤.

    –°–¢–†–£–ö–¢–£–†–ê –ö–ê–ñ–î–û–ì–û –ë–õ–û–ö–ê:
    1. –ó–∞–≥–æ–ª–æ–≤–æ–∫ (<h2> –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–ª–æ–∫–∞, <h3> –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö).
    2. –ê–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞.
    3. –í–≤–æ–¥–Ω–∞—è —Ñ—Ä–∞–∑–∞ (–∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –¥–≤–æ–µ—Ç–æ—á–∏–µ–º).
    4. –°–ø–∏—Å–æ–∫ <ul> –∏–ª–∏ <ol> (—ç–ª–µ–º–µ–Ω—Ç—ã –∑–∞–∫–∞–Ω—á–∏–≤–∞—é—Ç—Å—è —Ç–æ—á–∫–æ–π —Å –∑–∞–ø—è—Ç–æ–π, –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ—á–∫–æ–π).
    5. –ó–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–π –∞–±–∑–∞—Ü.

    –í–´–í–û–î:
    –†–∞–∑–¥–µ–ª–∏ –±–ª–æ–∫–∏ —Å—Ç—Ä–æ–≥–æ —Å—Ç—Ä–æ–∫–æ–π: |||BLOCK_SEP|||
    """

    try:
        response = client.chat.completions.create(
            model="sonar-pro", 
            messages=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7
        )
        content = response.choices[0].message.content
        
        # –ß–∏—Å—Ç–∫–∞ –æ—Ç –º–∞—Ä–∫–¥–∞—É–Ω–∞
        content = content.replace("```html", "").replace("```", "")
        
        blocks = content.split("|||BLOCK_SEP|||")
        clean_blocks = [b.strip() for b in blocks if b.strip()]
        
        while len(clean_blocks) < 5:
            clean_blocks.append("")
            
        return clean_blocks[:5]

    except Exception as e:
        return [f"API Error: {str(e)}"] * 5


# ==========================================
# 6. –ì–õ–ê–í–ù–´–ô –ò–ù–¢–ï–†–§–ï–ô–°
# ==========================================

# –°–∞–π–¥–±–∞—Ä –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–æ–≤
with st.sidebar:
    st.markdown("## üõ† –ú–µ–Ω—é")
    app_mode = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:", ["üìä SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (–ì–ê–†)", "ü§ñ AI –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (Perplexity)"])
    st.markdown("---")

# ------------------------------------------
# –†–ï–ñ–ò–ú 1: SEO –ê–ù–ê–õ–ò–ó–ê–¢–û–† (–í–∞—à —Å—Ç–∞—Ä—ã–π –∫–æ–¥)
# ------------------------------------------
if app_mode == "üìä SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (–ì–ê–†)":
    # (–ó–¥–µ—Å—å –≤—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –í–ï–°–¨ UI –∫–æ–¥ –∏–∑ –±–ª–æ–∫–∞ `with col_main:` –≤–∞—à–µ–≥–æ —Ñ–∞–π–ª–∞)
    # –î–ª—è —Ä–∞–±–æ—Ç—ã –ø—Ä–∏–º–µ—Ä–∞ —è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥—É –æ—Å–Ω–æ–≤–Ω—ã–µ —á–∞—Å—Ç–∏ UI
    
    col_main, col_sidebar_seo = st.columns([65, 35])
    
    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")
        st.info("–†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞ –≤–∫–ª—é—á–µ–Ω. –ó–∞–ø–æ–ª–Ω–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∏–∂–µ.")
        
        my_input_type = st.radio("–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], horizontal=True, label_visibility="collapsed")
        
        my_url = ""
        my_content = ""
        if "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è" in my_input_type:
            my_url = st.text_input("URL –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã", key="seo_my_url")
        elif "–ò—Å—Ö–æ–¥–Ω—ã–π" in my_input_type:
            my_content = st.text_area("HTML –∫–æ–¥", key="seo_my_html")
            
        st.markdown("### –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å")
        query = st.text_input("–ó–∞–ø—Ä–æ—Å", key="seo_query")
        
        st.markdown("### –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã")
        source_type = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫", ["API Arsenkin", "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"], horizontal=True, key="seo_source")
        
        if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", type="primary"):
            st.warning("‚ö†Ô∏è –í —ç—Ç–æ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ —Ñ—É–Ω–∫—Ü–∏—è `calculate_metrics` –∑–∞–≥–ª—É—à–µ–Ω–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞. –í—Å—Ç–∞–≤—å—Ç–µ —Å–≤–æ—é –ø–æ–ª–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Ä–∞–±–æ—Ç—ã –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞.")
            # –ó–¥–µ—Å—å –≤—ã–∑–æ–≤ calculate_metrics(....)
    
    with col_sidebar_seo:
        st.markdown("##### ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ SEO")
        st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google"], key="seo_engine")
        st.selectbox("–†–µ–≥–∏–æ–Ω", list(REGION_MAP.keys()), key="seo_region")

# ------------------------------------------
# –†–ï–ñ–ò–ú 2: AI –ì–ï–ù–ï–†–ê–¶–ò–Ø (–ù–æ–≤—ã–π –∫–æ–¥)
# ------------------------------------------
elif app_mode == "ü§ñ AI –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (Perplexity)":
    st.title("AI –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¢–µ–∫—Å—Ç–æ–≤ (Perplexity)")
    st.markdown("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML-–±–ª–æ–∫–æ–≤ –¥–ª—è –ø–æ–¥—Ñ–∏–ª—å—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ —Å–∞–π–¥–±–∞—Ä–µ –¥–ª—è —ç—Ç–æ–≥–æ —Ä–µ–∂–∏–º–∞
    with st.sidebar:
        st.markdown("### üîë API –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        api_key_input = st.text_input("Perplexity API Key", type="password", help="–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∫–ª—é—á, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 'pplx-'")
        if not api_key_input:
            st.warning("–í–≤–µ–¥–∏—Ç–µ API –∫–ª—é—á –¥–ª—è —Ä–∞–±–æ—Ç—ã!")

    # –û—Å–Ω–æ–≤–Ω–∞—è —Ñ–æ—Ä–º–∞
    target_url = st.text_input("URL –°—Ç—Ä–∞–Ω–∏—Ü—ã (–≥–¥–µ –±—Ä–∞—Ç—å —Ç–µ–≥–∏/—Ç–æ–≤–∞—Ä—ã)", placeholder="https://site.ru/catalog/category/")
    
    if st.button("üöÄ –ù–∞—á–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é", type="primary", disabled=not api_key_input):
        if not openai:
            st.error("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ `openai` –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞! `pip install openai`")
            st.stop()
            
        if not target_url:
            st.error("–í–≤–µ–¥–∏—Ç–µ URL!")
            st.stop()
            
        # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞
        try:
            client = openai.OpenAI(api_key=api_key_input, base_url="https://api.perplexity.ai")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–∞: {e}")
            st.stop()

        # 2. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        with st.status("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã...", expanded=True) as status:
            base_text, tags, error = get_page_data_for_gen(target_url)
            
            if error:
                status.update(label="–û—à–∏–±–∫–∞!", state="error")
                st.error(error)
                st.stop()
                
            if not tags:
                status.update(label="–¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!", state="error")
                st.warning("–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ –Ω–∞–π–¥–µ–Ω –±–ª–æ–∫ `popular-tags-inner` –∏–ª–∏ —Å—Å—ã–ª–∫–∏ –≤ –Ω–µ–º.")
                st.stop()
                
            status.update(label=f"–ù–∞–π–¥–µ–Ω–æ —Ç–µ–≥–æ–≤: {len(tags)}. –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...", state="running")
            
            all_rows = []
            prog_bar = st.progress(0)
            
            # 3. –¶–∏–∫–ª –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            for i, tag in enumerate(tags):
                tag_name = tag['name']
                st.write(f"‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞: **{tag_name}** ({i+1}/{len(tags)})")
                
                blocks = generate_five_blocks(client, base_text, tag_name)
                
                # –°–±–æ—Ä —Å—Ç—Ä–æ–∫–∏
                row = {
                    'TagName': tag_name,
                    'URL': tag['url'],
                    'IP_PROP4839': blocks[0],
                    'IP_PROP4816': blocks[1],
                    'IP_PROP4838': blocks[2],
                    'IP_PROP4829': blocks[3],
                    'IP_PROP4831': blocks[4],
                    # –°—Ç–∞—Ç–∏–∫–∞
                    **STATIC_DATA_GEN
                }
                all_rows.append(row)
                
                prog_bar.progress((i + 1) / len(tags))
                time.sleep(0.5) # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å UI
            
            status.update(label="–ì–æ—Ç–æ–≤–æ!", state="complete")
            
        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
        if all_rows:
            st.success("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            
            df = pd.DataFrame(all_rows)
            # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
            cols = [
                'TagName', 'URL', 
                'IP_PROP4839', 'IP_PROP4817', 'IP_PROP4818', 'IP_PROP4819', 'IP_PROP4820', 
                'IP_PROP4821', 'IP_PROP4822', 'IP_PROP4823', 'IP_PROP4824',
                'IP_PROP4816', 'IP_PROP4825', 'IP_PROP4826', 
                'IP_PROP4834', 'IP_PROP4835', 'IP_PROP4836', 'IP_PROP4837',
                'IP_PROP4838', 'IP_PROP4829', 'IP_PROP4831'
            ]
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—Å—Ç—å –≤ df
            final_cols = [c for c in cols if c in df.columns]
            df = df[final_cols]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Excel –≤ –ø–∞–º—è—Ç—å
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                df.to_excel(writer, index=False, sheet_name='Sheet1')
            
            st.download_button(
                label="üì• –°–∫–∞—á–∞—Ç—å Excel —Ñ–∞–π–ª",
                data=buffer.getvalue(),
                file_name="seo_texts_result.xlsx",
                mime="application/vnd.ms-excel"
            )
            
            with st.expander("–ü—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫)"):
                st.dataframe(df.head())
