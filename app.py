# ==========================================
# –®–ê–ì 0: –ê–£–¢–ï–ù–¢–ò–§–ò–ö–ê–¶–ò–Ø
# ==========================================

CORRECT_PASSWORD = "garpro"

password_input = widgets.Password(
    placeholder='–í–≤–µ–¥–∏—Ç–µ –ø–∞—Ä–æ–ª—å –¥–ª—è –¥–æ—Å—Ç—É–ø–∞',
    description='–ü–∞—Ä–æ–ª—å:',
    layout=widgets.Layout(width='300px')
)
login_button = widgets.Button(
    description='–í–æ–π—Ç–∏',
    button_style='info',
    layout=widgets.Layout(width='100px')
)
auth_output = widgets.Output()

# –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ UI (–∏–∑–Ω–∞—á–∞–ª—å–Ω–æ —Å–∫—Ä—ã—Ç)
main_ui_container = widgets.VBox([], layout=widgets.Layout(display='none', border='1px solid #CCC', padding='15px', background_color='#F7F7F7'))
bn_run = widgets.Button(
    description='–ê–ù–ê–õ–ò–ó–ò–†–û–í–ê–¢–¨ üöÄ',
    button_style='warning',
    layout=widgets.Layout(width='99%', height='50px', margin='20px 0', display='none')
)
output_log = widgets.Output()


def check_password(b):
    with auth_output:
        clear_output()
        if password_input.value == CORRECT_PASSWORD:
            print("‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞. –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞...")
            
            # –°–∫—Ä—ã–≤–∞–µ–º –ª–æ–≥–∏–Ω-—Ñ–æ—Ä–º—É
            password_input.layout.display = 'none'
            login_button.layout.display = 'none'
            
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π UI –∏ –∫–Ω–æ–ø–∫—É –∑–∞–ø—É—Å–∫–∞
            main_ui_container.layout.display = 'block'
            bn_run.layout.display = 'block'
            
            # –í—ã–≤–æ–¥–∏–º –≥–ª–∞–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
            display(widgets.HTML("<h2>–ì–∏–±—Ä–∏–¥–Ω—ã–π –ê–Ω–∞–ª–∏–∑ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ PRO</h2>"))
            display(main_ui_container)
            display(bn_run)
            display(output_log)
            
        else:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")

login_button.on_click(check_password)

# –í—ã–≤–æ–¥–∏–º —Å–Ω–∞—á–∞–ª–∞ —Ç–æ–ª—å–∫–æ —Ñ–æ—Ä–º—É –ª–æ–≥–∏–Ω–∞
display(widgets.HTML("<h2>–ì–∏–±—Ä–∏–¥–Ω—ã–π –ê–Ω–∞–ª–∏–∑ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ PRO: –í—Ö–æ–¥</h2>"))
display(widgets.HBox([password_input, login_button]))
display(auth_output)

# ==========================================
# –®–ê–ì 1: –£–°–¢–ê–ù–û–í–ö–ê –ò –ì–ê–†–ê–ù–¢–ò–Ø –†–ê–ë–û–¢–´ PYMORPHY2
# ==========================================
print("‚è≥ –ó–∞–ø—É—Å–∫ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫...")
!pip install googlesearch-python beautifulsoup4 requests pandas numpy ipywidgets -q
!pip install pymorphy2 --upgrade --force-reinstall -q

import requests
from bs4 import BeautifulSoup
try:
    from googlesearch import search
    USE_SEARCH = True
except ImportError:
    USE_SEARCH = False

import pandas as pd
import numpy as np
import re
import ipywidgets as widgets
from IPython.display import display, clear_output
from collections import Counter
import math
import warnings
import inspect
import sys

try:
    if sys.version_info >= (3, 10):
        if not hasattr(inspect, 'getargspec'):
            def getargspec(func):
                spec = inspect.getfullargspec(func)
                return spec.args, spec.varargs, spec.varkw, spec.defaults
            inspect.getargspec = getargspec

    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    print("‚úÖ Pymorphy2 —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
    USE_NLP = True
except Exception as e:
    print("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: Pymorphy2 –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø—Ä–æ—Å—Ç–æ–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–∏–µ.")
    morph = None
    USE_NLP = False

warnings.filterwarnings("ignore")

BLACKLIST_DOMAINS = [
    'avito.ru', 'wikipedia.org', 'yandex.ru', 'ozon.ru', 'wildberries.ru', 'tiu.ru',
    'beru.ru', 'aliexpress.com', 'youtube.com', 'dzen.ru', 'hh.ru',
    'market.yandex.ru', 'sbermegamarket.ru', 'rutube.ru', 't.me', 'instagram.com',
    'gosuslugi.ru', 'rambler.ru', '2gis.ru', 'sravni.ru', 'toshop.ru', 'price.ru',
    'pandao.ru', 'banki.ru', 'regmarkets.ru', 'zoon.ru', 'pulscen.ru', 'prodoctorov.ru',
    'blizko.ru', 'domclick.ru', 'satom.ru', 'quto.ru', 'edadeal.ru', 'cataloxy.ru', 
    'irr.ru', 'onliner.by', 'shop.by', 'deal.by', 'yell.ru', 'profi.ru', 
    'irecommend.ru', 'otzovik.com', 'auto.ru'
]

# ==========================================
# –®–ê–ì 2: –õ–û–ì–ò–ö–ê (BACKEND) - –ò–ó –§–ê–ô–õ–ê
# (–í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Å get_word_forms –¥–æ run_analysis)
# ==========================================

def get_word_forms(lemma):
    if not USE_NLP or not morph:
        return f"–¢–æ–∫–µ–Ω: {lemma}"
    if not lemma: return ""
    forms = []
    parses = morph.parse(lemma)
    if not parses: return ""
    base_parse = parses[0]
    for tag in base_parse.lexeme:
        forms.append(tag.word)
        if len(forms) >= 5:
            break
    return ", ".join(list(set(forms)))

def process_words(word_list, settings):
    base_stop_words = {
        '–∏', '–≤', '–Ω–∞', '—Å', '–∫', '–ø–æ', '–∑–∞', '–æ—Ç', '–¥–æ', '—ç—Ç–æ', '–º—ã', '–≤—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏', '–∏—Ö', '–µ–µ', '–µ–≥–æ', '–º–Ω–µ',
        '—Ç–µ–±–µ', '—Å–µ–±–µ', '–¥–ª—è', '—á—Ç–æ', '–∫–∞–∫', '—Ç–∞–∫', '–Ω–æ', '–∏–ª–∏', '–∞', '—á—Ç–æ–±—ã', '–∂–µ', '–±—ã', '–¥–∞', '–Ω–µ—Ç', '—É', '–±–µ–∑', '–ø–æ–¥',
        '–Ω–∞–¥', '–ø–µ—Ä–µ–¥', '–ø—Ä–∏', '—á–µ—Ä–µ–∑', '–º–µ–∂–¥—É', '—Å—Ä–µ–¥–∏', '–ø–æ—Å–ª–µ', '–≤–º–µ—Å—Ç–æ', '–æ–∫–æ–ª–æ', '–≤–æ–∫—Ä—É–≥', '—Å–æ', '–∏–∑', '–∏–∑-–∑–∞', '–∏–∑-–ø–æ–¥',
        '—Ç–æ–ª—å–∫–æ', '–¥–∞–∂–µ', '—Ö–æ—Ç—å', '–ª–∏', '–Ω–∏', '—Ä–∞–∑–≤–µ', '—É–∂–µ', '–µ—â–µ', '–≤—Å—ë', '–≤—Å–µ', '–∫–æ–≥–¥–∞', '–≥–¥–µ', '–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞', '–ø–æ—á–µ–º—É',
        '–∑–∞—á–µ–º', '–∫–∞–∫–æ–π', '–∫–æ—Ç–æ—Ä—ã–π', '–∫—Ç–æ', '—á—Ç–æ', '–≤–µ—Å—å', '—Å–≤–æ–π', '—Ç–∞–∫–æ–π', 
        '—Å–∞–º—ã–π', '–º–Ω–æ–≥–æ', '–º–∞–ª–æ', '–Ω–µ—Å–∫–æ–ª—å–∫–æ', '–Ω–µ–º–Ω–æ–≥–æ',
        '–æ—á–µ–Ω—å', '–ø—Ä–æ—Å—Ç–æ', '—Å–æ–≤—Å–µ–º', '–æ–ø—è—Ç—å', '—Å–Ω–æ–≤–∞', '–∑–¥–µ—Å—å', '—Ç–∞–º', '—Å—é–¥–∞', '—Ç—É–¥–∞', '–Ω–∏–∫–æ–≥–¥–∞', '–≤—Å–µ–≥–¥–∞', '–æ–±—ã—á–Ω–æ', '—á–∞—Å—Ç–æ',
        '—Ä–µ–¥–∫–æ', '–ø–æ—á—Ç–∏', '–ø–æ—ç—Ç–æ–º—É', '–ø–æ—Ç–æ–º', '—Ä–∞–Ω—å—à–µ', '–ø–æ–∑–∂–µ', '—Ä–∞–Ω–Ω–∏–π', '–ø–æ–∑–¥–Ω–∏–π', '–Ω–æ–≤—ã–π', '—Å—Ç–∞—Ä—ã–π', '–±–æ–ª—å—à–æ–π', '–º–∞–ª–µ–Ω—å–∫–∏–π',
        '—Ö–æ—Ä–æ—à–∏–π', '–ø–ª–æ—Ö–æ–π', '–ª—É—á—à–∏–π', '—Ö—É–¥—à–∏–π', '–æ–¥–∏–Ω', '–¥–≤–∞', '—Ç—Ä–∏', '—á–µ—Ç—ã—Ä–µ', '–ø—è—Ç—å', '—à–µ—Å—Ç—å', '—Å–µ–º—å', '–≤–æ—Å–µ–º—å', '–¥–µ–≤—è—Ç—å', '–¥–µ—Å—è—Ç—å',
        '—Ä—É–±–ª–µ–π', '—Ä—É–±', '—Å—Ç—Ä', '—É–ª', '—à—Ç', '—Å–º', '–º–º', '–º–ª', '–∫–≥', '–∫–≤', '–º', '–º2', '—Å–º2', '–º¬≤', '—Å–º¬≤'
    }
    
    if settings.get('custom_stops'):
        base_stop_words.update(set(settings['custom_stops']))

    if not USE_NLP or not morph:
        return [w.lower() for w in word_list if len(w) > 2 and w.lower() not in base_stop_words]

    lemmas = []
    for word in word_list:
        word_lower = word.lower()
        if len(word) > 2 and word_lower not in base_stop_words:
            p = morph.parse(word_lower)[0]
            if 'PREP' not in p.tag and 'CONJ' not in p.tag and 'NUMR' not in p.tag and 'PRCL' not in p.tag:
                lemmas.append(p.normal_form)
    return lemmas

def clean_and_tokenize(html_content, settings):
    soup = BeautifulSoup(html_content, 'html.parser')

    if settings.get('noindex', True):
        for noindex in soup.find_all('noindex'):
            noindex.decompose()

    for script in soup(["script", "style", "head", "footer", "nav", "header", "aside"]):
        script.extract()

    text_parts = [soup.get_text(separator=' ')]

    if settings.get('alt_title', False):
        for img in soup.find_all('img', alt=True):
            text_parts.append(img['alt'])
        for tag in soup.find_all(title=True):
            text_parts.append(tag['title'])

    full_text = " ".join(text_parts)

    if settings.get('numbers', False):
         words = re.findall(r'[–∞-—è–ê-–Ø—ë–Å0-9]+', full_text)
    else:
         words = re.findall(r'[–∞-—è–ê-–Ø—ë–Å]+', full_text)

    return " ".join(process_words(words, settings))

def clean_anchor_text(html_content, settings):
    soup = BeautifulSoup(html_content, 'html.parser')
    anchor_words = []
    for a_tag in soup.find_all('a'):
        text = a_tag.get_text(strip=True)
        if text:
            words = re.findall(r'[–∞-—è–ê-–Ø—ë–Å]+', text)
            anchor_words.extend(process_words(words, settings))

    return " ".join(anchor_words)

def get_page_data(url, user_agent, settings):
    headers = {'User-Agent': user_agent}
    try:
        response = requests.get(url.strip(), headers=headers, timeout=20)
        response.raise_for_status() 
        html = response.text
        return clean_and_tokenize(html, settings), clean_anchor_text(html, settings) 
    except Exception as e:
        return "", ""

def manual_vectorize_and_analyze(corpus_body, corpus_anchor, my_idx):

    all_tokens_list = [token for doc in corpus_body for token in doc.split()]
    feature_names = sorted(list(set(all_tokens_list)))

    count_vectors = []
    doc_freq = Counter()
    N_docs = len(corpus_body)

    for i, doc in enumerate(corpus_body):
        counts = Counter(doc.split())
        vector = [counts.get(token, 0) for token in feature_names]
        count_vectors.append(vector)

        for token in set(doc.split()):
            doc_freq[token] += 1

    anchor_vectors = []
    for doc in corpus_anchor:
        counts = Counter(doc.split())
        vector = [counts.get(token, 0) for token in feature_names]
        anchor_vectors.append(vector)


    idf_values = {}
    for token in feature_names:
        df = doc_freq[token]
        idf = math.log(N_docs / df) + 1
        idf_values[token] = idf

    tfidf_vectors = []
    for count_vector, doc in zip(count_vectors, corpus_body):
        doc_tokens = doc.split()
        doc_len = len(doc_tokens)

        tf_vector = [count / doc_len if doc_len > 0 else 0 for count in count_vector]
        tfidf_vector = [tf * idf_values.get(token, 0) for tf, token in zip(tf_vector, feature_names)]
        tfidf_vectors.append(tfidf_vector)

    dense_tfidf = np.array(tfidf_vectors)
    dense_count = np.array(count_vectors)
    dense_anchor = np.array(anchor_vectors)

    comp_tfidf_matrix = dense_tfidf[:my_idx]
    my_tfidf_vector = dense_tfidf[my_idx]

    comp_count_matrix = dense_count[:my_idx]
    my_count_vector = dense_count[my_idx]

    comp_anchor_matrix = dense_anchor[:my_idx]
    my_anchor_vector = dense_anchor[my_idx]

    return feature_names, comp_tfidf_matrix, my_tfidf_vector, comp_count_matrix, my_count_vector, comp_anchor_matrix, my_anchor_vector

def run_analysis(my_url_id, competitors_urls, settings, my_body_content=None, my_anchor_content=None):
    
    if my_url_id == "No_Page_Mode":
        my_body, my_anchor = "", ""
    elif my_body_content is not None:
        my_body, my_anchor = my_body_content, my_anchor_content
    else:
        print(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –í–∞—à–µ–≥–æ —Å–∞–π—Ç–∞: {my_url_id}...")
        my_body, my_anchor = get_page_data(my_url_id, settings['user_agent'], settings)
    
    if not my_body and my_url_id not in ["No_Page_Mode", "Manual_Code_Input"]:
        print("‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL –∏–ª–∏ User-Agent.")
        return None

    corpus_body, corpus_anchor = [], []

    print(f"üì• –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(competitors_urls)} –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤...")
    for url in competitors_urls:
        body_text, anchor_text = get_page_data(url, settings['user_agent'], settings)
        if len(body_text) > 50:
            corpus_body.append(body_text)
            corpus_anchor.append(anchor_text)

    if len(corpus_body) < 2:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—Å–∫–∞—á–∞–Ω–æ –º–µ–Ω–µ–µ 2 –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤).")
        return None

    my_idx = len(corpus_body)

    my_body_len = len(my_body.split())
    comp_body_lengths = [len(doc.split()) for doc in corpus_body] 
    avg_comp_body_len = np.mean(comp_body_lengths) if comp_body_lengths else 1.0

    length_normalization_factor = 1.0
    if settings.get('normalize', False) and avg_comp_body_len > 0:
        length_normalization_factor = my_body_len / avg_comp_body_len 
        
    corpus_body.append(my_body)
    corpus_anchor.append(my_anchor)

    feature_names, comp_tfidf_matrix, my_tfidf_vector, comp_count_matrix, my_count_vector, comp_anchor_matrix, my_anchor_vector = \
        manual_vectorize_and_analyze(corpus_body, corpus_anchor, my_idx)

    results = []
    TARGET_FACTOR = 1.3
    PERCENT_OUTPUT = settings.get('percent_output', False)

    for col in range(len(feature_names)):
        token = feature_names[col]

        comp_tfidf_col = comp_tfidf_matrix[:, col]
        comp_count_col = comp_count_matrix[:, col]
        comp_anchor_col = comp_anchor_matrix[:, col]

        median_tfidf = float(np.median(comp_tfidf_col)) if comp_tfidf_col.size > 0 else 0
        median_count = float(np.median(comp_count_col)) if comp_count_col.size > 0 else 0
        median_anchor_count = float(np.median(comp_anchor_col)) if comp_anchor_col.size > 0 else 0

        my_tfidf = float(my_tfidf_vector[col])
        my_count = float(my_count_vector[col])
        my_anchor_count = float(my_anchor_vector[col])

        # --- BODY COUNT CALCULATION ---
        target_body_count = int(median_count * TARGET_FACTOR * length_normalization_factor)
        rec_body_count = target_body_count - int(my_count)

        rec_body_text = "0"
        if PERCENT_OUTPUT:
            if target_body_count > 0:
                current_coverage_percent = (my_count / target_body_count) * 100
                if current_coverage_percent < 100: rec_body_text = f" +{abs(100 - current_coverage_percent):.0f}%"
                elif current_coverage_percent > 100: rec_body_text = f" -{abs(current_coverage_percent - 100):.0f}%"
                else: rec_body_text = "0%"
            elif my_count > 0: rec_body_text = f" -100%" 
            else: rec_body_text = "0%"
        else:
            if rec_body_count > 0: rec_body_text = f" +{abs(rec_body_count)}"
            elif rec_body_count < 0: rec_body_text = f" {rec_body_count}"
            else: rec_body_text = "0"


        # --- ANCHOR COUNT CALCULATION ---
        target_anchor_count = int(median_anchor_count * TARGET_FACTOR * length_normalization_factor)
        rec_anchor_count = target_anchor_count - int(my_anchor_count)

        rec_anchor_text = "0"
        if PERCENT_OUTPUT:
            if target_anchor_count > 0:
                current_coverage_percent = (my_anchor_count / target_anchor_count) * 100
                if current_coverage_percent < 100: rec_anchor_text = f" +{abs(100 - current_coverage_percent):.0f}%"
                elif current_coverage_percent > 100: rec_anchor_text = f" -{abs(current_coverage_percent - 100):.0f}%"
                else: rec_anchor_text = "0%"
            elif my_anchor_count > 0: rec_anchor_text = f" -100%" 
            else: rec_anchor_text = "0%"
        else:
            if rec_anchor_count > 0: rec_anchor_text = f" +{abs(rec_anchor_count)}"
            elif rec_anchor_count < 0: rec_anchor_text = f" {rec_anchor_count}"
            else: rec_anchor_text = "0"


        # 6. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Å–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        is_relevant = median_tfidf > 0.05
        is_actionable = (rec_body_text != '0' and rec_body_text != '0%') or \
                        (rec_anchor_text != '0' and rec_anchor_text != '0%')

        if is_relevant or is_actionable:

            lemma_name = token if USE_NLP else f"–¢–æ–∫–µ–Ω: {token}"

            results.append({
                "–°–ª–æ–≤–æ (–õ–µ–º–º–∞)": lemma_name,
                "–°–ª–æ–≤–æ—Ñ–æ—Ä–º—ã": get_word_forms(token),
                "TF-IDF (–í—ã)": round(my_tfidf, 3),
                "–ú–µ–¥–∏–∞–Ω–∞ (–¢–û–ü)": round(median_tfidf, 3),
                "–¢–µ–∫—Å—Ç (–†–µ–∫.)": rec_body_text,
                "–¢–µ–∫—Å—Ç (–ü–æ–≤—Ç.)": int(my_count),
                "–¢–µ–≥ <a> (–†–µ–∫.)": rec_anchor_text,
                "–¢–µ–≥ <a> (–ü–æ–≤—Ç.)": int(my_anchor_count)
            })

    # 7. –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
    df = pd.DataFrame(results)
    if not df.empty:
        def extract_abs_value(rec_str):
            if rec_str == '0' or rec_str == '0%': return 0
            return abs(float(re.sub(r'[+\- %]', '', rec_str)))

        df['Sort_Body_Abs'] = df['–¢–µ–∫—Å—Ç (–†–µ–∫.)'].apply(extract_abs_value)
        df['Sort_Anchor_Abs'] = df['–¢–µ–≥ <a> (–†–µ–∫.)'].apply(extract_abs_value)

        df = df.sort_values(by=['Sort_Body_Abs', 'Sort_Anchor_Abs', '–ú–µ–¥–∏–∞–Ω–∞ (–¢–û–ü)'], ascending=[False, False, False])
        df = df.drop(columns=['Sort_Body_Abs', 'Sort_Anchor_Abs'])

        df_filtered = df[(df['–¢–µ–∫—Å—Ç (–†–µ–∫.)'] != '0') & (df['–¢–µ–∫—Å—Ç (–†–µ–∫.)'] != '0%') |
                         (df['–¢–µ–≥ <a> (–†–µ–∫.)'] != '0') & (df['–¢–µ–≥ <a> (–†–µ–∫.)'] != '0%')]

        return df_filtered
    return None


# ==========================================
# –®–ê–ì 3: –ò–ù–¢–ï–†–§–ï–ô–° (UI) - –£–õ–£–ß–®–ï–ù–ù–´–ô –î–ò–ó–ê–ô–ù
# ==========================================

style_header = "font-size: 16px; font-weight: bold; margin-top: 10px; margin-bottom: 5px; color: #1E293B;"
w_layout = widgets.Layout(width='99%')
w_half_layout = widgets.Layout(width='50%')

# --- 1. –°–µ–∫—Ü–∏—è: –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏ ---
html_task = widgets.HTML(f"<div style='{style_header}'>1Ô∏è‚É£ –í–∞—à–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏ –∑–∞–ø—Ä–æ—Å</div>")
r_input_type = widgets.RadioButtons(options=['–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ', '–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç', '–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã'], value='–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ', layout=widgets.Layout(width='100%'))
w_my_url = widgets.Text(placeholder="https://site.ru/catalog/page", layout=w_layout) 
w_source_code = widgets.Textarea(placeholder="–í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ HTML –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏...", layout=widgets.Layout(width='99%', height='200px', display='none')) 
w_query = widgets.Text(placeholder="–û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å", layout=w_layout)
chk_extra_queries = widgets.Checkbox(value=False, description='–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã')
w_extra_queries_text = widgets.Textarea(placeholder="–ö–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏", layout=widgets.Layout(width='99%', height='60px', display='none'))

def toggle_input_mode(change):
    mode = change['new']
    w_my_url.layout.display = 'block' if mode == '–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ' else 'none'
    w_source_code.layout.display = 'block' if mode == '–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç' else 'none'
    chk_norm.disabled = mode == '–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã'
    chk_norm.value = not chk_norm.disabled

r_input_type.observe(toggle_input_mode, names='value')
def toggle_extra_queries(change): w_extra_queries_text.layout.display = 'block' if change['new'] else 'none'
chk_extra_queries.observe(toggle_extra_queries, names='value')

task_box = widgets.VBox([
    html_task, 
    r_input_type, 
    w_my_url, 
    w_source_code, 
    w_query,
    chk_extra_queries,
    w_extra_queries_text,
    widgets.HTML("<hr style='border-top: 1px solid #DDD;'>")
], layout=widgets.Layout(border='1px solid #CCC', padding='10px', margin='0 0 10px 0', background_color='#FFFFFF'))


# --- 2. –°–µ–∫—Ü–∏—è: –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã ---
html_comp = widgets.HTML(f"<div style='{style_header}'>2Ô∏è‚É£ –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ–∏–ª—å—Ç—Ä—ã</div>")
r_comp_source = widgets.RadioButtons(options=['–ü–æ–∏—Å–∫', '–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤'], value='–ü–æ–∏—Å–∫', layout=widgets.Layout(width='100%'))
w_engine = widgets.Dropdown(options=['Google', '–Ø–Ω–¥–µ–∫—Å (–ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç!)'], value='Google', description='–°–∏—Å—Ç–µ–º–∞:', layout=w_half_layout)
w_region = widgets.Dropdown(options=['–ú–æ—Å–∫–≤–∞', '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥', '–†–æ—Å—Å–∏—è', '–°–ù–ì'], value='–ú–æ—Å–∫–≤–∞', description='–†–µ–≥–∏–æ–Ω:', layout=w_half_layout)
w_device = widgets.Dropdown(options=['Desktop', 'Mobile'], value='Desktop', description='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:', layout=w_half_layout)
w_top_count = widgets.Dropdown(options=[5, 10, 15, 20, 30], value=10, description='–¢–û–ü:', layout=w_half_layout)
w_exclude_domains = widgets.Textarea(value="\n".join(BLACKLIST_DOMAINS), description='–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å:', placeholder='–î–æ–º–µ–Ω—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è', layout=w_layout)
w_manual_comps = widgets.Textarea(placeholder="https://competitor1.ru\nhttps://competitor2.ru", layout=widgets.Layout(width='99%', height='150px', display='none'))

comp_settings_col = widgets.VBox([
    widgets.HTML("<b>–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞</b>"),
    widgets.HBox([w_engine, w_region], layout=w_layout),
    widgets.HBox([w_device, w_top_count], layout=w_layout),
], layout=w_half_layout)

comp_exclude_col = widgets.VBox([
    widgets.HTML("<b>–î–æ–º–µ–Ω—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è (–ø–æ –¥–æ–º–µ–Ω—É/URL)</b>"),
    w_exclude_domains,
], layout=w_half_layout)

comp_search_settings = widgets.VBox([
    widgets.HBox([comp_settings_col, comp_exclude_col]),
], layout=widgets.Layout(display='block'))

def toggle_comp_source(change):
    comp_search_settings.layout.display = 'block' if change['new'] == '–ü–æ–∏—Å–∫' else 'none'
    w_manual_comps.layout.display = 'block' if change['new'] == '–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤' else 'none'
r_comp_source.observe(toggle_comp_source, names='value')

comp_box = widgets.VBox([
    html_comp, 
    r_comp_source,
    comp_search_settings,
    w_manual_comps,
    widgets.HTML("<hr style='border-top: 1px solid #DDD;'>")
], layout=widgets.Layout(border='1px solid #CCC', padding='10px', margin='0 0 10px 0', background_color='#FFFFFF'))


# --- 3. –°–µ–∫—Ü–∏—è: –ù–∞—Å—Ç—Ä–æ–π–∫–∏ (–î–í–ï –ö–û–õ–û–ù–ö–ò) ---
html_settings = widgets.HTML(f"<div style='{style_header}'>3Ô∏è‚É£ –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏</div>")
w_perfect_url = widgets.Text(placeholder="https://site.ru/ (–ì–ª–∞–≤–Ω—ã–π –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç)", layout=w_layout)
chk_norm = widgets.Checkbox(value=True, description='–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ–±—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è (–ú–µ–¥–∏–∞–Ω–∞, –ø–µ—Ä–µ—Å–ø–∞–º)')
chk_percent = widgets.Checkbox(value=False, description='–í—ã–≤–æ–¥–∏—Ç—å –æ–±—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö')
chk_aggr = widgets.Checkbox(value=True, description='–ò—Å–∫–ª—é—á–∏—Ç—å –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã –∏ type-in —Ç—Ä–∞—Ñ–∏–∫ (—Å–ø–∏—Å–æ–∫ –≤—ã—à–µ)')
chk_noindex = widgets.Checkbox(value=True, description='–ò—Å–∫–ª—é—á–∞—Ç—å —Ç–µ–∫—Å—Ç –≤ —Ç–µ–≥–µ noindex')
chk_alt = widgets.Checkbox(value=False, description='–£—á–∏—Ç—ã–≤–∞—Ç—å –∞—Ç—Ä–∏–±—É—Ç—ã alt –∏ title')
chk_num = widgets.Checkbox(value=False, description='–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞')
chk_stop_pos = widgets.Checkbox(value=True, description='–ò—Å–∫–ª—é—á–∞—Ç—å —Å–ª—É–∂–µ–±–Ω—ã–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏') 
chk_extra_data = widgets.Checkbox(value=False, description='–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ)')

main_settings_col = widgets.VBox([
    widgets.HTML("<b>–û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã</b>"),
    w_perfect_url,
    chk_norm, 
    chk_percent,
    chk_aggr,
    chk_noindex, 
    chk_alt, 
    chk_num,
    chk_extra_data,
], layout=widgets.Layout(width='50%', padding='5px'))

# –ö–æ–ª–æ–Ω–∫–∞ –ë: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏ –°—Ç–æ–ø-—Å–ª–æ–≤–∞
w_user_agent = widgets.Text(value="Mozilla/5.0 (compatible; Artur2k/1.0;)", description='User-Agent:', layout=w_layout)
chk_stop_custom = widgets.Checkbox(value=True, description='–ò—Å–∫–ª—é—á–∞—Ç—å —Å–≤–æ–π —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤')
default_stops = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n—Å—Ç—Ä\n—É–ª\n—à—Ç\n—Å–º\n–º–º\n–º–ª\n–∫–≥\n–∫–≤\n–º¬≤\n—Å–º¬≤\n–º2\n—Å–º2"
w_stop_custom_text = widgets.Textarea(value=default_stops, layout=w_layout)
chk_depth_formula = widgets.Checkbox(value=False, description='–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–æ—Ä–º—É–ª—ã –≥–ª—É–±–∏–Ω—ã')
w_depth_top = widgets.Dropdown(options=['–¢–û–ü3', '–¢–û–ü5', '–¢–û–ü10', '–¢–û–ü20'], value='–¢–û–ü5', description='–°–ª–æ–≤:', layout=widgets.Layout(display='none', width='100%'))
w_depth_count = widgets.Checkbox(value=True, description='–£—á–∏—Ç—ã–≤–∞—Ç—å –∫–æ–ª-–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤', layout=widgets.Layout(display='none', width='100%'))

def toggle_stop_custom(change): w_stop_custom_text.layout.display = 'block' if change['new'] else 'none'
chk_stop_custom.observe(toggle_stop_custom, names='value')

def toggle_depth(change):
    vis = 'block' if change['new'] else 'none'
    w_depth_top.layout.display = vis
    w_depth_count.layout.display = vis
chk_depth_formula.observe(toggle_depth, names='value')

tech_settings_col = widgets.VBox([
    widgets.HTML("<b>–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏</b>"),
    w_user_agent,
    chk_stop_pos,
    chk_stop_custom, 
    w_stop_custom_text,
    chk_depth_formula, 
    w_depth_top, 
    w_depth_count,
], layout=widgets.Layout(width='50%', padding='5px'))

settings_hbox = widgets.HBox([main_settings_col, tech_settings_col], layout=widgets.Layout(justify_content='space-between', width='100%'))

settings_box = widgets.VBox([
    html_settings,
    settings_hbox
], layout=widgets.Layout(border='1px solid #CCC', padding='10px', margin='0 0 10px 0', background_color='#FFFFFF'))


# --- –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–ë–û–†–ö–ê –ò–ù–¢–ï–†–§–ï–ô–°–ê ---
ui_elements = widgets.VBox([
    task_box,
    comp_box,
    settings_box
])

main_ui_container.children = [ui_elements]

# --- –û–ë–†–ê–ë–û–¢–ß–ò–ö –ó–ê–ü–£–°–ö–ê ---

def on_btn_click(b):
    with output_log:
        clear_output()
        print("‚öôÔ∏è –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∏...")

        settings = {
            'top': w_top_count.value,
            'noindex': chk_noindex.value,
            'alt_title': chk_alt.value,
            'numbers': chk_num.value,
            'normalize': chk_norm.value,
            'percent_output': chk_percent.value,
            'user_agent': w_user_agent.value,
            'exclude': [x.strip() for x in w_exclude_domains.value.split('\n') if x.strip()],
            'custom_stops': [x.strip() for x in w_stop_custom_text.value.split('\n') if x.strip()] if chk_stop_custom.value else []
        }
        
        if chk_aggr.value: 
            settings['exclude'].extend(BLACKLIST_DOMAINS)
        settings['exclude'] = list(set([item for item in settings['exclude'] if item]))

        # 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ "–ú–æ–µ–π –°—Ç—Ä–∞–Ω–∏—Ü—ã"
        input_mode = r_input_type.value
        my_body_content = None
        my_anchor_content = None
        my_url_id = "" 

        if input_mode == '–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ':
            my_url_id = w_my_url.value
            if not my_url_id:
                print("‚ùå –û—à–∏–±–∫–∞: –í–≤–µ–¥–∏—Ç–µ URL –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞!")
                return
        elif input_mode == '–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç':
            raw_code = w_source_code.value
            if not raw_code:
                print("‚ùå –û—à–∏–±–∫–∞: –í—Å—Ç–∞–≤—å—Ç–µ HTML –∫–æ–¥ –∏–ª–∏ —Ç–µ–∫—Å—Ç!")
                return
            my_url_id = "Manual_Code_Input"
            my_body_content = clean_and_tokenize(raw_code, settings)
            my_anchor_content = clean_anchor_text(raw_code, settings) 
        elif input_mode == '–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã':
            my_url_id = "No_Page_Mode"
            my_body_content = "" 
            my_anchor_content = ""
        
        # 3. –°–±–æ—Ä –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        competitors_urls = []
        if r_comp_source.value == '–ü–æ–∏—Å–∫':
            query = w_query.value
            if not query:
                print("‚ùå –û—à–∏–±–∫–∞: –í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å!")
                return
            
            if USE_SEARCH and w_engine.value == 'Google':
                try:
                    print(f"üîé –ü–æ–∏—Å–∫ –≤ Google –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
                    raw_urls = search(query, num_results=settings['top'] + 10, lang="ru") 

                    count_collected = 0
                    for u in raw_urls:
                        if u == my_url_id or any(ex in u for ex in settings['exclude']):
                            continue

                        competitors_urls.append(u)
                        count_collected += 1

                        if count_collected >= settings['top']: break
                    
                    if not competitors_urls:
                        print("‚ùå Google Search –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")
                        return

                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ Google: {e}.")
                    return
            else:
                 print("‚ùå –ü–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –Ø–Ω–¥–µ–∫—Å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è.")
                 return

        else: # –°–ø–∏—Å–æ–∫ URL –≤—Ä—É—á–Ω—É—é
            raw_list = w_manual_comps.value.split('\n')
            
            for comp_url in [u.strip() for u in raw_list if u.strip()]:
                if comp_url == my_url_id or any(ex in comp_url for ex in settings['exclude']):
                    continue
                competitors_urls.append(comp_url)


        if not competitors_urls:
            print("‚ùå –°–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –ø—É—Å—Ç.")
            return

        # 4. –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞
        print(f"\nüöÄ –°—Ç–∞—Ä—Ç –∞–Ω–∞–ª–∏–∑–∞. –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(competitors_urls)} URL.")
        
        df = run_analysis(
            my_url_id, 
            competitors_urls, 
            settings, 
            my_body_content=my_body_content, 
            my_anchor_content=my_anchor_content
        )

        if df is not None:
            print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ì–ò–ë–†–ò–î–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê:")
            display(df)
        else:
            print("\n‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑ –Ω–µ –¥–∞–ª –∑–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")

bn_run.on_click(on_btn_click)
