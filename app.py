import streamlit as st
import pymorphy2
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin, unquote
import inspect
import time
import json
import io
import csv
from google import genai
import os
import requests
proxy_url = "http://QYnojH:Uekp4k@196.18.3.35:8000" 

os.environ["http_proxy"] = proxy_url
os.environ["https_proxy"] = proxy_url

try:
    my_ip = requests.get("https://api.ipify.org", timeout=5).text
    st.info(f"üïµÔ∏è –í–ê–® IP –î–õ–Ø –°–ö–†–ò–ü–¢–ê: {my_ip}")
except Exception as e:
    st.error(f"‚ùå –ü—Ä–æ–∫—Å–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {e}")
    
import random
import streamlit.components.v1 as components
import copy
import plotly.graph_objects as go
import pickle
import datetime

# ==========================================
# FIX FOR PYTHON 3.11+
# ==========================================
if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except ImportError:
    morph = None
    USE_NLP = False

try:
    import openai
except ImportError:
    openai = None

try:
    from google import genai
except ImportError:
    genai = None

# ==========================================
# 0. –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ==========================================

def transliterate_text(text):
    mapping = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }
    result = []
    for char in text.lower():
        if char in mapping:
            result.append(mapping[char])
        elif char.isalnum() or char == '-':
            result.append(char)
    return "".join(result)

def force_cyrillic_name_global(slug_text):
    raw = unquote(slug_text).lower()
    raw = raw.replace('.html', '').replace('.php', '')
    if re.search(r'[–∞-—è]', raw):
        return raw.replace('-', ' ').replace('_', ' ').capitalize()

    words = re.split(r'[-_]', raw)
    rus_words = []
    
    exact_map = {
        'nikel': '–Ω–∏–∫–µ–ª—å', 'stal': '—Å—Ç–∞–ª—å', 'med': '–º–µ–¥—å', 'latun': '–ª–∞—Ç—É–Ω—å',
        'bronza': '–±—Ä–æ–Ω–∑–∞', 'svinec': '—Å–≤–∏–Ω–µ—Ü', 'titan': '—Ç–∏—Ç–∞–Ω', 'tsink': '—Ü–∏–Ω–∫',
        'dural': '–¥—é—Ä–∞–ª—å', 'dyural': '–¥—é—Ä–∞–ª—å', 'chugun': '—á—É–≥—É–Ω',
        'alyuminiy': '–∞–ª—é–º–∏–Ω–∏–π', 'al': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è', 'alyuminievaya': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è',
        'nerzhaveyushchiy': '–Ω–µ—Ä–∂–∞–≤–µ—é—â–∏–π', 'nerzhaveyka': '–Ω–µ—Ä–∂–∞–≤–µ–π–∫–∞',
        'profil': '–ø—Ä–æ—Ñ–∏–ª—å', 'shveller': '—à–≤–µ–ª–ª–µ—Ä', 'ugolok': '—É–≥–æ–ª–æ–∫',
        'polosa': '–ø–æ–ª–æ—Å–∞', 'krug': '–∫—Ä—É–≥', 'kvadrat': '–∫–≤–∞–¥—Ä–∞—Ç',
        'list': '–ª–∏—Å—Ç', 'truba': '—Ç—Ä—É–±–∞', 'setka': '—Å–µ—Ç–∫–∞',
        'provoloka': '–ø—Ä–æ–≤–æ–ª–æ–∫–∞', 'armatura': '–∞—Ä–º–∞—Ç—É—Ä–∞', 'balka': '–±–∞–ª–∫–∞',
        'katanka': '–∫–∞—Ç–∞–Ω–∫–∞', 'otvod': '–æ—Ç–≤–æ–¥', 'perehod': '–ø–µ—Ä–µ—Ö–æ–¥',
        'flanec': '—Ñ–ª–∞–Ω–µ—Ü', 'zaglushka': '–∑–∞–≥–ª—É—à–∫–∞', 'metiz': '–º–µ—Ç–∏–∑—ã',
        'profnastil': '–ø—Ä–æ—Ñ–Ω–∞—Å—Ç–∏–ª', 'shtrips': '—à—Ç—Ä–∏–ø—Å', 'lenta': '–ª–µ–Ω—Ç–∞',
        'shina': '—à–∏–Ω–∞', 'prutok': '–ø—Ä—É—Ç–æ–∫', 'shestigrannik': '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫',
        'vtulka': '–≤—Ç—É–ª–∫–∞', 'kabel': '–∫–∞–±–µ–ª—å', 'panel': '–ø–∞–Ω–µ–ª—å',
        'detal': '–¥–µ—Ç–∞–ª—å', 'set': '—Å–µ—Ç—å', 'cep': '—Ü–µ–ø—å', 'svyaz': '—Å–≤—è–∑—å',
        'rezba': '—Ä–µ–∑—å–±–∞', 'gost': '–ì–û–°–¢',
        'polipropilenovye': '–ø–æ–ª–∏–ø—Ä–æ–ø–∏–ª–µ–Ω–æ–≤—ã–µ', 'truby': '—Ç—Ä—É–±—ã',
        'ocinkovannaya': '–æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω–∞—è', 'riflenyy': '—Ä–∏—Ñ–ª–µ–Ω—ã–π'
    }

    for w in words:
        if not w: continue
        if w in exact_map:
            rus_words.append(exact_map[w])
            continue
        
        processed_w = w
        if processed_w.endswith('yy'): processed_w = processed_w[:-2] + '—ã–π'
        elif processed_w.endswith('iy'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('ij'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('yi'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('aya'): processed_w = processed_w[:-3] + '–∞—è'
        elif processed_w.endswith('oye'): processed_w = processed_w[:-3] + '–æ–µ'
        elif processed_w.endswith('ye'): processed_w = processed_w[:-2] + '—ã–µ'

        replacements = [
            ('shch', '—â'), ('sch', '—â'), ('yo', '—ë'), ('zh', '–∂'), ('ch', '—á'), ('sh', '—à'), 
            ('yu', '—é'), ('ya', '—è'), ('kh', '—Ö'), ('ts', '—Ü'), ('ph', '—Ñ'),
            ('a', '–∞'), ('b', '–±'), ('v', '–≤'), ('g', '–≥'), ('d', '–¥'), ('e', '–µ'), 
            ('z', '–∑'), ('i', '–∏'), ('j', '–π'), ('k', '–∫'), ('l', '–ª'), ('m', '–º'), 
            ('n', '–Ω'), ('o', '–æ'), ('p', '–ø'), ('r', '—Ä'), ('s', '—Å'), ('t', '—Ç'), 
            ('u', '—É'), ('f', '—Ñ'), ('h', '—Ö'), ('c', '–∫'), ('w', '–≤'), ('y', '—ã'), ('x', '–∫—Å')
        ]
        
        temp_res = processed_w
        for eng, rus in replacements:
            temp_res = temp_res.replace(eng, rus)
        
        rus_words.append(temp_res)

    draft_phrase = " ".join(rus_words)
    draft_phrase = draft_phrase.replace('–ø—Ä–æ—Ñ–∏–ª', '–ø—Ä–æ—Ñ–∏–ª—å').replace('–ø—Ä–æ—Ñ–∏–ª—å–Ω', '–ø—Ä–æ—Ñ–∏–ª—å–Ω')
    draft_phrase = draft_phrase.replace('–µ–ª–Ω—ã–π', '–µ–ª—å–Ω—ã–π').replace('–∞–ª–Ω—ã–π', '–∞–ª—å–Ω—ã–π')
    draft_phrase = draft_phrase.replace('–µ–ª–Ω–∞—è', '–µ–ª—å–Ω–∞—è').replace('–∞–ª–Ω–∞—è', '–∞–ª—å–Ω–∞—è')
    draft_phrase = draft_phrase.replace('—Å—Ç–∞–ª–Ω', '—Å—Ç–∞–ª—å–Ω').replace('–º–µ–¥—å–Ω', '–º–µ–¥–Ω')
    draft_phrase = draft_phrase.replace('–π–∞', '—è').replace('–π–æ', '—ë')

    return draft_phrase.capitalize()

def get_breadcrumb_only(url, ua_settings="Mozilla/5.0"):
    try:
        session = requests.Session()
        retry = Retry(connect=3, read=3, redirect=3, backoff_factor=0.5)
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        
        headers = {'User-Agent': ua_settings}
        r = session.get(url, headers=headers, timeout=25)
        if r.status_code != 200: 
            return None
        
        soup = BeautifulSoup(r.text, 'html.parser')
        
        breadcrumbs = soup.find(class_=re.compile(r'breadcrumb|breadcrumbs|nav-path|nav-chain|bx-breadcrumb', re.I))
        if not breadcrumbs:
            breadcrumbs = soup.find(id=re.compile(r'breadcrumb|breadcrumbs|nav-path', re.I))

        if breadcrumbs:
            full_text = breadcrumbs.get_text(separator='|||', strip=True)
            parts = [p.strip() for p in full_text.split('|||') if p.strip()]
            clean_parts = [p for p in parts if p not in ['/', '\\', '>', '¬ª', '‚Ä¢', '-', '|']]
            
            if clean_parts:
                last_item = clean_parts[-1]
                if len(last_item) > 2 and last_item.lower() != "–≥–ª–∞–≤–Ω–∞—è":
                    return last_item
    except:
        return None
    return None

def render_clean_block(title, icon, words_list):
    unique_words = sorted(list(set(words_list))) if words_list else []
    count = len(unique_words)
    
    if count > 0:
        content_html = ", ".join(unique_words)
        # –ö–∞—Ä—Ç–æ—á–∫–∞ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è
        html_code = f"""
        <details class="details-card">
            <summary class="card-summary">
                <div>
                    <span class="arrow-icon">‚ñ∂</span>
                    {icon} {title}
                </div>
                <span class="count-tag">{count}</span>
            </summary>
            <div class="card-content">
                {content_html}
            </div>
        </details>
        """
    else:
        # –ï—Å–ª–∏ –ø—É—Å—Ç–æ - –∫–∞—Ä—Ç–æ—á–∫–∞ –Ω–µ–∞–∫—Ç–∏–≤–Ω–∞ (–±–µ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
        html_code = f"""
        <div class="details-card">
            <div class="card-summary" style="cursor: default; color: #9ca3af;">
                <div>{icon} {title}</div>
                <span class="count-tag">0</span>
            </div>
        </div>
        """
    
    st.markdown(html_code, unsafe_allow_html=True)

def render_relevance_chart(df_rel, unique_key="default"):
    if df_rel.empty:
        return

    # 1. –ñ–ï–°–¢–ö–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ > 0
    # –í–∞—à —Å–∞–π—Ç (–ø–æ–∑–∏—Ü–∏—è 0) —É–¥–∞–ª—è–µ—Ç—Å—è –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
    df = df_rel[df_rel['–ü–æ–∑–∏—Ü–∏—è'] > 0].copy()
    
    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø—É—Å—Ç–∞ - –≤—ã—Ö–æ–¥–∏–º
    if df.empty:
        return

    df = df.sort_values(by='–ü–æ–∑–∏—Ü–∏—è')
    x_indices = np.arange(len(df))
    
    tick_links = []
    
    for _, row in df.iterrows():
        # –ß–∏—Å—Ç–∏–º –∏–º—è –¥–æ–º–µ–Ω–∞
        raw_name = row['–î–æ–º–µ–Ω'].replace(' (–í—ã)', '').strip()
        clean_domain = raw_name.replace('www.', '').split('/')[0]
        
        # –§–æ—Ä–º–∞—Ç: "1. site.ru" (–±–µ–∑ #)
        label_text = f"{row['–ü–æ–∑–∏—Ü–∏—è']}. {clean_domain}"
        
        # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å, —Ç–∞–∫ –∫–∞–∫ —à—Ä–∏—Ñ—Ç —Ç–µ–ø–µ—Ä—å –∫—Ä—É–ø–Ω–µ–µ
        if len(label_text) > 25: label_text = label_text[:23] + ".."
        
        url_target = row.get('URL', f"https://{raw_name}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º CSS-–∫–ª–∞—Å—Å .chart-link –≤–º–µ—Å—Ç–æ style="..." –¥–ª—è —Ä–∞–±–æ—Ç—ã hover
        link_html = f"<a href='{url_target}' target='_blank' class='chart-link'>{label_text}</a>"
        tick_links.append(link_html)

    # –ú–µ—Ç—Ä–∏–∫–∏
    df['Total_Rel'] = (df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] + df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)']) / 2
    
    # –¢—Ä–µ–Ω–¥
    z = np.polyfit(x_indices, df['Total_Rel'], 1)
    p = np.poly1d(z)
    df['Trend'] = p(x_indices)

    # 2. –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
    fig = go.Figure()

    # --- –ü–ê–õ–ò–¢–†–ê (Premium) ---
    COLOR_MAIN = '#4F46E5'  # –ò–Ω–¥–∏–≥–æ
    COLOR_WIDTH = '#0EA5E9' # –ì–æ–ª—É–±–æ–π
    COLOR_DEPTH = '#E11D48' # –ú–∞–ª–∏–Ω–æ–≤—ã–π
    COLOR_TREND = '#15803d' # –ó–µ–ª–µ–Ω—ã–π (Forest Green)

    COMMON_CONFIG = dict(
        mode='lines+markers',
        line=dict(width=3, shape='spline'), 
        marker=dict(size=8, line=dict(width=2, color='white'), symbol='circle')
    )

    # 1. –û–ë–©–ê–Ø
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['Total_Rel'],
        name='–û–±—â–∞—è',
        line=dict(color=COLOR_MAIN, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_MAIN, **COMMON_CONFIG['marker']),
        mode='lines+markers'
    ))

    # 2. –®–ò–†–ò–ù–ê
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'],
        name='–®–∏—Ä–∏–Ω–∞',
        line=dict(color=COLOR_WIDTH, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_WIDTH, **COMMON_CONFIG['marker']),
        mode='lines+markers'
    ))

    # 3. –ì–õ–£–ë–ò–ù–ê
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'],
        name='–ì–ª—É–±–∏–Ω–∞',
        line=dict(color=COLOR_DEPTH, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_DEPTH, **COMMON_CONFIG['marker']),
        mode='lines+markers'
    ))

    # 4. –¢–†–ï–ù–î
    fig.add_trace(go.Scatter(
        x=x_indices, y=df['Trend'],
        name='–¢—Ä–µ–Ω–¥',
        line=dict(color=COLOR_TREND, **COMMON_CONFIG['line']),
        marker=dict(color=COLOR_TREND, **COMMON_CONFIG['marker']),
        mode='lines+markers',
        opacity=0.8
    ))

# 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Layout (–ö–û–ú–ü–ê–ö–¢–ù–ê–Ø –í–ï–†–°–ò–Ø)
    fig.update_layout(
        template="plotly_white",
        legend=dict(
            orientation="h",
            yanchor="bottom", y=1.02, # –õ–µ–≥–µ–Ω–¥–∞ –ø—Ä—è–º–æ –Ω–∞–¥ –≥—Ä–∞—Ñ–∏–∫–æ–º
            xanchor="center", x=0.5,
            font=dict(size=12, color="#111827", family="Inter, sans-serif")
        ),
        xaxis=dict(
            showgrid=True,
            gridcolor='#F3F4F6',
            linecolor='#E5E7EB',
            tickmode='array',
            tickvals=x_indices,
            ticktext=tick_links, 
            
            tickfont=dict(size=11), # –ß—É—Ç—å –º–µ–Ω—å—à–µ —à—Ä–∏—Ñ—Ç –ø–æ–¥–ø–∏—Å–µ–π
            tickangle=-45, 
            
            fixedrange=True,
            dtick=1, 
            range=[-0.5, len(df) - 0.5], 
            automargin=False 
        ),
        yaxis=dict(
            range=[0, 115], 
            showgrid=True, 
            gridcolor='#F3F4F6', 
            gridwidth=1,
            zeroline=False,
            fixedrange=True
        ),
        # === –í–û–¢ –¢–£–¢ –ú–ï–ù–Ø–ï–ú –†–ê–ó–ú–ï–†–´ ===
        # l/r - –±–æ–∫–∞, t - –≤–µ—Ä—Ö, b - –Ω–∏–∑ (–ø–æ–¥ –ø–æ–¥–ø–∏—Å–∏)
        margin=dict(l=10, r=10, t=30, b=110),
        
        hovermode="x unified",
        
        # –û–±—â–∞—è –≤—ã—Å–æ—Ç–∞ –≥—Ä–∞—Ñ–∏–∫–∞ (–±—ã–ª–æ 550)
        height=400 
    )
    
    # use_container_width=True —Ä–∞—Å—Ç—è–≥–∏–≤–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –Ω–∞ –≤—Å—é —à–∏—Ä–∏–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    st.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False}, key=f"rel_chart_{unique_key}")

def analyze_serp_anomalies(df_rel):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–í–µ—Ä—Å–∏—è v5 - Robust).
    –ü–æ—Ä–æ–≥: 75% –æ—Ç –ª–∏–¥–µ—Ä–∞. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∏–ø–∏–∑–∞—Ü–∏—è.
    """
    if df_rel.empty:
        return [], [], {"type": "none", "msg": ""}

    # –ò—Å–∫–ª—é—á–∞–µ–º "–í–∞—à —Å–∞–π—Ç" –∏–∑ —Ä–∞—Å—á–µ—Ç–æ–≤ —ç—Ç–∞–ª–æ–Ω–∞
    df = df_rel[~df_rel['–î–æ–º–µ–Ω'].str.contains("\(–í—ã\)", na=False)].copy()
    
    if df.empty:
        return [], [], {"type": "none", "msg": ""}

    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –¥–µ–ª–∞–µ–º —á–∏—Å–ª–∞–º–∏ (–∑–∞—â–∏—Ç–∞ –æ—Ç —Å–±–æ–µ–≤)
    df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] = pd.to_numeric(df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'], errors='coerce').fillna(0)
    df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'] = pd.to_numeric(df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'], errors='coerce').fillna(0)

    # –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª
    df['Total'] = (df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] + df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)']) / 2
    
    # 1. –ò–©–ï–ú –õ–ò–î–ï–†–ê
    max_score = df['Total'].max()
    if max_score < 1: max_score = 1 # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
    
    # 2. –ñ–ï–°–¢–ö–ò–ô –ü–û–†–û–ì: 75% –æ—Ç –ª–∏–¥–µ—Ä–∞.
    # –ï—Å–ª–∏ –õ–∏–¥–µ—Ä=100, –ø–æ—Ä–æ–≥=75. –í—Å–µ —á—Ç–æ < 75 - —É–¥–∞–ª—è–µ–º.
    threshold = max(max_score * 0.75, 40) 
    
    anomalies = []
    normal_urls = []
    
    debug_counts = 0
    
    for _, row in df.iterrows():
        # –î–æ—Å—Ç–∞–µ–º —Å—Å—ã–ª–∫—É. –ó–∞—â–∏—Ç–∞ –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤.
        current_url = str(row.get('URL', '')).strip()
        if not current_url or current_url.lower() == 'nan':
             current_url = f"https://{row['–î–æ–º–µ–Ω']}" 

        score = row['Total']
        
        # –ê–ù–ê–õ–ò–ó
        if score < threshold:
            reason = f"–°–∫–æ—Ä {int(score)} < {int(threshold)} (–õ–∏–¥–µ—Ä {int(max_score)})"
            anomalies.append({'url': current_url, 'reason': reason, 'score': score})
            debug_counts += 1
        else:
            normal_urls.append(current_url)

    # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—è–º–∏
    if anomalies:
        st.toast(f"üóëÔ∏è –§–∏–ª—å—Ç—Ä (–õ–∏–¥–µ—Ä {int(max_score)} / –ü–æ—Ä–æ–≥ {int(threshold)}). –ò—Å–∫–ª—é—á–µ–Ω–æ: {len(anomalies)}", icon="‚ö†Ô∏è")
    else:
        # –ï—Å–ª–∏ –Ω–∏–∫–æ–≥–æ –Ω–µ –∏—Å–∫–ª—é—á–∏–ª–∏, –ø–∏—à–µ–º –ø–æ—á–µ–º—É
        st.toast(f"‚úÖ –í—Å–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –æ–∫. (–õ–∏–¥–µ—Ä {int(max_score)} / –ü–æ—Ä–æ–≥ {int(threshold)}). –ú–∏–Ω. –±–∞–ª–ª: {int(df['Total'].min())}", icon="‚ÑπÔ∏è")
    
    # –¢—Ä–µ–Ω–¥
    x = np.arange(len(df)); y = df['Total'].values
    slope = np.polyfit(x, y, 1)[0] if len(x) > 1 else 0
    trend_msg = "üìâ –ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–æ–ø" if slope < -1 else ("üìà –ü–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç—ã–π —Ç–æ–ø" if slope > 1 else "‚û°Ô∏è –†–æ–≤–Ω—ã–π —Ç–æ–ø")

    return normal_urls, anomalies, {"type": "info", "msg": trend_msg}

@st.cache_data
def load_lemmatized_dictionaries():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_path = os.path.join(script_dir, "data")
    
    # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞
    sets = {
        "products": set(),
        "commercial": set(),
        "specs": set(),
        "geo": set(),
        "services": set(),
        "sensitive": set()
    }

    # –ö–∞—Ä—Ç–∞ —Ñ–∞–π–ª–æ–≤
    files_map = {
        "metal_products.json": "products",
        "commercial_triggers.json": "commercial",
        "geo_locations.json": "geo",
        "services_triggers.json": "services",
        "tech_specs.json": "specs",
        "SENSITIVE_STOPLIST.json": "sensitive"
    }

    for filename, set_key in files_map.items():
        full_path = os.path.join(base_path, filename)
        if not os.path.exists(full_path):
            continue
        
        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                data = json.load(f) 
                
                words_bucket = []
                if isinstance(data, dict):
                    for cat_list in data.values():
                        words_bucket.extend(cat_list)
                elif isinstance(data, list):
                    words_bucket = data
                
                for phrase in words_bucket:
                    w_clean = str(phrase).lower().strip().replace('—ë', '–µ')
                    if not w_clean: continue
                    sets[set_key].add(w_clean)
                    if morph:
                        normal_form = morph.parse(w_clean)[0].normal_form.replace('—ë', '–µ')
                        sets[set_key].add(normal_form)
                    if ' ' in w_clean:
                        parts = w_clean.split()
                        for p in parts:
                            sets[set_key].add(p)
                            if morph: 
                                sets[set_key].add(morph.parse(p)[0].normal_form.replace('—ë', '–µ'))
        except: pass

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 6 –Ω–∞–±–æ—Ä–æ–≤
    return sets["products"], sets["commercial"], sets["specs"], sets["geo"], sets["services"], sets["sensitive"]

def classify_semantics_with_api(words_list, yandex_key):
    # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º 6 —Å–ª–æ–≤–∞—Ä–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Ä–Ω—É–ª–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
    PRODUCTS_SET, COMM_SET, SPECS_SET, GEO_SET, SERVICES_SET, SENS_SET = load_lemmatized_dictionaries()
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ —Ñ–∞–π–ª–∞ –∏ –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –≤ –∫–æ–¥–µ
    FULL_SENSITIVE = SENS_SET.union(SENSITIVE_STOPLIST)

    if 'debug_geo_count' not in st.session_state:
        st.session_state.debug_geo_count = len(GEO_SET)
    
    st.sidebar.info(f"–°–ª–æ–≤–∞—Ä–∏ (–∏–∑ —Ñ–∞–π–ª–æ–≤):\nüì¶ –¢–æ–≤–∞—Ä—ã: {len(PRODUCTS_SET)}\nüí∞ –ö–æ–º–º–µ—Ä—Ü–∏—è: {len(COMM_SET)}\nüõ†Ô∏è –£—Å–ª—É–≥–∏: {len(SERVICES_SET)}\nüåç –ì–æ—Ä–æ–¥–∞: {len(GEO_SET)}")

    dim_pattern = re.compile(r'\d+(?:[\.\,]\d+)?\s?[—Öx\*√ó]\s?\d+', re.IGNORECASE)
    grade_pattern = re.compile(r'^([–∞-—èa-z]{1,4}\-?\d+[–∞-—èa-z0-9]*)$', re.IGNORECASE)
    
    categories = {'products': set(), 'services': set(), 'commercial': set(), 
                  'dimensions': set(), 'geo': set(), 'general': set(), 'sensitive': set()}
    
    for word in words_list:
        word_lower = word.lower()
        
        # 1. –°–¢–û–ü-–°–õ–û–í–ê
        is_sensitive = False
        if word_lower in FULL_SENSITIVE: is_sensitive = True
        else:
            for stop_w in FULL_SENSITIVE:
                if len(stop_w) > 3 and stop_w in word_lower: is_sensitive = True; break
        if is_sensitive: categories['sensitive'].add(word_lower); continue
        
        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
        lemma = word_lower
        if morph:
            p = morph.parse(word_lower)[0]
            lemma = p.normal_form

        # 2. –†–ê–ó–ú–ï–†–´ / –ì–û–°–¢
        if word_lower in SPECS_SET or lemma in SPECS_SET:
            categories['dimensions'].add(word_lower); continue
        if dim_pattern.search(word_lower) or grade_pattern.match(word_lower) or word_lower.isdigit():
            categories['dimensions'].add(word_lower); continue

        # 3. –¢–û–í–ê–†–´ (–£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞)
        if word_lower in PRODUCTS_SET or lemma in PRODUCTS_SET:
            categories['products'].add(word_lower); continue
        
        is_product_root = False
        for prod in PRODUCTS_SET:
            check_root = prod[:-1] if len(prod) > 4 else prod
            if len(check_root) > 3 and check_root in word_lower:
                categories['products'].add(word_lower)
                is_product_root = True
                break
        if is_product_root: continue

        # 4. –ì–ï–û
        if lemma in GEO_SET or word_lower in GEO_SET:
            categories['geo'].add(word_lower); continue
        
        # 5. –£–°–õ–£–ì–ò
        if lemma in SERVICES_SET or word_lower in SERVICES_SET:
             categories['services'].add(word_lower); continue
        if lemma.endswith('–æ–±—Ä–∞–±–æ—Ç–∫–∞') or lemma.endswith('–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ') or lemma == "—Ä–µ–∑–∫–∞":
            categories['services'].add(word_lower); continue

        # 6. –ö–û–ú–ú–ï–†–¶–ò–Ø
        if lemma in COMM_SET or word_lower in COMM_SET:
            categories['commercial'].add(word_lower); continue
            
        # 7. –û–ë–©–ò–ï
        categories['general'].add(word_lower)

    return {k: sorted(list(v)) for k, v in categories.items()}

# ==========================================
# STATE INIT
# ==========================================
if 'sidebar_gen_df' not in st.session_state: st.session_state.sidebar_gen_df = None
if 'sidebar_excel_bytes' not in st.session_state: st.session_state.sidebar_excel_bytes = None
if 'analysis_results' not in st.session_state: st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state: st.session_state.analysis_done = False
if 'ai_generated_df' not in st.session_state: st.session_state.ai_generated_df = None
if 'ai_excel_bytes' not in st.session_state: st.session_state.ai_excel_bytes = None
if 'tags_html_result' not in st.session_state: st.session_state.tags_html_result = None
if 'table_html_result' not in st.session_state: st.session_state.table_html_result = None
if 'tags_generated_df' not in st.session_state: st.session_state.tags_generated_df = None
if 'tags_excel_data' not in st.session_state: st.session_state.tags_excel_data = None

# Current lists
if 'categorized_products' not in st.session_state: st.session_state.categorized_products = []
if 'categorized_services' not in st.session_state: st.session_state.categorized_services = []
if 'categorized_commercial' not in st.session_state: st.session_state.categorized_commercial = []
if 'categorized_dimensions' not in st.session_state: st.session_state.categorized_dimensions = []
if 'categorized_geo' not in st.session_state: st.session_state.categorized_geo = []
if 'categorized_general' not in st.session_state: st.session_state.categorized_general = []
if 'categorized_sensitive' not in st.session_state: st.session_state.categorized_sensitive = []

# Original lists (for restoration)
if 'orig_products' not in st.session_state: st.session_state.orig_products = []
if 'orig_services' not in st.session_state: st.session_state.orig_services = []
if 'orig_commercial' not in st.session_state: st.session_state.orig_commercial = []
if 'orig_dimensions' not in st.session_state: st.session_state.orig_dimensions = []
if 'orig_geo' not in st.session_state: st.session_state.orig_geo = []
if 'orig_general' not in st.session_state: st.session_state.orig_general = []

if 'auto_tags_words' not in st.session_state: st.session_state.auto_tags_words = []
if 'auto_promo_words' not in st.session_state: st.session_state.auto_promo_words = []
if 'persistent_urls' not in st.session_state: st.session_state['persistent_urls'] = ""

st.set_page_config(layout="wide", page_title="GAR PRO v2.6 (Mass Promo)", page_icon="üìä")

GARBAGE_LATIN_STOPLIST = {
    'whatsapp', 'viber', 'telegram', 'skype', 'vk', 'instagram', 'facebook', 'youtube', 'twitter',
    'cookie', 'cookies', 'policy', 'privacy', 'agreement', 'terms',
    'click', 'submit', 'send', 'zakaz', 'basket', 'cart', 'order', 'call', 'back', 'callback',
    'login', 'logout', 'sign', 'register', 'auth', 'account', 'profile',
    'search', 'menu', 'nav', 'navigation', 'footer', 'header', 'sidebar',
    'img', 'jpg', 'png', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'svg',
    'ok', 'error', 'undefined', 'null', 'true', 'false', 'var', 'let', 'const', 'function', 'return',
    'ru', 'en', 'com', 'net', 'org', 'biz', 'shop', 'store',
    'phone', 'email', 'tel', 'fax', 'mob', 'address', 'copyright', 'all', 'rights', 'reserved',
    'div', 'span', 'class', 'id', 'style', 'script', 'body', 'html', 'head', 'meta', 'link'
}

SENSITIVE_STOPLIST_RAW = {
    "—É–∫—Ä–∞–∏–Ω–∞", "ukraine", "ua", "–≤—Å—É", "–∑—Å—É", "–∞—Ç–æ",
    "–∫–∏–µ–≤", "–ª—å–≤–æ–≤", "—Ö–∞—Ä—å–∫–æ–≤", "–æ–¥–µ—Å—Å–∞", "–¥–Ω–µ–ø—Ä", "–º–∞—Ä–∏—É–ø–æ–ª—å",
    "–¥–æ–Ω–µ—Ü–∫", "–ª—É–≥–∞–Ω—Å–∫", "–¥–Ω—Ä", "–ª–Ω—Ä", "–¥–æ–Ω–±–∞—Å—Å", 
    "–º–µ–ª–∏—Ç–æ–ø–æ–ª—å", "–±–µ—Ä–¥—è–Ω—Å–∫", "–±–∞—Ö–º—É—Ç", "–∑–∞–ø–æ—Ä–æ–∂—å–µ", "—Ö–µ—Ä—Å–æ–Ω",
    "–∫—Ä—ã–º", "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å", "—Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å"
}
SENSITIVE_STOPLIST = {w.lower() for w in SENSITIVE_STOPLIST_RAW}

def check_password():
    if st.session_state.get("authenticated"):
        return True
    st.markdown("""<style>.main { display: flex; flex-direction: column; justify-content: center; align-items: center; } .auth-logo-box { text-align: center; margin-bottom: 1rem; padding-top: 0; }</style>""", unsafe_allow_html=True)
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="auth-logo-box"><h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3></div>', unsafe_allow_html=True)
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "ZVC01w4_pIquj0bMiaAu":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

if "arsenkin_token" in st.session_state:
    ARSENKIN_TOKEN = st.session_state.arsenkin_token
else:
    try: ARSENKIN_TOKEN = st.secrets["api"]["arsenkin_token"]
    except (FileNotFoundError, KeyError): ARSENKIN_TOKEN = None

if "yandex_dict_key" in st.session_state:
    YANDEX_DICT_KEY = st.session_state.yandex_dict_key
else:
    try: YANDEX_DICT_KEY = st.secrets["api"]["yandex_dict_key"]
    except (FileNotFoundError, KeyError): YANDEX_DICT_KEY = None

REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956},
    "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931},
    "–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É": {"ya": 39, "go": 1012028},
    "–£—Ñ–∞": {"ya": 172, "go": 1012091},
    "–ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫": {"ya": 62, "go": 1012001},
    "–í–æ—Ä–æ–Ω–µ–∂": {"ya": 193, "go": 1012134},
    "–ü–µ—Ä–º—å": {"ya": 50, "go": 1012015},
    "–í–æ–ª–≥–æ–≥—Ä–∞–¥": {"ya": 38, "go": 1012131},
    "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–°–∞—Ä–∞—Ç–æ–≤": {"ya": 194, "go": 1012046},
    "–¢—é–º–µ–Ω—å": {"ya": 283, "go": 1012089},
    "–¢–æ–ª—å—è—Ç—Ç–∏": {"ya": 240, "go": 1012080},
    "–ò–∂–µ–≤—Å–∫": {"ya": 44, "go": 1011979},
    "–ë–∞—Ä–Ω–∞—É–ª": {"ya": 197, "go": 1011855},
    "–ò—Ä–∫—É—Ç—Å–∫": {"ya": 63, "go": 1011977},
    "–£–ª—å—è–Ω–æ–≤—Å–∫": {"ya": 195, "go": 1012092},
    "–•–∞–±–∞—Ä–æ–≤—Å–∫": {"ya": 76, "go": 1011973},
    "–í–ª–∞–¥–∏–≤–æ—Å—Ç–æ–∫": {"ya": 75, "go": 1012129},
    "–Ø—Ä–æ—Å–ª–∞–≤–ª—å": {"ya": 16, "go": 1012140},
    "–ú–∞—Ö–∞—á–∫–∞–ª–∞": {"ya": 28, "go": 1011993},
    "–¢–æ–º—Å–∫": {"ya": 67, "go": 1012082},
    "–û—Ä–µ–Ω–±—É—Ä–≥": {"ya": 48, "go": 1012009},
    "–ö–µ–º–µ—Ä–æ–≤–æ": {"ya": 64, "go": 1011985},
    "–ù–æ–≤–æ–∫—É–∑–Ω–µ—Ü–∫": {"ya": 237, "go": 1011987},
    "–†—è–∑–∞–Ω—å": {"ya": 11, "go": 1012033},
    "–ù–∞–±–µ—Ä–µ–∂–Ω—ã–µ –ß–µ–ª–Ω—ã": {"ya": 234, "go": 1011905},
    "–ü–µ–Ω–∑–∞": {"ya": 49, "go": 1012013},
    "–õ–∏–ø–µ—Ü–∫": {"ya": 9, "go": 1011991},
    "–¢—É–ª–∞": {"ya": 15, "go": 1012085},
    "–ö–∏—Ä–æ–≤": {"ya": 46, "go": 1011989},
    "–ß–µ–±–æ–∫—Å–∞—Ä—ã": {"ya": 45, "go": 1011880},
    "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥": {"ya": 22, "go": 1011981},
    "–ö—É—Ä—Å–∫": {"ya": 8, "go": 1011988},
    "–£–ª–∞–Ω-–£–¥—ç": {"ya": 68, "go": 1012090},
    "–°—Ç–∞–≤—Ä–æ–ø–æ–ª—å": {"ya": 36, "go": 1012070},
    "–°–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å": {"ya": 959, "go": 1012048},
    "–°–æ—á–∏": {"ya": 239, "go": 1012053},
    "–†–æ—Å—Å–∏—è": {"ya": 225, "go": 2643},
    "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493},
    "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601},
    "–ê—Å—Ç–∞–Ω–∞ (KZ)": {"ya": 163, "go": 1014620}
}

DEFAULT_EXCLUDE_DOMAINS = {
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "aliexpress.ru", 
    "ebay.com", "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", 
    "pandao.ru", "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", 
    "banki.ru", "regmarkets.ru", "zoon.ru", "pulscen.ru", "prodoctorov.ru", 
    "blizko.ru", "domclick.ru", "satom.ru", "quto.ru", "edadeal.ru", 
    "cataloxy.ru", "irr.ru", "onliner.by", "shop.by", "deal.by", "yell.ru", 
    "profi.ru", "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by", 
    "market.yandex.ru", "youtube.com", "www.youtube.com", "gosuslugi.ru", 
    "www.gosuslugi.ru", "dzen.ru", "2gis.by", "wildberries.ru", "rutube.ru", 
    "vk.com", "facebook.com", "chipdip.ru"
    }

DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n—Å—Ç—Ä\n—É–ª\n—à—Ç\n—Å–º\n–º–º\n–º–ª\n–∫–≥\n–∫–≤\n–º¬≤\n—Å–º¬≤\n–º2\n—Å–º2"

PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE"

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif; color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important; }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important; color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] div[role="gridcell"] {{
            background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; border-bottom: 1px solid {ROW_BORDER_COLOR} !important;
        }}
        .legend-box {{ padding: 10px; background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px; }}
        .text-red {{ color: #D32F2F; font-weight: bold; }}
        .text-green {{ color: #2E7D32; font-weight: bold; }}
        .text-bold {{ font-weight: 600; }}
        .sort-container {{ background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 8px; margin-bottom: 10px; border: 1px solid {BORDER_COLOR}; }}
        
        .stApp > header {{ background-color: transparent !important; }}
        .stTextInput input:disabled, .stTextArea textarea:disabled, .stSelectbox div[aria-disabled="true"] {{
            opacity: 1 !important; background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; cursor: text !important; -webkit-text-fill-color: {TEXT_COLOR} !important; border-color: {BORDER_COLOR} !important;
        }}
        .stButton button:disabled {{ opacity: 1 !important; background-color: {PRIMARY_COLOR} !important; color: white !important; cursor: progress !important; }}
        div[data-testid="stAppViewContainer"] {{ filter: none !important; opacity: 1 !important; transition: none !important; }}
        /* –°—Ç–∏–ª–∏ –¥–ª—è —Å—Å—ã–ª–æ–∫ –≤–Ω—É—Ç—Ä–∏ –≥—Ä–∞—Ñ–∏–∫–∞ Plotly */
        .chart-link {{
            color: #277EFF !important;
            font-weight: 600 !important;
            text-decoration: none !important;
            border-bottom: 4px solid #CBD5E1 !important; 
            display: inline-block !important;
            transition: border-color 0.2s ease !important;
        }}
        .chart-link:hover {{
            border-bottom-color: #277EFF !important;
            cursor: pointer !important;
        }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# PARSING & METRICS
# ==========================================
# ... (–û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ —Ñ—É–Ω–∫—Ü–∏–π –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)

def get_yandex_dict_info(text, api_key):
    if not api_key: return {'lemma': text, 'pos': 'unknown'}
    url = "https://dictionary.yandex.net/api/v1/dicservice.json/lookup"
    params = {'key': api_key, 'lang': 'ru-ru', 'text': text, 'ui': 'ru'}
    try:
        r = requests.get(url, params=params, timeout=2)
        if r.status_code == 200:
            data = r.json()
            if data.get('def'):
                first_def = data['def'][0]
                return {'lemma': first_def.get('text', text), 'pos': first_def.get('pos', 'unknown')}
    except: pass
    return {'lemma': text, 'pos': 'unknown'}

def get_arsenkin_urls(query, engine_type, region_name, api_token, depth_val=10):
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check"
    url_get = "https://arsenkin.ru/api/tools/get"
    headers = {"Authorization": f"Bearer {api_token}", "Content-type": "application/json"}
    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []
    if "–Ø–Ω–¥–µ–∫—Å" in engine_type: se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type: se_params.append({"type": 11, "region": reg_ids['go']})

    payload = {"tools_name": "check-top", "data": {"queries": [query], "is_snippet": False, "noreask": True, "se": se_params, "depth": depth_val}}
    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        resp_json = r.json()
        if "error" in resp_json or "task_id" not in resp_json: st.error(f"‚ùå –û—à–∏–±–∫–∞ API: {resp_json}"); return []
        task_id = resp_json["task_id"]
        st.toast(f"–ó–∞–¥–∞—á–∞ ID {task_id} –∑–∞–ø—É—â–µ–Ω–∞")
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}"); return []

    status = "process"
    attempts = 0
    # Timeout increased to 10 minutes (120 * 5s)
    while status == "process" and attempts < 120:
        time.sleep(5); attempts += 1
        try:
            r_check = requests.post(url_check, headers=headers, json={"task_id": task_id})
            res_check_data = r_check.json()
            if res_check_data.get("status") == "finish": status = "done"; break
        except: pass

    if status != "done": st.error(f"‚è≥ –¢–∞–π–º-–∞—É—Ç API"); return []

    try:
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id}, timeout=30)
        res_data = r_final.json()
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}"); return []

    results_list = []
    try:
        collect = res_data.get('result', {}).get('result', {}).get('collect')
        if not collect: return []
        final_url_list = []
        if isinstance(collect, list) and len(collect) > 0 and isinstance(collect[0], list): final_url_list = collect[0][0]
        else:
             unique_urls = set()
             for engine_data in collect:
                 if isinstance(engine_data, dict):
                     for _, serps in engine_data.items():
                         for item in serps:
                             if item.get('url') and item.get('url') not in unique_urls:
                                 results_list.append({'url': item['url'], 'pos': item['pos']})
                                 unique_urls.add(item['url'])
             return results_list

        if final_url_list:
            for index, url in enumerate(final_url_list): results_list.append({'url': url, 'pos': index + 1})
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}"); return []
    return results_list

def process_text_detailed(text, settings, n_gram=1):
    text = text.lower().replace('—ë', '–µ')
    words = re.findall(r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+', text)
    stops = set(w.lower().replace('—ë', '–µ') for w in settings['custom_stops'])
    lemmas = []
    forms_map = defaultdict(set)
    for w in words:
        if len(w) < 2: continue
        if not settings['numbers'] and w.isdigit(): continue
        if w in stops: continue
        lemma = w
        if USE_NLP and n_gram == 1:
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            lemma = p.normal_form.replace('—ë', '–µ')
        lemmas.append(lemma)
        forms_map[lemma].add(w)
    return lemmas, forms_map

def check_positions_NO_ALT(query, target_url, region_name, api_token):
    """
    –ê–±—Å–æ–ª—é—Ç–Ω–æ –Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è.
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç alt_urls.
    """
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check"
    url_get = "https://arsenkin.ru/api/tools/get"
    headers = {"Authorization": f"Bearer {api_token}", "Content-type": "application/json"}
    
    # –†–µ–≥–∏–æ–Ω
    reg_ids = REGION_MAP.get(region_name, {"ya": 213})
    region_id_int = int(reg_ids['ya'])
    
    # === JSON –°–¢–†–û–ì–û –ë–ï–ó ALT_URLS ===
    payload = {
        "tools_name": "positions",
        "data": {
            "queries": [str(query)],
            "url": str(target_url).strip(),
            # –°–¢–†–û–ö–ê alt_urls –ü–û–õ–ù–û–°–¢–¨–Æ –£–î–ê–õ–ï–ù–ê –û–¢–°–Æ–î–ê
            "subdomain": True,
            "se": [{"type": 2, "region": region_id_int}],
            "format": 0
        }
    }

    try:
        # 1. –ó–ê–ü–£–°–ö
        r = requests.post(url_set, headers=headers, json=payload, timeout=20)
        
        # –ï—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –≤–µ—Ä–Ω—É–ª 500 –∏–ª–∏ 400
        if r.status_code != 200:
            return 0, {"error": f"HTTP {r.status_code}", "text": r.text}
            
        resp = r.json()
        if "error" in resp: return 0, resp
        
        task_id = resp.get("task_id")
        if not task_id: return 0, {"error": "No Task ID", "resp": resp}
        
        # 2. –û–ñ–ò–î–ê–ù–ò–ï
        for i in range(40):
            time.sleep(2)
            r_c = requests.post(url_check, headers=headers, json={"task_id": task_id})
            if r_c.json().get("status") == "finish":
                break
        else:
            return 0, {"error": "Timeout"}

        # 3. –†–ï–ó–£–õ–¨–¢–ê–¢
        r_g = requests.post(url_get, headers=headers, json={"task_id": task_id})
        data = r_g.json()
        
        res_list = data.get("result", [])
        if not res_list: return 0, data
            
        item = res_list[0]
        pos = item.get('position')
        if pos is None: pos = item.get('pos')
        
        if str(pos) in ['0', '-', '', 'None']:
            return 0, item 
            
        return int(pos), None

    except Exception as e:
        return 0, {"error": f"Crash: {str(e)}"}

def parse_page(url, settings, query_context=""):
    import streamlit as st
    try:
        from curl_cffi import requests as cffi_requests
        headers = {
            'User-Agent': settings['ua'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        }
        r = cffi_requests.get(url, headers=headers, timeout=20, impersonate="chrome110")
        if r.status_code == 403: raise Exception("CURL_CFFI –ø–æ–ª—É—á–∏–ª 403 Forbidden")
        if r.status_code != 200: return None
        content = r.content
        encoding = r.encoding if r.encoding else 'utf-8'
    except Exception:
        try:
            import requests
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            session = requests.Session()
            headers = {'User-Agent': settings['ua']}
            r = session.get(url, headers=headers, timeout=20, verify=False)
            if r.status_code != 200: return None
            content = r.content
            encoding = r.apparent_encoding
        except Exception: return None

    try:
        # 1. –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Soup (–ü–æ–ª–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞)
        soup = BeautifulSoup(content, 'html.parser', from_encoding=encoding)
        
        # === –ù–û–í–û–ï: –°–æ–±–∏—Ä–∞–µ–º Title –∏ Description –æ—Ç–¥–µ–ª—å–Ω–æ ===
        page_title = soup.title.string.strip() if soup.title and soup.title.string else ""
        
        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})
        page_desc = meta_desc_tag['content'].strip() if meta_desc_tag and meta_desc_tag.get('content') else ""
        # ====================================================

        # === –õ–û–ì–ò–ö–ê –¢–ê–ë–õ–ò–¶–´ 2 (–ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ URL/–°—Å—ã–ª–∫–µ) ===
        product_titles = []
        search_roots = set()
        if query_context:
            clean_q = query_context.lower().replace('–∫—É–ø–∏—Ç—å', '').replace('—Ü–µ–Ω–∞', '').replace(' –≤ ', ' ')
            words = re.findall(r'[–∞-—èa-z]+', clean_q)
            for w in words:
                if len(w) > 3: search_roots.add(w[:-1])
                else: search_roots.add(w)
        
        parsed_current = urlparse(url)
        current_path_clean = parsed_current.path.rstrip('/')
        seen_titles = set()
        
        for a in soup.find_all('a', href=True):
            txt = a.get_text(strip=True)
            raw_href = a['href']
            if len(txt) < 5 or len(txt) > 300: continue
            if raw_href.startswith('#') or raw_href.startswith('javascript'): continue
            
            abs_href = urljoin(url, raw_href)
            parsed_href = urlparse(abs_href)
            href_path_clean = parsed_href.path.rstrip('/')
            
            is_child_path = href_path_clean.startswith(current_path_clean)
            is_deeper = len(href_path_clean) > len(current_path_clean)
            is_not_query_param_only = (href_path_clean != current_path_clean)

            if is_child_path and is_deeper and is_not_query_param_only:
                txt_lower = txt.lower()
                href_lower = abs_href.lower()
                has_keywords = False
                if search_roots:
                    for root in search_roots:
                        if root in txt_lower or root in href_lower:
                            has_keywords = True; break
                else:
                    if re.search(r'\d', txt): has_keywords = True

                is_buy_button = txt_lower in {'–∫—É–ø–∏—Ç—å', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ', '–≤ –∫–æ—Ä–∑–∏–Ω—É', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Ü–µ–Ω–∞'}
                if has_keywords and not is_buy_button:
                    if txt not in seen_titles:
                        product_titles.append(txt)
                        seen_titles.add(txt)
        # ========================================================
        
        h1_tag = soup.find('h1')
        h1_text = h1_tag.get_text(strip=True) if h1_tag else ""

        # 2. –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –¢–∞–±–ª–∏—Ü—ã 2 (–£–¥–∞–ª—è–µ–º –±–ª–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤)
        soup_no_grid = BeautifulSoup(content, 'html.parser', from_encoding=encoding)
        grid_div = soup_no_grid.find('div', class_='an-container-fluid an-container-xl')
        if grid_div: grid_div.decompose()
        
        # === [–í–ê–ñ–ù–û] –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ö–û–ù–¢–ï–ù–¢–ê –ü–û –ì–ê–õ–û–ß–ö–ê–ú ===
        tags_to_remove = []
        if settings['noindex']: tags_to_remove.append('noindex')
        
        for s in [soup, soup_no_grid]:
            for c in s.find_all(string=lambda text: isinstance(text, Comment)): c.extract()
            if tags_to_remove:
                for t in s.find_all(tags_to_remove): t.decompose()
            for script in s(["script", "style", "svg", "path", "noscript"]): script.decompose()

        # –¢–µ–∫—Å—Ç —Å—Å—ã–ª–æ–∫ (–∞–Ω–∫–æ—Ä—ã)
        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)
        
        # –°–±–æ—Ä –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ì–û —Ç–µ–∫—Å—Ç–∞ (Description, Alt, Title)
        extra_text = []
        # Description –¥–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Ç–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–∂–µ
        if page_desc: extra_text.append(page_desc)

        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])

        # –°–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç
        body_text_raw = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text = re.sub(r'\s+', ' ', body_text_raw).strip()

        body_text_no_grid_raw = soup_no_grid.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text_no_grid = re.sub(r'\s+', ' ', body_text_no_grid_raw).strip()

        if not body_text: return None
            
        return {
            'url': url, 
            'domain': urlparse(url).netloc, 
            'body_text': body_text, 
            'body_text_no_grid': body_text_no_grid,
            'anchor_text': anchor_text,
            'h1': h1_text,
            'product_titles': product_titles,
            # !!! –ù–û–í–´–ï –ü–û–õ–Ø –î–õ–Ø DASHBOARD !!!
            'meta_title': page_title,
            'meta_desc': page_desc
        }
    except Exception:
        return None

def analyze_meta_gaps(comp_data_full, my_data, settings):
    """
    –£–ú–ù–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† META-–¢–ï–ì–û–í v2.1
    1. –£—á–∏—Ç—ã–≤–∞–µ—Ç –≤–µ—Å –ø–æ–∑–∏—Ü–∏–∏ (—Å–ª–æ–≤–∞ —Ç–æ–ø–æ–≤ –≤–∞–∂–Ω–µ–µ).
    2. –ü–æ—Ä–æ–≥ –≤—Ö–æ–∂–¥–µ–Ω–∏—è: –°–¢–†–û–ì–û 50% (—Å–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —É –ø–æ–ª–æ–≤–∏–Ω—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤).
    3. –§–∏–ª—å—Ç—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–≥–∏ –∏ —Å–æ—é–∑—ã.
    """
    if not comp_data_full: return None
    
    # === 1. –ù–ê–°–¢–†–û–ô–ö–ò –ê–õ–ì–û–†–ò–¢–ú–ê ===
    TOTAL_COMPS = len(comp_data_full)
    
    # !!! –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–¢–†–û–ì–û 50% !!!
    MIN_OCCURRENCE_PCT = 0.4 
    
    # –ú–∏–Ω–∏–º—É–º 2 —Å–∞–π—Ç–∞, –¥–∞–∂–µ –µ—Å–ª–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤—Å–µ–≥–æ 3
    MIN_COUNT = max(2, int(TOTAL_COMPS * MIN_OCCURRENCE_PCT))

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (–ß–∏—Å—Ç–∫–∞ –º—É—Å–æ—Ä–∞)
    def fast_tokenize(text):
        if not text: return set()
        
        # –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
        stop_garbage = {
            '–≤', '–Ω–∞', '–∏', '—Å', '—Å–æ', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '—É', 
            '–æ', '–æ–±', '–∑–∞', '–Ω–∞–¥', '–ø–æ–¥', '–ø—Ä–∏', '–ø—Ä–æ', '–±–µ–∑', '—á–µ—Ä–µ–∑', '–º–µ–∂–¥—É',
            '–∞', '–Ω–æ', '–∏–ª–∏', '–¥–∞', '–∫–∞–∫', '—á—Ç–æ', '—á—Ç–æ–±—ã', '–µ—Å–ª–∏', '—Ç–æ', '–ª–∏', '–±—ã', '–∂–µ', 
            '–≥', '–æ–±–ª', '—Ä', '—Ä—É–±', '–º–º', '—Å–º', '–º', '–∫–≥', '—Ç', '—à—Ç', '–¥–Ω',
            '–≤–µ—Å—å', '–≤—Å–µ', '–≤—Å—ë', '—Å–≤–æ–π', '–≤–∞—à', '–Ω–∞—à', '–º—ã', '–≤—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏',
            '–∫—É–ø–∏—Ç—å', '—Ü–µ–Ω–∞', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø—Ä–æ–¥–∞–∂–∞', '–Ω–µ–¥–æ—Ä–æ–≥–æ', 
            '–º–æ—Å–∫–≤–∞', '—Å–ø–±' 
        }
        # –£–±–∏—Ä–∞–µ–º –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —à—Ç–∞–º–ø—ã –∏–∑ —Å—Ç–æ–ø-–ª–∏—Å—Ç–∞, —á—Ç–æ–±—ã –æ–Ω–∏ –ø–æ–ø–∞–¥–∞–ª–∏ –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if '–∫—É–ø–∏—Ç—å' in stop_garbage: stop_garbage.remove('–∫—É–ø–∏—Ç—å') 
        if '—Ü–µ–Ω–∞' in stop_garbage: stop_garbage.remove('—Ü–µ–Ω–∞')
        
        if settings.get('custom_stops'):
            stop_garbage.update(set(settings['custom_stops']))

        lemmas = set()
        words = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9]+', text.lower())
        
        for w in words:
            if len(w) < 2: continue 
            if w in stop_garbage: continue
            
            # NLP –§–∏–ª—å—Ç—Ä
            if morph:
                try:
                    p = morph.parse(w)[0]
                    # –ò—Å–∫–ª—é—á–∞–µ–º –ü—Ä–µ–¥–ª–æ–≥–∏, –°–æ—é–∑—ã, –ß–∞—Å—Ç–∏—Ü—ã, –ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è, –ú–µ–∂–¥–æ–º–µ—Ç–∏—è
                    if p.tag.POS in {'PREP', 'CONJ', 'PRCL', 'NPRO', 'INTJ'}:
                        continue
                    if p.normal_form in stop_garbage:
                        continue
                    lemmas.add(p.normal_form)
                except: 
                    lemmas.add(w)
            else:
                lemmas.add(w)
        return lemmas

    # === 2. –°–ë–û–† –î–ê–ù–ù–´–• –° –í–ï–°–ê–ú–ò ===
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞: word -> {'count': 0, 'score': 0.0}
    stats_map = {
        'title': defaultdict(lambda: {'count': 0, 'score': 0.0}),
        'desc': defaultdict(lambda: {'count': 0, 'score': 0.0}),
        'h1': defaultdict(lambda: {'count': 0, 'score': 0.0})
    }
    
    detailed_rows = []

    for i, item in enumerate(comp_data_full):
        # –í–µ—Å –ø–æ–∑–∏—Ü–∏–∏: 1-–µ –º–µ—Å—Ç–æ = –≤–µ—Å–æ–º–µ–µ, —á–µ–º 10-–µ
        rank_weight = 1.0 + ( (TOTAL_COMPS - i) / TOTAL_COMPS ) * 1.5
        
        t_tok = fast_tokenize(item.get('meta_title', ''))
        d_tok = fast_tokenize(item.get('meta_desc', ''))
        h_tok = fast_tokenize(item.get('h1', ''))
        
        for w in t_tok:
            stats_map['title'][w]['count'] += 1
            stats_map['title'][w]['score'] += rank_weight
            
        for w in d_tok:
            stats_map['desc'][w]['count'] += 1
            stats_map['desc'][w]['score'] += rank_weight
            
        for w in h_tok:
            stats_map['h1'][w]['count'] += 1
            stats_map['h1'][w]['score'] += rank_weight

        detailed_rows.append({
            'URL': item['url'],
            'Title': item.get('meta_title', ''),
            'Description': item.get('meta_desc', ''),
            'H1': item.get('h1', '')
        })

    # === 3. –ê–ù–ê–õ–ò–ó –†–ê–ó–†–´–í–û–í (GAPS) ===
    
    my_tokens = {
        'title': fast_tokenize(my_data.get('meta_title', '')),
        'desc': fast_tokenize(my_data.get('meta_desc', '')),
        'h1': fast_tokenize(my_data.get('h1', ''))
    }

    def process_category(cat_key):
        data = stats_map[cat_key]
        important_words = []
        
        for word, metrics in data.items():
            # 1. –û—Ç—Å–µ–∫–∞–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ä–µ–∂–µ, —á–µ–º —É 50% –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
            if metrics['count'] < MIN_COUNT:
                continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–æ –∏ –µ–≥–æ "–≤–∞–∂–Ω–æ—Å—Ç—å" (Score)
            important_words.append((word, metrics['score']))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (Score)
        important_words.sort(key=lambda x: x[1], reverse=True)
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —è–¥—Ä–æ (–¢–æ–ø-15 —Å–ª–æ–≤, –ø—Ä–æ—à–µ–¥—à–∏—Ö —Ñ–∏–ª—å—Ç—Ä 50%)
        core_semantics = [x[0] for x in important_words[:15]]
        
        if not core_semantics:
            return 100, [] 
            
        matches = 0
        missing = []
        
        for w in core_semantics:
            if w in my_tokens[cat_key]:
                matches += 1
            else:
                missing.append(w)
        
        if len(core_semantics) > 0:
            score = int((matches / len(core_semantics)) * 100)
        else:
            score = 100
            
        return score, missing

    s_t, m_t = process_category('title')
    s_d, m_d = process_category('desc')
    s_h, m_h = process_category('h1')

    return {
        'scores': {'title': s_t, 'desc': s_d, 'h1': s_h},
        'missing': {'title': m_t, 'desc': m_d, 'h1': m_h},
        'detailed': detailed_rows,
        'my_data': {
            'Title': my_data.get('meta_title', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'),
            'Description': my_data.get('meta_desc', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'),
            'H1': my_data.get('h1', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω')
        }
    }
        
def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    # --- –ò–ú–ü–û–†–¢–´ ---
    import math
    import pandas as pd
    import numpy as np
    from collections import Counter, defaultdict
    import re
    from urllib.parse import urlparse

    # 1. –°–¢–†–û–ì–û–ï –û–ö–†–£–ì–õ–ï–ù–ò–ï –î–û –°–û–¢–´–•
    def strict_round(num):
        return round(num, 2)

    # --- 2. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–†–§–û–õ–û–ì–ò–ò ---
    try:
        import pymorphy2
        if 'local_morph' not in locals():
            local_morph = pymorphy2.MorphAnalyzer()
    except ImportError:
        local_morph = None

    # –ö–∞—Ä—Ç–∞ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏
    POS_MAP = {
        'NOUN': '–°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ', 'ADJF': '–ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ', 'ADJS': '–ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ',
        'VERB': '–ì–ª–∞–≥–æ–ª', 'INFN': '–ì–ª–∞–≥–æ–ª', 'PRTF': '–ü—Ä–∏—á–∞—Å—Ç–∏–µ',
        'PRTS': '–ü—Ä–∏—á–∞—Å—Ç–∏–µ', 'GRND': '–î–µ–µ–ø—Ä–∏—á–∞—Å—Ç–∏–µ', 'NUMR': '–ß–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–µ',
        'ADVB': '–ù–∞—Ä–µ—á–∏–µ', 'NPRO': '–ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ', 'PREP': '–ü—Ä–µ–¥–ª–æ–≥',
        'CONJ': '–°–æ—é–∑', 'PRCL': '–ß–∞—Å—Ç–∏—Ü–∞', 'INTJ': '–ú–µ–∂–¥–æ–º–µ—Ç–∏–µ'
    }

    # –ì–†–£–ü–ü–ò–†–û–í–ö–ê: –õ–µ–º–º–∞ + –ß–∞—Å—Ç—å —Ä–µ—á–∏
    def get_lemma_pos_key(word):
        if not local_morph:
            return (word.lower(), "–ü—Ä–æ—á–µ–µ")
        try:
            p = local_morph.parse(word)[0]
            # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ (—Ç—Ä—É–±—ã -> —Ç—Ä—É–±–∞)
            lemma = p.normal_form.replace('—ë', '–µ')
            # –ß–∞—Å—Ç—å —Ä–µ—á–∏ (—á—Ç–æ–±—ã "—Ç—Ä—É–±–Ω—ã–π" –∏ "—Ç—Ä—É–±–∞" –Ω–µ —Å–º–µ—à–∏–≤–∞–ª–∏—Å—å)
            pos_tag = p.tag.POS
            pos_ru = POS_MAP.get(pos_tag, '–ü—Ä–æ—á–µ–µ')
            return (lemma, pos_ru)
        except:
            return (word.lower(), "–ü—Ä–æ—á–µ–µ")

    # –ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–ê
    def analyze_text_structure(text):
        if not text: return [], {}, 0
        
        words = re.findall(r'[–∞-—è–ê-–Ø—ë–Åa-zA-Z0-9\-]+', text.lower())
        
        lemma_pos_list = []      
        forms_map = defaultdict(set)
        valid_word_count = 0     

        for w in words:
            if len(w) < 2: continue
            if not settings['numbers'] and w.isdigit(): continue
            
            # –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏ –∏–∑ –ø–æ–¥—Å—á–µ—Ç–∞
            if local_morph:
                p = local_morph.parse(w)[0]
                if p.tag.POS in {'PREP', 'CONJ', 'PRCL', 'INTJ', 'NPRO'}:
                    continue
            
            key = get_lemma_pos_key(w)
            
            lemma_pos_list.append(key)
            forms_map[key].add(w)
            valid_word_count += 1
            
        return lemma_pos_list, forms_map, valid_word_count

    # ==========================================
    # 3. –°–ë–û–† –°–¢–ê–¢–ò–°–¢–ò–ö–ò (TF)
    # ==========================================
    
    # –•—Ä–∞–Ω–∏–ª–∏—â–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –¢–û–ü—É
    global_stats = defaultdict(lambda: {
        'docs_containing': 0,  # DF
        'sum_tf': 0.0,         # –°—É–º–º–∞ TF –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ
        'forms': set(),        
        'counts_list': []      # –î–ª—è –º–µ–¥–∏–∞–Ω—ã
    })

    N = len(comp_data_full) # –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    if N == 0: N = 1 

    # --- –ü—Ä–æ—Ö–æ–¥ –ø–æ –ö–û–ù–ö–£–†–ï–ù–¢–ê–ú ---
    for p in comp_data_full:
        if not p.get('body_text'): continue
        
        doc_tokens, doc_forms, doc_len = analyze_text_structure(p['body_text'])
        
        if doc_len == 0: continue

        # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—Ç–∏–ª–æ—Å—å –≤ –≠–¢–û–ú –¥–æ–∫—É–º–µ–Ω—Ç–µ
        doc_counter = Counter(doc_tokens) 

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º TF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –≠–¢–û–ú –¥–æ–∫—É–º–µ–Ω—Ç–µ
        for key, count in doc_counter.items():
            # TF = –ö–æ–ª-–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π —Å–ª–æ–≤–∞ / –í—Å–µ–≥–æ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            tf = count / doc_len
            
            global_stats[key]['docs_containing'] += 1
            global_stats[key]['sum_tf'] += tf 
            global_stats[key]['forms'].update(doc_forms[key])
            global_stats[key]['counts_list'].append(count)

    # --- –ü—Ä–æ—Ö–æ–¥ –ø–æ –í–ê–®–ï–ú–£ —Å–∞–π—Ç—É ---
    my_counts_map = Counter()
    my_clean_domain = "local"
    
    if my_data and my_data.get('body_text'):
        my_tokens, my_forms, my_len = analyze_text_structure(my_data['body_text'])
        my_counts_map = Counter(my_tokens)
        if my_data.get('domain'):
            my_clean_domain = my_data.get('domain').lower().replace('www.', '').split(':')[0]

    # ==========================================
    # 4. –†–ê–°–ß–ï–¢ –ú–ï–¢–†–ò–ö –ò –¢–ê–ë–õ–ò–¶
    # ==========================================
    
    table_depth = []
    table_hybrid = []
    missing_semantics_high = []
    missing_semantics_low = []
    
    words_with_median_gt_0 = set()
    my_found_words = set()

    sorted_keys = sorted(global_stats.keys(), key=lambda x: x[0])

    for key in sorted_keys:
        lemma, pos = key
        data = global_stats[key]
        
        # 1. –†–∞—Å—á–µ—Ç IDF = log10(–í—Å–µ–≥–æ –¥–æ–∫–æ–≤ / –î–æ–∫–æ–≤ —Å–æ —Å–ª–æ–≤–æ–º)
        df = data['docs_containing']
        if df == 0: continue 
        
        idf = math.log10(N / df)
        
        # 2. –†–∞—Å—á–µ—Ç –°—Ä–µ–¥–Ω–µ–≥–æ TF = (–°—É–º–º–∞ TF –≤—Å–µ—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤) / –ö–æ–ª-–≤–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        avg_tf = data['sum_tf'] / N
        
        # 3. –ò—Ç–æ–≥–æ–≤—ã–π TF-IDF = Avg_TF * IDF
        tf_idf_value = avg_tf * idf
        
        my_count = my_counts_map[key]
        
        # --- –¢–ê–ë–õ–ò–¶–ê TF-IDF (–¢–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏) ---
        table_hybrid.append({
            "–°–ª–æ–≤–æ": lemma,
            "–ß–∞—Å—Ç—å —Ä–µ—á–∏": pos,
            "TF-IDF –¢–û–ü": strict_round(tf_idf_value), # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ 2 –∑–Ω–∞–∫–æ–≤ (1.23)
            "IDF": strict_round(idf),                 # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ 2 –∑–Ω–∞–∫–æ–≤
            "–ö–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤": df,
            "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_count
        })
        
        # --- –¢–ê–ë–õ–ò–¶–ê –ì–õ–£–ë–ò–ù–´ (–ú–µ–¥–∏–∞–Ω—ã) ---
        raw_counts = data['counts_list']
        zeros_to_add = N - len(raw_counts)
        full_counts = raw_counts + [0] * zeros_to_add
        full_counts.sort()
        
        rec_median = int(np.median(full_counts) + 0.5)
        obs_max = max(full_counts) if full_counts else 0
        
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        if obs_max == 0 and my_count == 0: continue

        if rec_median >= 1:
            words_with_median_gt_0.add(lemma)
            if my_count > 0: my_found_words.add(lemma)

        forms_str = ", ".join(sorted(list(data['forms'])))[:100]

        # –õ–æ–≥–∏–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (–£–ø—É—â–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞)
        if my_count == 0:
            weight = tf_idf_value * (rec_median if rec_median > 0 else 0.5)
            item = {'word': lemma, 'weight': weight}
            if rec_median >= 1: missing_semantics_high.append(item)
            else: missing_semantics_low.append(item)

        diff = rec_median - my_count
        if diff == 0: status = "–ù–æ—Ä–º–∞"; action_text = "‚úÖ"; sort_val = 0
        elif diff > 0: status = "–ù–µ–¥–æ—Å–ø–∞–º"; action_text = f"+{diff}"; sort_val = diff
        else: status = "–ü–µ—Ä–µ—Å–ø–∞–º"; action_text = f"{diff}"; sort_val = abs(diff)

        table_depth.append({
            "–°–ª–æ–≤–æ": lemma,
            "–°–ª–æ–≤–æ—Ñ–æ—Ä–º—ã": forms_str,
            "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_count,
            "–ú–µ–¥–∏–∞–Ω–∞": rec_median,
            "–ú–∞–∫—Å–∏–º—É–º (–∫–æ–Ω–∫—É—Ä.)": obs_max,
            "–°—Ç–∞—Ç—É—Å": status,
            "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": action_text,
            "is_missing": (my_count == 0),
            "sort_val": sort_val
        })

    # ============================================
    # 5. –§–ò–ù–ê–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò
    # ============================================
    total_needed = len(words_with_median_gt_0)
    total_found = len(my_found_words)
    my_width_score = int(min(100, (total_found / total_needed) * 105)) if total_needed > 0 else 0
    
    table_rel = []
    my_site_found = False
    
    for item in original_results:
        url = item['url']
        doc_data = next((x for x in comp_data_full if x['url'] == url), None)
        width_val = 0
        
        if doc_data and doc_data.get('body_text'):
             toks, _, _ = analyze_text_structure(doc_data['body_text'])
             lemmas_only = [t[0] for t in toks]
             inter = set(lemmas_only).intersection(words_with_median_gt_0)
             width_val = int(min(100, (len(inter) / total_needed) * 105)) if total_needed > 0 else 0
             
        d_name = urlparse(url).netloc
        if my_clean_domain != "local" and my_clean_domain in d_name:
            d_name += " (–í—ã)"; my_site_found = True
            
        table_rel.append({ 
            "–î–æ–º–µ–Ω": d_name, 
            "URL": url, 
            "–ü–æ–∑–∏—Ü–∏—è": item['pos'], 
            "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": width_val, 
            "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": width_val 
        })

    if not my_site_found:
        my_l = "–í–∞—à —Å–∞–π—Ç"
        my_u_val = my_data.get('url', '#') if my_data else '#'
        table_rel.append({ 
            "–î–æ–º–µ–Ω": my_l, 
            "URL": my_u_val, 
            "–ü–æ–∑–∏—Ü–∏—è": my_serp_pos, 
            "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": my_width_score, 
            "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": my_width_score 
        })

    missing_semantics_high.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low.sort(key=lambda x: x['weight'], reverse=True)
    
    good_urls, bad_urls_dicts, trend_info = analyze_serp_anomalies(pd.DataFrame(table_rel))

    return { 
        "depth": pd.DataFrame(table_depth), 
        "hybrid": pd.DataFrame(table_hybrid), 
        "relevance_top": pd.DataFrame(table_rel).sort_values(by='–ü–æ–∑–∏—Ü–∏—è'), 
        "my_score": {"width": my_width_score, "depth": my_width_score}, 
        "missing_semantics_high": missing_semantics_high, 
        "missing_semantics_low": missing_semantics_low[:500],
        "debug_width": {"found": total_found, "needed": total_needed}
    }
    
def get_hybrid_word_type(word, main_marker_root, specs_dict=None):
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä 3.1 (–§–∏–∫—Å –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤).
    """
    w = word.lower()
    specs_dict = specs_dict or set()
    
    # 1. –ú–ê–†–ö–ï–†
    if w == main_marker_root: return "1. üíé –ú–∞—Ä–∫–µ—Ä (–¢–æ–≤–∞—Ä)"
    if morph:
        norm = morph.parse(w)[0].normal_form
        if norm == main_marker_root: return "1. üíé –ú–∞—Ä–∫–µ—Ä (–¢–æ–≤–∞—Ä)"

    # 2. –°–¢–ê–ù–î–ê–†–¢–´
    if re.search(r'(gost|din|iso|en|tu|astm|aisi|–≥–æ—Å—Ç|–æ—Å—Ç|—Ç—É|–¥–∏–Ω)', w):
        return "6. üìú –°—Ç–∞–Ω–¥–∞—Ä—Ç"

    # 3. –†–ê–ó–ú–ï–†–´ / –¢–ï–•. –ü–ê–†–ê–ú–ï–¢–†–´
    # –ê. –ì–æ–ª—ã–µ —Ü–∏—Ñ—Ä—ã (10, 50.5)
    if re.fullmatch(r'\d+([.,]\d+)?', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    # –ë. –†–∞–∑–º–µ—Ä—ã —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ (10—Ö20, 10*20, 10-20, 10/20) <--- –î–û–ë–ê–í–ò–õ –¢–ò–†–ï –ò –°–õ–ï–®
    if re.search(r'^\d+[x—Ö*\-/]\d+', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    # –í. –ï–¥–∏–Ω–∏—Ü—ã (–º–º, –∫–≥)
    if re.search(r'\d+(–º–º|mm|–º|m|kg|–∫–≥|bar|–±–∞—Ä|–∞—Ç–º)$', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    # –ì. –ü—Ä–µ—Ñ–∏–∫—Å—ã (–î—É, –†—É, SDR)
    if re.match(r'^(d|dn|pn|sn|sdr|–¥—É|—Ä—É|√∏)\d+', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"

    # 4. –ú–ê–†–ö–ò / –°–ü–õ–ê–í–´
    if w in specs_dict: return "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –º–∞—Ä–æ–∫ (–ë—É–∫–≤—ã+–¶–∏—Ñ—Ä—ã)
    if re.search(r'\d', w): return "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"

    # 5. –õ–ê–¢–ò–ù–ò–¶–ê (–ë—Ä–µ–Ω–¥—ã)
    if re.search(r'^[a-z\-]+$', w): return "7. üî† –õ–∞—Ç–∏–Ω–∏—Ü–∞/–ë—Ä–µ–Ω–¥"

    # 6. –¢–ï–ö–°–¢
    if morph:
        p = morph.parse(w)[0]
        tag = p.tag
        if {'PREP'} in tag or {'CONJ'} in tag: return "SKIP"
        if {'ADJF'} in tag or {'PRTF'} in tag or {'ADJS'} in tag: return "2. üé® –°–≤–æ–π—Å—Ç–≤–∞"
        if {'NOUN'} in tag: return "4. üîó –î–æ–ø–æ–ª–Ω–µ–Ω–∏—è"

    if w.endswith(('–∏–π', '—ã–π', '–∞—è', '–æ–µ', '—ã–µ', '–∞—è')): return "2. üé® –°–≤–æ–π—Å—Ç–≤–∞"
    return "4. üîó –î–æ–ø–æ–ª–Ω–µ–Ω–∏—è"
    
def calculate_naming_metrics(comp_data_full, my_data, settings):
    """
    –¢–∞–±–ª–∏—Ü–∞ 2. –ë–µ–∑ "–æ–±—Ä–µ–∑–∞–Ω–∏—è" —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤.
    """
    # –ü–æ–¥–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è
    SPECS_SET = st.session_state.get('categorized_dimensions', set())
    if not SPECS_SET: _, _, SPECS_SET, _, _, _ = load_lemmatized_dictionaries()

    # 1. –ú–æ–π —Å–∞–π—Ç
    my_tokens = []
    if my_data and my_data.get('body_text_no_grid'):
        # –°–≤–æ—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –î—É50
        raw_w = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', my_data['body_text_no_grid'].lower())
        for w in raw_w:
            # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–ª–æ–≤–∞
            if not re.search(r'\d', w) and morph:
                my_tokens.append(morph.parse(w)[0].normal_form)
            else:
                my_tokens.append(w)

    # 2. –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã
    all_words_flat = []
    site_vocab_map = []
    
    for p in comp_data_full:
        titles = p.get('product_titles', [])
        valid_titles = [t for t in titles if 5 < len(t) < 150]
        
        if not valid_titles:
            site_vocab_map.append(set())
            continue
            
        curr_site_tokens = set()
        for t in valid_titles:
            words = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower())
            for w in words:
                if len(w) < 2: continue
                
                # –õ–û–ì–ò–ö–ê –°–û–•–†–ê–ù–ï–ù–ò–Ø –§–û–†–ú–´:
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Ü–∏—Ñ—Ä–∞ -> —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å (d50 -> d50)
                if re.search(r'\d', w):
                    token = w
                elif re.search(r'^[a-z]+$', w): # –õ–∞—Ç–∏–Ω–∏—Ü–∞ -> –∫–∞–∫ –µ—Å—Ç—å
                    token = w
                elif morph: # –†—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ -> –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º (—Å—Ç–∞–ª—å–Ω–∞—è -> —Å—Ç–∞–ª—å–Ω–æ–π)
                    token = morph.parse(w)[0].normal_form
                else:
                    token = w
                
                all_words_flat.append(token)
                curr_site_tokens.add(token)
                
        site_vocab_map.append(curr_site_tokens)

    if not all_words_flat: return pd.DataFrame()
    N_sites = len(site_vocab_map)

    # 3. –ú–∞—Ä–∫–µ—Ä (–°–∞–º–æ–µ —á–∞—Å—Ç–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–ª–æ–≤–æ)
    counts = Counter([w for w in all_words_flat if not re.search(r'\d', w)])
    main_marker_root = ""
    # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
    for w, c in counts.most_common(10):
        if morph and 'NOUN' in morph.parse(w)[0].tag: main_marker_root = w; break
    if not main_marker_root and counts: main_marker_root = counts.most_common(1)[0][0]

    # 4. –°–±–æ—Ä —Ç–∞–±–ª–∏—Ü—ã
    vocab = sorted(list(set(all_words_flat)))
    table_rows = []
    
    for token in vocab:
        if token in GARBAGE_LATIN_STOPLIST: continue
        
        # –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å
        sites_with_word = sum(1 for s_set in site_vocab_map if token in s_set)
        freq_percent = int((sites_with_word / N_sites) * 100)
        
        # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø
        cat = get_hybrid_word_type(token, main_marker_root, SPECS_SET)
        
        if cat == "SKIP": continue
        
        # –§–∏–ª—å—Ç—Ä—ã –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        # –ú–∞—Ä–∫–∏ –∏ –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç 5%
        is_spec = "–ú–∞—Ä–∫–∞" in cat or "–°—Ç–∞–Ω–¥–∞—Ä—Ç" in cat
        if is_spec and freq_percent < 5: continue
        
        # –û–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞ –æ—Ç 15%
        if not is_spec and "–†–∞–∑–º–µ—Ä—ã" not in cat and freq_percent < 15: continue
        
        # –†–∞–∑–º–µ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ —Ä–µ–∞–ª—å–Ω–æ —á–∞—Å—Ç—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ö–æ–¥–æ–≤–æ–π –¥–∏–∞–º–µ—Ç—Ä)
        # –ò–Ω–∞—á–µ —Ç–∞–±–ª–∏—Ü–∞ –±—É–¥–µ—Ç –∑–∞–±–∏—Ç–∞ —Ü–∏—Ñ—Ä–∞–º–∏ 10, 11, 12...
        if "–†–∞–∑–º–µ—Ä—ã" in cat and freq_percent < 15: continue

        rec_median = 1 if freq_percent > 30 else 0
        my_tf = my_tokens.count(token)
        diff = rec_median - my_tf
        action_text = f"+{diff}" if diff > 0 else ("‚úÖ" if diff == 0 else f"{diff}")
        
        table_rows.append({
            "–¢–∏–ø —Ö–∞—Ä-–∫–∏": cat[3:],
            "–°–ª–æ–≤–æ": token, # –í—ã–≤–æ–¥–∏–º —Ç–æ–∫–µ–Ω –∫–∞–∫ –µ—Å—Ç—å (—Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏ –±—É–∫–≤–∞–º–∏)
            "–ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å (%)": f"{freq_percent}%",
            "–£ –í–∞—Å": my_tf,
            "–ú–µ–¥–∏–∞–Ω–∞": rec_median,
            "–î–æ–±–∞–≤–∏—Ç—å": action_text,
            "raw_freq": freq_percent,
            "cat_sort": int(cat[0])
        })
        
    df = pd.DataFrame(table_rows)
    if not df.empty:
        df = df.sort_values(by=["cat_sort", "raw_freq"], ascending=[True, False])
        
    return df

def analyze_ideal_name(comp_data_full):
    """
    –°—Ç—Ä–æ–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å —É—á–µ—Ç–æ–º –ú–∞—Ä–æ–∫ –∏ –ì–û–°–¢–æ–≤.
    """
    # –ü–æ–¥–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
    SPECS_SET = st.session_state.get('categorized_dimensions', set())
    if not SPECS_SET: _, _, SPECS_SET, _, _, _ = load_lemmatized_dictionaries()

    titles = []
    for d in comp_data_full:
        ts = d.get('product_titles', [])
        titles.extend([t for t in ts if 5 < len(t) < 150])
    
    if not titles: return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö", []

    # –ú–∞—Ä–∫–µ—Ä
    all_w = []
    for t in titles: all_w.extend(re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower()))
    c = Counter(all_w)
    main_marker_root = ""
    for w, _ in c.most_common(5):
        if not re.search(r'\d', w):
             if morph and 'NOUN' in morph.parse(w)[0].tag: main_marker_root = w; break
             elif not morph: main_marker_root = w; break
    if not main_marker_root and c: main_marker_root = c.most_common(1)[0][0]

    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    structure_counter = Counter()
    vocab_by_type = defaultdict(Counter)
    
    sample = titles[:500]
    
    for t in sample:
        words = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower())
        pattern = []
        
        for w in words:
            if len(w) < 2: continue
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º —Å–ª–æ–≤–∞—Ä—è
            cat_full = get_hybrid_word_type(w, main_marker_root, SPECS_SET)
            if cat_full == "SKIP": continue
            
            # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∏–º—è —Ç–∏–ø–∞ ("–°–≤–æ–π—Å—Ç–≤–∞", "–ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤", "–°—Ç–∞–Ω–¥–∞—Ä—Ç")
            # "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤" -> "–ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"
            try:
                cat_short = cat_full.split('.', 1)[1].strip().split(' ', 1)[1] # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –∏–∫–æ–Ω–∫–∏
            except:
                cat_short = cat_full # Fallback
            
            vocab_by_type[cat_short][w] += 1
            
            if not pattern or pattern[-1] != cat_short:
                pattern.append(cat_short)
        
        if pattern:
            structure_str = " + ".join(pattern)
            structure_counter[structure_str] += 1
            
    # –°–±–æ—Ä–∫–∞
    if not structure_counter: return "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞", []
    
    best_struct_str, _ = structure_counter.most_common(1)[0]
    best_struct_list = best_struct_str.split(" + ")
    
    final_parts = []
    used_words = set()
    
    for block in best_struct_list:
        # –î–ª—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç–∞–≤–∏–º –∑–∞–≥–ª—É—à–∫—É
        if "–†–∞–∑–º–µ—Ä—ã" in block or "–°—Ç–∞–Ω–¥–∞—Ä—Ç" in block or "–ú–∞—Ä–∫–∞" in block:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –æ–Ω –æ—á–µ–Ω—å –ø–æ–ø—É–ª—è—Ä–µ–Ω
            top_cand = vocab_by_type[block].most_common(1)
            if top_cand and top_cand[0][1] > (len(sample) * 0.3): # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —É 30%
                 final_parts.append(top_cand[0][0])
            else:
                 final_parts.append(f"[{block.upper()}]")
            continue
            
        # –î–ª—è —Å–ª–æ–≤ (–ú–∞—Ä–∫–µ—Ä, –°–≤–æ–π—Å—Ç–≤–∞) –±–µ—Ä–µ–º –¢–û–ü-1
        candidates = vocab_by_type[block].most_common(3)
        for w, cnt in candidates:
            if w not in used_words:
                if "–ú–∞—Ä–∫–µ—Ä" in block: w = w.capitalize()
                final_parts.append(w)
                used_words.add(w)
                break
                
    ideal_name = " ".join(final_parts)
    
    # –û—Ç—á–µ—Ç
    report = []
    report.append(f"**–°—Ö–µ–º–∞:** {best_struct_str}")
    report.append("")
    report.append("**–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**")
    for block in best_struct_list:
        if "–†–∞–∑–º–µ—Ä—ã" in block: continue
        top = [f"{w}" for w, c in vocab_by_type[block].most_common(3)]
        report.append(f"- **{block}**: {', '.join(top)}")
            
    return ideal_name, report

def render_paginated_table(df, title_text, key_prefix, default_sort_col=None, use_abs_sort_default=False, default_sort_order="–£–±—ã–≤–∞–Ω–∏–µ", show_controls=True):
    if df.empty: st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö."); return
    col_t1, col_t2 = st.columns([7, 3])
    with col_t1: st.markdown(f"### {title_text}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ñ–æ–ª—Ç–æ–≤ –≤ Session State
    if f'{key_prefix}_sort_col' not in st.session_state: 
        st.session_state[f'{key_prefix}_sort_col'] = default_sort_col if (default_sort_col and default_sort_col in df.columns) else df.columns[0]
    
    if f'{key_prefix}_sort_order' not in st.session_state: 
        st.session_state[f'{key_prefix}_sort_order'] = default_sort_order

    search_query = st.text_input(f"üîç –ü–æ–∏—Å–∫ ({title_text})", key=f"{key_prefix}_search")
    if search_query:
        mask = df.astype(str).apply(lambda x: x.str.contains(search_query, case=False, na=False)).any(axis=1)
        df_filtered = df[mask].copy()
    else: df_filtered = df.copy()

    if df_filtered.empty: st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."); return

    # === –õ–û–ì–ò–ö–ê –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø –ö–û–ù–¢–†–û–õ–û–í ===
    if show_controls:
        with st.container():
            st.markdown("<div class='sort-container'>", unsafe_allow_html=True)
            col_s1, col_s2, col_sp = st.columns([2, 2, 4])
            with col_s1:
                current_sort = st.session_state[f'{key_prefix}_sort_col']
                if current_sort not in df_filtered.columns: current_sort = df_filtered.columns[0]
                sort_col = st.selectbox("üóÇ –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ:", df_filtered.columns, key=f"{key_prefix}_sort_box", index=list(df_filtered.columns).index(current_sort))
                st.session_state[f'{key_prefix}_sort_col'] = sort_col
            with col_s2:
                def_index = 0 if st.session_state[f'{key_prefix}_sort_order'] == "–£–±—ã–≤–∞–Ω–∏–µ" else 1
                sort_order = st.radio("–ü–æ—Ä—è–¥–æ–∫:", ["–£–±—ã–≤–∞–Ω–∏–µ", "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ"], horizontal=True, key=f"{key_prefix}_order_box", index=def_index)
                st.session_state[f'{key_prefix}_sort_order'] = sort_order
            st.markdown("</div>", unsafe_allow_html=True)
    else:
        # –ï—Å–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª—ã —Å–∫—Ä—ã—Ç—ã, –±–µ—Ä–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ session_state (–¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ)
        sort_col = st.session_state[f'{key_prefix}_sort_col']
        sort_order = st.session_state[f'{key_prefix}_sort_order']

    ascending = (sort_order == "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ")
    
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    if use_abs_sort_default and sort_col == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è" and "sort_val" in df_filtered.columns: 
        df_filtered = df_filtered.sort_values(by="sort_val", ascending=ascending)
    elif ("–î–æ–±–∞–≤–∏—Ç—å" in sort_col or "+/-" in sort_col) and df_filtered[sort_col].dtype == object:
        try:
            df_filtered['_temp_sort'] = df_filtered[sort_col].astype(str).str.replace(r'[^\d]', '', regex=True)
            df_filtered['_temp_sort'] = pd.to_numeric(df_filtered['_temp_sort'], errors='coerce').fillna(0)
            df_filtered = df_filtered.sort_values(by='_temp_sort', ascending=ascending).drop(columns=['_temp_sort'])
        except: df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)
    else: 
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–ª–æ–Ω–∫–∏ (–Ω–∞ —Å–ª—É—á–∞–π —Å–º–µ–Ω—ã –¥–∞–Ω–Ω—ã—Ö)
        if sort_col in df_filtered.columns:
            df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)

    df_filtered = df_filtered.reset_index(drop=True); df_filtered.index = df_filtered.index + 1
    
    # –≠–∫—Å–ø–æ—Ä—Ç
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        export_df = df_filtered.copy()
        if "is_missing" in export_df.columns: del export_df["is_missing"]
        if "sort_val" in export_df.columns: del export_df["sort_val"]
        export_df.to_excel(writer, index=False, sheet_name='Data')
    excel_data = buffer.getvalue()
    with col_t2: st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel", data=excel_data, file_name=f"{key_prefix}_export.xlsx", mime="application/vnd.ms-excel", key=f"{key_prefix}_down")

    # –ü–∞–≥–∏–Ω–∞—Ü–∏—è
    ROWS_PER_PAGE = 20
    if f'{key_prefix}_page' not in st.session_state: st.session_state[f'{key_prefix}_page'] = 1
    total_rows = len(df_filtered); total_pages = math.ceil(total_rows / ROWS_PER_PAGE)
    if total_pages == 0: total_pages = 1
    current_page = st.session_state[f'{key_prefix}_page']
    if current_page > total_pages: current_page = total_pages
    if current_page < 1: current_page = 1
    st.session_state[f'{key_prefix}_page'] = current_page
    start_idx = (current_page - 1) * ROWS_PER_PAGE
    end_idx = start_idx + ROWS_PER_PAGE
    df_view = df_filtered.iloc[start_idx:end_idx]

    def highlight_rows(row):
        base_style = 'background-color: #FFFFFF; color: #3D4858; border-bottom: 1px solid #DBEAFE;'
        styles = []
        status = row.get("–°—Ç–∞—Ç—É—Å", "")
        for col_name in row.index:
            cell_style = base_style
            if col_name == "–°—Ç–∞—Ç—É—Å":
                if status == "–ù–µ–¥–æ—Å–ø–∞–º": cell_style += "color: #D32F2F; font-weight: bold;"
                elif status == "–ü–µ—Ä–µ—Å–ø–∞–º": cell_style += "color: #E65100; font-weight: bold;"
                elif status == "–ù–æ—Ä–º–∞": cell_style += "color: #2E7D32; font-weight: bold;"
            styles.append(cell_style)
        return styles

    cols_to_hide = [c for c in ["is_missing", "sort_val"] if c in df_view.columns]
    try: styled_df = df_view.style.apply(highlight_rows, axis=1)
    except: styled_df = df_view
    st.dataframe(styled_df, use_container_width=True, height=(len(df_view) * 35) + 40, column_config={c: None for c in cols_to_hide})
    
    c_spacer, c_btn_prev, c_info, c_btn_next = st.columns([6, 1, 1, 1])
    with c_btn_prev:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_prev", disabled=(current_page <= 1), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] -= 1
            st.rerun()
    with c_info: st.markdown(f"<div style='text-align: center; margin-top: 10px;'><b>{current_page}</b> / {total_pages}</div>", unsafe_allow_html=True)
    with c_btn_next:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_next", disabled=(current_page >= total_pages), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] += 1
            st.rerun()
    st.markdown("---")
# ==========================================
# PERPLEXITY GEN
# ==========================================
STATIC_DATA_GEN = {
    'IP_PROP4817': "–£—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏",
    'IP_PROP4818': "–û–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –≤ —Ä–µ–≥–∏–æ–Ω—ã —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4819': """<p>–ù–∞–¥–µ–∂–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫–∞–∑–∞ –≤ –ª—é–±—É—é —Ç–æ—á–∫—É —Å—Ç—Ä–∞–Ω—ã: "–°—Ç–∞–ª—å–º–µ—Ç—É—Ä–∞–ª" –æ—Ç–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–≤–∞—Ä 24 —á–∞—Å–∞ –≤ —Å—É—Ç–∫–∏, 7 –¥–Ω–µ–π –≤ –Ω–µ–¥–µ–ª—é. –ë–æ–ª–µ–µ 4 000 –æ—Ç–≥—Ä—É–∑–æ–∫ –≤ –≥–æ–¥. –ü—Ä–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–∏ –∑–∞–∫–∞–∑–∞ –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –≤–∞–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ä—à—Ä—É—Ç.</p>""",
    'IP_PROP4820': """<p>–ù–∞—à–∏ –∏–∑–¥–µ–ª–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è—Ö –£—Ä–∞–ª–∞, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞, –ü–æ–≤–æ–ª–∂—å—è, –°–∏–±–∏—Ä–∏. –ü–∞—Ä—Ç–Ω–µ—Ä—ã –ø–æ –ª–æ–≥–∏—Å—Ç–∏–∫–µ –ø—Ä–µ–¥–ª–æ–∂–∞—Ç –¥–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–∫–∞–∑ —Å–∞–º—ã–º —É–¥–æ–±–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º ‚Äì –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–º, –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω—ã–º, –¥–∞–∂–µ –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º. –î–ª—è –≤–∞—Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—É—é —Å—Ö–µ–º—É –ø–æ–¥ —É–¥–æ–±–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è. –ü–æ–≥—Ä—É–∑–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ç–µ—Ö–Ω–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.</p><div class="h4"><h4>–°–∞–º–æ–≤—ã–≤–æ–∑</h4></div><p>–ï—Å–ª–∏ –æ–±—ã—á–Ω–æ —Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∑–∞–±—Ä–∞—Ç—å —Ç–æ–≤–∞—Ä –∏–ª–∏ –¥–∞–µ—Ç–µ —ç—Ç–æ –ø—Ä–∞–≤–æ —É–ø–æ–ª–Ω–æ–º–æ—á–µ–Ω–Ω—ã–º, –∞–¥—Ä–µ—Å –∏ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã —Å–∫–ª–∞–¥–∞ –≤ —Å–≤–æ–µ–º –≥–æ—Ä–æ–¥–µ —É—Ç–æ—á–Ω—è–π—Ç–µ —É –º–µ–Ω–µ–¥–∂–µ—Ä–∞.</p><div class="h4"><h4>–ì—Ä—É–∑–æ–≤–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏</h4></div><p>–û—Ç–ø—Ä–∞–≤–∏–º –ø—Ä–æ–∫–∞—Ç –Ω–∞ –≤–∞—à –æ–±—ä–µ–∫—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –∞–≤—Ç–æ–ø–∞—Ä–∫–æ–º. –ü–æ–ª—É—á–µ–Ω–∏–µ –≤ —É–ø–∞–∫–æ–≤–∫–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏, –∞ –∏–º–µ–Ω–Ω–æ –Ω–∞ –¥–µ—Ä–µ–≤—è–Ω–Ω–æ–º –ø–æ–¥–¥–æ–Ω–µ.</p><div class="h4"><h4>–°–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å –¢–ö</h4></div><p>–î–æ—Å—Ç–∞–≤–∫–∞ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –†–æ—Å—Å–∏–∏ –∏ –°–ù–ì. –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å—Å—è, —Ç–∞–∫ –∫–∞–∫ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç, –æ–¥–Ω–∞–∫–æ, —Å—Ä–∞–≤–Ω–∏–º —Å—Ç–æ–∏–º–æ—Å—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª—É–∂–± –∏ –≤—ã–±–µ—Ä–µ–º –ª—É—á—à—É—é.</p>""",
    'IP_PROP4821': "–û–ø–ª–∞—Ç–∞ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤:",
    'IP_PROP4822': """<p>–ù–∞—à–∞ –∫–æ–º–ø–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞ –ø—Ä–∏–Ω—è—Ç—å –ª—é–±—ã–µ –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–µ –≤–∏–¥—ã –æ–ø–ª–∞—Ç—ã –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏—Ü: –ø–æ —Å—á–µ—Ç—É, –Ω–∞–ª–∏—á–Ω–∞—è –∏ –±–µ–∑–Ω–∞–ª–∏—á–Ω–∞—è, –Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–π –ø–ª–∞—Ç–µ–∂, —Ç–∞–∫–∂–µ –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞ –∏ –æ—Ç—Å—Ä–æ—á–∫–∞ –ø–ª–∞—Ç–µ–∂–∞.</p>""",
    'IP_PROP4823': """<div class="h4"><h3>–ü—Ä–∏–º–µ—Ä—ã –≤–æ–∑–º–æ–∂–Ω–æ–π –æ–ø–ª–∞—Ç—ã</h3></div><div class="an-col-12"><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–° –ø–æ–º–æ—â—å—é –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤ —Ü–µ–Ω—Ç—Ä–∞—Ö –ø—Ä–æ–¥–∞–∂</span></p></li></ul><p>–í–∞–∂–Ω–æ! –¶–µ–Ω–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—É–±–ª–∏—á–Ω–æ–π –æ—Ñ–µ—Ä—Ç–æ–π. –ü—Ä–∏—Ö–æ–¥–∏—Ç–µ –≤ –Ω–∞—à –æ—Ñ–∏—Å, —á—Ç–æ–±—ã —É—Ç–æ—á–Ω–∏—Ç—å –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ, –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ—á—Ç–∏ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å, —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –≤–æ–∑–≤—Ä–∞—Ç, —Å—á–µ—Ç, —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ª–æ–≥–∏—Å—Ç–∏–∫—É.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞ —Ä–∞—Å—á–µ—Ç–Ω—ã–π —Å—á–µ—Ç</span></p></li></ul><p>–ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É —Å—á–µ—Ç—É –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏ –±–∞–Ω–∫–∞ –∏–ª–∏ –ø—É—Ç–µ–º –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥—Å—Ç–≤ —á–µ—Ä–µ–∑ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç (—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∑–∞—â–∏—â–µ–Ω—ã, —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –æ—Ç–¥–µ–ª–µ–Ω–∏—è). –î–ª—è –ø—Ä–∞–≤–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å —Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –ø–ª–∞—Ç–µ–∂–Ω–æ–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ —Å –æ—Ç–º–µ—Ç–∫–æ–π –±–∞–Ω–∫–∞.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞–ª–∏—á–Ω—ã–º–∏ –∏–ª–∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∫–∞—Ä—Ç–æ–π –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏</span></p></li></ul><p><span style="font-weight: 400;">–ü–æ–º–æ–∂–µ–º —Å –æ–ø–ª–∞—Ç–æ–π: –æ–±—ä–µ–º –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ. –ö—Ä—É–ø–Ω—ã–º –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º ‚Äì –¥–µ–Ω—å–≥–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–µ–º–∫–∏ —Ç–æ–≤–∞—Ä–∞.</span></p><p>–ú–µ–Ω–µ–¥–∂–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.</p><p>–ó–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ —á–µ—Ä–µ–∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç:</p><p><a class="btn btn-blue" href="/catalog/">–ö–∞—Ç–∞–ª–æ–≥ (–º–∞–≥–∞–∑–∏–Ω-–º–µ–Ω—é):</a></p></div></div><br>""",
    'IP_PROP4824': "–ö–æ–Ω—Ç–∞–∫—Ç—ã –¥–ª—è —Å–≤—è–∑–∏",
    'IP_PROP4825': "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä",
    'IP_PROP4826': "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥",
    'IP_PROP4834': "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä",
    'IP_PROP4835': "–¢–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4836': "–ì–∏–±–∫–∏–µ —É—Å–ª–æ–≤–∏—è —Ä–∞—Å—á–µ—Ç–∞",
    'IP_PROP4837': "–ü–æ—Ä—è–¥–æ–∫ –≤ –ì–û–°–¢–∞—Ö"
}

def get_page_data_for_gen(url):
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π requests, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–ª –Ω–æ—Ä–º–∞–ª—å–Ω–æ
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        # verify=False –ø–æ–º–æ–≥–∞–µ—Ç –æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö SSL –æ—à–∏–±–æ–∫, –Ω–æ –Ω–µ –ª–æ–º–∞–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∫—É
        response = requests.get(url, headers=headers, timeout=20, verify=False)
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞ —á–∞—Å—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –Ω–∞ —Ä—É-—Å–∞–π—Ç–∞—Ö
        if response.encoding != 'utf-8':
            response.encoding = response.apparent_encoding
    except Exception as e: 
        return None, None, None, f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}"
    
    if response.status_code != 200: 
        return None, None, None, f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—É—Å–∞: {response.status_code}"
    
    try:
        soup = BeautifulSoup(response.text, 'html.parser')
    except:
        return None, None, None, "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"
    
    # 1. –ó–ê–ì–û–õ–û–í–û–ö
    description_div = soup.find('div', class_='description-container')
    target_h2 = None
    if description_div:
        target_h2 = description_div.find('h2')
    
    if not target_h2:
        target_h2 = soup.find('h2')
    
    # –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ï—Å–ª–∏ H2 –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None, —á—Ç–æ–±—ã —Å–∫—Ä–∏–ø—Ç –≤–∑—è–ª –∏–º—è —Ç–æ–≤–∞—Ä–∞ –∏–∑ —Å—Å—ã–ª–∫–∏
    page_header = target_h2.get_text(strip=True) if target_h2 else None

    # 2. –§–∞–∫—Ç—É—Ä–∞ (—Ç–µ–∫—Å—Ç)
    if description_div:
        base_text = description_div.get_text(separator="\n", strip=True)
    else:
        # –ß–∏—Å—Ç–∏–º —Å–∫—Ä–∏–ø—Ç—ã, —á—Ç–æ–±—ã –≤ —Ç–µ–∫—Å—Ç –Ω–µ –ø–æ–ø–∞–ª –º—É—Å–æ—Ä
        for s in soup(['script', 'style']): s.decompose()
        base_text = soup.body.get_text(separator="\n", strip=True)[:6000]
    
    # 3. –¢–µ–≥–∏
    tags_container = soup.find(class_='popular-tags-inner')
    tags_data = []
    if tags_container:
        links = tags_container.find_all('a')
        for link in links:
            tag_url = urljoin(url, link.get('href')) if link.get('href') else None
            if tag_url: tags_data.append({'name': link.get_text(strip=True), 'url': tag_url})
            
    return base_text, tags_data, page_header, None

def generate_ai_content_blocks(api_key, base_text, tag_name, forced_header, num_blocks=5, seo_words=None):
    if not base_text: return ["Error: No base text"] * num_blocks
    
    from openai import OpenAI
    client = OpenAI(api_key=api_key, base_url="https://litellm.tokengate.ru/v1")
    
    seo_words = seo_words or []
    seo_instruction_block = ""
    
    # === 1. –ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û SEO (–ò–°–ü–†–ê–í–õ–ï–ù–û: –£–ë–†–ê–ù–û –¢–†–ï–ë–û–í–ê–ù–ò–ï –í–´–î–ï–õ–ï–ù–ò–Ø) ===
    if seo_words:
        seo_list_str = ", ".join(seo_words)
        seo_instruction_block = f"""
--- –í–ê–ñ–ù–ê–Ø –ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û SEO-–°–õ–û–í–ê–ú ---
–¢–µ–±–µ –Ω—É–∂–Ω–æ –≤–Ω–µ–¥—Ä–∏—Ç—å –≤ —Ç–µ–∫—Å—Ç —Å–ª–µ–¥—É—é—â–∏–µ —Å–ª–æ–≤–∞ –≤ –ª—é–±–æ–π –ø–æ–¥—Ö–æ–¥—è—â–µ–π –ø–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ª–µ–º–º–µ: {{{seo_list_str}}}

–ü–†–ê–í–ò–õ–ê –í–ù–ï–î–†–ï–ù–ò–Ø –ò –í–´–î–ï–õ–ï–ù–ò–Ø:
1. –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï: –†–∞—Å–∫–∏–¥–∞–π —Å–ª–æ–≤–∞ –ø–æ –≤—Å–µ–º {num_blocks} –±–ª–æ–∫–∞–º.
2. –°–¢–†–û–ì–û –ó–ê–ü–†–ï–©–ï–ù–û: –ù–µ –≤—ã–¥–µ–ª—è–π –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∂–∏—Ä–Ω—ã–º —à—Ä–∏—Ñ—Ç–æ–º (**—Ç–µ–∫—Å—Ç** –∏–ª–∏ <b>—Ç–µ–∫—Å—Ç</b>). –í–ø–∏—Å—ã–≤–∞–π –∏—Ö –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç.
4. –ï–°–¢–ï–°–¢–í–ï–ù–ù–û–°–¢–¨: –ú–µ–Ω—è–π —Å–ª–æ–≤–æ—Ñ–æ—Ä–º—ã –ø–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç. –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∏ –ª–æ–≥–∏—á–Ω—ã–º, –Ω–µ –ø–∏—à–∏ —á—É—à—å.
–ü–†–ò–ú–ï–†–´ –¢–û–ì–û, –ö–ê–ö –ù–ê–î–û –ò –ù–ï –ù–ê–î–û –î–ï–õ–ê–¢–¨:
1. –ö–ª—é—á: "—Ç–æ–Ω–Ω–∞"
   ‚ùå –ü–õ–û–•–û: "–¶–µ–Ω–∞ –∑–∞ —Ç–æ–Ω–Ω–∞..." (–û—à–∏–±–∫–∞ –ø–∞–¥–µ–∂–∞)
   ‚úÖ –•–û–†–û–®–û: "–¶–µ–Ω–∞ –∑–∞ —Ç–æ–Ω–Ω—É..." (–í–∏–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂)
   
2. –ö–ª—é—á: "–∫–∞—á–µ—Å—Ç–≤–æ"
   ‚ùå –ü–õ–û–•–û: "–ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–∞..." (–ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å)
   ‚úÖ –•–û–†–û–®–û: "–ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ..."

3. –ö–ª—é—á: "–ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å"
   ‚ùå –ü–õ–û–•–û: "–í–æ –º–Ω–æ–≥–∏—Ö –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å..."
   ‚úÖ –•–û–†–û–®–û: "–í–æ –º–Ω–æ–≥–∏—Ö –æ—Ç—Ä–∞—Å–ª—è—Ö –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏..."
-------------------------------------------
"""

    # === 2. –°–ò–°–¢–ï–ú–ù–ê–Ø –†–û–õ–¨ (–í–∞—à –≤–∞—Ä–∏–∞–Ω—Ç) ===
    system_instruction = (
        "–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä –∏ –≤–µ—Ä—Å—Ç–∞–ª—å—â–∏–∫. "
        "–¢–≤–æ—è —Ü–µ–ª—å ‚Äî –ø–∏—Å–∞—Ç—å –≥–ª—É–±–æ–∫–∏–π, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –ø–æ–ª–µ–∑–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–æ–≤, –Ω–∞—Å—ã—â–µ–Ω–Ω—ã–π —Ñ–∞–∫—Ç–∞–º–∏ –∏ —Ü–∏—Ñ—Ä–∞–º–∏. "
        "–¢—ã –≤—ã–¥–∞–µ—à—å –¢–û–õ–¨–ö–û HTML-–∫–æ–¥. "
        "–°—Ç–∏–ª—å: –î–µ–ª–æ–≤–æ–π, —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π, –Ω–æ \"—á–µ–ª–æ–≤–µ—á–Ω—ã–π\" –∏ –ø–æ–Ω—è—Ç–Ω—ã–π. –ò–∑–±–µ–≥–∞–π –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º–æ–≤ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. "
        "–§–∞–∫—Ç—ã –∏ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞: –í—Å–µ —Å—É–∂–¥–µ–Ω–∏—è –ø–æ–¥–∫—Ä–µ–ø–ª—è–π –∏–∑–º–µ—Ä–∏–º—ã–º–∏ —Ñ–∞–∫—Ç–∞–º–∏, —Ü–∏—Ñ—Ä–∞–º–∏, —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –ì–û–°–¢—ã, –º–∞—Ä–∫–∏ —Å—Ç–∞–ª–∏ –∏ –¥—Ä—É–≥–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤—ã. –ò—Å–ø–æ–ª—å–∑—É–π –ø–æ–∏—Å–∫–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π. "
        "–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å: –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–¥–∞–≤–∞—Ç—å. –ì–æ–≤–æ—Ä–∏ –æ—Ç –ª–∏—Ü–∞ –∫–æ–º–ø–∞–Ω–∏–∏-–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è/–ø–æ—Å—Ç–∞–≤—â–∏–∫–∞. –í–º–µ—Å—Ç–æ \"–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫\" –∏—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—â–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –∏ —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É. "
        "–§–æ—Ä–º—É–ª–∞ –ì–ª–∞–≤—Ä–µ–¥–∞ –¥–ª—è B2B: –í —Ç–µ–∫—Å—Ç–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã: —á—Ç–æ —ç—Ç–æ? –∫–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç? –∫–æ–º—É –ø–æ–¥–æ–π–¥–µ—Ç? –∫–∞–∫–∏–µ –µ—Å—Ç—å —Ä–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç–∏? –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å–∫—Ä–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞—Ö –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞, —Å–∫–ª–∞–¥—Å–∫–∏—Ö –∑–∞–ø–∞—Å–∞—Ö –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è –ø–æ–¥ –∑–∞–∫–∞–∑. "
        "–°–¢–†–û–ì–ò–ï –ó–ê–ü–†–ï–¢–´: "
        "1. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –£–∫—Ä–∞–∏–Ω—ã, —É–∫—Ä–∞–∏–Ω—Å–∫–∏—Ö –≥–æ—Ä–æ–¥–æ–≤ (–ö–∏–µ–≤, –õ—å–≤–æ–≤ –∏ –¥—Ä.), "
        "–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–º—ã, –≤–∞–ª—é—Ç—É –≥—Ä–∏–≤–Ω—É. –ö–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–æ–≥–æ –¥–ª—è –†–§. "
        "2. –ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∏ –≤ —Ç–µ–∫—Å—Ç–µ, –Ω–∏ –≤ —Å–ø–∏—Å–∫–∞—Ö. –ß–∏—Å—Ç–∏ —Ç–µ–∫—Å—Ç –æ—Ç –Ω–∏—Ö –ø–æ–ª–Ω–æ—Å—Ç—å—é. "
        "3. –ò–º–µ–Ω–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ, –Ω–∞–∑–≤–∞–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤ –ø–∏—à–∏ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã. –ú–∞—Ä–∫–∏ –ø–∏—à–∏ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –º–∞—Ä–æ—á–Ω–∏–∫–∞–º–∏. –ì–û–°–¢ –≤—Å–µ–≥–¥–∞ –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏."
    )

    # === 3. –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–¨–°–ö–ò–ô –ü–†–û–ú–¢ (–í–∞—à –≤–∞—Ä–∏–∞–Ω—Ç + –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ) ===
    user_prompt = f"""
    –ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:
    –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: "{tag_name}"
    –ë–∞–∑–æ–≤—ã–π —Ç–µ–∫—Å—Ç (—Ñ–∞–∫—Ç—É—Ä–∞): \"\"\"{base_text[:3500]}\"\"\"
    
    {seo_instruction_block}
    
    –ó–ê–î–ê–ß–ê:
    –ù–∞–ø–∏—à–∏ {num_blocks} HTML-–±–ª–æ–∫–æ–≤, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º: |||BLOCK_SEP|||
    
    –û–ë–©–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
    1. –û–ë–™–ï–ú: –ö–∞–∂–¥—ã–π –±–ª–æ–∫ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∞–∫—Å–∏–º—É–º 800 —Å–∏–º–≤–æ–ª–æ–≤. –†–∞—Å–∫—Ä—ã–≤–∞–π —Ç–µ–º—É –ø–æ–¥—Ä–æ–±–Ω–æ.
    2. –ß–ò–°–¢–û–¢–ê: –ò—Å–∫–ª—é—á–∏ –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.
    3. –ü–û–õ–¨–ó–ê: –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –≥—Ä–∞–º–æ—Ç–Ω—ã–º –∏ –ø–æ–ª–µ–∑–Ω—ã–º –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –ø–æ –∑–∞–∫—É–ø–∫–∞–º. –ò–∑–±–µ–≥–∞–π "–≤–æ–¥—ã".
    
    –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –°–¢–†–£–ö–¢–£–†–ï –ö–ê–ñ–î–û–ì–û –ë–õ–û–ö–ê:
    –ö–∞–∂–¥—ã–π –∏–∑ {num_blocks} –±–ª–æ–∫–æ–≤ –¥–æ–ª–∂–µ–Ω —Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –ø–æ—Ä—è–¥–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤:
    1. –ó–∞–≥–æ–ª–æ–≤–æ–∫ (<h2> —Ç–æ–ª—å–∫–æ –¥–ª—è 1-–≥–æ –±–ª–æ–∫–∞, <h3> –¥–ª—è –±–ª–æ–∫–æ–≤ 2-{num_blocks}).
    2. –ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞ (<p>) - —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π.
    3. –í–≤–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –ø–æ–¥–≤–æ–¥—è—â–µ–µ –∫ —Å–ø–∏—Å–∫—É (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:", "–°—Ñ–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è:").
    4. –ú–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ (<ul> c <li>).
    5. –í—Ç–æ—Ä–æ–π (–∑–∞–≤–µ—Ä—à–∞—é—â–∏–π) –∞–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞ (<p>) - —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π.
    
    –¢–ï–ú–´ –ë–õ–û–ö–û–í:
    --- –ë–õ–û–ö 1 (–í–≤–æ–¥–Ω—ã–π) ---
    - –ó–∞–≥–æ–ª–æ–≤–æ–∫: <h2>{forced_header}</h2>
    - –û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞, –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ, –∫–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏.
    
    --- –ë–õ–û–ö–ò 2, 3, 4, 5 (–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏) ---
    - –ó–∞–≥–æ–ª–æ–≤–∫–∏: <h3> (–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ, –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ, –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏, –°–æ—Ä—Ç–∞–º–µ–Ω—Ç –∏ —Ç.–¥.).
    - –ò—Å–ø–æ–ª—å–∑—É–π —Ñ–∞–∫—Ç—É—Ä—É –∏–∑ "–ë–∞–∑–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞".
    
    –§–ò–ù–ê–õ–¨–ù–´–ï –£–°–õ–û–í–ò–Ø:
    - –ù–∏–∫–∞–∫–∏—Ö –≤–≤–æ–¥–Ω—ã—Ö —Å–ª–æ–≤ —Ç–∏–ø–∞ "–í–æ—Ç –≤–∞—à –∫–æ–¥".
    - –ù–∏–∫–∞–∫–æ–≥–æ Markdown (```).
    - –ù–ò–ö–ê–ö–û–ì–û –ñ–ò–†–ù–û–ì–û –¢–ï–ö–°–¢–ê.
    - –¢–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π HTML, —Ä–∞–∑–±–∏—Ç—ã–π —á–µ—Ä–µ–∑ |||BLOCK_SEP|||.
    """
    
    try:
        response = client.chat.completions.create(
            model="google/gemini-2.5-pro",
            messages=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3 
        )
        content = response.choices[0].message.content
        
        # === –ß–ò–°–¢–ö–ê –û–¢ MARKDOWN –ò –ú–£–°–û–†–ê ===
        content = re.sub(r'^```[a-zA-Z]*\s*', '', content.strip())
        content = re.sub(r'\s*```$', '', content.strip())
        
        blocks = [b.strip() for b in content.split("|||BLOCK_SEP|||") if b.strip()]
        
        cleaned_blocks = []
        for b in blocks:
            cb = re.sub(r'^```[a-zA-Z]*', '', b).strip().lstrip('`.').strip()
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª—å H2, –µ—Å–ª–∏ –ò–ò –µ–≥–æ –≤—Å–µ-—Ç–∞–∫–∏ –Ω–∞–ø–∏—Å–∞–ª
            cb = re.sub(r'^<h2.*?>.*?</h2>', '', cb, flags=re.DOTALL | re.IGNORECASE).strip()
            
            # === –§–ò–ó–ò–ß–ï–°–ö–û–ï –£–î–ê–õ–ï–ù–ò–ï –ñ–ò–†–ù–û–ì–û –¢–ï–ö–°–¢–ê ===
            # 1. –£–¥–∞–ª—è–µ–º Markdown –∂–∏—Ä–Ω—ã–π (**—Ç–µ–∫—Å—Ç**)
            cb = cb.replace("**", "")
            # 2. –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏ –∂–∏—Ä–Ω–æ–≥–æ (<b>, </b>, <strong>, </strong>)
            cb = re.sub(r'</?(b|strong)>', '', cb, flags=re.IGNORECASE)
            
            if cb: cleaned_blocks.append(cb)
            
        while len(cleaned_blocks) < num_blocks: cleaned_blocks.append("")
        
        # === –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –í–°–¢–ê–í–ö–ê –ó–ê–ì–û–õ–û–í–ö–ê ===
        if cleaned_blocks:
            final_h2_text = forced_header if forced_header else tag_name
            cleaned_blocks[0] = f"<h2>{final_h2_text}</h2>\n{cleaned_blocks[0]}"

        return cleaned_blocks[:num_blocks]
        
    except Exception as e:
        return [f"API Error: {str(e)}"] * num_blocks

# ==========================================
# 7. UI TABS RESTRUCTURED
# ==========================================
tab_seo_main, tab_wholesale_main, tab_projects, tab_monitoring, tab_lsi_gen = st.tabs(["üìä SEO –ê–Ω–∞–ª–∏–∑", "üè≠ –û–ø—Ç–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä", "üìÅ –ü—Ä–æ–µ–∫—Ç—ã", "üìâ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ–∑–∏—Ü–∏–π", "üìù LSI –¢–µ–∫—Å—Ç—ã"])

# ------------------------------------------
# TAB 1: SEO ANALYSIS (KEPT AS IS)
# ------------------------------------------
with tab_seo_main:
    col_main, col_sidebar = st.columns([65, 35])
    
    # === –õ–ï–í–ê–Ø –ö–û–õ–û–ù–ö–ê (–û–°–ù–û–í–ù–ê–Ø) ===
    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
        
        # –°–±—Ä–æ—Å –∫—ç—à–∞ –¥–ª—è —Å–ª–æ–≤–∞—Ä–µ–π
        if st.button("üßπ –û–±–Ω–æ–≤–∏—Ç—å —Å–ª–æ–≤–∞—Ä–∏ (–ö—ç—à)", key="clear_cache_btn"):
            st.cache_data.clear()
            st.rerun()

        my_input_type = st.radio("–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], horizontal=True, label_visibility="collapsed", key="my_page_source_radio")
        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            st.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã", placeholder="https://site.ru/catalog/tovar", label_visibility="collapsed", key="my_url_input")
        elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏–ª–∏ —Ç–µ–∫—Å—Ç", height=200, label_visibility="collapsed", placeholder="–í—Å—Ç–∞–≤—å—Ç–µ HTML", key="my_content_input")

        st.markdown("### –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å")
        st.text_input("–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞", label_visibility="collapsed", key="query_input")
        
        st.markdown("### –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        
        # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ-–ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è ---
        if st.session_state.get('force_radio_switch'):
            st.session_state["competitor_source_radio"] = "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"
            st.session_state['force_radio_switch'] = False
        # -----------------------------------------------

        source_type_new = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫", ["–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)", "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"], horizontal=True, label_visibility="collapsed", key="competitor_source_radio")
        source_type = "API" if "API" in source_type_new else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"
        
        if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
            # --- –í–°–¢–ê–í–ò–¢–¨ –≠–¢–û–¢ –ë–õ–û–ö –¢–£–¢ ---
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç —Ñ–∏–ª—å—Ç—Ä–∞
            if 'temp_update_urls' in st.session_state:
                st.session_state['persistent_urls'] = st.session_state['temp_update_urls']
                del st.session_state['temp_update_urls']

            # –ö–Ω–æ–ø–∫–∞ —Å–±—Ä–æ—Å–∞
            if st.session_state.get('analysis_done'):
                col_reset, _ = st.columns([1, 4])
                with col_reset:
                    if st.button("üîÑ –ù–æ–≤—ã–π –ø–æ–∏—Å–∫ (–°–±—Ä–æ—Å)", type="secondary"):
                        keys_to_clear = ['analysis_done', 'analysis_results', 'persistent_urls', 'excluded_urls_auto', 'detected_anomalies']
                        for k in keys_to_clear:
                            if k in st.session_state: del st.session_state[k]
                        st.rerun()

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–µ—Å–ª–∏ –Ω–µ—Ç)
            if 'persistent_urls' not in st.session_state:
                st.session_state['persistent_urls'] = ""

            has_exclusions = st.session_state.get('excluded_urls_auto') and len(st.session_state.get('excluded_urls_auto')) > 5
            
            if has_exclusions:
                c_url_1, c_url_2 = st.columns(2)
                with c_url_1:
                    # –ü–†–û–°–¢–û –í–ò–î–ñ–ï–¢. –ë–µ–∑ value=..., —Ç–∞–∫ –∫–∞–∫ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º key.
                    # –ó–Ω–∞—á–µ–Ω–∏–µ —Å–∞–º–æ –ø–æ–¥—Ç—è–Ω–µ—Ç—Å—è –∏–∑ st.session_state['persistent_urls']
                    st.text_area(
                        "‚úÖ –ê–∫—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã (–î–ª—è –∞–Ω–∞–ª–∏–∑–∞)", 
                        height=200, 
                        key="persistent_urls" 
                    )
                with c_url_2:
                    st.text_area(
                        "üö´ –ê–≤—Ç–æ-–∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ", 
                        height=200, 
                        value=st.session_state.get('excluded_urls_auto', ""),
                        disabled=True # –°–¥–µ–ª–∞–ª –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–º, —á—Ç–æ–±—ã –Ω–µ –ø—É—Ç–∞—Ç—å
                    )
            else:
                st.text_area(
                    "–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", 
                    height=200, 
                    key="persistent_urls"
                )

        # –ì–†–ê–§–ò–ö
        if st.session_state.get('analysis_done') and st.session_state.get('analysis_results'):
            results = st.session_state.analysis_results
            if 'relevance_top' in results and not results['relevance_top'].empty:
                st.markdown("<br>", unsafe_allow_html=True)
                with st.expander("üìä –ì—Ä–∞—Ñ–∏–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–ù–∞–∂–º–∏—Ç–µ, —á—Ç–æ–±—ã —Ä–∞—Å–∫—Ä—ã—Ç—å)", expanded=False):                    
                    graph_data = st.session_state.get('full_graph_data', results['relevance_top'])
                    render_relevance_chart(graph_data, unique_key="main")
                st.markdown("<br>", unsafe_allow_html=True)

        # --- –ö–ù–û–ü–ö–ê –ó–ê–ü–£–°–ö–ê ---
        def run_analysis_callback():
            saved_filter_state = st.session_state.get('settings_auto_filter', True)
            keys_to_clear = [
                'analysis_results', 'analysis_done', 'naming_table_df',
                'ideal_h1_result', 'gen_result_df', 'unified_excel_data',
                'detected_anomalies', 'serp_trend_info',
                'excluded_urls_auto'
            ]
            for k in keys_to_clear:
                if k in st.session_state: del st.session_state[k]
            st.session_state.settings_auto_filter = saved_filter_state
            for k in list(st.session_state.keys()):
                if k.endswith('_page'): st.session_state[k] = 1
            st.session_state.start_analysis_flag = True

        st.markdown("<br>", unsafe_allow_html=True) # –û—Ç—Å—Ç—É–ø –ø–µ—Ä–µ–¥ –∫–Ω–æ–ø–∫–æ–π
        st.button(
            "–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", 
            type="primary", 
            use_container_width=True, 
            key="start_analysis_btn",
            on_click=run_analysis_callback 
        )

    # === –ü–†–ê–í–ê–Ø –ö–û–õ–û–ù–ö–ê (–°–ê–ô–î–ë–ê–†) ===
    with col_sidebar:
        if not ARSENKIN_TOKEN:
             new_arsenkin = st.text_input("Arsenkin Token", type="password", key="input_arsenkin")
             if new_arsenkin: st.session_state.arsenkin_token = new_arsenkin; ARSENKIN_TOKEN = new_arsenkin 
        if not YANDEX_DICT_KEY:
             new_yandex = st.text_input("Yandex Dict Key", type="password", key="input_yandex")
             if new_yandex: st.session_state.yandex_dict_key = new_yandex; YANDEX_DICT_KEY = new_yandex
        
        st.markdown("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
        st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
        st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
        st.selectbox("–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞", list(REGION_MAP.keys()), key="settings_region")
        st.selectbox("–ö–æ–ª-–≤–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", [10, 20], index=0, key="settings_top_n")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–µ–∫–±–æ–∫—Å–æ–≤
        if "settings_noindex" not in st.session_state: st.session_state.settings_noindex = True
        if "settings_alt" not in st.session_state: st.session_state.settings_alt = False
        if "settings_numbers" not in st.session_state: st.session_state.settings_numbers = False
        if "settings_norm" not in st.session_state: st.session_state.settings_norm = True
        if "settings_auto_filter" not in st.session_state: st.session_state.settings_auto_filter = True

        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å <noindex>", key="settings_noindex")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", key="settings_alt")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", key="settings_numbers")
        st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", key="settings_norm")
        st.checkbox("–ê–≤—Ç–æ-—Ñ–∏–ª—å—Ç—Ä —Å–ª–∞–±—ã—Ö —Å–∞–π—Ç–æ–≤", key="settings_auto_filter", help="–°–∞–π—Ç—ã —Å –Ω–∏–∑–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã –≤ —Å–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö.")
        
        # === [–ò–ó–ú–ï–ù–ï–ù–ò–ï] –°–ü–ò–°–ö–ò –ü–ï–†–ï–ù–ï–°–ï–ù–´ –°–Æ–î–ê ===
        st.markdown("---")
        st.markdown("üõë **–ò—Å–∫–ª—é—á–µ–Ω–∏—è**")
        
        if "settings_excludes" not in st.session_state: st.session_state.settings_excludes = DEFAULT_EXCLUDE
        if "settings_stops" not in st.session_state: st.session_state.settings_stops = DEFAULT_STOPS

        st.text_area("–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–æ–º–µ–Ω—ã", height=100, key="settings_excludes", help="–î–æ–º–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–∞—Ä—Å–µ—Ä –ø—Ä–æ–ø—É—Å—Ç–∏—Ç —Å—Ä–∞–∑—É.")
        st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞", height=100, key="settings_stops", help="–°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–ø–∞–¥—É—Ç –≤ –∞–Ω–∞–ª–∏–∑.")
# ==========================================
    # –ë–õ–û–ö 1: –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
    # ==========================================
    if st.session_state.analysis_done and st.session_state.analysis_results:
        results = st.session_state.analysis_results
        
        d_score = results['my_score']['depth']
        w_score = results['my_score']['width']
        
        # –¶–≤–µ—Ç–∞ –±–∞–ª–ª–æ–≤
        w_color = "#2E7D32" if w_score >= 80 else ("#E65100" if w_score >= 50 else "#D32F2F")
        
        if 75 <= d_score <= 88:
            d_color = "#2E7D32"; d_status = "–ò–î–ï–ê–õ (–¢–æ–ø)"
        elif 88 < d_score <= 100:
            d_color = "#D32F2F"; d_status = "–ü–ï–†–ï–°–ü–ê–ú (–†–∏—Å–∫)"
        elif 55 <= d_score < 75:
            d_color = "#F9A825"; d_status = "–°—Ä–µ–¥–Ω—è—è"
        else:
            d_color = "#D32F2F"; d_status = "–ù–∏–∑–∫–∞—è"

        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        
        # –°—Ç–∏–ª–∏
        st.markdown("""
        <style>
            details > summary { list-style: none; }
            details > summary::-webkit-details-marker { display: none; }
            .details-card { background-color: #f8f9fa; border: 1px solid #e9ecef; border-radius: 8px; margin-bottom: 10px; }
            .card-summary { padding: 12px 15px; cursor: pointer; font-weight: 700; display: flex; justify-content: space-between; }
            .count-tag { background: #e5e7eb; padding: 2px 8px; border-radius: 10px; font-size: 12px; }
            .flat-card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; height: 340px; display: flex; flex-direction: column; }
            .flat-header { height: 50px; padding: 0 20px; font-weight: 700; border-bottom: 1px solid #f3f4f6; display: flex; align-items: center; justify-content: space-between; }
            .flat-content { flex-grow: 1; padding: 15px 20px; overflow-y: auto; font-size: 13px; line-height: 1.4; }
            .flat-footer { height: 150px; padding: 12px 20px; border-top: 1px solid #f3f4f6; background: #fafafa; }
            .flat-len-badge { padding: 2px 8px; border-radius: 4px; font-weight: 700; font-size: 10px; }
            .flat-miss-tag { border: 1px solid #fecaca; color: #991b1b; padding: 2px 6px; font-size: 11px; border-radius: 4px; margin: 2px; display: inline-block; }
        </style>
        """, unsafe_allow_html=True)

# –í—ã–≤–æ–¥ –û–¢–õ–ê–î–ö–ò –¥–ª—è –®–∏—Ä–∏–Ω—ã (—á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –ø–æ—á–µ–º—É 95)
        if 'debug_width' in results:
            found = results['debug_width']['found']
            needed = results['debug_width']['needed']
            pct = int((found / needed * 100)) if needed > 0 else 0
            st.caption(f"üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –®–∏—Ä–∏–Ω—ã: –ù–∞–π–¥–µ–Ω–æ **{found}** –∏–∑ **{needed}** –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ ({pct}%).")
        
        # –í—ã–≤–æ–¥ –±–∞–ª–ª–æ–≤
        st.markdown(f"""
        <div style='display: flex; gap: 20px; flex-wrap: wrap; margin-bottom: 20px;'>
            <div style='flex: 1; background:{LIGHT_BG_MAIN}; padding:15px; border-radius:8px; border-left: 5px solid {w_color};'>
                <div style='font-size: 12px; color: #666;'>–®–ò–†–ò–ù–ê (–û—Ö–≤–∞—Ç —Ç–µ–º)</div>
                <div style='font-size: 24px; font-weight: bold; color: {w_color};'>{w_score}/100</div>
            </div>
            <div style='flex: 1; background:{LIGHT_BG_MAIN}; padding:15px; border-radius:8px; border-left: 5px solid {d_color};'>
                <div style='font-size: 12px; color: #666;'>–ì–õ–£–ë–ò–ù–ê (–¶–µ–ª—å: ~80)</div>
                <div style='font-size: 24px; font-weight: bold; color: {d_color};'>{d_score}/100 <span style='font-size:14px; font-weight:normal;'>({d_status})</span></div>
            </div>
        </div>
        """, unsafe_allow_html=True)

        # --- –†–ê–°–ß–ï–¢ META (–ß—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å –∏—Ö –ø–µ—Ä–≤—ã–º–∏) ---
        my_data_saved = st.session_state.get('saved_my_data')
        meta_res = None
        
        if 'raw_comp_data' in st.session_state and my_data_saved:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            s_meta = {
                'noindex': True, 'alt_title': False, 'numbers': False, 'norm': True, 
                'ua': "Mozilla/5.0", 'custom_stops': st.session_state.get('settings_stops', "").split()
            }
            meta_res = analyze_meta_gaps(st.session_state['raw_comp_data'], my_data_saved, s_meta)

        # --- –í–´–í–û–î META DASHBOARD (–ö–ê–†–¢–û–ß–ö–ò) ---
        if meta_res:
            st.markdown("### üß¨ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ Title, Description –∏ H1")
            
            # –•–µ–ª–ø–µ—Ä—ã –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏
            def check_len_status(text, type_key):
                length = len(text) if text else 0
                limits = {'Title': (30, 70), 'Description': (150, 250), 'H1': (20, 60)}
                mn, mx = limits.get(type_key, (0,0))
                if mn <= length <= mx: return length, "–•–û–†–û–®–û", "#059669", "#ECFDF5"
                return length, "–ü–õ–û–•–û", "#DC2626", "#FEF2F2"

            def render_flat_card(col, label, type_key, icon, txt, score, missing):
                length, status, col_txt, col_bg = check_len_status(txt, type_key)
                rel_col = "#10B981" if score >= 90 else ("#F59E0B" if score >= 50 else "#EF4444")
                
                miss_html = ""
                if missing:
                    tags = "".join([f'<span class="flat-miss-tag">{w}</span>' for w in missing[:10]])
                    miss_html = f"<div style='margin-top:5px;'>{tags}</div>"
                else:
                    miss_html = "<div style='color:#059669; font-weight:bold; margin-top:10px;'>‚úî –í—Å—ë –æ—Ç–ª–∏—á–Ω–æ</div>"

                html = f"""
                <div class="flat-card">
                    <div class="flat-header">
                        <div>{icon} {label}</div>
                        <span class="flat-len-badge" style="background:{col_bg}; color:{col_txt}">{length} –∑–Ω.</span>
                    </div>
                    <div class="flat-content">{txt if txt else '<span style="color:#ccc">–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö</span>'}</div>
                    <div class="flat-footer">
                        <div style="display:flex; justify-content:space-between; font-weight:bold; font-size:11px; color:#9ca3af;">
                            <span>–†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–¨</span> 
                            <span style="color:{rel_col}">{score}%</span>
                        </div>
                        <div style="width:100%; height:6px; background:#e5e7eb; border-radius:3px; margin-top:5px; overflow:hidden;">
                            <div style="width:{score}%; height:100%; background:{rel_col};"></div>
                        </div>
                        {miss_html}
                    </div>
                </div>
                """
                col.markdown(html, unsafe_allow_html=True)

            c1, c2, c3 = st.columns(3)
            m_s = meta_res['scores']; m_m = meta_res['missing']; m_d = meta_res['my_data']
            
            render_flat_card(c1, "Title", "Title", "üìë", m_d['Title'], m_s['title'], m_m['title'])
            render_flat_card(c2, "Description", "Description", "üìù", m_d['Description'], m_s['desc'], m_m['desc'])
            render_flat_card(c3, "H1 –ó–∞–≥–æ–ª–æ–≤–æ–∫", "H1", "#Ô∏è‚É£", m_d['H1'], m_s['h1'], m_m['h1'])
            
            st.markdown("<br>", unsafe_allow_html=True)

# 1. –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ï –Ø–î–†–û
        with st.expander("üõí –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ", expanded=True):
            if not st.session_state.get('orig_products') and not st.session_state.get('categorized_general'):
                st.info("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑.")
            else:
                # --- –§–£–ù–ö–¶–ò–Ø –ü–ï–†–ï–°–ß–ï–¢–ê (CALLBACK) ---
                def sync_semantics_with_stoplist():
                    # 1. –°—á–∏—Ç—ã–≤–∞–µ–º, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Å—Ç–∞–≤–∏–ª/–Ω–∞–ø–∏—Å–∞–ª –≤ –ø–æ–ª–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
                    raw_input = st.session_state.get('sensitive_words_input_final', "")
                    # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç (–º–Ω–æ–∂–µ—Å—Ç–≤–æ) –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞, –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
                    current_stop_set = set(w.strip().lower() for w in raw_input.split('\n') if w.strip())

                    # 2. –ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã–µ —Å–ø–∏—Å–∫–∏ –∏–∑ –ú–∞—Å—Ç–µ—Ä-—Å–ø–∏—Å–∫–æ–≤ (orig_...)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º: –µ—Å–ª–∏ —Å–ª–æ–≤–∞ –Ω–µ—Ç –≤ —Å—Ç–æ–ø-–ª–∏—Å—Ç–µ ‚Äî –æ–Ω–æ –∏–¥–µ—Ç –≤ —Ä–∞–±–æ—Ç—É
                    st.session_state.categorized_products = [w for w in st.session_state.orig_products if w.lower() not in current_stop_set]
                    st.session_state.categorized_services = [w for w in st.session_state.orig_services if w.lower() not in current_stop_set]
                    st.session_state.categorized_commercial = [w for w in st.session_state.orig_commercial if w.lower() not in current_stop_set]
                    st.session_state.categorized_geo = [w for w in st.session_state.orig_geo if w.lower() not in current_stop_set]
                    st.session_state.categorized_dimensions = [w for w in st.session_state.orig_dimensions if w.lower() not in current_stop_set]
                    st.session_state.categorized_general = [w for w in st.session_state.orig_general if w.lower() not in current_stop_set]

                    # 3. –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º (—á—Ç–æ–±—ã –º—É—Å–æ—Ä –Ω–µ –ø–æ–ø–∞–ª –≤ —Ç–µ–≥–∏)
                    all_active_products = st.session_state.categorized_products
                    if len(all_active_products) < 20:
                        st.session_state.auto_tags_words = all_active_products
                        st.session_state.auto_promo_words = []
                    else:
                        mid = math.ceil(len(all_active_products) / 2)
                        st.session_state.auto_tags_words = all_active_products[:mid]
                        st.session_state.auto_promo_words = all_active_products[mid:]
                    
                    st.toast("–°–ø–∏—Å–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã!", icon="‚úÖ")

                # --- –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –ö–ê–†–¢–û–ß–ï–ö ---
                c1, c2, c3 = st.columns(3)
                with c1: render_clean_block("–¢–æ–≤–∞—Ä—ã", "üß±", st.session_state.categorized_products)
                with c2: render_clean_block("–ì–µ–æ", "üåç", st.session_state.categorized_geo)
                with c3: render_clean_block("–ö–æ–º–º–µ—Ä—Ü–∏—è", "üí∞", st.session_state.categorized_commercial)
                
                c4, c5, c6 = st.columns(3)
                with c4: render_clean_block("–£—Å–ª—É–≥–∏", "üõ†Ô∏è", st.session_state.categorized_services)
                with c5: render_clean_block("–†–∞–∑–º–µ—Ä—ã/–ì–û–°–¢", "üìè", st.session_state.categorized_dimensions)
                with c6: render_clean_block("–û–±—â–∏–µ", "üìÇ", st.session_state.categorized_general)

                # --- –ë–õ–û–ö –°–¢–û–ü-–°–õ–û–í (–†–ï–î–ê–ö–¢–ò–†–£–ï–ú–´–ô) ---
                st.markdown("<br>", unsafe_allow_html=True)
                st.markdown("#### üõë –°—Ç–æ–ø-–ª–∏—Å—Ç")
                st.caption("–°—é–¥–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ø–∞–ª–∏ —Å–ª–æ–≤–∞ –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Å–ø–∏—Å–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤, –æ–Ω–∏ –Ω–µ –±—É–¥—É—Ç –≤–ª–∏—è—Ç—å –Ω–∞ —Ä–∞—Å—á–µ—Ç—ã –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ. –î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ –∏–ª–∏ —É–¥–∞–ª–∏—Ç–µ –ª–∏—à–Ω–∏–µ.")

                col_text, col_btn = st.columns([4, 1])
                
                with col_text:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º key, —á—Ç–æ–±—ã –∑–Ω–∞—á–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω—è–ª–æ—Å—å –≤ session_state
                    st.text_area(
                        "–°–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π",
                        height=150,
                        key="sensitive_words_input_final", 
                        label_visibility="collapsed"
                    )
                
                with col_btn:
                    st.write("") # –û—Ç—Å—Ç—É–ø
                    st.button(
                        "üîÑ –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∏ –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å", 
                        type="primary", 
                        use_container_width=True,
                        on_click=sync_semantics_with_stoplist
                    )
                    st.info("–£–¥–∞–ª–∏—Ç–µ —Å–ª–æ–≤–æ –∏–∑ —Å–ø–∏—Å–∫–∞ —Å–ª–µ–≤–∞, —á—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å –µ–≥–æ –≤ –≥—Ä—É–ø–ø—ã –≤—ã—à–µ.")

        # 2. –¢–ê–ë–õ–ò–¶–ê –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò
        with st.expander("üèÜ 4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–¢–∞–±–ª–∏—Ü–∞)", expanded=True):
            render_paginated_table(results['relevance_top'], "4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", "tbl_rel", 
                                   default_sort_col="–ü–æ–∑–∏—Ü–∏—è", default_sort_order="–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ", show_controls=False)

        st.markdown("<br>", unsafe_allow_html=True)
        st.caption("üëá –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")

        # 3. –ù–ê–ô–ú–ò–ù–ì
        with st.expander("üè∑Ô∏è –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ç–æ–≤–∞—Ä–æ–≤", expanded=False):
            if 'naming_table_df' in st.session_state and not st.session_state.naming_table_df.empty:
                st.dataframe(st.session_state.naming_table_df, use_container_width=True, hide_index=True)
            else:
                st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")

        # 4. –î–ï–¢–ê–õ–ò META (–¢–ê–ë–õ–ò–¶–ê) - –í–û–¢ –¢–£–¢ –ë–´–õ–ê –û–®–ò–ë–ö–ê
        with st.expander("üïµÔ∏è –ú–µ—Ç–∞-–¥–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤", expanded=False):
            # –í—Å—Ç–∞–≤–ª—è–µ–º –∑–∞—â–∏—Ç—É: –µ—Å–ª–∏ meta_res –Ω–µ—Ç, –Ω–µ —Å—Ç—Ä–æ–∏–º —Ç–∞–±–ª–∏—Ü—É
            if meta_res and 'detailed' in meta_res:
                df_meta_table = pd.DataFrame(meta_res['detailed'])
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É "–í–∞—à —Å–∞–π—Ç"
                my_row = pd.DataFrame([{
                    'URL': '–í–ê–® –°–ê–ô–¢', 
                    'Title': meta_res['my_data']['Title'], 
                    'Description': meta_res['my_data']['Description'], 
                    'H1': meta_res['my_data']['H1']
                }])
                df_meta_table = pd.concat([my_row, df_meta_table], ignore_index=True)
                
                st.dataframe(
                    df_meta_table, 
                    use_container_width=True, 
                    column_config={
                        "URL": st.column_config.LinkColumn("–°—Å—ã–ª–∫–∞"),
                        "Title": st.column_config.TextColumn("Title", width="medium"),
                        "Description": st.column_config.TextColumn("Description", width="large"),
                        "H1": st.column_config.TextColumn("H1", width="small"),
                    }
                )
            else:
                st.warning("–î–∞–Ω–Ω—ã–µ –ø–æ –º–µ—Ç–∞-—Ç–µ–≥–∞–º –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã (–≤–æ–∑–º–æ–∂–Ω–æ, –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ).")

        # 5. –£–ü–£–©–ï–ù–ù–ê–Ø –°–ï–ú–ê–ù–¢–ò–ö–ê
        high = results.get('missing_semantics_high', [])
        low = results.get('missing_semantics_low', [])
        
        if high or low:
            # –°—á–∏—Ç–∞–µ–º –æ–±—â—É—é —Å—É–º–º—É
            total_missing = len(high) + len(low)
            
            with st.expander(f"üß© –£–ø—É—â–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ ({total_missing})", expanded=False):
                # 1. –í–ê–ñ–ù–´–ï (–ú–µ–¥–∏–∞–Ω–∞ >= 1) - –°–∏–Ω—è—è –ø–ª–∞—à–∫–∞
                if high: 
                    words_high = ", ".join([x['word'] for x in high])
                    st.markdown(f"""
                    <div style='background:#EBF5FF; padding:12px; border-radius:8px; border:1px solid #BFDBFE; color:#1E40AF; margin-bottom:10px;'>
                        <div style='font-weight:bold; margin-bottom:4px;'>üî• –í–∞–∂–Ω—ã–µ (–ï—Å—Ç—å —É –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤):</div>
                        <div style='font-size:14px; line-height:1.5;'>{words_high}</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                # 2. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï (–ú–µ–¥–∏–∞–Ω–∞ < 1) - –°–µ—Ä–∞—è –ø–ª–∞—à–∫–∞
                if low: 
                    words_low = ", ".join([x['word'] for x in low])
                    st.markdown(f"""
                    <div style='background:#F8FAFC; padding:12px; border-radius:8px; border:1px solid #E2E8F0; color:#475569;'>
                        <div style='font-weight:bold; margin-bottom:4px;'>üî∏ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ (–í—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ä–µ–∂–µ):</div>
                        <div style='font-size:13px; line-height:1.5;'>{words_low}</div>
                    </div>
                    """, unsafe_allow_html=True)

# 6. –ì–õ–£–ë–ò–ù–ê (–ó–ê–ö–†–´–¢–û)
        with st.expander("üìâ 1. –ì–ª—É–±–∏–Ω–∞ (–î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞)", expanded=False):
            render_paginated_table(
                results['depth'], 
                "–ì–ª—É–±–∏–Ω–∞", 
                "tbl_depth_1", 
                default_sort_col="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è", 
                use_abs_sort_default=True
            )

        # 7. TF-IDF (–ó–ê–ö–†–´–¢–û)
        with st.expander("üßÆ 3. TF-IDF –ê–Ω–∞–ª–∏–∑", expanded=False):
            render_paginated_table(
                results['hybrid'], 
                "3. TF-IDF", 
                "tbl_hybrid", 
                default_sort_col="TF-IDF –¢–û–ü", 
                show_controls=False 
            )
# ==========================================
    # –ë–õ–û–ö 2: –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ï –ò –†–ê–°–ß–ï–¢
    # ==========================================
    if st.session_state.get('start_analysis_flag'):
        st.session_state.start_analysis_flag = False
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        settings = {
            'noindex': st.session_state.settings_noindex, 
            'alt_title': st.session_state.settings_alt, 
            'numbers': st.session_state.settings_numbers, 
            'norm': st.session_state.settings_norm, 
            'ua': st.session_state.settings_ua, 
            'custom_stops': st.session_state.settings_stops.split()
        }
        
        my_data, my_domain, my_serp_pos = None, "", 0
        current_input_type = st.session_state.get("my_page_source_radio")
        
        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –í–ê–®–ï–ô —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if current_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
                my_data = parse_page(st.session_state.my_url_input, settings, st.session_state.query_input)
                if not my_data: st.error("–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."); st.stop()
                my_domain = urlparse(st.session_state.my_url_input).netloc
        elif current_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            my_data = {'url': 'Local', 'domain': 'local', 'body_text': st.session_state.my_content_input, 'anchor_text': ''}

        st.session_state['saved_my_data'] = my_data 
            
        # 2. –°–±–æ—Ä –ö–ê–ù–î–ò–î–ê–¢–û–í
        candidates_pool = []
        current_source_val = st.session_state.get("competitor_source_radio")
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–µ—Ä–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (10 –∏–ª–∏ 20) –¥–ª—è –§–ò–ù–ê–õ–ê
        user_target_top_n = st.session_state.settings_top_n
        # –ê —Å–∫–∞—á–∏–≤–∞–µ–º –≤—Å–µ–≥–¥–∞ –ú–ê–ö–°–ò–ú–£–ú (30), —á—Ç–æ–±—ã –±—ã–ª–æ –∏–∑ —á–µ–≥–æ –≤—ã–±–∏—Ä–∞—Ç—å
        download_limit = 30 
        
        if "API" in current_source_val:
            if not ARSENKIN_TOKEN: st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç API —Ç–æ–∫–µ–Ω Arsenkin."); st.stop()
            with st.spinner(f"API Arsenkin (–ó–∞–ø—Ä–æ—Å –¢–æ–ø-30)..."):
                raw_top = get_arsenkin_urls(st.session_state.query_input, st.session_state.settings_search_engine, st.session_state.settings_region, ARSENKIN_TOKEN, depth_val=30)
                
                if not raw_top: st.stop()
                
                excl = [d.strip() for d in st.session_state.settings_excludes.split('\n') if d.strip()]
                agg_list = [
                    "avito", "ozon", "wildberries", "market.yandex", "tiu", "youtube", "vk.com", "yandex",
                    "leroymerlin", "petrovich", "satom", "pulscen", "blizko", "deal.by", "satu.kz", "prom.ua",
                    "wikipedia", "dzen", "rutube", "kino", "otzovik", "irecommend", "profi.ru", "zoon", "2gis",
                    "megamarket.ru", "lamoda.ru", "utkonos.ru", "vprok.ru", "allbiz.ru", "all-companies.ru",
                    "orgpage.ru", "list-org.com", "rusprofile.ru", "e-katalog.ru", "kufar.by", "wildberries.kz",
                    "ozon.kz", "kaspi.kz", "pulscen.kz", "allbiz.kz", "wildberries.uz", "olx.uz", "pulscen.uz",
                    "allbiz.uz", "wildberries.kg", "pulscen.kg", "allbiz.kg", "all.biz", "b2b-center.ru"
                ]
                excl.extend(agg_list)
                for res in raw_top:
                    dom = urlparse(res['url']).netloc.lower()
                    if my_domain and (my_domain in dom or dom in my_domain):
                        if my_serp_pos == 0 or res['pos'] < my_serp_pos: 
                            my_serp_pos = res['pos']
                    is_garbage = False
                    for x in excl:
                        if x.lower() in dom:
                            is_garbage = True
                            break
                    if is_garbage: continue
                    candidates_pool.append(res)
        else:
            raw_input_urls = st.session_state.get("persistent_urls", "")
            candidates_pool = [{'url': u.strip(), 'pos': i+1} for i, u in enumerate(raw_input_urls.split('\n')) if u.strip()]

        if not candidates_pool: st.error("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤."); st.stop()
        
        # 3. –°–ö–ê–ß–ò–í–ê–ù–ò–ï (–í—Å–µ—Ö 30)
        comp_data_valid = []
        with st.status(f"üïµÔ∏è –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (–í—Å–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidates_pool)})...", expanded=True) as status:
            with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
                futures = {
                    executor.submit(parse_page, item['url'], settings, st.session_state.query_input): item 
                    for item in candidates_pool
                }
                done_count = 0
                for f in concurrent.futures.as_completed(futures):
                    original_item = futures[f]
                    try:
                        res = f.result()
                        if res:
                            res['pos'] = original_item['pos']
                            comp_data_valid.append(res)
                    except: pass
                    done_count += 1
                    status.update(label=f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {done_count}/{len(candidates_pool)} | –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ: {len(comp_data_valid)}")

            comp_data_valid.sort(key=lambda x: x['pos'])
            # –°–Ω–∞—á–∞–ª–∞ –±–µ—Ä–µ–º –í–°–ï–•, –∫—Ç–æ —Å–∫–∞—á–∞–ª—Å—è (–¥–æ 30), –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
            data_for_graph = comp_data_valid[:download_limit]
            targets_for_graph = [{'url': d['url'], 'pos': d['pos']} for d in data_for_graph]

        # 5. –†–ê–°–ß–ï–¢ –ú–ï–¢–†–ò–ö (–î–í–û–ô–ù–û–ô –ü–†–û–ì–û–ù)
        with st.spinner("–ê–Ω–∞–ª–∏–∑ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è..."):
            
            # --- –≠–¢–ê–ü 1: –ß–µ—Ä–Ω–æ–≤–æ–π –ø—Ä–æ–≥–æ–Ω (–ø–æ –≤—Å–µ–º 30 —Å–∞–π—Ç–∞–º) ---
            # –≠—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ –∏ –Ω–∞–π—Ç–∏ –∞–Ω–æ–º–∞–ª–∏–∏
            results_full = calculate_metrics(data_for_graph, my_data, settings, my_serp_pos, targets_for_graph)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ü–û–õ–ù–´–ï –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ (—á—Ç–æ–±—ã –Ω–∞ –Ω–µ–º –±—ã–ª–∏ –≤—Å–µ)
            st.session_state['full_graph_data'] = results_full['relevance_top']
            
            # –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π –ø–æ –ø–æ–ª–Ω–æ–º—É —Å–ø–∏—Å–∫—É
            df_rel_check = results_full['relevance_top']
            good_urls, bad_urls_dicts, trend = analyze_serp_anomalies(df_rel_check)
            st.session_state['serp_trend_info'] = trend
            
            # --- –≠–¢–ê–ü 2: –û—Ç–±–æ—Ä —á–∏—Å—Ç–æ–≤—ã—Ö (–¢–æ–ø-10/20 –±–µ–∑ –º—É—Å–æ—Ä–∞) ---
            
# 1. –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–µ—Ö —Å–∞–π—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –≤ —Å–ø–∏—Å–∫–µ –ø–ª–æ—Ö–∏—Ö
            bad_urls_set = set(item['url'] for item in bad_urls_dicts)
            
            # === –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –§–ò–õ–¨–¢–†–ê–¶–ò–ò ===
            # –ï—Å–ª–∏ —ç—Ç–æ API - –º—ã —Ñ–∏–ª—å—Ç—Ä—É–µ–º –∏ —Ä–µ–∂–µ–º —Ç–æ–ø.
            # –ï—Å–ª–∏ —ç—Ç–æ –†–£–ß–ù–û–ô —Ä–µ–∂–∏–º - –º—ã –ù–ï —Ñ–∏–ª—å—Ç—Ä—É–µ–º (–¥–æ–≤–µ—Ä—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é).
            if "API" in current_source_val:
                clean_data_pool = [d for d in data_for_graph if d['url'] not in bad_urls_set]
                final_clean_data = clean_data_pool[:user_target_top_n]
            else:
                # –í —Ä—É—á–Ω–æ–º —Ä–µ–∂–∏–º–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï–• —Å–∫–∞—á–∞–Ω–Ω—ã—Ö, –Ω–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º "—Å–ª–∞–±—ã—Ö"
                final_clean_data = data_for_graph 
            
            # <--- –í–ê–ñ–ù–û: –°—Ç—Ä–æ–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–¥–µ—Ç –°–¢–†–û–ì–û –ü–û–°–õ–ï –±–ª–æ–∫–∞ if/else --->
            st.session_state['raw_comp_data'] = final_clean_data
            # ------------------------------------------------------------------

            final_clean_targets = [{'url': d['url'], 'pos': d['pos']} for d in final_clean_data]
            
            # 3. –§–ò–ù–ê–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ (–¢–æ–ª—å–∫–æ –ø–æ —ç–ª–∏—Ç–µ)
            results_final = calculate_metrics(final_clean_data, my_data, settings, my_serp_pos, final_clean_targets)
            
# 3. –§–ò–ù–ê–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ (–¢–æ–ª—å–∫–æ –ø–æ —ç–ª–∏—Ç–µ)
            results_final = calculate_metrics(final_clean_data, my_data, settings, my_serp_pos, final_clean_targets)
            st.session_state.analysis_results = results_final
            
            # --- –û—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ (–Ω–µ–π–º–∏–Ω–≥, —Å–µ–º–∞–Ω—Ç–∏–∫–∞) ---
            naming_df = calculate_naming_metrics(final_clean_data, my_data, settings)
            st.session_state.naming_table_df = naming_df 
            st.session_state.ideal_h1_result = analyze_ideal_name(final_clean_data)
            st.session_state.analysis_done = True
            
            # ==========================================
            # üî• –ë–õ–û–ö: –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –°–ï–ú–ê–ù–¢–ò–ö–ò (–°–¢–†–û–ì–û –ó–î–ï–°–¨)
            # ==========================================
            words_to_check = [x['word'] for x in results_final.get('missing_semantics_high', [])]
            
            # –ï—Å–ª–∏ "–≤–∞–∂–Ω—ã—Ö" —Å–ª–æ–≤ –º–∞–ª–æ, –±–µ—Ä–µ–º –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ, —á—Ç–æ–±—ã –∑–∞–ø–æ–ª–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã
            if len(words_to_check) < 5:
                words_to_check.extend([x['word'] for x in results_final.get('missing_semantics_low', [])[:20]])

            if not words_to_check:
                st.session_state.categorized_products = []; st.session_state.categorized_services = []
                st.session_state.categorized_commercial = []; st.session_state.categorized_dimensions = []
            else:
                with st.spinner("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏..."):
                    categorized = classify_semantics_with_api(words_to_check, YANDEX_DICT_KEY)
                
                st.session_state.categorized_products = categorized['products']
                st.session_state.categorized_services = categorized['services']
                st.session_state.categorized_commercial = categorized['commercial']
                st.session_state.categorized_geo = categorized['geo']
                st.session_state.categorized_dimensions = categorized['dimensions']
                st.session_state.categorized_general = categorized['general']
                st.session_state.categorized_sensitive = categorized['sensitive']

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—ã
                st.session_state.orig_products = categorized['products'] + categorized['sensitive']
                st.session_state.orig_services = categorized['services'] + categorized['sensitive']
                st.session_state.orig_commercial = categorized['commercial'] + categorized['sensitive']
                st.session_state.orig_geo = categorized['geo'] + categorized['sensitive']
                st.session_state.orig_dimensions = categorized['dimensions'] + categorized['sensitive']
                st.session_state.orig_general = categorized['general'] + categorized['sensitive']
                
                st.session_state['sensitive_words_input_final'] = "\n".join(categorized['sensitive'])

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
            all_found_products = st.session_state.categorized_products
            count_prods = len(all_found_products)
            if count_prods < 20:
                st.session_state.auto_tags_words = all_found_products
                st.session_state.auto_promo_words = []
            else:
                half_count = int(math.ceil(count_prods / 2))
                st.session_state.auto_tags_words = all_found_products[:half_count]
                st.session_state.auto_promo_words = all_found_products[half_count:]
            
            st.session_state['tags_products_edit_final'] = "\n".join(st.session_state.auto_tags_words)
            st.session_state['promo_keywords_area_final'] = "\n".join(st.session_state.auto_promo_words)
            # ==========================================
            # –ö–û–ù–ï–¶ –ë–õ–û–ö–ê –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò
            # ==========================================
            
            
            # === –£–ú–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø (Smart Filter Logic) ===
            
            # 1. –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–Ω–æ–º–∞–ª–∏–π
            if "API" in current_source_val and 'full_graph_data' in st.session_state:
                df_rel_check = st.session_state['full_graph_data']
            else:
                df_rel_check = st.session_state.analysis_results['relevance_top']
            
            # 2. –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π
            good_urls, bad_urls_dicts, trend = analyze_serp_anomalies(df_rel_check)
            st.session_state['serp_trend_info'] = trend
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞
            is_filter_enabled = st.session_state.get("settings_auto_filter", True)
            
            def get_strict_key(u):
                if not u: return ""
                return str(u).lower().strip().replace("https://", "").replace("http://", "").replace("www.", "").rstrip('/')

            final_clean_text = ""
            
            # --- –õ–û–ì–ò–ö–ê –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø ---
            if is_filter_enabled and bad_urls_dicts:
                # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–ª–æ—Ö–∏—Ö
                st.session_state['detected_anomalies'] = bad_urls_dicts
                
                blacklist_keys = set()
                excluded_display_list = []
                for item in bad_urls_dicts:
                    raw_u = item.get('url', '')
                    if raw_u:
                        blacklist_keys.add(get_strict_key(raw_u))
                        excluded_display_list.append(str(raw_u).strip())
                
                st.session_state['excluded_urls_auto'] = "\n".join(excluded_display_list)
                
                # 2. –°–æ–±–∏—Ä–∞–µ–º —Ö–æ—Ä–æ—à–∏—Ö
                clean_active_list = []
                seen_keys = set()
                for u in good_urls:
                    key = get_strict_key(u)
                    if key and key not in blacklist_keys and key not in seen_keys:
                        clean_active_list.append(str(u).strip())
                        seen_keys.add(key)
                
                final_clean_text = "\n".join(clean_active_list)
                st.toast(f"–§–∏–ª—å—Ç—Ä —Å—Ä–∞–±–æ—Ç–∞–ª. –ò—Å–∫–ª—é—á–µ–Ω–æ: {len(blacklist_keys)}", icon="‚úÇÔ∏è")
            
            else:
                # –§–∏–ª—å—Ç—Ä –≤—ã–∫–ª—é—á–µ–Ω –∏–ª–∏ –ø–ª–æ—Ö–∏—Ö –Ω–µ—Ç - –±–µ—Ä–µ–º –≤—Å—ë
                clean_all = []
                seen_all = set()
                combined_pool = good_urls + [x['url'] for x in (bad_urls_dicts or [])]
                for u in combined_pool:
                    key = get_strict_key(u)
                    if key and key not in seen_all:
                        clean_all.append(str(u).strip())
                        seen_all.add(key)
                
                final_clean_text = "\n".join(clean_all)
                # –ß–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—ã–µ –æ—à–∏–±–∫–∏
                st.session_state.pop('excluded_urls_auto', None)
                st.session_state.pop('detected_anomalies', None)

            # === –§–ò–ù–ê–õ–¨–ù–ê–Ø –ó–ê–ü–ò–°–¨ –ò –ü–ï–†–ï–ó–ê–ì–†–£–ó–ö–ê ===
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –í–†–ï–ú–ï–ù–ù–£–Æ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
            st.session_state['temp_update_urls'] = final_clean_text
            
            # –°—Ç–∞–≤–∏–º —Ñ–ª–∞–≥ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–∞–¥–∏–æ-–∫–Ω–æ–ø–∫–∏
            st.session_state['force_radio_switch'] = True
            
            # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É, —á—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –°–í–ï–†–•–£
            st.rerun()

# ------------------------------------------
# TAB 2: WHOLESALE GENERATOR (COMBINED)
# ------------------------------------------
with tab_wholesale_main:
    st.header("üè≠ –ï–¥–∏–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    
    # ==========================================
    # 0. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• (–ò–ó –¢–ï–ö–£–©–ï–ì–û –°–û–°–¢–û–Ø–ù–ò–Ø)
    # ==========================================
    cat_products = st.session_state.get('categorized_products', [])
    cat_services = st.session_state.get('categorized_services', [])
    
    # 1. –î–ª—è –¢–µ–≥–æ–≤ –∏ –ü—Ä–æ–º–æ (–°–∞–π–¥–±–∞—Ä –∏—Å–∫–ª—é—á–µ–Ω)
    structure_keywords = cat_products + cat_services
    count_struct = len(structure_keywords)

    if 'auto_tags_words' in st.session_state and st.session_state.auto_tags_words:
         tags_list_source = st.session_state.auto_tags_words
         promo_list_source = st.session_state.auto_promo_words
    else:
         if count_struct > 0:
            if count_struct < 10:
                tags_list_source = structure_keywords
                promo_list_source = []
            else:
                # –î–µ–ª–∏–º –≤—Å–µ–≥–¥–∞ –ø–æ–ø–æ–ª–∞–º (–¢–µ–≥–∏ / –ü—Ä–æ–º–æ), –°–∞–π–¥–±–∞—Ä –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
                mid = math.ceil(count_struct / 2)
                tags_list_source = structure_keywords[:mid]
                promo_list_source = structure_keywords[mid:]
         else:
             tags_list_source = []
             promo_list_source = []
    
    # –°–∞–π–¥–±–∞—Ä –≤—Å–µ–≥–¥–∞ –ø—É—Å—Ç–æ–π
    sidebar_default_text = ""

    tags_default_text = ", ".join(tags_list_source)
    promo_default_text = ", ".join(promo_list_source)

    # 2. –î–ª—è –¢–∞–±–ª–∏—Ü (–†–∞–∑–º–µ—Ä—ã/–ì–û–°–¢)
    cat_dimensions = st.session_state.get('categorized_dimensions', [])
    tech_context_default = ", ".join(cat_dimensions) if cat_dimensions else ""

    # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ö–æ–º–º–µ—Ä—Ü–∏–∏/–û–±—â–∏—Ö –∏ –ì–ï–û
    cat_commercial = st.session_state.get('categorized_commercial', [])
    cat_general = st.session_state.get('categorized_general', [])
    cat_geo = st.session_state.get('categorized_geo', [])
    
    # –ò–°–ö–õ–Æ–ß–ê–ï–ú –ì–ï–û –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    text_context_list_raw = cat_commercial + cat_general
    text_context_default = ", ".join(text_context_list_raw)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ñ–æ–ª—Ç –¥–ª—è –ì–ï–û –±–ª–æ–∫–∞
    geo_context_default = ", ".join(cat_geo)

    # --- –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ê–ö–¢–ò–í–ù–û–°–¢–ò –ú–û–î–£–õ–ï–ô ---
    auto_check_text = bool(text_context_list_raw)
    auto_check_tags = bool(tags_list_source)
    auto_check_tables = bool(cat_dimensions)
    auto_check_promo = bool(promo_list_source)
    auto_check_geo = bool(cat_geo)

    # ==========================================
    # 1. –í–í–û–î–ù–´–ï –î–ê–ù–ù–´–ï
    # ==========================================
    with st.container(border=True):
        st.subheader("1. –ò—Å—Ç–æ—á–Ω–∏–∫ –∏ –î–æ—Å—Ç—É–ø—ã")
        
        col_source, col_key = st.columns([3, 1])
        
        use_manual_html = st.checkbox("üìù –í—Å—Ç–∞–≤–∏—Ç—å HTML –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", key="cb_manual_html_mode", value=False)
        
        with col_source:
            if use_manual_html:
                manual_html_source = st.text_area(
                    "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (HTML)", 
                    height=200, 
                    placeholder="<html>...</html>", 
                    help="–°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Å—é–¥–∞ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."
                )
                main_category_url = None
            else:
                main_category_url = st.text_input(
                    "URL –ö–∞—Ç–µ–≥–æ—Ä–∏–∏", 
                    placeholder="https://site.ru/catalog/...", 
                    help="–°–∫—Ä–∏–ø—Ç —Å–æ–±–µ—Ä–µ—Ç —Ç–æ–≤–∞—Ä—ã —Å —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
                )
                manual_html_source = None

        with col_key:
            try:
                key_from_secrets = st.secrets["GEMINI_KEY"]
            except (FileNotFoundError, KeyError):
                key_from_secrets = ""

            default_key = st.session_state.get('gemini_key_cache', key_from_secrets)
            gemini_api_key = st.text_input("Google Gemini API Key", value=default_key, type="password")

    # ==========================================
    # 2. –í–´–ë–û–† –ú–û–î–£–õ–ï–ô
    # ==========================================
    st.subheader("2. –ö–∞–∫–∏–µ –±–ª–æ–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º?")
    st.info("‚ÑπÔ∏è **–ê–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞:** –ì–∞–ª–æ—á–∫–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ç–∞–º, –≥–¥–µ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—à–ª–∏—Å—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–ª–æ–≤–∞.")
    col_ch1, col_ch2, col_ch3, col_ch4, col_ch5, col_ch6 = st.columns(6)
    
    with col_ch1: use_text = st.checkbox("ü§ñ AI –¢–µ–∫—Å—Ç—ã", value=auto_check_text)
    with col_ch2: use_tags = st.checkbox("üè∑Ô∏è –¢–µ–≥–∏", value=auto_check_tags)
    with col_ch3: use_tables = st.checkbox("üß© –¢–∞–±–ª–∏—Ü—ã", value=auto_check_tables)
    with col_ch4: use_promo = st.checkbox("üî• –ü—Ä–æ–º–æ", value=auto_check_promo)
    
    # –û–¢–ö–õ–Æ–ß–ê–ï–ú –°–ê–ô–î–ë–ê–† –ó–î–ï–°–¨
    with col_ch5: use_sidebar = st.checkbox("üìë –°–∞–π–¥–±–∞—Ä (–û—Ç–∫–ª)", value=False, disabled=True, key="sidebar_disabled_ui")
    
    with col_ch6: use_geo = st.checkbox("üåç –ì–µ–æ-–±–ª–æ–∫", value=auto_check_geo)

    # ==========================================
    # 3. –ù–ê–°–¢–†–û–ô–ö–ò –ú–û–î–£–õ–ï–ô
    # ==========================================
    global_tags_list = []
    global_promo_list = []
    global_sidebar_list = []
    global_geo_list = []
    tags_file_content = ""
    table_prompts = []
    df_db_promo = None
    promo_title = "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º"
    sidebar_content = ""
    text_context_final_list = []
    tech_context_final_str = ""
    
    # –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–ª–æ–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞ (–ø–æ –¥–µ—Ñ–æ–ª—Ç—É 5)
    num_text_blocks_val = 5 

    if any([use_text, use_tags, use_tables, use_promo, use_sidebar, use_geo]):
        st.subheader("3. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥—É–ª–µ–π")

        # --- AI TEXT ---
        if use_text:
            with st.container(border=True):
                st.markdown("#### ü§ñ 1. AI –¢–µ–∫—Å—Ç—ã")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–±–æ—Ä –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–ª–æ–∫–æ–≤
                col_txt1, col_txt2 = st.columns([1, 4])
                with col_txt1:
                    num_text_blocks_val = st.selectbox("–ö–æ–ª-–≤–æ –±–ª–æ–∫–æ–≤", [1, 2, 3, 4, 5], index=4, key="sb_num_blocks")
                
                with col_txt2:
                    ai_words_input = st.text_area(
                        "–°–ª–æ–≤–∞ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è (–ö–æ–º–º–µ—Ä—Ü–∏—è + –û–±—â–∏–µ)", 
                        value=text_context_default, 
                        height=100, 
                        key="ai_text_context_editable",
                        help="–≠—Ç–∏ —Å–ª–æ–≤–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –ø–æ—Å—Ç–∞—Ä–∞–µ—Ç—Å—è –≤–Ω–µ–¥—Ä–∏—Ç—å –≤ —Ç–µ–∫—Å—Ç."
                    )
                
                text_context_final_list = [x.strip() for x in re.split(r'[,\n]+', ai_words_input) if x.strip()]

        # --- TAGS ---
        if use_tags:
            with st.container(border=True):
                st.markdown("#### üè∑Ô∏è 2. –¢–µ–≥–∏")
                kws_input_tags = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=tags_default_text, 
                    height=100, 
                    key="kws_tags_auto"
                )
                global_tags_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_tags) if x.strip()]
                if not global_tags_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                col_t1, col_t2 = st.columns([1, 2])
                with col_t1: u_manual = st.checkbox("–°–≤–æ—è –±–∞–∑–∞ —Å—Å—ã–ª–æ–∫ (.txt)", key="cb_tags_vert")
                with col_t2:
                    default_tags_path = "data/links_base.txt"
                    if not u_manual and os.path.exists(default_tags_path):
                        st.success(f"‚úÖ –ë–∞–∑–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (`links_base.txt`)")
                        with open(default_tags_path, "r", encoding="utf-8") as f: tags_file_content = f.read()
                    elif u_manual:
                        up_t = st.file_uploader("–§–∞–π–ª .txt", type=["txt"], key="up_tags_vert", label_visibility="collapsed")
                        if up_t: tags_file_content = up_t.getvalue().decode("utf-8")
                    else: st.error("‚ùå –§–∞–π–ª –±–∞–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω!")

# --- –§–£–ù–ö–¶–ò–Ø –ì–õ–£–ë–û–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê –ö–û–ù–¢–ï–ö–°–¢–ê –î–õ–Ø –¢–ê–ë–õ–ò–¶ ---
        def generate_context_aware_headers(count, query, dimensions_list, general_list):
            """
            –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ò –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (—Ä–∞–∑–º–µ—Ä—ã, –æ–±—â–∏–µ), 
            —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —Ç–∏–ø—ã —Ç–∞–±–ª–∏—Ü –Ω—É–∂–Ω—ã.
            """
            query_lower = query.lower()
            
            # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–∫–∏ —Å–ª–æ–≤ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            dims_str = " ".join(dimensions_list).lower()
            gen_str = " ".join(general_list).lower()
            full_context = f"{dims_str} {gen_str} {query_lower}"
            
            # --- 1. –î–ï–¢–ï–ö–¢–û–†–´ –°–ò–ì–ù–ê–õ–û–í (–ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Å–µ–º–∞–Ω—Ç–∏–∫–µ) ---
            
            # –ü—Ä–∏–∑–Ω–∞–∫ —Ä–∞–∑–º–µ—Ä–æ–≤: –µ—Å—Ç—å —Ü–∏—Ñ—Ä—ã —Å '—Ö' (10—Ö20), —Å–ª–æ–≤–∞ –º–º, –∫–≥, —Ç–æ–Ω–Ω–∞, —Ä–∞–∑–º–µ—Ä
            has_sizes_signal = (
                len(dimensions_list) > 0 or 
                bool(re.search(r'\d+[x—Ö*]\d+', full_context)) or 
                any(x in full_context for x in ['—Ä–∞–∑–º–µ—Ä', '–≥–∞–±–∞—Ä–∏—Ç', '—Ç–æ–ª—â–∏–Ω', '–¥–∏–∞–º–µ—Ç—Ä', '—Ä–∞—Å–∫—Ä–æ–π', '–≤–µ—Å', '–º–∞—Å—Å'])
            )
            
            # –ü—Ä–∏–∑–Ω–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤: –ì–û–°–¢, –û–°–¢, –¢–£, DIN, AISI
            has_gost_signal = any(x in full_context for x in ['–≥–æ—Å—Ç', 'din', 'aisi', 'astm', '—Ç—É ', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç'])
            
            # –ü—Ä–∏–∑–Ω–∞–∫ –º–∞—Ä–æ–∫/–º–∞—Ç–µ—Ä–∏–∞–ª–∞: —Å—Ç–∞–ª—å, —Å–ø–ª–∞–≤, –º–∞—Ä–∫–∞, —Å—Ç.3, 09–≥2—Å
            has_grade_signal = any(x in full_context for x in ['–º–∞—Ä–∫', '—Å–ø–ª–∞–≤', '—Å—Ç–∞–ª—å', '—Å—Ç.', '–º–∞—Ç–µ—Ä–∏–∞–ª', '—Ö–∏–º–∏—á', '—Å–æ—Å—Ç–∞–≤'])
            
            # –ü—Ä–∏–∑–Ω–∞–∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è: –¥–ª—è —á–µ–≥–æ, —Å—Ñ–µ—Ä–∞, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            has_usage_signal = any(x in full_context for x in ['–ø—Ä–∏–º–µ–Ω–µ–Ω', '—Å—Ñ–µ—Ä', '–Ω–∞–∑–Ω–∞—á–µ–Ω', '–∏—Å–ø–æ–ª—å–∑'])

            # --- 2. –°–ë–û–†–ö–ê –û–ß–ï–†–ï–î–ò (PRIORITY QUEUE) ---
            # –ú—ã —Ä–∞—Å—Å—Ç–∞–≤–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—ã –≤ –ø–æ—Ä—è–¥–∫–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
            priority_stack = []
            
            # –ï—Å–ª–∏ —ç—Ç–æ –º–µ—Ç–∞–ª–ª–æ–ø—Ä–æ–∫–∞—Ç (–µ—Å—Ç—å –º–∞—Ä–∫–∏/—Å–ø–ª–∞–≤—ã), –æ–±—ã—á–Ω–æ —Å–Ω–∞—á–∞–ª–∞ —Å—Ç–∞–≤—è—Ç –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–ª–∏ –ú–∞—Ä–∫–∏
            if has_grade_signal:
                priority_stack.append("–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã")
                
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã - —ç—Ç–æ —Å—É–ø–µ—Ä –≤–∞–∂–Ω–æ, —Å—Ç–∞–≤–∏–º –≤ –Ω–∞—á–∞–ª–æ
            if has_sizes_signal:
                # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –ú–∞—Ä–∫–∏, —Ç–æ –†–∞–∑–º–µ—Ä—ã –≤—Ç–æ—Ä—ã–º–∏. –ï—Å–ª–∏ –Ω–µ—Ç - –ø–µ—Ä–≤—ã–º–∏.
                priority_stack.append("–¢–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤")
                
            # –ï—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ì–û–°–¢–æ–≤
            if has_gost_signal:
                priority_stack.append("–ì–û–°–¢—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã")
                
            # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—Ä–æ —Ö–∏–º —Å–æ—Å—Ç–∞–≤ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ)
            if "—Ö–∏–º" in full_context and "—Å–æ—Å—Ç–∞–≤" in full_context:
                 # –í—Å—Ç–∞–≤–ª—è–µ–º "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤" –≤–º–µ—Å—Ç–æ "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã" –∏–ª–∏ —Ä—è–¥–æ–º
                 if "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã" in priority_stack:
                     idx = priority_stack.index("–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã")
                     priority_stack.insert(idx+1, "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤")
                 else:
                     priority_stack.append("–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤")

            # --- 3. –ó–ê–ü–û–õ–ù–ï–ù–ò–ï –ü–£–°–¢–û–¢ (DEFAULTS) ---
            # –ï—Å–ª–∏ –º—ã –≤—ã–±—Ä–∞–ª–∏ 5 —Ç–∞–±–ª–∏—Ü, –∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞—à–ª–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ 2, –Ω—É–∂–Ω–æ –¥–æ–±–∏—Ç—å –æ—Å—Ç–∞–ª—å–Ω–æ–µ
            defaults = [
                "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞
                "–°–≤–æ–π—Å—Ç–≤–∞",
                "–°—Ñ–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è",
                "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–¥–µ–ª–∏—è",
                "–ê–Ω–∞–ª–æ–≥–∏"
            ]
            
            final_headers = []
            # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ, —á—Ç–æ –Ω–∞—à–ª–∏ —É–º–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
            for p in priority_stack:
                if p not in final_headers: final_headers.append(p)
            
            # –î–æ–±–∏–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏
            for d in defaults:
                if d not in final_headers: final_headers.append(d)
                
            # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –≤—Å—ë —Ä–∞–≤–Ω–æ –º–∞–ª–æ (—Ä–µ–¥–∫–∏–π —Å–ª—É—á–∞–π)
            while len(final_headers) < count:
                final_headers.append("–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏")
                
            return final_headers[:count]

        # --- –ë–õ–û–ö –ò–ù–¢–ï–†–§–ï–ô–°–ê TABLES ---
        if use_tables:
            with st.container(border=True):
                st.markdown("#### üß© 3. –¢–∞–±–ª–∏—Ü—ã")
                
                # –î–ê–ù–ù–´–ï –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê (–ë–µ—Ä–µ–º –∏–∑ session_state, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞)
                raw_query = st.session_state.get('query_input', '')
                found_dims = st.session_state.get('categorized_dimensions', []) # –°–ª–æ–≤–∞—Ä—å —Ä–∞–∑–º–µ—Ä–æ–≤
                found_general = st.session_state.get('categorized_general', []) # –°–ª–æ–≤–∞—Ä—å –æ–±—â–∏—Ö —Å–ª–æ–≤
                
                col_ctx, col_cnt = st.columns([3, 1]) 
                
                with col_ctx:
                    tech_context_final_str = st.text_area(
                        "–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ç–∞–±–ª–∏—Ü (–ú–∞—Ä–∫–∏, –ì–û–°–¢, –†–∞–∑–º–µ—Ä—ã)", 
                        value=tech_context_default, # –ó–¥–µ—Å—å –ª–µ–∂–∞—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
                        height=68, 
                        key="table_context_editable",
                        help="–≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–º–æ–≥—É—Ç AI —Å–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É."
                    )
                
                with col_cnt:
                    cnt_options = [1, 2, 3, 4, 5]
                    cnt = st.selectbox("–ö–æ–ª-–≤–æ —Ç–∞–±–ª–∏—Ü", cnt_options, index=1, key="num_tbl_vert_select")

                # --- –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê ---
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢–û–ì–û, –ß–¢–û –ù–ê–®–õ–ò –í –°–ï–ú–ê–ù–¢–ò–ö–ï
                smart_headers_list = generate_context_aware_headers(cnt, raw_query, found_dims, found_general)

                table_presets = [
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–°–≤–æ–π—Å—Ç–≤–∞", "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–¥–µ–ª–∏—è",
                    "–û–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ", "–¢–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤", "–°–æ—Ä—Ç–∞–º–µ–Ω—Ç",
                    "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤", "–§–∏–∑–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞", "–ú–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞",
                    "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã", "–°–æ—Å—Ç–∞–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞", "–ì–û–°–¢—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã",
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –ì–û–°–¢", "–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ",
                    "–°—Ñ–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", "–£—Å–ª–æ–≤–∏—è —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏", "–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è",
                    "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", "–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏", "–ê–Ω–∞–ª–æ–≥–∏",
                    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π", "–†–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç–∏"
                ]
                
                table_prompts = []
                st.write("") 
                
                cols = st.columns(cnt)
                
                for i, col in enumerate(cols):
                    with col:
                        st.caption(f"**–¢–∞–±–ª–∏—Ü–∞ {i+1}**")
                        
                        # –ê–≤—Ç–æ-–≤—ã–±–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
                        suggested_topic = smart_headers_list[i]
                        
                        try: default_idx = table_presets.index(suggested_topic)
                        except: default_idx = 0
                        
                        is_manual = st.checkbox("–°–≤–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫", key=f"cb_tbl_manual_{i}")
                        
                        if is_manual:
                            selected_topic = st.text_input(
                                f"–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª. {i+1}", value="", 
                                key=f"tbl_topic_custom_{i}", label_visibility="collapsed"
                            )
                            if not selected_topic.strip(): selected_topic = "–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏" 
                        else:
                            selected_topic = st.selectbox(
                                f"–¢–µ–º–∞ —Ç–∞–±–ª. {i+1}", 
                                table_presets, 
                                index=default_idx, # <--- –£–ú–ù–´–ô –ò–ù–î–ï–ö–°
                                key=f"tbl_topic_select_{i}",
                                label_visibility="collapsed"
                            )
                        
                        table_prompts.append(selected_topic)

# --- PROMO (–° –ê–ù–ê–õ–ò–ó–û–ú –ö–û–ú–ú–ï–†–ß–ï–°–ö–ò–• –§–ê–ö–¢–û–†–û–í) ---
        if use_promo:
            with st.container(border=True):
                st.markdown("#### üî• 4. –ü—Ä–æ–º–æ-–±–ª–æ–∫")
                
                kws_input_promo = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=promo_default_text, 
                    height=100, 
                    key="kws_promo_auto"
                )
                global_promo_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_promo) if x.strip()]
                if not global_promo_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                
                col_p1, col_p2 = st.columns([1, 2])
                with col_p1:
                    promo_presets = [
                        "–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ", "–ü–æ—Ö–æ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã", "–í–∞—Å –º–æ–∂–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å",
                        "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º", "–î—Ä—É–≥–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", "–í–∞–º –º–æ–∂–µ—Ç –ø—Ä–∏–≥–æ–¥–∏—Ç—å—Å—è",
                        "–¢–∞–∫–∂–µ –≤ —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ", "–° —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º –ø–æ–∫—É–ø–∞—é—Ç", "–ß–∞—Å—Ç–æ –ø–æ–∫—É–ø–∞—é—Ç –≤–º–µ—Å—Ç–µ",
                        "–°–æ–ø—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–≤–∞—Ä—ã", "–•–∏—Ç—ã –ø—Ä–æ–¥–∞–∂", "–í—ã–±–æ—Ä –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π",
                        "–õ–∏–¥–µ—Ä—ã —Å–ø—Ä–æ—Å–∞", "–ü–æ–ø—É–ª—è—Ä–Ω–æ–µ —Å–µ–π—á–∞—Å", "–¢–æ–ø —Ç–æ–≤–∞—Ä–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏",
                        "–õ—É—á—à–∞—è —Ü–µ–Ω–∞", "–°–ø–µ—Ü–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", "–£—Å–ø–µ–π—Ç–µ –∑–∞–∫–∞–∑–∞—Ç—å",
                        "–ù–µ –∑–∞–±—É–¥—å—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å", "–í—ã –Ω–µ–¥–∞–≤–Ω–æ —Å–º–æ—Ç—Ä–µ–ª–∏"
                    ]

                    # --- –õ–û–ì–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê –ö–û–ú–ú–ï–†–¶–ò–ò ---
                    # –ë–µ—Ä–µ–º –∑–∞–ø—Ä–æ—Å + —Å–ø–∏—Å–æ–∫ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ (Tab 1)
                    raw_query = st.session_state.get('query_input', '').lower()
                    comm_words = st.session_state.get('categorized_commercial', [])
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                    comm_context = f"{raw_query} {' '.join(comm_words)}".lower()
                    
                    target_header = "–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ" # –î–µ—Ñ–æ–ª—Ç (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π)

                    # 1. –Ø–≤–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è (–µ—Å—Ç—å —Å–ª–æ–≤–∞ '—Ü–µ–Ω–∞', '–∫—É–ø–∏—Ç—å' –≤ —Å–µ–º–∞–Ω—Ç–∏–∫–µ –∏–ª–∏ –∑–∞–ø—Ä–æ—Å–µ)
                    is_commercial = any(x in comm_context for x in ["–∫—É–ø–∏—Ç—å", "—Ü–µ–Ω–∞", "–∑–∞–∫–∞–∑", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–ø—Ä–∞–π—Å", "–º–∞–≥–∞–∑–∏–Ω", "–∫–æ—Ä–∑–∏–Ω–∞"])
                    
                    # 2. –ê–∫—Ü–∏–æ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
                    is_promo = any(x in comm_context for x in ["–∞–∫—Ü–∏—è", "—Å–∫–∏–¥–∫", "—Ä–∞—Å–ø—Ä–æ–¥–∞–∂", "–≤—ã–≥–æ–¥–Ω"])
                    
                    # 3. –†–µ–π—Ç–∏–Ω–≥–æ–≤—ã–µ —Å–ª–æ–≤–∞
                    is_top = any(x in comm_context for x in ["—Ç–æ–ø", "–ª—É—á—à", "—Ä–µ–π—Ç–∏–Ω–≥", "–ø–æ–ø—É–ª—è—Ä–Ω"])

                    if is_promo:
                        target_header = "–°–ø–µ—Ü–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"
                    elif is_top:
                        target_header = "–õ–∏–¥–µ—Ä—ã —Å–ø—Ä–æ—Å–∞"
                    elif is_commercial:
                        # –ï—Å–ª–∏ —ç—Ç–æ —è–≤–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è, –ª—É—á—à–µ "–ü–æ–∫—É–ø–∞—é—Ç –≤–º–µ—Å—Ç–µ" –∏–ª–∏ "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º"
                        target_header = "–° —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º –ø–æ–∫—É–ø–∞—é—Ç"
                    
                    try: promo_smart_idx = promo_presets.index(target_header)
                    except: promo_smart_idx = 0

                    use_custom_header = st.checkbox("–í–≤–µ—Å—Ç–∏ —Å–≤–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫", key="cb_custom_header")
                    
                    if use_custom_header:
                        promo_title = st.text_input("–í–∞—à –∑–∞–≥–æ–ª–æ–≤–æ–∫", placeholder="–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ", key="pr_tit_vert")
                    else:
                        promo_title = st.selectbox(
                            "–í–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–≥–æ–ª–æ–≤–∫–∞", 
                            promo_presets, 
                            index=promo_smart_idx, # <--- –£–ú–ù–´–ô –í–´–ë–û–†
                            key="promo_header_select"
                        )

                    st.markdown("<br>", unsafe_allow_html=True)
                    u_img_man = st.checkbox("–°–≤–æ—è –±–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫", key="cb_img_vert")

                with col_p2:
                    default_img_db = "data/images_db.xlsx"
                    if not u_img_man and os.path.exists(default_img_db):
                        st.success("‚úÖ –ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ (`images_db.xlsx`)")
                        try: df_db_promo = pd.read_excel(default_img_db)
                        except: pass
                    elif u_img_man:
                        up_i = st.file_uploader("–§–∞–π–ª .xlsx", type=['xlsx'], key="up_img_vert", label_visibility="collapsed")
                        if up_i: df_db_promo = pd.read_excel(up_i)
                    else: st.error("‚ùå –ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")

        # --- SIDEBAR ---
        if use_sidebar:
            with st.container(border=True):
                st.markdown("#### üìë 5. –°–∞–π–¥–±–∞—Ä")
                kws_input_sidebar = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏", 
                    value=sidebar_default_text, 
                    height=100, 
                    key="kws_sidebar_auto"
                )
                global_sidebar_list = [x.strip() for x in kws_input_sidebar.split('\n') if x.strip()]
                if not global_sidebar_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                col_s1, col_s2 = st.columns([1, 2])
                with col_s1: u_sb_man = st.checkbox("–°–≤–æ–π —Ñ–∞–π–ª –º–µ–Ω—é (.txt)", key="cb_sb_vert")
                with col_s2:
                    def_menu = "data/menu_structure.txt"
                    if not u_sb_man and os.path.exists(def_menu):
                        st.success("‚úÖ –ú–µ–Ω—é —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (`menu_structure.txt`)")
                        with open(def_menu, "r", encoding="utf-8") as f: sidebar_content = f.read()
                    elif u_sb_man:
                        up_s = st.file_uploader("–§–∞–π–ª .txt", type=['txt'], key="up_sb_vert", label_visibility="collapsed")
                        if up_s: sidebar_content = up_s.getvalue().decode("utf-8")
                    else: st.error("‚ùå –§–∞–π–ª –º–µ–Ω—é –Ω–µ –Ω–∞–π–¥–µ–Ω!")

        # --- GEO BLOCK ---
        if use_geo:
            with st.container(border=True):
                st.markdown("#### üåç 6. –ì–µ–æ-–±–ª–æ–∫")
                kws_input_geo = st.text_area(
                    "–°–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤/—Ä–µ–≥–∏–æ–Ω–æ–≤ (–∏–∑ –≤–∫–ª–∞–¥–∫–∏ –ê–Ω–∞–ª–∏–∑) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=geo_context_default, 
                    height=100, 
                    key="kws_geo_auto"
                )
                global_geo_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_geo) if x.strip()]
                
                if not global_geo_list:
                    st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤ –ø—É—Å—Ç!")
                else:
                    st.info(f"–ë—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ç–µ–∫—Å—Ç –¥–æ—Å—Ç–∞–≤–∫–∏ –¥–ª—è –ø–æ–ª—è IP_PROP4819 —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º —ç—Ç–∏—Ö –≥–æ—Ä–æ–¥–æ–≤.")

    st.markdown("---")
    
# ==========================================
    # 4. –ó–ê–ü–£–°–ö –ì–ï–ù–ï–†–ê–¶–ò–ò (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê)
    # ==========================================
    
    ready_to_go = True
    if use_manual_html:
        if not manual_html_source: ready_to_go = False
    else:
        if not main_category_url: ready_to_go = False

    if (use_text or use_tables or use_geo) and not gemini_api_key: ready_to_go = False
    if use_promo and df_db_promo is None: ready_to_go = False

# ==========================================
    # üÜò –ë–õ–û–ö –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò (Gemini 2.0 Flash)
    # ==========================================
    st.markdown("---")
    with st.expander("üõ†Ô∏è –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê API (–ï—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∏)", expanded=True):
        if st.button("üì° –ü–†–û–í–ï–†–ò–¢–¨ GEMINI 2.0"):
            if not gemini_api_key:
                st.error("‚ùå –ö–ª—é—á API –Ω–µ –≤–≤–µ–¥–µ–Ω!")
            else:
                try:
                    from openai import OpenAI
                    client = OpenAI(api_key=gemini_api_key, base_url="https://litellm.tokengate.ru/v1")
                    response = client.chat.completions.create(
                        model="google/gemini-2.5-pro",
                        messages=[{"role": "user", "content": "Say OK"}]
                    )
                    st.success(f"‚úÖ –£–°–ü–ï–•! –û—Ç–≤–µ—Ç: {response.choices[0].message.content}")
                except Exception as e:
                    st.error(f"‚ùå –û–®–ò–ë–ö–ê: {str(e)}")
                    if "404" in str(e):
                        st.info("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω—É–∂–Ω–∞ –¥—Ä—É–≥–∞—è):")
                        try:
                            models = [m.name for m in genai.list_models()]
                            st.code("\n".join(models))
                        except: pass

    st.markdown("---")

# ==========================================
    # 4. –ó–ê–ü–£–°–ö –ì–ï–ù–ï–†–ê–¶–ò–ò (–ü–ê–ö–ï–¢–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê + LOGS)
    # ==========================================

    # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –æ–Ω–∞ –±—ã–ª–∞ —Ç–æ–ª—å–∫–æ –≤—ã—à–µ)
    if use_manual_html:
        if not manual_html_source: ready_to_go = False
    else:
        if not main_category_url: ready_to_go = False
    if (use_text or use_tables or use_geo) and not gemini_api_key: ready_to_go = False
    if use_promo and df_db_promo is None: ready_to_go = False

# ==========================================
    # 4. –£–ú–ù–´–ô –ó–ê–ü–£–°–ö (–°–ò–°–¢–ï–ú–ê STOP/RESUME + FULL GENERATION)
    # ==========================================
    st.markdown("### üöÄ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø—É—Å–∫–æ–º (–ê–≤—Ç–æ-—Ü–µ–ø–æ—á–∫–∞)")
    st.markdown("---")

    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π
    if 'auto_run_active' not in st.session_state: st.session_state.auto_run_active = False
    if 'auto_current_index' not in st.session_state: st.session_state.auto_current_index = 0
    if 'last_stopped_index' not in st.session_state: st.session_state.last_stopped_index = 0

    # 2. –ë–õ–û–ö –í–û–ó–û–ë–ù–û–í–õ–ï–ù–ò–Ø (–ü–æ—è–≤–ª—è–µ—Ç—Å—è, –µ—Å–ª–∏ –º—ã –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–ª–∏—Å—å)
    if not st.session_state.auto_run_active and st.session_state.last_stopped_index > 0:
        with st.container(border=True):
            st.warning(f"‚ö†Ô∏è **–ü—Ä–æ—Ü–µ—Å—Å –±—ã–ª –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.** –ü–æ—Å–ª–µ–¥–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å: {st.session_state.last_stopped_index}")
            
            col_res_btn, col_res_info = st.columns([1, 2])
            with col_res_btn:
                if st.button(f"‚èØÔ∏è –ü–†–û–î–û–õ–ñ–ò–¢–¨ —Å ‚Ññ {st.session_state.last_stopped_index}", type="primary", use_container_width=True):
                    st.session_state.auto_current_index = st.session_state.last_stopped_index
                    st.session_state.auto_run_active = True
                    st.rerun()
            with col_res_info:
                st.caption("–ù–∞–∂–º–∏—Ç–µ, —á—Ç–æ–±—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.")

# 3. –ù–ê–°–¢–†–û–ô–ö–ò –ó–ê–ü–£–°–ö–ê
    col_batch1, col_batch2, col_batch3 = st.columns([1, 1, 2])
    
    with col_batch1:
        # –ï—Å–ª–∏ –∑–∞–ø—É—â–µ–Ω–æ - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å (—Ä–∏–¥ –æ–Ω–ª–∏), –µ—Å–ª–∏ –Ω–µ—Ç - –ø–æ–ª–µ –≤–≤–æ–¥–∞
        if st.session_state.auto_run_active:
            st.text_input("üü¢ –í –ø—Ä–æ—Ü–µ—Å—Å–µ (–°—Ç–∞—Ä—Ç):", value=str(st.session_state.auto_current_index), disabled=True)
            start_index = st.session_state.auto_current_index
        else:
            # –ï—Å–ª–∏ —Ö–æ—Ç–∏–º –Ω–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ –∏–ª–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –º–µ—Å—Ç–∞ –≤—Ä—É—á–Ω—É—é
            start_index = st.number_input("–ù–∞—á–∞—Ç—å —Å —Ç–æ–≤–∞—Ä–∞ ‚Ññ (—Å 0)", min_value=0, value=st.session_state.last_stopped_index, step=1)

    with col_batch2:
        safe_batch_size = st.number_input("–†–∞–∑–º–µ—Ä –ø–∞—á–∫–∏ (—à—Ç)", min_value=1, value=5, help="–õ—É—á—à–µ 3-5 —à—Ç.")
        
    with col_batch3:
        st.write("")
        st.write("")
        enable_auto_chain = st.checkbox("üîÑ –ê–≤—Ç–æ-–ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–π –ø–∞—á–∫–µ", value=True, help="–ï—Å–ª–∏ –∞–∫—Ç–∏–≤–Ω–æ, —Å–∫—Ä–∏–ø—Ç –±—É–¥–µ—Ç —Å–∞–º –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ–π–¥–µ—Ç –≤—Å–µ —Ç–æ–≤–∞—Ä—ã.")

# --- –ö–ù–û–ü–ö–ê –°–ë–†–û–°–ê –ö–≠–®–ê ---
    st.markdown("---")
    col_clear, _ = st.columns([2, 3])
    with col_clear:
        if st.button("üóëÔ∏è –û–ß–ò–°–¢–ò–¢–¨ –ö–≠–® –ì–ï–ù–ï–†–ê–¶–ò–ò (–°–±—Ä–æ—Å —Ç–∞–±–ª–∏—Ü—ã)", type="secondary", use_container_width=True, help="–£–¥–∞–ª—è–µ—Ç –≤—Å–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ü–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ –Ω–∞ —Ç–µ –∂–µ —Ç–æ–≤–∞—Ä—ã –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–∞ –¥—É–±–ª–µ–π."):
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
            st.session_state.gen_result_df = pd.DataFrame(columns=[
                'Page URL', 'Product Name', 'IP_PROP4839', 'IP_PROP4817', 'IP_PROP4818', 
                'IP_PROP4819', 'IP_PROP4820', 'IP_PROP4821', 'IP_PROP4822', 'IP_PROP4823', 
                'IP_PROP4824', 'IP_PROP4816', 'IP_PROP4825', 'IP_PROP4826', 'IP_PROP4834', 
                'IP_PROP4835', 'IP_PROP4836', 'IP_PROP4837', 'IP_PROP4838', 'IP_PROP4829', 'IP_PROP4831'
            ])
            st.session_state.unified_excel_data = None
            st.session_state.auto_current_index = 0
            st.session_state.last_stopped_index = 0
            
            # === –í–ê–ñ–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï ===
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã–∫–ª—é—á–∞–µ–º —Ñ–ª–∞–≥ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª –∞–≤—Ç–æ-–∑–∞–ø—É—Å–∫
            st.session_state.auto_run_active = False 
            # ==========================
            
            st.toast("‚úÖ –ö—ç—à –æ—á–∏—â–µ–Ω! –ü—Ä–æ—Ü–µ—Å—Å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.", icon="üóëÔ∏è")
            time.sleep(1)
            st.rerun()

    st.markdown("---")
    c_start, c_stop = st.columns([2, 1])
    with c_start:
        # –ö–Ω–æ–ø–∫–∞ –°–¢–ê–†–¢ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º—ã –ù–ï —Ä–∞–±–æ—Ç–∞–µ–º
        if not st.session_state.auto_run_active:
            if st.button("üöÄ –ó–ê–ü–£–°–¢–ò–¢–¨ –ù–û–í–´–ô –ü–†–û–¶–ï–°–°", type="primary", disabled=(not ready_to_go), use_container_width=True):
                st.session_state.auto_current_index = start_index
                st.session_state.auto_run_active = True
                st.session_state.last_stopped_index = start_index # –°–±—Ä–æ—Å –ø–∞–º—è—Ç–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–∏ –Ω–æ–≤–æ–º —Å—Ç–∞—Ä—Ç–µ
                st.rerun()
        else:
            st.info("‚è≥ –ü—Ä–æ—Ü–µ—Å—Å –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è...")

    # =========================================================
    # –ì–õ–ê–í–ù–´–ô –ò–°–ü–û–õ–ù–Ø–Æ–©–ò–ô –ë–õ–û–ö
    # –ó–∞—Ö–æ–¥–∏–º —Å—é–¥–∞ –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –∞–∫—Ç–∏–≤–µ–Ω —Ñ–ª–∞–≥ auto_run_active
    # =========================================================

    if st.session_state.auto_run_active:
        
        # 0. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DataFrame –µ—Å–ª–∏ –Ω–µ—Ç
        if 'gen_result_df' not in st.session_state or st.session_state.gen_result_df is None:
             st.session_state.gen_result_df = pd.DataFrame(columns=[
                'Page URL', 'Product Name', 'IP_PROP4839', 'IP_PROP4817', 'IP_PROP4818', 
                'IP_PROP4819', 'IP_PROP4820', 'IP_PROP4821', 'IP_PROP4822', 'IP_PROP4823', 
                'IP_PROP4824', 'IP_PROP4816', 'IP_PROP4825', 'IP_PROP4826', 'IP_PROP4834', 
                'IP_PROP4835', 'IP_PROP4836', 'IP_PROP4837', 'IP_PROP4838', 'IP_PROP4829', 'IP_PROP4831'
            ])

        EXCEL_COLUMN_ORDER = st.session_state.gen_result_df.columns.tolist()
        TEXT_CONTAINERS = ['IP_PROP4839', 'IP_PROP4816', 'IP_PROP4838', 'IP_PROP4829', 'IP_PROP4831']

        # === 1. –ó–ê–ì–†–£–ó–ö–ê –ë–ê–ó (–ü–æ–≤—Ç–æ—Ä –ª–æ–≥–∏–∫–∏) ===
        all_tags_links = []
        if use_tags:
            if tags_file_content: 
                all_tags_links = [l.strip() for l in io.StringIO(tags_file_content).readlines() if l.strip()]
            elif os.path.exists("data/links_base.txt"):
                with open("data/links_base.txt", "r", encoding="utf-8") as f: 
                    all_tags_links = [l.strip() for l in f.readlines() if l.strip()]

        p_img_map = {}
        if use_promo and df_db_promo is not None:
            for _, row in df_db_promo.iterrows():
                u = str(row.iloc[0]).strip(); img = str(row.iloc[1]).strip()
                if u and u != 'nan' and img and img != 'nan': p_img_map[u.rstrip('/')] = img

        # === 2. –ü–û–î–ì–û–¢–û–í–ö–ê –°–ü–ò–°–ö–û–í –°–ï–ú–ê–ù–¢–ò–ö–ò ===
        raw_txt = st.session_state.get("ai_text_context_editable", "")
        list_text_initial = [x.strip() for x in re.split(r'[,\n]+', raw_txt) if x.strip()]
        
        raw_tags = st.session_state.get("kws_tags_auto", "")
        list_tags_initial = [x.strip() for x in re.split(r'[,\n]+', raw_tags) if x.strip()]
        
        raw_tables = st.session_state.get("table_context_editable", "")
        list_tables_final = [x.strip() for x in re.split(r'[,\n]+', raw_tables) if x.strip()] 
        str_tables_final = ", ".join(list_tables_final)

        raw_promo = st.session_state.get("kws_promo_auto", "")
        list_promo_initial = [x.strip() for x in re.split(r'[,\n]+', raw_promo) if x.strip()]

        raw_geo = st.session_state.get("kws_geo_auto", "")
        list_geo_final = [x.strip() for x in re.split(r'[,\n]+', raw_geo) if x.strip()]

        # –ü–æ–¥—Å—á–µ—Ç —Ü–µ–ª–µ–π SEO
        unique_seo_goals = set()
        if use_text: unique_seo_goals.update(list_text_initial)
        if use_tags: unique_seo_goals.update(list_tags_initial)
        if use_tables: unique_seo_goals.update(list_tables_final)
        if use_promo: unique_seo_goals.update(list_promo_initial)
        total_seo_goal = len(unique_seo_goals)

        # –ü–µ—Ä–µ–Ω–æ—Å —Å–ª–æ–≤
        final_tags_prepared = []
        final_text_seo_list = list(list_text_initial)
        
        if use_tags:
            for kw in list_tags_initial:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                matches = [u for u in all_tags_links if tr in u.lower()]
                if matches:
                    final_tags_prepared.append((kw, matches))
                else:
                    if kw not in final_text_seo_list: final_text_seo_list.append(kw)

        if use_promo and p_img_map:
            for kw in list_promo_initial:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                found_link = False
                for link in p_img_map.keys():
                    if tr in link.lower():
                        found_link = True; break
                if not found_link:
                    if kw not in final_text_seo_list: final_text_seo_list.append(kw)
        elif list_promo_initial: 
             for kw in list_promo_initial:
                 if kw not in final_text_seo_list: final_text_seo_list.append(kw)

        seo_keywords_string = ", ".join(final_text_seo_list)
        user_num_blocks = st.session_state.get("sb_num_blocks", 5)

        # –ü–õ–ï–ô–°–•–û–õ–î–ï–†–´ –î–õ–Ø –õ–û–ì–û–í
        live_download_placeholder = st.empty()
        live_table_placeholder = st.empty()
        log_container = st.status(f"üöÄ –í –†–ê–ë–û–¢–ï... –ü–∞—á–∫–∞ —Å {start_index}", expanded=True)

        # API CLIENT
        client = None
        if (use_text or use_tables or use_geo) and gemini_api_key:
            try:
                from openai import OpenAI
                client = OpenAI(api_key=gemini_api_key, base_url="https://litellm.tokengate.ru/v1")
            except Exception as e:
                log_container.error(f"–û—à–∏–±–∫–∞ API: {e}")
                st.session_state.auto_run_active = False
                st.stop()

        # Helper functions
        def resolve_real_names(urls_list, status_msg=""):
            if not urls_list: return {}
            results_map = {}
            if status_msg: log_container.write(status_msg)
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                future_to_url = {executor.submit(get_breadcrumb_only, u, st.session_state.settings_ua): u for u in urls_list}
                for future in concurrent.futures.as_completed(future_to_url):
                    url_key = future_to_url[future]
                    try:
                        extracted_name = future.result()
                        if extracted_name: results_map[url_key] = extracted_name
                    except: pass
            return results_map

# === –°–ë–û–† –°–¢–†–ê–ù–ò–¶ (–ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–ê–©–ò–¢–ê –û–¢ SSL –û–®–ò–ë–û–ö) ===
        log_container.write("üì• –°–±–æ—Ä —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü...")
        target_pages = []
        try:
            if use_manual_html:
                soup_main = BeautifulSoup(manual_html_source, 'html.parser')
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º curl_cffi –¥–ª—è –æ–±—Ö–æ–¥–∞ SSL –æ—à–∏–±–æ–∫
                try:
                    from curl_cffi import requests as cffi_requests
                    r = cffi_requests.get(
                        main_category_url, 
                        impersonate="chrome110", 
                        timeout=30,
                        headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'}
                    )
                    html_content = r.content
                except:
                    # Fallback
                    session = requests.Session()
                    r = session.get(main_category_url, timeout=30, verify=False)
                    html_content = r.text

                if r.status_code == 200: 
                    soup_main = BeautifulSoup(html_content, 'html.parser')
                else: 
                    log_container.error(f"–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {r.status_code}")
                    st.session_state.auto_run_active = False
                    st.stop()
            
            if soup_main:
                # –°–±–æ—Ä —Å—Å—ã–ª–æ–∫
                tags_container = soup_main.find(class_='popular-tags-inner')
                if tags_container:
                    for link in tags_container.find_all('a'):
                        href = link.get('href')
                        if href:
                            full_url = urljoin(main_category_url or "http://localhost", href)
                            target_pages.append({'url': full_url, 'name': link.get_text(strip=True)})
                
                # –ï—Å–ª–∏ —Ç–µ–≥–æ–≤ –Ω–µ—Ç, –∏—â–µ–º —Ö–æ—Ç—è –±—ã H1
                if not target_pages:
                    h1_found = soup_main.find('h1')
                    target_pages.append({'url': main_category_url or "local", 'name': h1_found.get_text(strip=True) if h1_found else "–¢–æ–≤–∞—Ä"})
                    
        except Exception as e:
            log_container.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–±–æ—Ä–∞: {e}")
            st.session_state.auto_run_active = False
            st.stop()

        total_found = len(target_pages)
        if start_index >= total_found:
             st.session_state.auto_run_active = False
             st.session_state.last_stopped_index = total_found
             st.success("üéâ –í—Å–µ —Ç–æ–≤–∞—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
             st.stop()

        end_index = min(start_index + safe_batch_size, total_found)
        target_pages_batch = target_pages[start_index:end_index]
        
        log_container.write(f"üìä –ü–ê–ß–ö–ê: {start_index+1} ‚Äî {end_index} –∏–∑ {total_found}")

# === –¶–ò–ö–õ –ü–û –ü–ê–ß–ö–ï (v9.0: –ò–ï–†–ê–†–•–ò–Ø –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–• + –õ–û–ì–ò–ß–ù–´–ï –ö–õ–Æ–ß–ò) ===
        for i, page in enumerate(target_pages_batch):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–µ–π
            current_urls_in_df = st.session_state.gen_result_df['Page URL'].values
            if page['url'] in current_urls_in_df:
                log_container.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –¥—É–±–ª—è: {page['name']}")
                continue 

            current_num = start_index + i + 1
            log_container.write(f"‚ñ∂Ô∏è **[{current_num}/{total_found}] {page['name']}**")
            
            # --- –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ---
            try:
                # 1. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π requests
                base_text_raw, _, real_header_h2, _ = get_page_data_for_gen(page['url'])
                header_for_ai = real_header_h2 if real_header_h2 else page['name']
                row_data = {col: "" for col in EXCEL_COLUMN_ORDER}
                row_data['Page URL'] = page['url']; row_data['Product Name'] = header_for_ai
                for k, v in STATIC_DATA_GEN.items():
                    if k in row_data: row_data[k] = v
                
                injections = []
                generated_full_text = "" 
                blocks = [""] * 5

                # =========================================================
                # –®–ê–ì 1. –ì–ï–ù–ï–†–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê
                # =========================================================
                if use_text and client:
                    log_container.write(f"   ‚Ü≥ ü§ñ –ü–∏—à–µ–º —Ç–µ–∫—Å—Ç...")
                    blocks_raw = generate_ai_content_blocks(
                        gemini_api_key, 
                        base_text_raw or "", 
                        page['name'], 
                        header_for_ai, 
                        user_num_blocks, 
                        final_text_seo_list
                    )
                    cleaned_blocks = [b.replace("```html", "").replace("```", "").strip() for b in blocks_raw]
                    for i_b in range(len(cleaned_blocks)):
                        if i_b < 5: blocks[i_b] = cleaned_blocks[i_b]
                    
                    generated_full_text = " ".join(blocks)

                # =========================================================
                # –®–ê–ì 2. –ì–ï–ù–ï–†–ê–¶–ò–Ø –¢–ê–ë–õ–ò–¶ (–°–¢–†–û–ì–ê–Ø –¢–ï–•–ù–ò–ß–ï–°–ö–ê–Ø –õ–û–ì–ò–ö–ê)
                # =========================================================
                if use_tables and client:
                    previous_tables_context = ""
                    keys_already_inserted = False 
                    
                    for t_topic in table_prompts:
                        context_snippet = generated_full_text[:3500] if generated_full_text else ""

                        # –õ–æ–≥–∏–∫–∞ –∫–ª—é—á–µ–π (1 —Ä–∞–∑, –Ω–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
                        if not keys_already_inserted and str_tables_final.strip():
                            curr_keys = str_tables_final
                            keys_instr = "–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –Ω–∞–π–¥–∏ –º–µ—Å—Ç–æ –¥–ª—è —ç—Ç–∏—Ö –∫–ª—é—á–µ–π. –ü–æ–¥–±–µ—Ä–∏ –¥–ª—è –Ω–∏—Ö –ª–æ–≥–∏—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ (–í–∏–¥, –¢–∏–ø, –ê–Ω–∞–ª–æ–≥–∏)."
                        else:
                            curr_keys = ""
                            keys_instr = ""

                        topic_guide = "–†–∞–∑–º–µ—Ä—ã, –¥–æ–ø—É—Å–∫–∏, –≤–µ—Å." if "–†–∞–∑–º–µ—Ä" in t_topic else ("–•–∏–º–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã." if "–•–∏–º" in t_topic else "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")

                        # === –ü–†–û–ú–¢ v9.0 (DATA QUALITY HIERARCHY) ===
                        prompt_tbl = f"""
    –¢–´ ‚Äî –°–¢–†–û–ì–ò–ô –¢–ï–•–ù–û–õ–û–ì. –ó–∞–¥–∞—á–∞: HTML-—Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è "{header_for_ai}".
    –¢–ï–ú–ê: {t_topic} ({topic_guide})
    
    –í–í–û–î–ù–´–ï:
    1. –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context_snippet} (–ò—â–∏ —Ñ–∞–∫—Ç—ã –∑–¥–µ—Å—å).
    2. –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–∏: [{curr_keys}] -> {keys_instr}
    3. –ò–≥–Ω–æ—Ä-–ª–∏—Å—Ç: {previous_tables_context}
    
    --- –ê–õ–ì–û–†–ò–¢–ú –ó–ê–ü–û–õ–ù–ï–ù–ò–Ø –Ø–ß–ï–ô–ö–ò (–ü–†–ò–û–†–ò–¢–ï–¢–´) ---
    1. üíé –ò–î–ï–ê–õ (–§–ê–ö–¢–´): –ü–∏—à–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, –¥–∏–∞–ø–∞–∑–æ–Ω—ã, –º–∞—Ä–∫–∏, –ì–û–°–¢—ã.
       - –ü—Ä–∏–º–µ—Ä: "HB 255", "–¥–æ 450 –ú–ü–∞", "–°—Ç3—Å–ø".
       
    2. üîß –ù–û–†–ú–ê (–¢–ï–†–ú–ò–ù–´): –ï—Å–ª–∏ —Ü–∏—Ñ—Ä –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ä–º–∏–Ω.
       - –í–º–µ—Å—Ç–æ "–•–æ—Ä–æ—à–∞—è —Å–≤–∞—Ä–∏–≤–∞–µ–º–æ—Å—Ç—å" -> –ø–∏—à–∏ "–ë–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π".
       - –í–º–µ—Å—Ç–æ "–¢–≤–µ—Ä–¥–∞—è —Å—Ç–∞–ª—å" -> –ø–∏—à–∏ "–í—ã—Å–æ–∫–æ–ø—Ä–æ—á–Ω–∞—è".
       
    3. ‚õî –ö–†–ê–ô–ù–ò–ô –°–õ–£–ß–ê–ô (–ü–†–û–ß–ï–†–ö): –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–æ–≤—Å–µ–º –Ω–µ—Ç.
       - –°—Ç–∞–≤—å "‚Äî".
       - –≠–¢–û –õ–£–ß–®–ï, —á–µ–º –ø–∏—Å–∞—Ç—å –≤–æ–¥—É ("–í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ", "–û—Ç–ª–∏—á–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞").
       
    --- –ü–†–ê–í–ò–õ–ê ---
    1. –ö–õ–Æ–ß–ò: –í—Å—Ç–∞–≤—å –∏—Ö –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ. –ï—Å–ª–∏ –∫–ª—é—á –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–Ω–∞–ø—Ä. –í–ì–ü –≤ –∫—Ä—É–≥–µ), —Å–æ–∑–¥–∞–π —Å—Ç—Ä–æ–∫—É "–°–º–µ–∂–Ω—ã–µ –≤–∏–¥—ã" –∏–ª–∏ "–¢–∞–∫–∂–µ –Ω–∞ —Å–∫–ª–∞–¥–µ".
    2. –û–§–û–†–ú–õ–ï–ù–ò–ï: 
       - –ú–∞—Ä–∫–∏/–ì–û–°–¢—ã ‚Äî –ó–ê–ì–õ–ê–í–ù–´–ú–ò.
       - –¢–æ–ª—å–∫–æ <table> —Å –∫–ª–∞—Å—Å–æ–º 'brand-accent-table' –∏ <thead>.
    """
                        try:
                            resp = client.chat.completions.create(model="google/gemini-2.5-pro", messages=[{"role": "user", "content": prompt_tbl}], temperature=0.25)
                            raw_table = resp.choices[0].message.content.replace("```html", "").replace("```", "").strip()
                            
                            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–∞ –ø–µ—Ä–µ–¥ —Ç–∞–±–ª–∏—Ü–µ–π
                            start_idx = raw_table.find("<table")
                            end_idx = raw_table.find("</table>")
                            
                            if start_idx != -1 and end_idx != -1:
                                clean_table_inner = raw_table[start_idx:end_idx+8]
                                if "brand-accent-table" not in clean_table_inner:
                                    clean_table_inner = clean_table_inner.replace("<table", "<table class='brand-accent-table'", 1)
                                
                                final_table_html = f'<div class="table-full-width-wrapper">{clean_table_inner}</div>'
                                injections.append(final_table_html)
                                
                                # –ó–∞–ø–∏—Å—å –≤ –ø–∞–º—è—Ç—å
                                content_stripped = re.sub(r'<[^>]+>', ' ', clean_table_inner)
                                previous_tables_context += f"\n[–¢–∞–±–ª–∏—Ü–∞ {t_topic}]: {content_stripped[:600]}..."
                                
                                # –ö–ª—é—á–∏ —Å—á–∏—Ç–∞–µ–º –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏
                                if curr_keys: keys_already_inserted = True

                        except Exception as e: 
                            log_container.write(f"–û—à–∏–±–∫–∞ —Ç–∞–±–ª–∏—Ü—ã: {e}")

                # =========================================================
                # –®–ê–ì 3. –û–°–¢–ê–õ–¨–ù–´–ï –ë–õ–û–ö–ò (–¢–ï–ì–ò, –ü–†–û–ú–û, –ì–ï–û)
                # =========================================================
                
                if use_tags and all_tags_links:
                    tags_cands_all = [u for u in all_tags_links if u.rstrip('/') != page['url'].rstrip('/')]
                    if tags_cands_all:
                        target_tag_urls = []
                        for kw in list_tags_initial:
                            tr_kw = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                            for url in tags_cands_all:
                                if tr_kw in url.lower() and url not in target_tag_urls:
                                    target_tag_urls.append(url); break 
                        needed_tags = 15
                        if len(target_tag_urls) < needed_tags:
                            pool_random = [u for u in tags_cands_all if u not in target_tag_urls]
                            if pool_random: target_tag_urls.extend(random.sample(pool_random, min(needed_tags - len(target_tag_urls), len(pool_random))))
                        if target_tag_urls:
                            tags_names_map = resolve_real_names(target_tag_urls)
                            html_t = []
                            for u in target_tag_urls:
                                name = tags_names_map.get(u, force_cyrillic_name_global(u.split("/")[-1]))
                                html_t.append(f'<a href="{u}" class="tag-item">{name}</a>')
                            injections.append(f'''<div class="popular-tags-text"><div class="popular-tags-inner-text"><div class="tag-items">{"\n".join(html_t)}</div></div></div>''')

# =========================================================
                # –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ë–õ–û–ö –ü–†–û–ú–û (–° –ß–ï–°–¢–ù–´–ú –†–ê–ù–î–û–ú–û–ú)
                # =========================================================
                if use_promo and p_img_map:
                    # 1. –ë–µ—Ä–µ–º –≤—Å–µ—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –∫—Ä–æ–º–µ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    p_cands_all = [u for u in p_img_map.keys() if u.rstrip('/') != page['url'].rstrip('/')]
                    
                    if p_cands_all:
                        target_urls = []
                        
                        # –®–ê–ì –ê: –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, —á—Ç–æ–±—ã –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–µ–º –º–µ–Ω—è–ª—Å—è
                        # (–ù–∞–ø—Ä–∏–º–µ—Ä, —á—Ç–æ–±—ã –Ω–µ –≤—Å–µ–≥–¥–∞ —Å–Ω–∞—á–∞–ª–∞ –∏—Å–∫–∞–ª–∞—Å—å "–¢—Ä—É–±–∞", –∞ –ø–æ—Ç–æ–º "–õ–∏—Å—Ç")
                        shuffled_keywords = list(list_promo_initial)
                        random.shuffle(shuffled_keywords)

                        # –®–ê–ì –ë: –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã —Å —Ä–∞–Ω–¥–æ–º–æ–º –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã
                        for kw in shuffled_keywords:
                            # –û–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—å, —á—Ç–æ–±—ã –Ω–µ –Ω–∞–±–∏—Ä–∞—Ç—å —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ, –µ—Å–ª–∏ –∫–ª—é—á–µ–π —Å–æ—Ç–Ω–∏
                            if len(target_urls) >= 10: break 

                            tr_kw = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                            
                            # –ù–∞—Ö–æ–¥–∏–º –í–°–ï –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å—Å—ã–ª–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —ç—Ç–æ—Ç –∫–ª—é—á (–∏ –∫–æ—Ç–æ—Ä—ã—Ö –µ—â–µ –Ω–µ—Ç –≤ —Å–ø–∏—Å–∫–µ)
                            all_matches_for_kw = [u for u in p_cands_all if tr_kw in u.lower() and u not in target_urls]
                            
                            if all_matches_for_kw:
                                # –í–ê–ñ–ù–û: –ë–µ—Ä–µ–º –°–õ–£–ß–ê–ô–ù–£–Æ —Å—Å—ã–ª–∫—É –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö, –∞ –Ω–µ –ø–µ—Ä–≤—É—é
                                target_urls.append(random.choice(all_matches_for_kw))
                        
                        # –®–ê–ì –í: –î–æ–±–∏–≤–∫–∞ –¥–æ –º–∏–Ω–∏–º—É–º–∞ (–æ–±—ã—á–Ω–æ 5 –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–π –≥–∞–ª–µ—Ä–µ–∏)
                        needed_total = 5
                        if len(target_urls) < needed_total:
                            pool_random = [u for u in p_cands_all if u not in target_urls]
                            if pool_random: 
                                count_to_add = min(needed_total - len(target_urls), len(pool_random))
                                target_urls.extend(random.sample(pool_random, count_to_add))
                        
                        # –®–ê–ì –ì: –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ –≤—ã–≤–æ–¥–æ–º, —á—Ç–æ–±—ã –ø–æ—Ä—è–¥–æ–∫ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –±—ã–ª —Ä–∞–∑–Ω—ã–º
                        random.shuffle(target_urls)

                        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∏–ª–µ–π)
                        if target_urls:
                            promo_names_map = resolve_real_names(target_urls)
                            gallery_items = []
                            for u in target_urls:
                                # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –ª–∏–±–æ –∏–∑ –ø–∞—Ä—Å–∏–Ω–≥–∞, –ª–∏–±–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑ URL
                                nm = promo_names_map.get(u, force_cyrillic_name_global(u.split("/")[-1]))
                                img_src = p_img_map[u]
                                gallery_items.append(f'''<div class="gallery-item"><h3><a href="{u}" target="_blank">{nm}</a></h3><figure><a href="{u}" target="_blank"><picture><img src="{img_src}" loading="lazy"></picture></a></figure></div>''')
                            
                            # –í—Å—Ç–∞–≤–ª—è–µ–º HTML
                            injections.append(f'''<style>.outer-full-width-section {{ padding: 25px 0; width: 100%; }}.gallery-content-wrapper {{ max-width: 1400px; margin: 0 auto; padding: 25px 15px; box-sizing: border-box; border-radius: 10px; overflow: hidden; background-color: #F6F7FC; }}h3.gallery-title {{ color: #3D4858; font-size: 1.8em; font-weight: normal; padding: 0; margin-top: 0; margin-bottom: 15px; text-align: left; }}.five-col-gallery {{ display: flex; justify-content: flex-start; align-items: flex-start; gap: 20px; margin-bottom: 0; padding: 0; list-style: none; flex-wrap: nowrap !important; overflow-x: auto !important; padding-bottom: 15px; }}.gallery-item {{ flex: 0 0 260px !important; box-sizing: border-box; text-align: center; scroll-snap-align: start; }}.gallery-item h3 {{ font-size: 1.1em; margin-bottom: 8px; font-weight: normal; text-align: center; line-height: 1.1em; display: block; min-height: 40px; }}.gallery-item h3 a {{ text-decoration: none; color: #333; display: block; height: 100%; display: flex; align-items: center; justify-content: center; transition: color 0.2s ease; }}.gallery-item h3 a:hover {{ color: #007bff; }}.gallery-item figure {{ width: 100%; margin: 0; float: none !important; height: 260px; overflow: hidden; margin-bottom: 5px; border-radius: 8px; }}.gallery-item figure a {{ display: block; height: 100%; text-decoration: none; }}.gallery-item img {{ width: 100%; height: 100%; display: block; margin: 0 auto; object-fit: cover; transition: transform 0.3s ease; border-radius: 8px; }}.gallery-item figure a:hover img {{ transform: scale(1.05); }}</style><div class="outer-full-width-section"><div class="gallery-content-wrapper"><h3 class="gallery-title">{promo_title}</h3><div class="five-col-gallery">{"".join(gallery_items)}</div></div></div>''')

                if use_geo and client:
                    log_container.write(f"   ‚Ü≥ üåç –ü–∏—à–µ–º –¥–æ—Å—Ç–∞–≤–∫—É...")
                    try:
                         cities = ", ".join(random.sample(list_geo_final, min(15, len(list_geo_final))))
                         prompt_geo = f"–ù–∞–ø–∏—à–∏ –æ–¥–∏–Ω HTML –ø–∞—Ä–∞–≥—Ä–∞—Ñ (<p>) –æ –¥–æ—Å—Ç–∞–≤–∫–µ —Ç–æ–≤–∞—Ä–∞ '{header_for_ai}' –≤ —Å–ª–µ–¥—É—é—â–∏–µ –≥–æ—Ä–æ–¥–∞: {cities}. –í–ø–∏—à–∏ –∫–ª—é—á–µ–≤–∏–∫–∏ {seo_keywords_string} (–≤—ã–¥–µ–ª–∏ <b>). –í—ã–¥–∞–π —Ç–æ–ª—å–∫–æ HTML."
                         resp = client.chat.completions.create(model="google/gemini-2.5-pro", messages=[{"role": "user", "content": prompt_geo}], temperature=0.5)
                         row_data['IP_PROP4819'] = resp.choices[0].message.content.replace("```html", "").replace("```", "").strip()
                    except: pass

                # =========================================================
                # –°–ë–û–†–ö–ê –ò –°–û–•–†–ê–ù–ï–ù–ò–ï
                # =========================================================
                effective_blocks_count = max(1, user_num_blocks)
                for i_inj, inj in enumerate(injections):
                    target_idx = i_inj % effective_blocks_count
                    blocks[target_idx] = blocks[target_idx] + "\n\n" + inj

                for i_c, c_name in enumerate(TEXT_CONTAINERS):
                    row_data[c_name] = blocks[i_c]

                new_row_df = pd.DataFrame([row_data])
                st.session_state.gen_result_df = pd.concat([st.session_state.gen_result_df, new_row_df], ignore_index=True)
                
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                    st.session_state.gen_result_df.to_excel(writer, index=False)
                st.session_state.unified_excel_data = buffer.getvalue()
                
                live_table_placeholder.dataframe(st.session_state.gen_result_df.tail(3), use_container_width=True)
                full_row_html = "".join([str(val) for val in row_data.values()])
                bolds_fact = full_row_html.count("<b>")
                with live_download_placeholder.container():
                     st.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {page['name']} (SEO-—Ç–µ–≥–æ–≤: {bolds_fact}/{total_seo_goal})")

            except Exception as e:
                log_container.error(f"–°–±–æ–π –Ω–∞ —Ç–æ–≤–∞—Ä–µ {page['name']}: {e}")

        log_container.update(label=f"‚úÖ –ü–∞—á–∫–∞ {start_index}-{end_index} –≥–æ—Ç–æ–≤–∞!", state="complete", expanded=False)
        
        # === –õ–û–ì–ò–ö–ê –ê–í–¢–û-–ü–ï–†–ï–ó–ê–ü–£–°–ö–ê (RELOAD) ===
        if enable_auto_chain:
            # –°–Ω–æ–≤–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞–∂–∞–ª –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –°–¢–û–ü –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–∞—á–∫–∏
            if st.session_state.auto_run_active:
                next_start = end_index
                if next_start < total_found:
                    st.session_state.auto_current_index = next_start
                    st.session_state.last_stopped_index = next_start # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è Resume
                    st.info(f"‚è≥ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ 1 —Å–µ–∫... –°–ª–µ–¥—É—é—â–∞—è –ø–∞—á–∫–∞ —Å {next_start}.")
                    time.sleep(1)
                    st.rerun() 
                else:
                    st.session_state.auto_run_active = False
                    st.session_state.last_stopped_index = total_found
                    st.balloons()
                    st.success("üèÅ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–û–õ–ù–û–°–¢–¨–Æ –ó–ê–í–ï–†–®–ï–ù–ê!")
            else:
                st.warning("‚õî –¶–µ–ø–æ—á–∫–∞ –±—ã–ª–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –≤—Ä—É—á–Ω—É—é.")

    # =========================================================
    # 5. –ë–õ–û–ö –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (–û–¢–û–ë–†–ê–ñ–ê–ï–¢–°–Ø –í–°–ï–ì–î–ê, –ï–°–õ–ò –ï–°–¢–¨ –î–ê–ù–ù–´–ï)
    # –≠—Ç–æ—Ç –∫–æ–¥ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç, –¥–∞–∂–µ –µ—Å–ª–∏ –≤—ã –Ω–∞–∂–∞–ª–∏ –°–¢–û–ü –∏ —Å–∫—Ä–∏–ø—Ç –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏–ª—Å—è
    # =========================================================

    has_data = (
        'gen_result_df' in st.session_state 
        and st.session_state.gen_result_df is not None 
        and not st.session_state.gen_result_df.empty
    )

    if has_data:
        st.markdown("---")
        st.success(f"üíæ **–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å:** –ì–æ—Ç–æ–≤–æ —Å—Ç—Ä–æ–∫: {len(st.session_state.gen_result_df)}")

        # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –±–∏–Ω–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ excel –ø–æ—Ç–µ—Ä—è–ª–∏—Å—å, –ø–µ—Ä–µ—Å–æ–∑–¥–∞–¥–∏–º –∏—Ö –∏–∑ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
        if st.session_state.get('unified_excel_data') is None:
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                st.session_state.gen_result_df.to_excel(writer, index=False)
            st.session_state.unified_excel_data = buffer.getvalue()

        col_dl_final, col_mon_final = st.columns([1, 1])

        with col_dl_final:
            st.download_button(
                label=f"üì• –°–ö–ê–ß–ê–¢–¨ –í–°–Å ({len(st.session_state.gen_result_df)} —à—Ç.)",
                data=st.session_state.unified_excel_data,
                file_name=f"wholesale_result_FULL_{int(time.time())}.xlsx",
                mime="application/vnd.ms-excel",
                key="btn_dl_persistent_v2",
                type="primary",
                use_container_width=True
            )

        with col_mon_final:
            if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å –≤ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", key="btn_add_mon_persistent", use_container_width=True):
                count_added = 0
                for idx, row in st.session_state.gen_result_df.iterrows():
                    u_val = str(row.get('Page URL', '')).strip()
                    kw_val = str(row.get('Product Name', '')).strip()
                    if u_val and kw_val and u_val != 'nan':
                        add_to_tracking(u_val, kw_val)
                        count_added += 1
                if count_added > 0:
                    st.toast(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {count_added} —Ç–æ–≤–∞—Ä–æ–≤!", icon="üìâ")

# === –ü–†–ï–î–ü–†–û–°–ú–û–¢–†–ê (–¢–û–ñ–ï –°–û–•–†–ê–ù–Ø–ï–¢–°–Ø) ===
        with st.expander("üëÄ –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Ç–æ–≥–æ, —á—Ç–æ —É–∂–µ –≥–æ—Ç–æ–≤–æ", expanded=False):
            # --- –í–°–¢–ê–í–õ–Ø–ï–ú –°–¢–ò–õ–ò CSS –î–õ–Ø –ö–†–ê–°–ò–í–´–• –¢–ê–ë–õ–ò–¶ ---
            st.markdown("""
            <style>
                /* –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞ */
                .preview-box {
                    border: 1px solid #e2e8f0;
                    background-color: #ffffff;
                    padding: 20px;
                    border-radius: 8px;
                    max-height: 600px;
                    overflow-y: auto;
                    box-shadow: inset 0 2px 4px 0 rgba(0, 0, 0, 0.06);
                }
                
                /* –í–ê–®–ò –°–¢–ò–õ–ò –î–õ–Ø –¢–ê–ë–õ–ò–¶ */
                .table-full-width-wrapper {
                    display: block !important;
                    width: 100% !important;
                    margin: 20px 0 !important;
                }
                .brand-accent-table {
                    width: 100% !important;
                    border-collapse: separate !important;
                    border-spacing: 0 !important;
                    background: white;
                    border-radius: 8px;
                    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
                    font-family: 'Inter', sans-serif;
                    border: 0 !important;
                }
                .brand-accent-table th {
                    background-color: #277EFF;
                    color: white;
                    text-align: left;
                    padding: 16px;
                    font-weight: 500;
                    font-size: 15px;
                    border: none;
                }
                .brand-accent-table th:first-child { border-top-left-radius: 8px; }
                .brand-accent-table th:last-child { border-top-right-radius: 8px; }
                .brand-accent-table td {
                    padding: 16px;
                    border-bottom: 1px solid #e5e7eb;
                    color: #4b5563;
                    font-size: 15px;
                    line-height: 1.4;
                }
                .brand-accent-table tr:last-child td { border-bottom: none; }
                .brand-accent-table tr:last-child td:first-child { border-bottom-left-radius: 8px; }
                .brand-accent-table tr:last-child td:last-child { border-bottom-right-radius: 8px; }
                .brand-accent-table tr:hover td { background-color: #f8faff; }
            </style>
            """, unsafe_allow_html=True)

            st.dataframe(st.session_state.gen_result_df, use_container_width=True)
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä –ø–æ –æ–¥–Ω–æ–º—É —Ç–æ–≤–∞—Ä—É
            df_p = st.session_state.gen_result_df
            if 'Product Name' in df_p.columns:
                all_products = df_p['Product Name'].tolist()
                # –í—ã–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                safe_index = len(all_products)-1 if len(all_products) > 0 else 0
                sel_p = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–æ–≤–∞—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–µ–∫—Å—Ç–∞:", all_products, index=safe_index, key="safe_preview_sel")
                
                if sel_p:
                    row_p = df_p[df_p['Product Name'] == sel_p].iloc[0]
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                    cols_to_show = ['IP_PROP4839', 'IP_PROP4816', 'IP_PROP4838', 'IP_PROP4829', 'IP_PROP4831', 'IP_PROP4819']
                    active_cols = [c for c in cols_to_show if str(row_p.get(c, "")).strip() != ""]
                    
                    if active_cols:
                        tabs = st.tabs([c.replace("IP_PROP", "") for c in active_cols])
                        for i, col in enumerate(active_cols):
                            with tabs[i]:
                                # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º HTML –≤–Ω—É—Ç—Ä–∏ —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                                st.markdown(f"<div class='preview-box'>{str(row_p[col])}</div>", unsafe_allow_html=True)
# ==========================================
# TAB 3: PROJECT MANAGER (SAVE/LOAD)
# ==========================================
with tab_projects:
    st.header("üìÅ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞–º–∏")
    st.markdown("–ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –≤ —Ñ–∞–π–ª –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–∞–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–µ–∫—Ç.")

    col_save, col_load = st.columns(2)

    # --- –§–£–ù–ö–¶–ò–Ø –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–Ø (CALLBACK) ---
    def restore_state_callback(data_to_restore):
        """
        –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –î–û –ø–µ—Ä–µ—Ä–∏—Å–æ–≤–∫–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.
        –ü–æ—ç—Ç–æ–º—É –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å session_state.
        """
        try:
            state_dict = data_to_restore["state"]
            restored_count = 0
            
            # 1. –û–±–Ω–æ–≤–ª—è–µ–º session_state
            for k, v in state_dict.items():
                st.session_state[k] = v
                restored_count += 1
            
            # 2. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–ª–∞–≥–∏
            st.session_state['analysis_done'] = True
            
            # 3. –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ (–ø–æ—è–≤–∏—Ç—Å—è –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏)
            st.toast(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {restored_count} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤!", icon="üéâ")
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –≤–Ω—É—Ç—Ä–∏ callback: {e}")

    # --- –ë–õ–û–ö –°–û–•–†–ê–ù–ï–ù–ò–Ø ---
    with col_save:
        with st.container(border=True):
            st.subheader("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–æ–µ–∫—Ç")
            
            if not st.session_state.get('analysis_done'):
                st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ–¥–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ (–í–∫–ª–∞–¥–∫–∞ SEO), —á—Ç–æ–±—ã –±—ã–ª–æ —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å.")
            else:
                st.info("–ë—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã, —Å–ø–∏—Å–∫–∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∏, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, —Å—Å—ã–ª–∫–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.")
                
                timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M")
                query_slug = transliterate_text(st.session_state.get('query_input', 'project'))[:20]
                default_filename = f"GAR_PRO_{query_slug}_{timestamp}.pkl"
                
                project_snapshot = {
                    "meta": {
                        "version": "2.6",
                        "date": str(datetime.datetime.now())
                    },
                    "state": {}
                }
                
                # –ö–ª—é—á–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                keys_to_save = [
                    'analysis_results', 'analysis_done', 'naming_table_df', 'ideal_h1_result',
                    'detected_anomalies', 'serp_trend_info', 'full_graph_data',
                    'categorized_products', 'categorized_services', 'categorized_commercial',
                    'categorized_dimensions', 'categorized_geo', 'categorized_general', 'categorized_sensitive',
                    'orig_products', 'orig_services', 'orig_commercial', 
                    'orig_dimensions', 'orig_geo', 'orig_general',
                    'sensitive_words_input_final', 'auto_tags_words', 'auto_promo_words',
                    'my_url_input', 'query_input', 'my_content_input', 'my_page_source_radio',
                    'competitor_source_radio', 'persistent_urls', 'excluded_urls_auto',
                    'settings_excludes', 'settings_stops', 'arsenkin_token', 'yandex_dict_key',
                    'settings_ua', 'settings_search_engine', 'settings_region', 'settings_top_n',
                    'settings_noindex', 'settings_alt', 'settings_numbers', 'settings_norm',
                    'gen_result_df', 'unified_excel_data'
                ]
                
                for k in keys_to_save:
                    if k in st.session_state:
                        project_snapshot["state"][k] = st.session_state[k]

                try:
                    pickle_data = pickle.dumps(project_snapshot)
                    st.download_button(
                        label="üì• –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª –ø—Ä–æ–µ–∫—Ç–∞ (.pkl)",
                        data=pickle_data,
                        file_name=default_filename,
                        mime="application/octet-stream",
                        type="primary",
                        use_container_width=True
                    )
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ø–∞–∫–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")

    # --- –ë–õ–û–ö –ó–ê–ì–†–£–ó–ö–ò ---
    with col_load:
        with st.container(border=True):
            st.subheader("üìÇ –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–µ–∫—Ç")
            
            uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª .pkl", type=["pkl"], key="project_loader")
            
            if uploaded_file is not None:
                try:
                    loaded_data = pickle.load(uploaded_file)
                    
                    if isinstance(loaded_data, dict) and "state" in loaded_data:
                        date_str = loaded_data['meta'].get('date', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        st.success(f"–ü—Ä–æ–µ–∫—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω! (–î–∞—Ç–∞: {date_str})")
                        
                        # –ò–°–ü–û–õ–¨–ó–£–ï–ú ON_CLICK –ò ARGS
                        # –≠—Ç–æ –≥–ª–∞–≤–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: —Ñ—É–Ω–∫—Ü–∏—è restore_state_callback –≤—ã–∑–æ–≤–µ—Ç—Å—è –î–û —Ç–æ–≥–æ,
                        # –∫–∞–∫ Streamlit –Ω–∞—á–Ω–µ—Ç –æ—Ç—Ä–∏—Å–æ–≤—ã–≤–∞—Ç—å –≤–∏–¥–∂–µ—Ç—ã –∑–∞–Ω–æ–≤–æ.
                        st.button(
                            "üöÄ –í–û–°–°–¢–ê–ù–û–í–ò–¢–¨ –°–û–°–¢–û–Ø–ù–ò–ï", 
                            type="primary", 
                            use_container_width=True,
                            on_click=restore_state_callback,
                            args=(loaded_data,)
                        )
                    else:
                        st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –ø—Ä–æ–µ–∫—Ç–∞.")
                except Exception as e:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")

# ==========================================
# –ú–û–ù–ò–¢–û–†–ò–ù–ì: –ß–ò–°–¢–ê–Ø –í–ï–†–°–ò–Ø (–ë–ï–ó –ú–£–°–û–†–ê)
# ==========================================
import os
import pandas as pd
import datetime
import time
import requests
import json
from urllib.parse import urlparse

TRACK_FILE = "monitoring.csv"

def add_to_tracking(url, keyword):
    if not os.path.exists(TRACK_FILE):
        with open(TRACK_FILE, "w", encoding="utf-8") as f:
            f.write("URL;Keyword;Date;Position\n")
    try:
        existing = pd.read_csv(TRACK_FILE, sep=";")
        if ((existing['URL'] == url) & (existing['Keyword'] == keyword)).any(): return
    except: pass
    with open(TRACK_FILE, "a", encoding="utf-8") as f:
        today = datetime.datetime.now().strftime("%Y-%m-%d")
        f.write(f"{url};{keyword};{today};0\n")

# –§—É–Ω–∫—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (—É–±–∏—Ä–∞–µ—Ç www –∏ http –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
def normalize_url(u):
    if not u: return ""
    u = str(u).lower().strip()
    u = u.replace("https://", "").replace("http://", "").replace("www.", "")
    if u.endswith("/"): u = u[:-1]
    return u

with tab_monitoring:
    st.header("üìâ –¢—Ä–µ–∫–µ—Ä –ø–æ–∑–∏—Ü–∏–π (DEBUG MODE)")

    # –í—ã–±–æ—Ä —Ä–µ–≥–∏–æ–Ω–∞
    default_reg_val = st.session_state.get('settings_region', '–ú–æ—Å–∫–≤–∞')
    try: def_index = list(REGION_MAP.keys()).index(default_reg_val)
    except: def_index = 0

    col_reg, col_btn, col_del = st.columns([2, 2, 1])
    
    with col_reg:
        selected_mon_region = st.selectbox("–†–µ–≥–∏–æ–Ω:", list(REGION_MAP.keys()), index=def_index, label_visibility="collapsed")

    # –§–æ—Ä–º–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
    with st.expander("‚ûï –î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –≤—Ä—É—á–Ω—É—é", expanded=False):
        with st.form("add_clean_manual"):
            col_u, col_k = st.columns(2)
            u_in = col_u.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã/—Å–∞–π—Ç–∞")
            k_in = col_k.text_input("–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ")
            if st.form_submit_button("–î–æ–±–∞–≤–∏—Ç—å –≤ —Å–ø–∏—Å–æ–∫"):
                if u_in and k_in:
                    add_to_tracking(u_in, k_in)
                    st.success(f"–î–æ–±–∞–≤–ª–µ–Ω–æ: {k_in}")
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –æ–±–∞ –ø–æ–ª—è")

    if not os.path.exists(TRACK_FILE):
        st.info("–°–ø–∏—Å–æ–∫ –ø—É—Å—Ç.")
    else:
        try: df_mon = pd.read_csv(TRACK_FILE, sep=";")
        except: df_mon = pd.DataFrame()

        if df_mon.empty:
            st.info("–§–∞–π–ª –±–∞–∑—ã –ø—É—Å—Ç.")
        else:
            with col_btn:
                if st.button("üöÄ –û–ë–ù–û–í–ò–¢–¨ –ü–û–ó–ò–¶–ò–ò", type="primary", use_container_width=True):
                    if not ARSENKIN_TOKEN:
                        st.error("‚ùå –û–®–ò–ë–ö–ê: –ù–µ—Ç —Ç–æ–∫–µ–Ω–∞!")
                    else:
                        status_container = st.status("üöÄ –ù–∞—á–∏–Ω–∞–µ–º...", expanded=True)
                        progress_bar = status_container.progress(0)
                        
                        reg_ids = REGION_MAP.get(selected_mon_region, {"ya": 213})
                        rid_int = int(reg_ids['ya'])
                        
                        total_rows = len(df_mon)

                        for i, row in df_mon.iterrows():
                            kw = str(row['Keyword']).strip()
                            target_url_raw = str(row['URL']).strip()
                            
                            # === 1. –í–´–î–ï–õ–Ø–ï–ú –ß–ò–°–¢–´–ô –î–û–ú–ï–ù –î–õ–Ø API ===
                            # API –≤ –ø–æ–ª–µ "url" —Ö–æ—á–µ—Ç "site.ru", –∞ –Ω–µ "site.ru/page"
                            parsed_url = urlparse(target_url_raw)
                            clean_domain = parsed_url.netloc.replace("www.", "")
                            if not clean_domain: clean_domain = target_url_raw.split('/')[0]

                            status_container.write(f"üì° –ó–∞–ø—Ä–æ—Å: **{kw}** (–î–æ–º–µ–Ω: {clean_domain})...")

                            payload = {
                                "tools_name": "positions",
                                "data": {
                                    "queries": [kw],
                                    "url": clean_domain, # <--- –û–¢–ü–†–ê–í–õ–Ø–ï–ú –¢–û–õ–¨–ö–û –î–û–ú–ï–ù
                                    "subdomain": True,
                                    "se": [{"type": 2, "region": rid_int}],
                                    "format": 0
                                }
                            }
                            
                            try:
                                # SET
                                r_set = requests.post("https://arsenkin.ru/api/tools/set", headers={"Authorization": f"Bearer {ARSENKIN_TOKEN}"}, json=payload, timeout=30)
                                if r_set.status_code != 200:
                                    st.error(f"HTTP Error: {r_set.status_code}")
                                    continue
                                
                                tid = r_set.json().get("task_id")
                                if not tid: 
                                    st.error(f"No Task ID: {r_set.json()}")
                                    continue

                                # CHECK
                                for _ in range(15):
                                    time.sleep(1.5)
                                    r_c = requests.post("https://arsenkin.ru/api/tools/check", headers={"Authorization": f"Bearer {ARSENKIN_TOKEN}"}, json={"task_id": tid})
                                    if r_c.json().get("status") == "finish": break
                                
                                # GET
                                r_get = requests.post("https://arsenkin.ru/api/tools/get", headers={"Authorization": f"Bearer {ARSENKIN_TOKEN}"}, json={"task_id": tid})
                                final_data = r_get.json()

                                # === üîç –û–¢–õ–ê–î–ö–ê: –í–´–í–û–î–ò–ú JSON –ù–ê –≠–ö–†–ê–ù ===
                                # –ï—Å–ª–∏ –∑–¥–µ—Å—å 0, –ø–æ—Å–º–æ—Ç—Ä–∏, —á—Ç–æ –≤–Ω—É—Ç—Ä–∏ JSON!
                                with status_container:
                                    st.write(f"üìù –û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è '{kw}':")
                                    st.json(final_data) 
                                
                                # –ü–ê–†–°–ò–ù–ì
                                res_data = final_data.get("result", [])
                                found_pos_val = 0
                                
                                if res_data and isinstance(res_data, list):
                                    item = res_data[0]
                                    
                                    # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –ø–æ –≤—Å–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–º –∫–ª—é—á–∞–º
                                    keys_to_check = ["position", "pos", str(rid_int)]
                                    
                                    for key in keys_to_check:
                                        val = item.get(key)
                                        if val is not None:
                                            # –ê—Ä—Å–µ–Ω–∫–∏–Ω –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å —á–∏—Å–ª–æ 11 –∏–ª–∏ —Å—Ç—Ä–æ–∫—É "11"
                                            if str(val).isdigit():
                                                found_pos_val = int(val)
                                                break
                                            # –ò–ª–∏ –≤–µ—Ä–Ω—É—Ç—å "-" –µ—Å–ª–∏ –Ω–µ –≤ —Ç–æ–ø–µ
                                            if str(val) in ["-", "0"]:
                                                found_pos_val = 0
                                                break
                                
                                df_mon.at[i, 'Position'] = found_pos_val
                                df_mon.at[i, 'Date'] = datetime.datetime.now().strftime("%Y-%m-%d")
                                df_mon.to_csv(TRACK_FILE, sep=";", index=False)
                                
                            except Exception as e:
                                st.error(f"Crash: {e}")
                            
                            progress_bar.progress((i + 1) / total_rows)

                        status_container.update(label="‚úÖ –ì–æ—Ç–æ–≤–æ!", state="complete", expanded=False)
                        st.rerun()

            # –¢–∞–±–ª–∏—Ü–∞
            def style_pos(v):
                try:
                    i = int(v)
                    if 0 < i <= 10: return 'color: #16a34a; font-weight: bold' 
                    if 10 < i <= 30: return 'color: #ca8a04' 
                    if i == 0: return 'color: #dc2626' 
                except: pass
                return ''

            st.dataframe(
                df_mon.style.map(style_pos, subset=['Position']),
                use_container_width=True,
                height=500,
                column_config={
                    "URL": st.column_config.LinkColumn("–°—Å—ã–ª–∫–∞"),
                    "Position": st.column_config.NumberColumn("–ü–æ–∑–∏—Ü–∏—è", format="%d"),
                    "Keyword": "–ö–ª—é—á",
                    "Date": "–î–∞—Ç–∞"
                }
            )
            
            with col_del:
                if st.button("üóëÔ∏è", help="–£–¥–∞–ª–∏—Ç—å –±–∞–∑—É"):
                    os.remove(TRACK_FILE); st.rerun()

# ==========================================
# TAB 5: BULK LSI GENERATOR (PRO - FINAL + FORMAT FIX)
# ==========================================
import requests
from bs4 import BeautifulSoup
import time
import pandas as pd
import io
import re
import streamlit as st

with tab_lsi_gen:
    st.header("üè≠ –ú–∞—Å—Å–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è B2B (Visual + Styles + Format Fix)")
    st.markdown("–ü—Ä–µ–¥–∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ LSI. –§–æ—Ä–º–∞—Ç: **–ß–∏—Å–ª–∞ 4-10 –º–º (–±–µ–∑ —Ç–∏—Ä–µ)**, **–¢–µ–∫—Å—Ç ‚Äì —á–µ—Ä–µ–∑ —Ç–∏—Ä–µ**, **–ü–ª–æ—Ç–Ω–æ—Å—Ç—å 2%**.")

    # --- 1. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø SESSION STATE ---
    if 'bg_tasks_queue' not in st.session_state:
        st.session_state.bg_tasks_queue = []
    if 'bg_results' not in st.session_state:
        st.session_state.bg_results = []
    if 'bg_batch_size' not in st.session_state:
        st.session_state.bg_batch_size = 2
    if 'bg_is_running' not in st.session_state:
        st.session_state.bg_is_running = False

    # --- 2. –§–£–ù–ö–¶–ò–ò ---
    def get_h2_from_url(url):
        try:
            from curl_cffi import requests as cffi_requests
            r = cffi_requests.get(
                url, impersonate="chrome110", 
                headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'},
                timeout=15
            )
            content = r.content; encoding = r.encoding if r.encoding else 'utf-8'
        except:
            try:
                import urllib3; urllib3.disable_warnings()
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
                r = requests.get(url, headers=headers, timeout=15, verify=False)
                content = r.content; encoding = r.apparent_encoding
            except Exception as e: return f"ERROR: Connect ({str(e)})"
        
        try:
            soup = BeautifulSoup(content, 'html.parser', from_encoding=encoding)
            desc_div = soup.find('div', class_='description-container')
            if desc_div and desc_div.find('h2'): return desc_div.find('h2').get_text(strip=True)
            if soup.find('h2'): return soup.find('h2').get_text(strip=True)
            if soup.find('h1'): return soup.find('h1').get_text(strip=True)
            return f"ERROR: H2 not found"
        except Exception as e: return f"ERROR: Parse ({str(e)})"

    def generate_full_article(api_key, exact_h2, lsi_list):
                if not api_key: return "Error: No API Key"
                try:
                    from openai import OpenAI
                    client = OpenAI(api_key=api_key, base_url="https://litellm.tokengate.ru/v1")
                except ImportError: return "Error: Library 'openai' not installed"
                
                lsi_string = ", ".join(lsi_list)
                
                stop_words_list = (
                    "—è–≤–ª—è–µ—Ç—Å—è, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π, –∫–ª—é—á–µ–≤–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç, —à–∏—Ä–æ–∫–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è, "
                    "–æ–±–ª–∞–¥–∞—é—Ç, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—Ç—Å—è, –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ, —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä, "
                    "–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π, —É–Ω–∏–∫–∞–ª—å–Ω—ã–π, –¥–∞–Ω–Ω—ã–π, —ç—Ç–æ—Ç, –∏–∑–¥–µ–ª–∏—è, –º–∞—Ç–µ—Ä–∏–∞–ª—ã, "
                    "–≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –¥–æ—Å—Ç—É–ø–Ω–∞—è —Ü–µ–Ω–∞, –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥, "
                    "–¥–æ—Å—Ç–∞–≤–∫–∞, –æ–ø–ª–∞—Ç–∞, —É—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏, –∑–≤–æ–Ω–∏—Ç–µ, –º–µ–Ω–µ–¥–∂–µ—Ä"
                )
        
                contact_html_block = (
                    '–ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é —Å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–º –ø–æ –Ω–æ–º–µ—Ä—É '
                    '<nobr><a href="tel:#PHONE#" onclick="ym(document.querySelector(\'#ya_counter\').getAttribute(\'data-counter\'),\'reachGoal\',\'tel\');gtag(\'event\', \'Click po nomeru telefona\', {{\'event_category\' : \'Click\', \'event_label\' : \'po nomeru telefona\'}});gtag(\'event\', \'Lead_Goal\', {{\'event_category\' : \'Click\', \'event_label\' : \'Leads Goal\'}});" class="a_404 ct_phone">#PHONE#</a></nobr>, '
                    '–ª–∏–±–æ –ø–∏—à–∏—Ç–µ –Ω–∞ –ø–æ—á—Ç—É <a href="mailto:#EMAIL#" onclick="ym(document.querySelector(\'#ya_counter\').getAttribute(\'data-counter\'),\'reachGoal\',\'email\');gtag(\'event\', \'Click napisat nam\', {{\'event_category\' : \'Click\', \'event_label\' : \'napisat nam\'}});gtag(\'event\', \'Lead_Goal\', {{\'event_category\' : \'Click\', \'event_label\' : \'Leads Goal\'}});" class="a_404">#EMAIL#</a>.'
                )
        
                system_instruction = (
                    "–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ä–µ–¥–∞–∫—Ç–æ—Ä. –¢–≤–æ–π —Å—Ç–∏–ª—å: —Ç–µ–ª–µ–≥—Ä–∞—Ñ–Ω—ã–π, —Å—É—Ö–æ–π, —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π. "
                    "–¢–´ –ù–ï–ù–ê–í–ò–î–ò–®–¨ –°–û–Æ–ó '–ò' –ø—Ä–∏ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–∏. –í 95% —Å–ª—É—á–∞–µ–≤ –∑–∞–º–µ–Ω—è–π '–∏' –Ω–∞ –∑–∞–ø—è—Ç—É—é. "
                    "–¢—ã —Å–æ–±–ª—é–¥–∞–µ—à—å HTML-—Å—Ç—Ä—É–∫—Ç—É—Ä—É."
                )
                
                user_prompt = f"""
                –ó–ê–î–ê–ß–ê: –ù–∞–ø–∏—à–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é —Å—Ç–∞—Ç—å—é –¥–ª—è —Ç–æ–≤–∞—Ä–∞: "{exact_h2}".
                
                [I] –ü–†–ê–í–ò–õ–ê –†–ê–ë–û–¢–´ –° –ö–õ–Æ–ß–û–ú ("{exact_h2}"):
                
                1. –ü–õ–û–¢–ù–û–°–¢–¨ (–î–û 5-6 –†–ê–ó –ù–ê –°–õ–û–í–û):
                   - –ö–∞–∂–¥–æ–µ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ –∏–∑ —Ñ—Ä–∞–∑—ã "{exact_h2}" –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ –î–û 5-6 –†–ê–ó.
                   - –í–ê–ñ–ù–û: –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–π –∏—Ö —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –ø–æ –≤—Å–µ–º—É —Ç–µ–∫—Å—Ç—É, –Ω–µ –ª–µ–ø–∏ –≤—Å—ë –≤ –æ–¥–∏–Ω –∞–±–∑–∞—Ü.
                
                2. –†–ê–ó–ë–ò–í–ö–ê –î–õ–ò–ù–ù–´–• –§–†–ê–ó (–ê–ù–¢–ò-–°–ü–ê–ú):
                   - –ï—Å–ª–∏ –∫–ª—é—á–µ–≤–∞—è —Ñ—Ä–∞–∑–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 3 –∏ –±–æ–ª–µ–µ —Å–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ü—Ä—É—Ç–æ–∫ –∏–∑ –º–∞–≥–Ω–∏—Ç–Ω–æ-—Ç–≤–µ—Ä–¥—ã—Ö —Å–ø–ª–∞–≤–æ–≤"), –ó–ê–ü–†–ï–©–ï–ù–û –ø–∏—Å–∞—Ç—å –µ—ë —Ü–µ–ª–∏–∫–æ–º –≤–Ω—É—Ç—Ä–∏ –∞–±–∑–∞—Ü–µ–≤.
                   - –¢—ã –æ–±—è–∑–∞–Ω —Ä–∞–∑–±–∏–≤–∞—Ç—å –µ—ë: –ø–∏—Å–∞—Ç—å "–ø—Ä—É—Ç–æ–∫" –æ—Ç–¥–µ–ª—å–Ω–æ, "—Å–ø–ª–∞–≤" –æ—Ç–¥–µ–ª—å–Ω–æ, "—Ç–≤–µ—Ä–¥—ã–π" –æ—Ç–¥–µ–ª—å–Ω–æ.
                   - –¶–µ–ª–∏–∫–æ–º —Ñ—Ä–∞–∑—É –º–æ–∂–Ω–æ –ø–∏—Å–∞—Ç—å –¢–û–õ–¨–ö–û –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö (H2, H3).
                   
                3. –ó–ê–ü–†–ï–¢ –ù–ê –°–û–Æ–ó "–ò":
                   - –ö–∞—Ç–µ–≥–æ—Ä–∏—á–µ—Å–∫–∏ –∏–∑–±–µ–≥–∞–π —Å–æ—é–∑–∞ "–∏" –ø—Ä–∏ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–∏ —Å–≤–æ–π—Å—Ç–≤. –ò—Å–ø–æ–ª—å–∑—É–π –∑–∞–ø—è—Ç—É—é.
                   - –ü–õ–û–•–û: "–ü—Ä–æ—á–Ω—ã–π, –ª–µ–≥–∫–∏–π –∏ –Ω–∞–¥–µ–∂–Ω—ã–π".
                   - –•–û–†–û–®–û: "–ü—Ä–æ—á–Ω—ã–π, –ª–µ–≥–∫–∏–π, –Ω–∞–¥–µ–∂–Ω—ã–π".
                   
                [II] –õ–û–ì–ò–ö–ê HTML (–°–¢–†–û–ì–û):
                
                1. –°–ü–ò–°–ö–ò:
                   - <ol>: –¢–û–õ–¨–ö–û –¥–ª—è –ø–æ—à–∞–≥–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.
                   - <ul>: –î–õ–Ø –í–°–ï–ì–û –û–°–¢–ê–õ–¨–ù–û–ì–û (—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, —Å—Ñ–µ—Ä—ã).
                   
                2. –¢–ê–ë–õ–ò–¶–ê:
                   - –ö–ª–∞—Å—Å: "brand-accent-table".
                   - –®–∞–ø–∫–∞ —á–µ—Ä–µ–∑ <thead> –∏ <th>.
        
                [III] –°–¢–†–£–ö–¢–£–†–ê –¢–ï–ö–°–¢–ê:
                
                1.1. –ó–∞–≥–æ–ª–æ–≤–æ–∫: <h2>{exact_h2}</h2>.
                
                1.2. –ë–≠–ù–ì–ï–†: 3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –°—É—Ç—å —Ç–æ–≤–∞—Ä–∞, –ì–û–°–¢, –º–∞—Ç–µ—Ä–∏–∞–ª. (–ö–ª—é—á —Ä–∞–∑–±–∏—Ç).
                
                1.3. –ê–±–∑–∞—Ü 1 + –ö–æ–Ω—Ç–∞–∫—Ç—ã: 
                {contact_html_block}
                
                1.4. –ü–æ–¥–≤–æ–¥–∫–∞ –∫ —Å–ø–∏—Å–∫—É 1 (:).
                
                1.5. –°–ø–∏—Å–æ–∫ ‚Ññ1 (6 –ø—É–Ω–∫—Ç–æ–≤): –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ü–ê–†–ê–ú–ï–¢–†–´.
                (–§–æ—Ä–º–∞—Ç: <ul>). –ë–µ–∑ —Å–æ—é–∑–∞ "–∏", –∏—Å–ø–æ–ª—å–∑—É–π –∑–∞–ø—è—Ç—ã–µ.
                   
                1.6. –ê–±–∑–∞—Ü 2. –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞.
                
                1.7. –¢–ê–ë–õ–ò–¶–ê –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö (–°–ü–†–ê–í–û–ß–ù–ê–Ø):
                4-5 —Å—Ç—Ä–æ–∫.
                –ö–û–î:
                <table class="brand-accent-table">
                    <thead>
                        <tr>
                            <th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th>
                            <th>–ó–Ω–∞—á–µ–Ω–∏–µ</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td>–ì–û–°–¢ / –¢–£</td><td>[–î–∞–Ω–Ω—ã–µ]</td></tr>
                        <tr><td>–ú–∞—Ä–∫–∞</td><td>[–î–∞–Ω–Ω—ã–µ]</td></tr>
                        <tr><td>[–ü–∞—Ä–∞–º–µ—Ç—Ä 3]</td><td>[–î–∞–Ω–Ω—ã–µ]</td></tr>
                        <tr><td>[–ü–∞—Ä–∞–º–µ—Ç—Ä 4]</td><td>[–î–∞–Ω–Ω—ã–µ]</td></tr>
                    </tbody>
                </table>
                
                1.8. –ü–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫ H3 (–®–ê–ë–õ–û–ù): 
                "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è {exact_h2} (–≤ —Ä–æ–¥. –ø–∞–¥–µ–∂–µ, —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã)"
                (–¢—É—Ç –∫–ª—é—á —Ü–µ–ª–∏–∫–æ–º).
                
                1.9. –ê–±–∑–∞—Ü 3. –í–∏–¥—ã, —Ç–∏–ø—ã. (–°—Ç—Ä–æ–≥–æ —Ä–∞–∑–±–∏–≤–∞–π –∫–ª—é—á –Ω–∞ —Å–ª–æ–≤–∞).
                
                1.10. –ü–æ–¥–≤–æ–¥–∫–∞ –∫ —Å–ø–∏—Å–∫—É 2 (:).
                
                1.11. –°–ø–∏—Å–æ–∫ ‚Ññ2 (6 –ø—É–Ω–∫—Ç–æ–≤): –°–§–ï–†–´ –ü–†–ò–ú–ï–ù–ï–ù–ò–Ø.
                (–§–æ—Ä–º–∞—Ç: <ul>).
                   
                1.12. –ê–±–∑–∞—Ü 4. –£—Å–ª–æ–≤–∏—è —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏.
                                      
                1.13. –ü–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫ H3 (–®–ê–ë–õ–û–ù):
                "–ú–æ–Ω—Ç–∞–∂ {exact_h2} (–≤ —Ä–æ–¥. –ø–∞–¥–µ–∂–µ)" –ò–õ–ò "–û–±—Ä–∞–±–æ—Ç–∫–∞ {exact_h2} (–≤ —Ä–æ–¥. –ø–∞–¥–µ–∂–µ)".
                
                1.14. –ê–±–∑–∞—Ü 5. –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è —Ä–∞–±–æ—Ç—ã.
                
                1.15. –ü–æ–¥–≤–æ–¥–∫–∞ –∫ —Å–ø–∏—Å–∫—É 3 (:).
                
                1.16. –°–ø–∏—Å–æ–∫ ‚Ññ3 (6 –ø—É–Ω–∫—Ç–æ–≤): –≠–ö–°–ü–õ–£–ê–¢–ê–¶–ò–û–ù–ù–´–ï –°–í–û–ô–°–¢–í–ê.
                (–ó–∞–ø—è—Ç—ã–µ –≤–º–µ—Å—Ç–æ "–∏"). –§–æ—Ä–º–∞—Ç: <ul>.
                   
                1.17. –ê–±–∑–∞—Ü 6. –†–µ–∑—é–º–µ –∏ –æ—Ç–≥—Ä—É–∑–∫–∞.
        
                [IV] –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û:
                1. LSI: {{{lsi_string}}} (–≤–ø–∏—Å–∞—Ç—å –æ—Ä–≥–∞–Ω–∏—á–Ω–æ).
                2. –°–¢–û–ü-–°–õ–û–í–ê: {stop_words_list}.
                3. –í–´–í–û–î: –¢–û–õ–¨–ö–û HTML –ö–û–î. –ë–µ–∑ markdown.
                """
                
                try:
                    response = client.chat.completions.create(
                        model="google/gemini-2.5-pro",
                        messages=[
                            {"role": "system", "content": system_instruction},
                            {"role": "user", "content": user_prompt}
                        ],
                        temperature=0.25
                    )
                    content = response.choices[0].message.content
                    content = re.sub(r'^```html', '', content.strip())
                    content = re.sub(r'^```', '', content.strip())
                    content = re.sub(r'```$', '', content.strip())
                    
                    # --- –°–ö–†–ò–ü–¢: –û–ß–ò–°–¢–ö–ê ---
                    content = content.replace(' - ', ' &ndash; ')
                    content = content.replace('‚Äî', '&ndash;')
                    content = content.replace('‚Äì', '&ndash;')
                    content = content.replace('&mdash;', '&ndash;')
                    content = content.replace('**', '').replace('__', '')
                    content = re.sub(r'<b\b[^>]*>', '', content, flags=re.IGNORECASE)
                    content = re.sub(r'</b>', '', content, flags=re.IGNORECASE)
                    content = re.sub(r'<strong\b[^>]*>', '', content, flags=re.IGNORECASE)
                    content = re.sub(r'</strong>', '', content, flags=re.IGNORECASE)
                    
                    return content
                except Exception as e:
                    return f"API Error: {str(e)}"

    # --- 3. UI: –ù–ê–°–¢–†–û–ô–ö–ò ---
    with st.expander("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ LSI", expanded=True):
        cached_key = st.session_state.get('gemini_key_cache', "")
        if not cached_key:
            try: cached_key = st.secrets["GEMINI_KEY"]
            except: pass
        
        default_lsi_text = "–≥–∞—Ä–∞–Ω—Ç–∏—è, –∑–≤–æ–Ω–∏—Ç–µ, –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è, –∫—É–ø–∏—Ç—å, –æ–ø–ª–∞—Ç–∞, –æ–ø—Ç–æ–º, –æ—Ç–≥—Ä—É–∑–∫–∞, –ø–æ–¥ –∑–∞–∫–∞–∑, –ø–æ—Å—Ç–∞–≤–∫–∞, –ø—Ä–∞–π—Å-–ª–∏—Å—Ç, –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º, —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å, —Ü–µ–Ω—ã"
        
        c1, c2 = st.columns([1, 2])
        with c1:
            lsi_api_key = st.text_input("Gemini API Key", value=cached_key, type="password", key="bulk_api_key_v16")
        with c2:
            raw_lsi_common = st.text_area("LSI (–æ–±—â–∏–π)", height=150, value=default_lsi_text)

    # --- 4. UI: –ó–ê–ì–†–£–ó–ö–ê ---
    st.subheader("1. –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–¥–∞—á")
    input_mode = st.radio("–†–µ–∂–∏–º:", ["–°–ø–∏—Å–æ–∫ URL", "–¢–µ–º—ã –≤—Ä—É—á–Ω—É—é"], horizontal=True)
    
    col_inp, col_act = st.columns([3, 1])
    with col_inp:
        raw_input_data = st.text_area("–°–ø–∏—Å–æ–∫ (—Å—Ç—Ä–æ–∫–∞ = –∑–∞–¥–∞—á–∞)", height=100)
    with col_act:
        st.write("¬†")
        if st.button("üì• –ó–∞–≥—Ä—É–∑–∏—Ç—å", use_container_width=True):
            lines = [l.strip() for l in raw_input_data.split('\n') if l.strip()]
            if lines:
                st.session_state.bg_tasks_queue = []
                st.session_state.bg_results = []
                st.session_state.bg_is_running = False
                t_type = 'url' if "URL" in input_mode else 'topic'
                for l in lines: st.session_state.bg_tasks_queue.append({'type': t_type, 'val': l})
                st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(lines)}")
                st.rerun()

    # --- 5. UI: –ü–†–û–¶–ï–°–° ---
    total_q = len(st.session_state.bg_tasks_queue)
    completed_q = len(st.session_state.bg_results)
    
    # --- –õ–û–ì–ò–ö–ê: –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –û–°–¢–ê–í–®–ò–•–°–Ø –ó–ê–î–ê–ß ---
    existing_sources = {r['source'] for r in st.session_state.bg_results if r['source'] != '-'}
    existing_h2s = {r['h2'] for r in st.session_state.bg_results}
    
    pending_indices = []
    for idx, task in enumerate(st.session_state.bg_tasks_queue):
        is_done = False
        if task['type'] == 'url' and task['val'] in existing_sources: is_done = True
        if task['type'] == 'topic' and task['val'] in existing_h2s: is_done = True
        
        if not is_done:
            pending_indices.append(idx)
            
    remaining_real_q = len(pending_indices)

    if total_q > 0:
        st.divider()
        st.subheader(f"2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (–ì–æ—Ç–æ–≤–æ: {completed_q} | –û—Å—Ç–∞–ª–æ—Å—å: {remaining_real_q})")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—á–∫–∏
        c_b1, c_b2 = st.columns([1, 3])
        with c_b1:
            st.session_state.bg_batch_size = st.number_input("–†–∞–∑–º–µ—Ä –ø–∞—á–∫–∏", 1, 20, st.session_state.bg_batch_size)
        with c_b2:
            auto_run_mode = st.checkbox("üîÑ –ê–≤—Ç–æ-–ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–π –ø–∞—á–∫–µ", value=True)
        
        # –ö–ù–û–ü–ö–ò –£–ü–†–ê–í–õ–ï–ù–ò–Ø
        c_act1, c_act2 = st.columns([2, 1])
        with c_act1:
            if not st.session_state.bg_is_running:
                label_btn = "‚ñ∂Ô∏è –ó–ê–ü–£–°–ö –¶–ò–ö–õ–ê"
                if remaining_real_q == 0: label_btn = "‚úÖ –í–°–ï –ó–ê–î–ê–ß–ò –í–´–ü–û–õ–ù–ï–ù–´"
                
                if st.button(label_btn, type="primary", disabled=(remaining_real_q == 0), use_container_width=True):
                    if not lsi_api_key:
                        st.error("–ù–µ—Ç API –∫–ª—é—á–∞!")
                    else:
                        st.session_state.bg_is_running = True
                        st.rerun()
            else:
                if st.button("‚õî –°–¢–û–ü (–ü–∞—É–∑–∞)", type="secondary", use_container_width=True):
                    st.session_state.bg_is_running = False
                    st.rerun()

        with c_act2:
            if st.button("üóëÔ∏è –°–±—Ä–æ—Å", use_container_width=True, disabled=st.session_state.bg_is_running):
                st.session_state.bg_tasks_queue = []
                st.session_state.bg_results = []
                st.session_state.bg_is_running = False
                st.rerun()

        st.write("üìä **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞:**")
        table_placeholder = st.empty()
        status_placeholder = st.empty()

        def render_live_table():
            if st.session_state.bg_results:
                disp_res = st.session_state.bg_results[::-1][:10]
                display_data = []
                for res in disp_res:
                    inp_val = res['source'] if res['source'] != '-' else res['h2']
                    content_preview = "..."
                    if res['status'] == 'OK':
                        clean_text = re.sub(r'<[^>]+>', '', res['content'])[:50] + "..."
                        content_preview = f"‚úÖ {clean_text}"
                    elif "Fail" in res['status']:
                        content_preview = f"‚ùå –û—à–∏–±–∫–∞: {res['content']}"
                    elif res['status'] == 'Skipped':
                        content_preview = "‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω (–î—É–±–ª—å)"
                    
                    display_data.append({
                        "–í—Ö–æ–¥": inp_val,
                        "H2 / –¢–µ–º–∞": res['h2'],
                        "–°—Ç–∞—Ç—É—Å": content_preview
                    })
                df_disp = pd.DataFrame(display_data)
                table_placeholder.dataframe(df_disp, use_container_width=True, hide_index=True)

        render_live_table()

        # --- –õ–û–ì–ò–ö–ê –ì–ï–ù–ï–†–ê–¶–ò–ò ---
        if st.session_state.bg_is_running and remaining_real_q > 0:
            lsi_arr = [x.strip() for x in raw_lsi_common.split(',') if x.strip()]
            
            current_batch_indices = pending_indices[:st.session_state.bg_batch_size]
            
            status_placeholder.info(f"üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(current_batch_indices)} —Å—Ç–∞—Ç–µ–π...")
            prog_bar = st.progress(0)
            
            for i, task_idx in enumerate(current_batch_indices):
                task = st.session_state.bg_tasks_queue[task_idx]
                val = task['val']; ttype = task['type']
                
                final_h2 = val; src_url = "-"
                
                if ttype == 'url':
                    h2_res = get_h2_from_url(val)
                    if h2_res.startswith("ERROR"):
                        st.session_state.bg_results.append({
                            "source": val, 
                            "h2": "ERROR", 
                            "content": h2_res, 
                            "status": "Parse Fail"
                        })
                        render_live_table()
                        continue
                    final_h2 = h2_res; src_url = val
                
                html_out = generate_full_article(lsi_api_key, final_h2, lsi_arr)
                
                st.session_state.bg_results.append({
                    "source": src_url,
                    "h2": final_h2,
                    "content": html_out,
                    "status": "OK" if not html_out.startswith("API Error") else "Gen Fail"
                })
                
                render_live_table()
                prog_bar.progress((i + 1) / len(current_batch_indices))
            
            if auto_run_mode:
                st.rerun()
            else:
                st.session_state.bg_is_running = False
                status_placeholder.success("‚úÖ –ü–∞—á–∫–∞ –≥–æ—Ç–æ–≤–∞!")
                st.rerun()
        
        elif st.session_state.bg_is_running and remaining_real_q == 0:
             st.session_state.bg_is_running = False
             st.success("–í—Å–µ –∑–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!")
             st.rerun()

    # --- 6. –ü–†–ï–í–¨–Æ –ò –≠–ö–°–ü–û–†–¢ ---
    if st.session_state.bg_results:
        st.divider()
        st.subheader("3. –ü—Ä–æ—Å–º–æ—Ç—Ä –∏ –≠–∫—Å–ø–æ—Ä—Ç")
        
        df = pd.DataFrame(st.session_state.bg_results)
        buf = io.BytesIO()
        with pd.ExcelWriter(buf, engine='xlsxwriter') as writer: df.to_excel(writer, index=False)
        st.download_button("üì• –°–∫–∞—á–∞—Ç—å Excel", data=buf.getvalue(), file_name="Gen_Result.xlsx", mime="application/vnd.ms-excel", type="primary")

        st.markdown("---")
        st.markdown("#### üëÅÔ∏è –í–∏–∑—É–∞–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä —Å—Ç–∞—Ç—å–∏")
        
        preview_options = [f"{i+1}. {r['h2']}" for i, r in enumerate(st.session_state.bg_results)]
        selected_option = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–∞—Ç—å—é –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞:", preview_options)
        
        table_css = """
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap');
            .brand-accent-table { display: table !important; width: 100% !important; border-collapse: separate !important; border-spacing: 0 !important; background: white; border-radius: 8px; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08); font-family: 'Inter', sans-serif; border: 0 !important; margin-top: 20px; margin-bottom: 20px; }
            .brand-accent-table th { background-color: #277EFF; color: white; text-align: left; padding: 16px; font-weight: 500; font-size: 15px; border: none; }
            .brand-accent-table th:first-child { border-top-left-radius: 8px; }
            .brand-accent-table th:last-child { border-top-right-radius: 8px; }
            .brand-accent-table td { padding: 16px; border-bottom: 1px solid #e5e7eb; color: #4b5563; font-size: 15px; line-height: 1.4; vertical-align: middle; word-wrap: break-word; }
            .brand-accent-table tr:last-child td { border-bottom: none; }
            .brand-accent-table tr:hover td { background-color: #f8faff; }
        </style>
        """
        
        if selected_option:
            idx = int(selected_option.split(".")[0]) - 1
            record = st.session_state.bg_results[idx]
            content_to_show = record['content']
            
            with st.container(border=True):
                if record['status'] == 'OK':
                    st.markdown(table_css + content_to_show, unsafe_allow_html=True)
                else:
                    st.error(f"–°—Ç–∞—Ç—É—Å: {record['status']}\n{content_to_show}")
            
            with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π HTML –∫–æ–¥"):
                st.code(content_to_show, language='html')



















