import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin, unquote
import inspect
import time
import json
import io
import os
import random
import streamlit.components.v1 as components
import copy
import plotly.graph_objects as go

# ==========================================
# FIX FOR PYTHON 3.11+
# ==========================================
if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except ImportError:
    morph = None
    USE_NLP = False

try:
    import openai
except ImportError:
    openai = None

# ==========================================
# 0. –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –ò –£–¢–ò–õ–ò–¢–´
# ==========================================

def transliterate_text(text):
    mapping = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }
    result = []
    for char in text.lower():
        if char in mapping:
            result.append(mapping[char])
        elif char.isalnum() or char == '-':
            result.append(char)
    return "".join(result)

def force_cyrillic_name_global(slug_text):
    raw = unquote(slug_text).lower()
    raw = raw.replace('.html', '').replace('.php', '')
    if re.search(r'[–∞-—è]', raw):
        return raw.replace('-', ' ').replace('_', ' ').capitalize()

    words = re.split(r'[-_]', raw)
    rus_words = []
    
    exact_map = {
        'nikel': '–Ω–∏–∫–µ–ª—å', 'stal': '—Å—Ç–∞–ª—å', 'med': '–º–µ–¥—å', 'latun': '–ª–∞—Ç—É–Ω—å',
        'bronza': '–±—Ä–æ–Ω–∑–∞', 'svinec': '—Å–≤–∏–Ω–µ—Ü', 'titan': '—Ç–∏—Ç–∞–Ω', 'tsink': '—Ü–∏–Ω–∫',
        'dural': '–¥—é—Ä–∞–ª—å', 'dyural': '–¥—é—Ä–∞–ª—å', 'chugun': '—á—É–≥—É–Ω',
        'alyuminiy': '–∞–ª—é–º–∏–Ω–∏–π', 'al': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è', 'alyuminievaya': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è',
        'nerzhaveyushchiy': '–Ω–µ—Ä–∂–∞–≤–µ—é—â–∏–π', 'nerzhaveyka': '–Ω–µ—Ä–∂–∞–≤–µ–π–∫–∞',
        'profil': '–ø—Ä–æ—Ñ–∏–ª—å', 'shveller': '—à–≤–µ–ª–ª–µ—Ä', 'ugolok': '—É–≥–æ–ª–æ–∫',
        'polosa': '–ø–æ–ª–æ—Å–∞', 'krug': '–∫—Ä—É–≥', 'kvadrat': '–∫–≤–∞–¥—Ä–∞—Ç',
        'list': '–ª–∏—Å—Ç', 'truba': '—Ç—Ä—É–±–∞', 'setka': '—Å–µ—Ç–∫–∞',
        'provoloka': '–ø—Ä–æ–≤–æ–ª–æ–∫–∞', 'armatura': '–∞—Ä–º–∞—Ç—É—Ä–∞', 'balka': '–±–∞–ª–∫–∞',
        'katanka': '–∫–∞—Ç–∞–Ω–∫–∞', 'otvod': '–æ—Ç–≤–æ–¥', 'perehod': '–ø–µ—Ä–µ—Ö–æ–¥',
        'flanec': '—Ñ–ª–∞–Ω–µ—Ü', 'zaglushka': '–∑–∞–≥–ª—É—à–∫–∞', 'metiz': '–º–µ—Ç–∏–∑—ã',
        'profnastil': '–ø—Ä–æ—Ñ–Ω–∞—Å—Ç–∏–ª', 'shtrips': '—à—Ç—Ä–∏–ø—Å', 'lenta': '–ª–µ–Ω—Ç–∞',
        'shina': '—à–∏–Ω–∞', 'prutok': '–ø—Ä—É—Ç–æ–∫', 'shestigrannik': '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫',
        'vtulka': '–≤—Ç—É–ª–∫–∞', 'kabel': '–∫–∞–±–µ–ª—å', 'panel': '–ø–∞–Ω–µ–ª—å',
        'detal': '–¥–µ—Ç–∞–ª—å', 'set': '—Å–µ—Ç—å', 'cep': '—Ü–µ–ø—å', 'svyaz': '—Å–≤—è–∑—å',
        'rezba': '—Ä–µ–∑—å–±–∞', 'gost': '–ì–û–°–¢',
        'polipropilenovye': '–ø–æ–ª–∏–ø—Ä–æ–ø–∏–ª–µ–Ω–æ–≤—ã–µ', 'truby': '—Ç—Ä—É–±—ã',
        'ocinkovannaya': '–æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω–∞—è', 'riflenyy': '—Ä–∏—Ñ–ª–µ–Ω—ã–π'
    }

    for w in words:
        if not w: continue
        if w in exact_map:
            rus_words.append(exact_map[w])
            continue
        
        processed_w = w
        if processed_w.endswith('yy'): processed_w = processed_w[:-2] + '—ã–π'
        elif processed_w.endswith('iy'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('ij'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('yi'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('aya'): processed_w = processed_w[:-3] + '–∞—è'
        elif processed_w.endswith('oye'): processed_w = processed_w[:-3] + '–æ–µ'
        elif processed_w.endswith('ye'): processed_w = processed_w[:-2] + '—ã–µ'

        replacements = [
            ('shch', '—â'), ('sch', '—â'), ('yo', '—ë'), ('zh', '–∂'), ('ch', '—á'), ('sh', '—à'), 
            ('yu', '—é'), ('ya', '—è'), ('kh', '—Ö'), ('ts', '—Ü'), ('ph', '—Ñ'),
            ('a', '–∞'), ('b', '–±'), ('v', '–≤'), ('g', '–≥'), ('d', '–¥'), ('e', '–µ'), 
            ('z', '–∑'), ('i', '–∏'), ('j', '–π'), ('k', '–∫'), ('l', '–ª'), ('m', '–º'), 
            ('n', '–Ω'), ('o', '–æ'), ('p', '–ø'), ('r', '—Ä'), ('s', '—Å'), ('t', '—Ç'), 
            ('u', '—É'), ('f', '—Ñ'), ('h', '—Ö'), ('c', '–∫'), ('w', '–≤'), ('y', '—ã'), ('x', '–∫—Å')
        ]
        
        temp_res = processed_w
        for eng, rus in replacements:
            temp_res = temp_res.replace(eng, rus)
        
        rus_words.append(temp_res)

    draft_phrase = " ".join(rus_words)
    draft_phrase = draft_phrase.replace('–ø—Ä–æ—Ñ–∏–ª', '–ø—Ä–æ—Ñ–∏–ª—å').replace('–ø—Ä–æ—Ñ–∏–ª—å–Ω', '–ø—Ä–æ—Ñ–∏–ª—å–Ω')
    draft_phrase = draft_phrase.replace('–µ–ª–Ω—ã–π', '–µ–ª—å–Ω—ã–π').replace('–∞–ª–Ω—ã–π', '–∞–ª—å–Ω—ã–π')
    draft_phrase = draft_phrase.replace('–µ–ª–Ω–∞—è', '–µ–ª—å–Ω–∞—è').replace('–∞–ª–Ω–∞—è', '–∞–ª—å–Ω–∞—è')
    draft_phrase = draft_phrase.replace('—Å—Ç–∞–ª–Ω', '—Å—Ç–∞–ª—å–Ω').replace('–º–µ–¥—å–Ω', '–º–µ–¥–Ω')
    draft_phrase = draft_phrase.replace('–π–∞', '—è').replace('–π–æ', '—ë')

    return draft_phrase.capitalize()

def get_breadcrumb_only(url, ua_settings="Mozilla/5.0"):
    try:
        session = requests.Session()
        retry = Retry(connect=3, read=3, redirect=3, backoff_factor=0.5)
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        
        headers = {'User-Agent': ua_settings}
        r = session.get(url, headers=headers, timeout=25)
        if r.status_code != 200: 
            return None
        
        soup = BeautifulSoup(r.text, 'html.parser')
        
        breadcrumbs = soup.find(class_=re.compile(r'breadcrumb|breadcrumbs|nav-path|nav-chain|bx-breadcrumb', re.I))
        if not breadcrumbs:
            breadcrumbs = soup.find(id=re.compile(r'breadcrumb|breadcrumbs|nav-path', re.I))

        if breadcrumbs:
            full_text = breadcrumbs.get_text(separator='|||', strip=True)
            parts = [p.strip() for p in full_text.split('|||') if p.strip()]
            clean_parts = [p for p in parts if p not in ['/', '\\', '>', '¬ª', '‚Ä¢', '-', '|']]
            
            if clean_parts:
                last_item = clean_parts[-1]
                if len(last_item) > 2 and last_item.lower() != "–≥–ª–∞–≤–Ω–∞—è":
                    return last_item
    except:
        return None
    return None

def render_clean_block(title, icon, words_list):
    unique_words = sorted(list(set(words_list))) if words_list else []
    count = len(unique_words)
    
    if count > 0:
        content_html = ", ".join(unique_words)
        html_code = f"""
        <details class="details-card">
            <summary class="card-summary">
                <div>
                    <span class="arrow-icon">‚ñ∂</span>
                    {icon} {title}
                </div>
                <span class="count-tag">{count}</span>
            </summary>
            <div class="card-content">
                {content_html}
            </div>
        </details>
        """
    else:
        html_code = f"""
        <div class="details-card">
            <div class="card-summary" style="cursor: default; color: #9ca3af;">
                <div>{icon} {title}</div>
                <span class="count-tag">0</span>
            </div>
        </div>
        """
    st.markdown(html_code, unsafe_allow_html=True)

def render_relevance_chart(df_rel, unique_key="default"):
    """
    –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø –ì–†–ê–§–ò–ö–ê.
    """
    if df_rel.empty:
        return

    # 1. –ñ–ï–°–¢–ö–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ > 0
    df = df_rel[df_rel['–ü–æ–∑–∏—Ü–∏—è'] > 0].copy()
    if df.empty: return

    df = df.sort_values(by='–ü–æ–∑–∏—Ü–∏—è')
    x_indices = np.arange(len(df))
    
    tick_links = []
    
    for _, row in df.iterrows():
        raw_name = row['–î–æ–º–µ–Ω'].replace(' (–í—ã)', '').strip()
        clean_domain = raw_name.replace('www.', '').split('/')[0]
        
        label_text = f"{row['–ü–æ–∑–∏—Ü–∏—è']}. {clean_domain}"
        if len(label_text) > 20: label_text = label_text[:18] + ".."
        
        url_target = row.get('URL', f"https://{raw_name}")
        
        link_html = f"<a href='{url_target}' target='_blank' class='chart-link'>{label_text}</a>"
        tick_links.append(link_html)

    # –ú–µ—Ç—Ä–∏–∫–∏
    df['Total_Rel'] = (df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] + df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)']) / 2
    
    # –¢—Ä–µ–Ω–¥
    z = np.polyfit(x_indices, df['Total_Rel'], 1)
    p = np.poly1d(z)
    df['Trend'] = p(x_indices)

    fig = go.Figure()

    COLOR_MAIN = '#4F46E5'  # –ò–Ω–¥–∏–≥–æ
    COLOR_WIDTH = '#0EA5E9' # –ì–æ–ª—É–±–æ–π
    COLOR_DEPTH = '#E11D48' # –ú–∞–ª–∏–Ω–æ–≤—ã–π
    COLOR_TREND = '#15803d' # –ó–µ–ª–µ–Ω—ã–π

    COMMON_CONFIG = dict(
        mode='lines+markers',
        line=dict(width=3, shape='spline'), 
        marker=dict(size=8, line=dict(width=2, color='white'), symbol='circle')
    )

    fig.add_trace(go.Scatter(x=x_indices, y=df['Total_Rel'], name='–û–±—â–∞—è', line=dict(color=COLOR_MAIN, **COMMON_CONFIG['line']), marker=dict(color=COLOR_MAIN, **COMMON_CONFIG['marker']), mode='lines+markers'))
    fig.add_trace(go.Scatter(x=x_indices, y=df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'], name='–®–∏—Ä–∏–Ω–∞', line=dict(color=COLOR_WIDTH, **COMMON_CONFIG['line']), marker=dict(color=COLOR_WIDTH, **COMMON_CONFIG['marker']), mode='lines+markers'))
    fig.add_trace(go.Scatter(x=x_indices, y=df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'], name='–ì–ª—É–±–∏–Ω–∞', line=dict(color=COLOR_DEPTH, **COMMON_CONFIG['line']), marker=dict(color=COLOR_DEPTH, **COMMON_CONFIG['marker']), mode='lines+markers'))
    fig.add_trace(go.Scatter(x=x_indices, y=df['Trend'], name='–¢—Ä–µ–Ω–¥', line=dict(color=COLOR_TREND, **COMMON_CONFIG['line']), marker=dict(color=COLOR_TREND, **COMMON_CONFIG['marker']), mode='lines+markers', opacity=0.8))

    fig.update_layout(
        template="plotly_white",
        legend=dict(
            orientation="h",
            yanchor="bottom", y=1.05,
            xanchor="center", x=0.5,
            font=dict(size=12, color="#111827", family="Inter, sans-serif")
        ),
        xaxis=dict(
            showgrid=False, 
            linecolor='#E5E7EB',
            tickmode='array',
            tickvals=x_indices,
            ticktext=tick_links, 
            tickfont=dict(size=12),
            fixedrange=True,
            range=[-0.5, len(df) - 0.5],
            automargin=True
        ),
        yaxis=dict(
            range=[0, 115], 
            showgrid=True, 
            gridcolor='#F3F4F6', 
            gridwidth=1,
            zeroline=False,
            fixedrange=True
        ),
        margin=dict(l=10, r=10, t=50, b=40),
        hovermode="x unified",
        height=280 # –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –≤—ã—Å–æ—Ç–∞
    )
    
    st.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False}, key=f"rel_chart_{unique_key}")

def render_paginated_table(df, title_text, key_prefix, default_sort_col=None, use_abs_sort_default=False):
    """
    –§—É–Ω–∫—Ü–∏—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü—ã. –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∞ –î–û –≤—ã–∑–æ–≤–∞.
    """
    if df.empty: st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö."); return
    col_t1, col_t2 = st.columns([7, 3])
    with col_t1: st.markdown(f"### {title_text}")
    if f'{key_prefix}_sort_col' not in st.session_state: st.session_state[f'{key_prefix}_sort_col'] = default_sort_col if (default_sort_col and default_sort_col in df.columns) else df.columns[0]
    if f'{key_prefix}_sort_order' not in st.session_state: st.session_state[f'{key_prefix}_sort_order'] = "–£–±—ã–≤–∞–Ω–∏–µ"

    search_query = st.text_input(f"üîç –ü–æ–∏—Å–∫ ({title_text})", key=f"{key_prefix}_search")
    if search_query:
        mask = df.astype(str).apply(lambda x: x.str.contains(search_query, case=False, na=False)).any(axis=1)
        df_filtered = df[mask].copy()
    else: df_filtered = df.copy()

    if df_filtered.empty: st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."); return

    with st.container():
        st.markdown("<div class='sort-container'>", unsafe_allow_html=True)
        col_s1, col_s2, col_sp = st.columns([2, 2, 4])
        with col_s1:
            current_sort = st.session_state[f'{key_prefix}_sort_col']
            if current_sort not in df_filtered.columns: current_sort = df_filtered.columns[0]
            sort_col = st.selectbox("üóÇ –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ:", df_filtered.columns, key=f"{key_prefix}_sort_box", index=list(df_filtered.columns).index(current_sort))
            st.session_state[f'{key_prefix}_sort_col'] = sort_col
        with col_s2:
            sort_order = st.radio("–ü–æ—Ä—è–¥–æ–∫:", ["–£–±—ã–≤–∞–Ω–∏–µ", "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ"], horizontal=True, key=f"{key_prefix}_order_box", index=0 if st.session_state[f'{key_prefix}_sort_order'] == "–£–±—ã–≤–∞–Ω–∏–µ" else 1)
            st.session_state[f'{key_prefix}_sort_order'] = sort_order
        st.markdown("</div>", unsafe_allow_html=True)

    ascending = (sort_order == "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ")
    if use_abs_sort_default and sort_col == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è" and "sort_val" in df_filtered.columns: df_filtered = df_filtered.sort_values(by="sort_val", ascending=ascending)
    elif ("–î–æ–±–∞–≤–∏—Ç—å" in sort_col or "+/-" in sort_col) and df_filtered[sort_col].dtype == object:
        try:
            df_filtered['_temp_sort'] = df_filtered[sort_col].astype(str).str.replace(r'[^\d]', '', regex=True)
            df_filtered['_temp_sort'] = pd.to_numeric(df_filtered['_temp_sort'], errors='coerce').fillna(0)
            df_filtered = df_filtered.sort_values(by='_temp_sort', ascending=ascending).drop(columns=['_temp_sort'])
        except: df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)
    else: df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)

    df_filtered = df_filtered.reset_index(drop=True); df_filtered.index = df_filtered.index + 1
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        export_df = df_filtered.copy()
        if "is_missing" in export_df.columns: del export_df["is_missing"]
        if "sort_val" in export_df.columns: del export_df["sort_val"]
        export_df.to_excel(writer, index=False, sheet_name='Data')
    excel_data = buffer.getvalue()
    with col_t2: st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel", data=excel_data, file_name=f"{key_prefix}_export.xlsx", mime="application/vnd.ms-excel", key=f"{key_prefix}_down")

    ROWS_PER_PAGE = 20
    if f'{key_prefix}_page' not in st.session_state: st.session_state[f'{key_prefix}_page'] = 1
    total_rows = len(df_filtered); total_pages = math.ceil(total_rows / ROWS_PER_PAGE)
    if total_pages == 0: total_pages = 1
    current_page = st.session_state[f'{key_prefix}_page']
    if current_page > total_pages: current_page = total_pages
    if current_page < 1: current_page = 1
    st.session_state[f'{key_prefix}_page'] = current_page
    start_idx = (current_page - 1) * ROWS_PER_PAGE
    end_idx = start_idx + ROWS_PER_PAGE
    df_view = df_filtered.iloc[start_idx:end_idx]

    def highlight_rows(row):
        base_style = 'background-color: #FFFFFF; color: #3D4858; border-bottom: 1px solid #DBEAFE;'
        styles = []
        status = row.get("–°—Ç–∞—Ç—É—Å", "")
        for col_name in row.index:
            cell_style = base_style
            if col_name == "–°—Ç–∞—Ç—É—Å":
                if status == "–ù–µ–¥–æ—Å–ø–∞–º": cell_style += "color: #D32F2F; font-weight: bold;"
                elif status == "–ü–µ—Ä–µ—Å–ø–∞–º": cell_style += "color: #E65100; font-weight: bold;"
                elif status == "–ù–æ—Ä–º–∞": cell_style += "color: #2E7D32; font-weight: bold;"
            styles.append(cell_style)
        return styles

    cols_to_hide = [c for c in ["is_missing", "sort_val"] if c in df_view.columns]
    try: styled_df = df_view.style.apply(highlight_rows, axis=1)
    except: styled_df = df_view
    st.dataframe(styled_df, use_container_width=True, height=(len(df_view) * 35) + 40, column_config={c: None for c in cols_to_hide})
    c_spacer, c_btn_prev, c_info, c_btn_next = st.columns([6, 1, 1, 1])
    with c_btn_prev:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_prev", disabled=(current_page <= 1), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] -= 1
            st.rerun()
    with c_info: st.markdown(f"<div style='text-align: center; margin-top: 10px;'><b>{current_page}</b> / {total_pages}</div>", unsafe_allow_html=True)
    with c_btn_next:
        if st.button("‚û°Ô∏è", key=f"{key_prefix}_next", disabled=(current_page >= total_pages), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] += 1
            st.rerun()
    st.markdown("---")

def analyze_serp_anomalies(df_rel):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–í–µ—Ä—Å–∏—è v5 - Robust).
    –ü–æ—Ä–æ–≥: 75% –æ—Ç –ª–∏–¥–µ—Ä–∞. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∏–ø–∏–∑–∞—Ü–∏—è.
    """
    if df_rel.empty:
        return [], [], {"type": "none", "msg": ""}

    # –ò—Å–∫–ª—é—á–∞–µ–º "–í–∞—à —Å–∞–π—Ç" –∏–∑ —Ä–∞—Å—á–µ—Ç–æ–≤ —ç—Ç–∞–ª–æ–Ω–∞
    df = df_rel[~df_rel['–î–æ–º–µ–Ω'].str.contains("\(–í—ã\)", na=False)].copy()
    
    if df.empty:
        return [], [], {"type": "none", "msg": ""}

    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –¥–µ–ª–∞–µ–º —á–∏—Å–ª–∞–º–∏ (–∑–∞—â–∏—Ç–∞ –æ—Ç —Å–±–æ–µ–≤)
    df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] = pd.to_numeric(df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'], errors='coerce').fillna(0)
    df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'] = pd.to_numeric(df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'], errors='coerce').fillna(0)

    # –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª
    df['Total'] = (df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] + df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)']) / 2
    
    # 1. –ò–©–ï–ú –õ–ò–î–ï–†–ê
    max_score = df['Total'].max()
    if max_score < 1: max_score = 1 # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
    
    # 2. –ñ–ï–°–¢–ö–ò–ô –ü–û–†–û–ì: 75% –æ—Ç –ª–∏–¥–µ—Ä–∞.
    # –ï—Å–ª–∏ –õ–∏–¥–µ—Ä=100, –ø–æ—Ä–æ–≥=75. –í—Å–µ —á—Ç–æ < 75 - —É–¥–∞–ª—è–µ–º.
    threshold = max(max_score * 0.75, 40) 
    
    anomalies = []
    normal_urls = []
    
    debug_counts = 0
    
    for _, row in df.iterrows():
        # –î–æ—Å—Ç–∞–µ–º —Å—Å—ã–ª–∫—É. –ó–∞—â–∏—Ç–∞ –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤.
        current_url = str(row.get('URL', '')).strip()
        if not current_url or current_url.lower() == 'nan':
             current_url = f"https://{row['–î–æ–º–µ–Ω']}" 

        score = row['Total']
        
        # –ê–ù–ê–õ–ò–ó
        if score < threshold:
            reason = f"–°–∫–æ—Ä {int(score)} < {int(threshold)} (–õ–∏–¥–µ—Ä {int(max_score)})"
            anomalies.append({'url': current_url, 'reason': reason, 'score': score})
            debug_counts += 1
        else:
            normal_urls.append(current_url)

    # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—è–º–∏
    if anomalies:
        st.toast(f"üóëÔ∏è –§–∏–ª—å—Ç—Ä (–õ–∏–¥–µ—Ä {int(max_score)} / –ü–æ—Ä–æ–≥ {int(threshold)}). –ò—Å–∫–ª—é—á–µ–Ω–æ: {len(anomalies)}", icon="‚ö†Ô∏è")
    else:
        # –ï—Å–ª–∏ –Ω–∏–∫–æ–≥–æ –Ω–µ –∏—Å–∫–ª—é—á–∏–ª–∏, –ø–∏—à–µ–º –ø–æ—á–µ–º—É
        st.toast(f"‚úÖ –í—Å–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –æ–∫. (–õ–∏–¥–µ—Ä {int(max_score)} / –ü–æ—Ä–æ–≥ {int(threshold)}). –ú–∏–Ω. –±–∞–ª–ª: {int(df['Total'].min())}", icon="‚ÑπÔ∏è")
    
    # –¢—Ä–µ–Ω–¥
    x = np.arange(len(df)); y = df['Total'].values
    slope = np.polyfit(x, y, 1)[0] if len(x) > 1 else 0
    trend_msg = "üìâ –ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–æ–ø" if slope < -1 else ("üìà –ü–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç—ã–π —Ç–æ–ø" if slope > 1 else "‚û°Ô∏è –†–æ–≤–Ω—ã–π —Ç–æ–ø")

    return normal_urls, anomalies, {"type": "info", "msg": trend_msg}

# ==========================================
# –ó–ê–ì–†–£–ó–ö–ê –°–õ–û–í–ê–†–ï–ô
# ==========================================
@st.cache_data
def load_lemmatized_dictionaries():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_path = os.path.join(script_dir, "data")
    
    # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞
    sets = {
        "products": set(),
        "commercial": set(),
        "specs": set(),
        "geo": set(),
        "services": set(),
        "sensitive": set()
    }

    # –ö–∞—Ä—Ç–∞ —Ñ–∞–π–ª–æ–≤
    files_map = {
        "metal_products.json": "products",
        "commercial_triggers.json": "commercial",
        "geo_locations.json": "geo",
        "services_triggers.json": "services",
        "tech_specs.json": "specs",
        "SENSITIVE_STOPLIST.json": "sensitive"
    }

    for filename, set_key in files_map.items():
        full_path = os.path.join(base_path, filename)
        if not os.path.exists(full_path):
            continue
        
        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                data = json.load(f) 
                
                words_bucket = []
                if isinstance(data, dict):
                    for cat_list in data.values():
                        words_bucket.extend(cat_list)
                elif isinstance(data, list):
                    words_bucket = data
                
                for phrase in words_bucket:
                    w_clean = str(phrase).lower().strip().replace('—ë', '–µ')
                    if not w_clean: continue
                    sets[set_key].add(w_clean)
                    if morph:
                        normal_form = morph.parse(w_clean)[0].normal_form.replace('—ë', '–µ')
                        sets[set_key].add(normal_form)
                    if ' ' in w_clean:
                        parts = w_clean.split()
                        for p in parts:
                            sets[set_key].add(p)
                            if morph: 
                                sets[set_key].add(morph.parse(p)[0].normal_form.replace('—ë', '–µ'))
        except: pass

    return sets["products"], sets["commercial"], sets["specs"], sets["geo"], sets["services"], sets["sensitive"]

# ==========================================
# –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† (–£–°–ò–õ–ï–ù–ù–´–ô)
# ==========================================
def classify_semantics_with_api(words_list, yandex_key):
    # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º 6 —Å–ª–æ–≤–∞—Ä–µ–π
    PRODUCTS_SET, COMM_SET, SPECS_SET, GEO_SET, SERVICES_SET, SENS_SET = load_lemmatized_dictionaries()
    FULL_SENSITIVE = SENS_SET.union(SENSITIVE_STOPLIST)

    if 'debug_geo_count' not in st.session_state:
        st.session_state.debug_geo_count = len(GEO_SET)
    
    st.sidebar.info(f"–°–ª–æ–≤–∞—Ä–∏ (–∏–∑ —Ñ–∞–π–ª–æ–≤):\nüì¶ –¢–æ–≤–∞—Ä—ã: {len(PRODUCTS_SET)}\nüí∞ –ö–æ–º–º–µ—Ä—Ü–∏—è: {len(COMM_SET)}\nüõ†Ô∏è –£—Å–ª—É–≥–∏: {len(SERVICES_SET)}\nüåç –ì–æ—Ä–æ–¥–∞: {len(GEO_SET)}")

    dim_pattern = re.compile(r'\d+(?:[\.\,]\d+)?\s?[—Öx\*√ó]\s?\d+', re.IGNORECASE)
    grade_pattern = re.compile(r'^([–∞-—èa-z]{1,4}\-?\d+[–∞-—èa-z0-9]*)$', re.IGNORECASE)
    
    categories = {'products': set(), 'services': set(), 'commercial': set(), 
                  'dimensions': set(), 'geo': set(), 'general': set(), 'sensitive': set()}
    
    for word in words_list:
        word_lower = word.lower()
        is_sensitive = False
        if word_lower in FULL_SENSITIVE: is_sensitive = True
        else:
            for stop_w in FULL_SENSITIVE:
                if len(stop_w) > 3 and stop_w in word_lower: is_sensitive = True; break
        if is_sensitive: categories['sensitive'].add(word_lower); continue
        
        lemma = word_lower
        if morph:
            p = morph.parse(word_lower)[0]
            lemma = p.normal_form

        if word_lower in SPECS_SET or lemma in SPECS_SET: categories['dimensions'].add(word_lower); continue
        if dim_pattern.search(word_lower) or grade_pattern.match(word_lower) or word_lower.isdigit(): categories['dimensions'].add(word_lower); continue

        if word_lower in PRODUCTS_SET or lemma in PRODUCTS_SET: categories['products'].add(word_lower); continue
        is_product_root = False
        for prod in PRODUCTS_SET:
            check_root = prod[:-1] if len(prod) > 4 else prod
            if len(check_root) > 3 and check_root in word_lower:
                categories['products'].add(word_lower); is_product_root = True; break
        if is_product_root: continue

        if lemma in GEO_SET or word_lower in GEO_SET: categories['geo'].add(word_lower); continue
        if lemma in SERVICES_SET or word_lower in SERVICES_SET: categories['services'].add(word_lower); continue
        if lemma.endswith('–æ–±—Ä–∞–±–æ—Ç–∫–∞') or lemma.endswith('–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ') or lemma == "—Ä–µ–∑–∫–∞": categories['services'].add(word_lower); continue
        if lemma in COMM_SET or word_lower in COMM_SET: categories['commercial'].add(word_lower); continue
        categories['general'].add(word_lower)

    return {k: sorted(list(v)) for k, v in categories.items()}

# ==========================================
# STATE INIT
# ==========================================
if 'sidebar_gen_df' not in st.session_state: st.session_state.sidebar_gen_df = None
if 'sidebar_excel_bytes' not in st.session_state: st.session_state.sidebar_excel_bytes = None
if 'analysis_results' not in st.session_state: st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state: st.session_state.analysis_done = False
if 'ai_generated_df' not in st.session_state: st.session_state.ai_generated_df = None
if 'ai_excel_bytes' not in st.session_state: st.session_state.ai_excel_bytes = None
if 'tags_html_result' not in st.session_state: st.session_state.tags_html_result = None
if 'table_html_result' not in st.session_state: st.session_state.table_html_result = None
if 'tags_generated_df' not in st.session_state: st.session_state.tags_generated_df = None
if 'tags_excel_data' not in st.session_state: st.session_state.tags_excel_data = None

# Current lists
if 'categorized_products' not in st.session_state: st.session_state.categorized_products = []
if 'categorized_services' not in st.session_state: st.session_state.categorized_services = []
if 'categorized_commercial' not in st.session_state: st.session_state.categorized_commercial = []
if 'categorized_dimensions' not in st.session_state: st.session_state.categorized_dimensions = []
if 'categorized_geo' not in st.session_state: st.session_state.categorized_geo = []
if 'categorized_general' not in st.session_state: st.session_state.categorized_general = []
if 'categorized_sensitive' not in st.session_state: st.session_state.categorized_sensitive = []

# Original lists
if 'orig_products' not in st.session_state: st.session_state.orig_products = []
if 'orig_services' not in st.session_state: st.session_state.orig_services = []
if 'orig_commercial' not in st.session_state: st.session_state.orig_commercial = []
if 'orig_dimensions' not in st.session_state: st.session_state.orig_dimensions = []
if 'orig_geo' not in st.session_state: st.session_state.orig_geo = []
if 'orig_general' not in st.session_state: st.session_state.orig_general = []

if 'auto_tags_words' not in st.session_state: st.session_state.auto_tags_words = []
if 'auto_promo_words' not in st.session_state: st.session_state.auto_promo_words = []
if 'persistent_urls' not in st.session_state: st.session_state['persistent_urls'] = ""

# ==========================================
# CONFIG & CONSTANTS
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO v2.6 (Mass Promo)", page_icon="üìä")

GARBAGE_LATIN_STOPLIST = {
    'whatsapp', 'viber', 'telegram', 'skype', 'vk', 'instagram', 'facebook', 'youtube', 'twitter',
    'cookie', 'cookies', 'policy', 'privacy', 'agreement', 'terms', 'click', 'submit', 'send', 'zakaz', 
    'basket', 'cart', 'order', 'call', 'back', 'callback', 'login', 'logout', 'sign', 'register', 'auth', 
    'account', 'profile', 'search', 'menu', 'nav', 'navigation', 'footer', 'header', 'sidebar',
    'img', 'jpg', 'png', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'svg', 'ok', 'error', 'undefined', 
    'null', 'true', 'false', 'var', 'let', 'const', 'function', 'return', 'ru', 'en', 'com', 'net', 
    'org', 'biz', 'shop', 'store', 'phone', 'email', 'tel', 'fax', 'mob', 'address', 'copyright', 
    'all', 'rights', 'reserved', 'div', 'span', 'class', 'id', 'style', 'script', 'body', 'html', 'head', 'meta', 'link'
}

SENSITIVE_STOPLIST_RAW = {
    "—É–∫—Ä–∞–∏–Ω–∞", "ukraine", "ua", "–≤—Å—É", "–∑—Å—É", "–∞—Ç–æ", "–∫–∏–µ–≤", "–ª—å–≤–æ–≤", "—Ö–∞—Ä—å–∫–æ–≤", "–æ–¥–µ—Å—Å–∞", "–¥–Ω–µ–ø—Ä", 
    "–º–∞—Ä–∏—É–ø–æ–ª—å", "–¥–æ–Ω–µ—Ü–∫", "–ª—É–≥–∞–Ω—Å–∫", "–¥–Ω—Ä", "–ª–Ω—Ä", "–¥–æ–Ω–±–∞—Å—Å", "–º–µ–ª–∏—Ç–æ–ø–æ–ª—å", "–±–µ—Ä–¥—è–Ω—Å–∫", "–±–∞—Ö–º—É—Ç", 
    "–∑–∞–ø–æ—Ä–æ–∂—å–µ", "—Ö–µ—Ä—Å–æ–Ω", "–∫—Ä—ã–º", "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å", "—Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å"
}
SENSITIVE_STOPLIST = {w.lower() for w in SENSITIVE_STOPLIST_RAW}

def check_password():
    if st.session_state.get("authenticated"):
        return True
    st.markdown("""<style>.main { display: flex; flex-direction: column; justify-content: center; align-items: center; } .auth-logo-box { text-align: center; margin-bottom: 1rem; padding-top: 0; }</style>""", unsafe_allow_html=True)
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="auth-logo-box"><h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3></div>', unsafe_allow_html=True)
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "ZVC01w4_pIquj0bMiaAu":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

if "arsenkin_token" in st.session_state: ARSENKIN_TOKEN = st.session_state.arsenkin_token
else:
    try: ARSENKIN_TOKEN = st.secrets["api"]["arsenkin_token"]
    except: ARSENKIN_TOKEN = None

if "yandex_dict_key" in st.session_state: YANDEX_DICT_KEY = st.session_state.yandex_dict_key
else:
    try: YANDEX_DICT_KEY = st.secrets["api"]["yandex_dict_key"]
    except: YANDEX_DICT_KEY = None

REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969}, "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868}, "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904}, "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956}, "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931}, "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–ö–∏–µ–≤ (UA)": {"ya": 143, "go": 1012852}, "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493}, "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601}
}

DEFAULT_EXCLUDE_DOMAINS = {
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "aliexpress.ru", 
    "ebay.com", "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", "pandao.ru", 
    "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", "banki.ru", "regmarkets.ru", 
    "zoon.ru", "pulscen.ru", "prodoctorov.ru", "blizko.ru", "domclick.ru", "satom.ru", 
    "quto.ru", "edadeal.ru", "cataloxy.ru", "irr.ru", "onliner.by", "shop.by", "deal.by", 
    "yell.ru", "profi.ru", "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by", "market.yandex.ru", 
    "youtube.com", "www.youtube.com", "gosuslugi.ru", "www.gosuslugi.ru", "dzen.ru", 
    "2gis.by", "wildberries.ru", "rutube.ru", "vk.com", "facebook.com", "chipdip.ru"
}
DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n—Å—Ç—Ä\n—É–ª\n—à—Ç\n—Å–º\n–º–º\n–º–ª\n–∫–≥\n–∫–≤\n–º¬≤\n—Å–º¬≤\n–º2\n—Å–º2"

PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE"

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif; color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important; }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important; color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] div[role="gridcell"] {{
            background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; border-bottom: 1px solid {ROW_BORDER_COLOR} !important;
        }}
        .legend-box {{ padding: 10px; background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px; }}
        .text-red {{ color: #D32F2F; font-weight: bold; }}
        .text-green {{ color: #2E7D32; font-weight: bold; }}
        .text-bold {{ font-weight: 600; }}
        .sort-container {{ background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 8px; margin-bottom: 10px; border: 1px solid {BORDER_COLOR}; }}
        
        .stApp > header {{ background-color: transparent !important; }}
        .stTextInput input:disabled, .stTextArea textarea:disabled, .stSelectbox div[aria-disabled="true"] {{
            opacity: 1 !important; background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; cursor: text !important; -webkit-text-fill-color: {TEXT_COLOR} !important; border-color: {BORDER_COLOR} !important;
        }}
        .stButton button:disabled {{ opacity: 1 !important; background-color: {PRIMARY_COLOR} !important; color: white !important; cursor: progress !important; }}
        div[data-testid="stAppViewContainer"] {{ filter: none !important; opacity: 1 !important; transition: none !important; }}
        /* –°—Ç–∏–ª–∏ –¥–ª—è —Å—Å—ã–ª–æ–∫ –≤–Ω—É—Ç—Ä–∏ –≥—Ä–∞—Ñ–∏–∫–∞ Plotly */
        .chart-link {{
            color: #277EFF !important;
            font-weight: 600 !important;
            text-decoration: none !important;
            border-bottom: 4px solid #CBD5E1 !important; 
            display: inline-block !important;
            transition: border-color 0.2s ease !important;
        }}
        .chart-link:hover {{
            border-bottom-color: #277EFF !important;
            cursor: pointer !important;
        }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 7. UI TABS RESTRUCTURED
# ==========================================
tab_seo_main, tab_wholesale_main = st.tabs(["üìä SEO –ê–Ω–∞–ª–∏–∑", "üè≠ –û–ø—Ç–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä"])

# ------------------------------------------
# TAB 1: SEO ANALYSIS (KEPT AS IS)
# ------------------------------------------
with tab_seo_main:
    col_main, col_sidebar = st.columns([68, 32])
    
    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
        
# 1. MY PAGE & QUERY
        st.markdown("### –î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        # 1. –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–æ—Ä —Ç–∏–ø–∞ (—á—Ç–æ–±—ã –Ω–µ —Å–¥–≤–∏–≥–∞–ª –ø–æ–ª—è –≤–≤–æ–¥–∞)
        my_input_type = st.radio(
            "–¢–∏–ø –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã", 
            ["URL –Ω–∞ —Å–∞–π—Ç–µ", "–¢–µ–∫—Å—Ç/HTML", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], 
            horizontal=True, 
            label_visibility="collapsed", 
            key="my_page_source_radio"
        )

        # 2. –¢–µ–ø–µ—Ä—å –∏–Ω–ø—É—Ç—ã —Ä–æ–≤–Ω–æ –≤ —Ä—è–¥
        c_req1, c_req2 = st.columns(2)
        
        with c_req1:
            if my_input_type == "URL –Ω–∞ —Å–∞–π—Ç–µ":
                # –£–±—Ä–∞–ª–∏ label_visibility="collapsed", —á—Ç–æ–±—ã –∑–∞–≥–æ–ª–æ–≤–æ–∫ "–í–∞—à URL" –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–ª –≤—ã—Å–æ—Ç—É
                st.text_input("–í–∞—à URL", placeholder="https://site.ru/...", key="my_url_input")
            elif my_input_type == "–¢–µ–∫—Å—Ç/HTML":
                st.text_area("–ö–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", height=100, placeholder="<html>...", key="my_content_input")
            else:
                st.info("–†–µ–∂–∏–º –±–µ–∑ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–æ —Å–≤–æ–∏–º —Å–∞–π—Ç–æ–º")
        
        with c_req2:
            st.text_input("–ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –Ω–∏–∫–µ–ª—å", key="query_input")

        # 2. COMPETITOR SOURCE
        st.markdown("### –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–≤—Ç–æ-–ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è
        if st.session_state.get('force_radio_switch'):
            st.session_state["competitor_source_radio"] = "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"
            st.session_state['force_radio_switch'] = False

        c_src, c_reset = st.columns([3, 1])
        with c_src:
            source_type_new = st.radio(
                "–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö", 
                ["–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)", "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"], 
                horizontal=True, 
                label_visibility="collapsed", 
                key="competitor_source_radio"
            )
            
        with c_reset:
            # –ö–Ω–æ–ø–∫–∞ –ø–æ—è–≤–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if st.session_state.get('analysis_done'):
                if st.button("üîÑ –°–±—Ä–æ—Å", type="secondary", use_container_width=True, help="–ù–∞—á–∞—Ç—å –Ω–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑"):
                    keys_to_clear = [
                        'analysis_done', 'analysis_results', 'excluded_urls_auto', 
                        'detected_anomalies', 'serp_trend_info', 'persistent_urls',
                        'naming_table_df', 'ideal_h1_result', 'full_graph_data'
                    ]
                    for k in keys_to_clear:
                        if k in st.session_state: del st.session_state[k]
                    st.rerun()

        source_type = "API" if "API" in source_type_new else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"
        
        # 3. –ü–û–õ–ï –í–í–û–î–ê –°–°–´–õ–û–ö (–õ–æ–≥–∏–∫–∞ —Å 2 –∫–æ–ª–æ–Ω–∫–∞–º–∏)
        if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
            has_exclusions = st.session_state.get('excluded_urls_auto') and len(st.session_state.get('excluded_urls_auto')) > 5
            
            if st.session_state.get('analysis_done') and has_exclusions:
                c_url_1, c_url_2 = st.columns(2)
                with c_url_1:
                    manual_val = st.text_area("‚úÖ –ê–∫—Ç–∏–≤–Ω—ã–µ (–ê–Ω–∞–ª–∏–∑)", height=200, key="manual_urls_widget", value=st.session_state.get('persistent_urls', ""))
                    st.session_state['persistent_urls'] = manual_val
                with c_url_2:
                    st.text_area("üö´ –ò—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ", height=200, key="excluded_urls_widget_display", value=st.session_state.get('excluded_urls_auto', ""))
            else:
                manual_val = st.text_area("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", height=200, key="manual_urls_widget", value=st.session_state.get('persistent_urls', ""))
                st.session_state['persistent_urls'] = manual_val

        st.markdown("<br>", unsafe_allow_html=True)
        
        # –ß–µ–∫–±–æ–∫—Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        use_smart_filter = st.checkbox("üõ°Ô∏è –ê–≤—Ç–æ-—Ñ–∏–ª—å—Ç—Ä (—Å–Ω—è—Ç—å, –µ—Å–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç–µ —Å–∞–π—Ç—ã)", value=True, key="cb_use_smart_filter")
        
        # –ö–ù–û–ü–ö–ê –ó–ê–ü–£–°–ö–ê
        if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", type="primary", use_container_width=True, key="start_analysis_btn"):
            # === –û–ß–ò–°–¢–ö–ê –í–°–ï–• –°–¢–ê–†–´–• –î–ê–ù–ù–´–• ===
            st.session_state.analysis_results = None
            st.session_state.analysis_done = False
            st.session_state.naming_table_df = None
            st.session_state.ideal_h1_result = None
            st.session_state.gen_result_df = None
            st.session_state.unified_excel_data = None

            # –ï—Å–ª–∏ –º—ã –≤ —Ä—É—á–Ω–æ–º —Ä–µ–∂–∏–º–µ –∏ –µ—Å—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏—è, –º—ã –∏—Ö –Ω–µ —Ç—Ä–æ–≥–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ —é–∑–µ—Ä –º–æ–≥ –∏—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å.
            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤—ã–π –∑–∞–ø—É—Å–∫, –æ–Ω–∏ –æ—á–∏—Å—Ç—è—Ç—Å—è.
            if not st.session_state.get('analysis_done'):
                 if 'excluded_urls_auto' in st.session_state: del st.session_state['excluded_urls_auto']
                 if 'detected_anomalies' in st.session_state: del st.session_state['detected_anomalies']
                 if 'serp_trend_info' in st.session_state: del st.session_state['serp_trend_info']
            
            # –°–±—Ä–æ—Å –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü
            for key in list(st.session_state.keys()):
                if key.endswith('_page'): st.session_state[key] = 1
            
            # –ó–∞–ø—É—Å–∫ —Ñ–ª–∞–≥–∞ –∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, —á—Ç–æ–±—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –æ—á–∏—Å—Ç–∏–ª—Å—è
            st.session_state.start_analysis_flag = True
            st.rerun()

    with col_sidebar:
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API")
        if not ARSENKIN_TOKEN:
             new_arsenkin = st.text_input("Arsenkin Token", type="password", key="input_arsenkin")
             if new_arsenkin: st.session_state.arsenkin_token = new_arsenkin; ARSENKIN_TOKEN = new_arsenkin 
        if not YANDEX_DICT_KEY:
             new_yandex = st.text_input("Yandex Dict Key", type="password", key="input_yandex")
             if new_yandex: st.session_state.yandex_dict_key = new_yandex; YANDEX_DICT_KEY = new_yandex
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
        st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
        st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
        st.selectbox("–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞", list(REGION_MAP.keys()), key="settings_region")
        
        # –ò–ó–ú–ï–ù–ï–ù–ò–ï: –£–±—Ä–∞–ª–∏ 30, –æ—Å—Ç–∞–≤–∏–ª–∏ —Ç–æ–ª—å–∫–æ 10 –∏ 20
        st.selectbox("–ö–æ–ª-–≤–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", [10, 20], index=0, key="settings_top_n")
        
        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å <noindex>", True, key="settings_noindex")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", False, key="settings_alt")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", False, key="settings_numbers")
        st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True, key="settings_norm")
        
        st.markdown("---")
        st.markdown("#####üö´ –§–∏–ª—å—Ç—Ä—ã")
        with st.expander("–°–ø–∏—Å–∫–∏ (Stop / Exclude)", expanded=False):
            st.text_area("–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–æ–º–µ–Ω—ã", DEFAULT_EXCLUDE, height=100, key="settings_excludes")
            st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞", DEFAULT_STOPS, height=100, key="settings_stops")

# ==========================================
    # –ë–õ–û–ö 1: –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (–¢–ï–ü–ï–†–¨ –ü–ï–†–í–´–ô)
    # ==========================================
    if st.session_state.analysis_done and st.session_state.analysis_results:
        results = st.session_state.analysis_results
        
        d_score = results['my_score']['depth']
        w_score = results['my_score']['width']
        
        # –®–∏—Ä–∏–Ω–∞
        w_color = "#2E7D32" if w_score >= 80 else ("#E65100" if w_score >= 50 else "#D32F2F")
        
        # –ì–ª—É–±–∏–Ω–∞ (–¶–µ–ª—å = 80)
        if 75 <= d_score <= 88:
            d_color = "#2E7D32" # –ó–µ–ª–µ–Ω—ã–π (–û—Ç–ª–∏—á–Ω–æ)
            d_status = "–ò–î–ï–ê–õ (–¢–æ–ø)"
        elif 88 < d_score <= 100:
            d_color = "#D32F2F" # –ö—Ä–∞—Å–Ω—ã–π (–†–∏—Å–∫ –ø–µ—Ä–µ—Å–ø–∞–º–∞)
            d_status = "–ü–ï–†–ï–°–ü–ê–ú (–†–∏—Å–∫)"
        elif 55 <= d_score < 75:
            d_color = "#F9A825" # –ñ–µ–ª—Ç—ã–π
            d_status = "–°—Ä–µ–¥–Ω—è—è"
        else:
            d_color = "#D32F2F" # –ö—Ä–∞—Å–Ω—ã–π
            d_status = "–ù–∏–∑–∫–∞—è"

        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        
        st.markdown("""
        <style>
            details > summary { list-style: none; }
            details > summary::-webkit-details-marker { display: none; }
            .details-card {
                background-color: #f8f9fa; border: 1px solid #e9ecef;
                border-radius: 8px; margin-bottom: 10px;
                overflow: hidden; transition: all 0.2s ease;
            }
            .details-card:hover { box-shadow: 0 2px 5px rgba(0,0,0,0.05); border-color: #d1d5db; }
            .card-summary {
                padding: 12px 15px; cursor: pointer; font-weight: 700;
                font-size: 15px; color: #111827; display: flex;
                justify-content: space-between; align-items: center;
                background-color: #ffffff;
            }
            .card-summary:hover { background-color: #f3f4f6; }
            .card-content {
                padding: 15px; border-top: 1px solid #e9ecef;
                font-size: 14px; color: #374151; line-height: 1.6;
                background-color: #fcfcfc;
            }
            .count-tag { 
                background: #e5e7eb; color: #374151; padding: 2px 8px; 
                border-radius: 10px; font-size: 12px; font-weight: 600;
                min-width: 25px; text-align: center;
            }
            .arrow-icon {
                font-size: 10px; margin-right: 8px; color: #9ca3af;
                transition: transform 0.2s;
            }
            details[open] .arrow-icon { transform: rotate(90deg); color: #277EFF; }
        </style>
        """, unsafe_allow_html=True)

        st.markdown(f"""
        <div style='display: flex; gap: 20px; flex-wrap: wrap;'>
            <div style='flex: 1; background:{LIGHT_BG_MAIN}; padding:15px; border-radius:8px; border-left: 5px solid {w_color};'>
                <div style='font-size: 12px; color: #666;'>–®–ò–†–ò–ù–ê (–û—Ö–≤–∞—Ç —Ç–µ–º)</div>
                <div style='font-size: 24px; font-weight: bold; color: {w_color};'>{w_score}/100</div>
            </div>
            <div style='flex: 1; background:{LIGHT_BG_MAIN}; padding:15px; border-radius:8px; border-left: 5px solid {d_color};'>
                <div style='font-size: 12px; color: #666;'>–ì–õ–£–ë–ò–ù–ê (–¶–µ–ª—å: ~80)</div>
                <div style='font-size: 24px; font-weight: bold; color: {d_color};'>{d_score}/100 <span style='font-size:14px; font-weight:normal;'>({d_status})</span></div>
            </div>
        </div>
        <br>
        """, unsafe_allow_html=True)

        with st.expander("üõí –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ –∏ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è", expanded=True):
            if not st.session_state.get('orig_products'):
                st.info("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑.")
            else:
                # –†—è–¥ 1
                c1, c2, c3 = st.columns(3)
                with c1: render_clean_block("–¢–æ–≤–∞—Ä—ã", "üß±", st.session_state.categorized_products)
                with c2: render_clean_block("–ì–µ–æ", "üåç", st.session_state.categorized_geo)
                with c3: render_clean_block("–ö–æ–º–º–µ—Ä—Ü–∏—è", "üí∞", st.session_state.categorized_commercial)
                
                # –†—è–¥ 2
                c4, c5, c6 = st.columns(3)
                with c4: render_clean_block("–£—Å–ª—É–≥–∏", "üõ†Ô∏è", st.session_state.categorized_services)
                with c5: render_clean_block("–†–∞–∑–º–µ—Ä—ã/–ì–û–°–¢", "üìè", st.session_state.categorized_dimensions)
                with c6: render_clean_block("–û–±—â–∏–µ", "üìÇ", st.session_state.categorized_general)

                st.markdown("<hr style='margin: 15px 0;'>", unsafe_allow_html=True)

                # –ë–ª–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
                cs1, cs2 = st.columns([1, 3])
                
                if 'sensitive_words_input_final' not in st.session_state:
                    current_list = st.session_state.get('categorized_sensitive', [])
                    st.session_state['sensitive_words_input_final'] = "\n".join(current_list)
                
                current_text_value = st.session_state['sensitive_words_input_final']
                
                with cs1:
                    count_excluded = len([x for x in current_text_value.split('\n') if x.strip()])
                    st.markdown(f"**‚õî –°—Ç–æ–ø-—Å–ª–æ–≤–∞**")
                    st.markdown(f"–ò—Å–∫–ª—é—á–µ–Ω–æ: **{count_excluded}**")
                    st.caption("–≠—Ç–∏ —Å–ª–æ–≤–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª–µ–Ω—ã.")
                
                with cs2:
                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º _2 –∫ –∫–ª—é—á—É —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è
                    new_sens_str = st.text_area(
                        "hidden_label", height=100,
                        key="sensitive_words_input_final_2",
                        label_visibility="collapsed",
                        placeholder="–°–ª–æ–≤–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è..."
                    )

                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á –∫–Ω–æ–ø–∫–µ –∏ —á–∏—Ç–∞–µ–º –∏–∑ –ø–æ–ª—è —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º _2
                    if st.button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä", type="primary", use_container_width=True, key="btn_update_filter_2"):
                        raw_input = st.session_state.get("sensitive_words_input_final_2", "")
                        new_stop_set = set([w.strip().lower() for w in raw_input.split('\n') if w.strip()])
                        
                        st.session_state.categorized_sensitive = sorted(list(new_stop_set))
                        
                        def apply_filter(orig_list_key, stop_set):
                            original = st.session_state.get(orig_list_key, [])
                            return [w for w in original if w.lower() not in stop_set]

                        st.session_state.categorized_products = apply_filter('orig_products', new_stop_set)
                        st.session_state.categorized_services = apply_filter('orig_services', new_stop_set)
                        st.session_state.categorized_commercial = apply_filter('orig_commercial', new_stop_set)
                        st.session_state.categorized_geo = apply_filter('orig_geo', new_stop_set)
                        st.session_state.categorized_dimensions = apply_filter('orig_dimensions', new_stop_set)
                        st.session_state.categorized_general = apply_filter('orig_general', new_stop_set)

                        # –û–±–Ω–æ–≤–ª—è–µ–º –≤–∫–ª–∞–¥–∫—É –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
                        all_prods = st.session_state.categorized_products
                        count_prods = len(all_prods)
                        if count_prods < 20:
                            st.session_state.auto_tags_words = all_prods
                            st.session_state.auto_promo_words = []
                        else:
                            half = int(math.ceil(count_prods / 2))
                            st.session_state.auto_tags_words = all_prods[:half]
                            st.session_state.auto_promo_words = all_prods[half:]

                        st.session_state['kws_tags_auto'] = "\n".join(st.session_state.auto_tags_words)
                        st.session_state['kws_promo_auto'] = "\n".join(st.session_state.auto_promo_words)

                        st.toast("–§–∏–ª—å—Ç—Ä –æ–±–Ω–æ–≤–ª–µ–Ω!", icon="‚úÖ")
                        time.sleep(0.5)
                        st.rerun()

        # === –£–ü–£–©–ï–ù–ù–ê–Ø –°–ï–ú–ê–ù–¢–ò–ö–ê ---
        high = results.get('missing_semantics_high', [])
        low = results.get('missing_semantics_low', [])
        if high or low:
            with st.expander(f"üß© –£–ø—É—â–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ ({len(high)+len(low)})", expanded=False):
                if high: st.markdown(f"<div style='background:#EBF5FF;padding:10px;border-radius:5px;'><b>–í–∞–∂–Ω—ã–µ:</b> {', '.join([x['word'] for x in high])}</div>", unsafe_allow_html=True)
                if low: st.markdown(f"<div style='background:#F7FAFC;padding:10px;border-radius:5px;margin-top:5px;'><b>–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞:</b> {', '.join([x['word'] for x in low])}</div>", unsafe_allow_html=True)

        render_paginated_table(results['depth'], "1. –ì–ª—É–±–∏–Ω–∞", "tbl_depth_1", default_sort_col="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è", use_abs_sort_default=True)
        
        # === –¢–ê–ë–õ–ò–¶–ê ‚Ññ2 (–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é) ===
        if 'naming_table_df' in st.session_state and st.session_state.naming_table_df is not None:
            df_naming = st.session_state.naming_table_df
            
            st.markdown("### 2. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ç–æ–≤–∞—Ä–æ–≤")
            
            # --- –ë–õ–û–ö 1: –§–û–†–ú–£–õ–ê (–ù–ê–¢–ò–í–ù–´–ô) ---
            if 'ideal_h1_result' in st.session_state:
                res_ideal = st.session_state.ideal_h1_result
                
                if isinstance(res_ideal, (tuple, list)) and len(res_ideal) >= 2:
                    example_name = res_ideal[0]
                    report_list = res_ideal[1]
                    
                    # –ß–∏—Å—Ç–∏–º —Å—Ç—Ä–æ–∫—É —Ñ–æ—Ä–º—É–ª—ã –æ—Ç –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
                    formula_str = "–§–æ—Ä–º—É–ª–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
                    for line in report_list:
                        if "—Å—Ç—Ä—É–∫—Ç—É—Ä–∞" in line or "–°—Ö–µ–º–∞" in line:
                            # –£–±–∏—Ä–∞–µ–º –∂–∏—Ä–Ω—ã–π —à—Ä–∏—Ñ—Ç –∏ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ–ª–µ–π
                            formula_str = line.replace("**–°–∞–º–∞—è —á–∞—Å—Ç–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**", "").replace("**–°—Ö–µ–º–∞:**", "").strip()
                            break
                    
                    # –í—ã–≤–æ–¥ —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å —Ä–∞–º–∫–æ–π
                    with st.container(border=True):
                        st.markdown("#### üß™ –ò–¥–µ–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ –Ω–∞–∑–≤–∞–Ω–∏—è")
                        # st.info –¥–µ–ª–∞–µ—Ç –∫—Ä–∞—Å–∏–≤—É—é —Å–∏–Ω—é—é –ø–ª–∞—à–∫—É –±–µ–∑ –ª–∏—à–Ω–µ–≥–æ HTML
                        st.info(f"**{formula_str}**", icon="üß©")
                        st.markdown(f"**–ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:** _{example_name}_")
                        
                else:
                    st.warning("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ —É—Å—Ç–∞—Ä–µ–ª–∏. –ù–∞–∂–º–∏—Ç–µ '–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó'.")

            # --- –ë–õ–û–ö 2: –¢–ê–ë–õ–ò–¶–ê ---
            st.markdown("##### –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫")
            
            if not df_naming.empty:
                col_ctrl1, col_ctrl2 = st.columns([1, 3])
                with col_ctrl1:
                    show_tech = st.toggle("–ü–æ–∫–∞–∑–∞—Ç—å —Ä–∞–∑–º–µ—Ä—ã –∏ —Ü–∏—Ñ—Ä—ã", value=False, key="toggle_show_tech_specs_unique")
                
                df_display = df_naming.copy()
                
                if not show_tech:
                    # –°–∫—Ä—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é "–†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
                    df_display = df_display[~df_display['–¢–∏–ø —Ö–∞—Ä-–∫–∏'].str.contains("–†–∞–∑–º–µ—Ä—ã", na=False)]

                if 'cat_sort' in df_display.columns:
                    df_display = df_display.sort_values(by=["cat_sort", "raw_freq"], ascending=[True, False])
                
                # –£–±–∏—Ä–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—Ç–æ–ª–±—Ü—ã
                cols_to_show = ["–¢–∏–ø —Ö–∞—Ä-–∫–∏", "–°–ª–æ–≤–æ", "–ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å (%)", "–£ –í–∞—Å", "–ú–µ–¥–∏–∞–Ω–∞", "–î–æ–±–∞–≤–∏—Ç—å"]
                existing_cols = [c for c in cols_to_show if c in df_display.columns]
                df_display = df_display[existing_cols]

                # –†–∞—Å–∫—Ä–∞—Å–∫–∞
                def style_rows(row):
                    val = str(row.get('–î–æ–±–∞–≤–∏—Ç—å', ''))
                    if "+" in val: return ['background-color: #fff1f2; color: #9f1239'] * len(row) # –ö—Ä–∞—Å–Ω—ã–π
                    if "‚úÖ" in val: return ['background-color: #f0fdf4; color: #166534'] * len(row) # –ó–µ–ª–µ–Ω—ã–π
                    return [''] * len(row)

                st.dataframe(
                    df_display.style.apply(style_rows, axis=1),
                    use_container_width=True,
                    hide_index=True,
                    height=(len(df_display) * 35) + 38 if len(df_display) < 15 else 500
                )
            else:
                st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")

        # === –ì–†–ê–§–ò–ö (–ù–ò–ñ–ù–ò–ô) ===
        if 'relevance_top' in results and not results['relevance_top'].empty:
             st.markdown("### üìä –ì—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑")
             with st.expander("üìà –ü–æ–∫–∞–∑–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–¢–û–ü-10)", expanded=True):
                  # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á, —á—Ç–æ–±—ã –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å —Å –≤–µ—Ä—Ö–Ω–∏–º –≥—Ä–∞—Ñ–∏–∫–æ–º
                  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –†–∏—Å—É–µ–º –≥—Ä–∞—Ñ–∏–∫ –ø–æ –ü–û–õ–ù–´–ú –¥–∞–Ω–Ω—ã–º (30 —Å–∞–π—Ç–æ–≤), –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                  graph_data = st.session_state.get('full_graph_data', results['relevance_top'])
                  render_relevance_chart(graph_data, unique_key="main")
             
             # === –ë–õ–û–ö –ü–û–î–°–ö–ê–ó–û–ö –ü–û –ì–†–ê–§–ò–ö–£ ===
             if 'serp_trend_info' in st.session_state:
                 trend = st.session_state['serp_trend_info']
                 anomalies = st.session_state.get('detected_anomalies', [])
                 
                 trend_color = "blue"
                 if trend['type'] == 'inverted': trend_color = "red"
                 if trend['type'] == 'normal': trend_color = "green"
                 
                 st.markdown(f"""
                 <div style="border: 1px solid #ddd; padding: 15px; border-radius: 8px; margin-top: 10px; background-color: #f9fafb;">
                     <h5 style="margin-top:0;">üß† AI-–ê–Ω–∞–ª–∏–∑ –≤—ã–¥–∞—á–∏</h5>
                     <p style="color: {trend_color}; font-weight: bold;">{trend['msg']}</p>
                 </div>
                 """, unsafe_allow_html=True)
                 
                 if anomalies:
                     st.warning(f"‚ö†Ô∏è **–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞–Ω–æ–º–∞–ª–∏–∏ ({len(anomalies)} —à—Ç.):** –°–∞–π—Ç—ã —Å –Ω–∏–∑–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –±—ã–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω—ã –∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.")

        # === –û–°–¢–ê–õ–¨–ù–´–ï –¢–ê–ë–õ–ò–¶–´ ===
        render_paginated_table(results['hybrid'], "3. TF-IDF", "tbl_hybrid", default_sort_col="TF-IDF –¢–û–ü")
        render_paginated_table(results['relevance_top'], "4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", "tbl_rel", default_sort_col="–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)")


    # ==========================================
    # –ë–õ–û–ö 2: –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ï –ò –†–ê–°–ß–ï–¢
    # ==========================================
    if st.session_state.get('start_analysis_flag'):
        st.session_state.start_analysis_flag = False
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        settings = {
            'noindex': st.session_state.settings_noindex, 
            'alt_title': st.session_state.settings_alt, 
            'numbers': st.session_state.settings_numbers, 
            'norm': st.session_state.settings_norm, 
            'ua': st.session_state.settings_ua, 
            'custom_stops': st.session_state.settings_stops.split()
        }
        
        my_data, my_domain, my_serp_pos = None, "", 0
        current_input_type = st.session_state.get("my_page_source_radio")
        
        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –í–ê–®–ï–ô —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if current_input_type == "URL –Ω–∞ —Å–∞–π—Ç–µ":
            with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
                my_data = parse_page(st.session_state.my_url_input, settings, st.session_state.query_input)
                if not my_data: st.error("–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."); st.stop()
                my_domain = urlparse(st.session_state.my_url_input).netloc
        elif current_input_type == "–¢–µ–∫—Å—Ç/HTML":
            my_data = {'url': 'Local', 'domain': 'local', 'body_text': st.session_state.my_content_input, 'anchor_text': ''}
            
        # 2. –°–±–æ—Ä –ö–ê–ù–î–ò–î–ê–¢–û–í
        candidates_pool = []
        current_source_val = st.session_state.get("competitor_source_radio")
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–µ—Ä–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (10 –∏–ª–∏ 20) –¥–ª—è –§–ò–ù–ê–õ–ê
        user_target_top_n = st.session_state.settings_top_n
        # –ê —Å–∫–∞—á–∏–≤–∞–µ–º –≤—Å–µ–≥–¥–∞ –ú–ê–ö–°–ò–ú–£–ú (30), —á—Ç–æ–±—ã –±—ã–ª–æ –∏–∑ —á–µ–≥–æ –≤—ã–±–∏—Ä–∞—Ç—å
        download_limit = 30 
        
        if "API" in current_source_val:
            if not ARSENKIN_TOKEN: st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç API —Ç–æ–∫–µ–Ω Arsenkin."); st.stop()
            with st.spinner(f"API Arsenkin (–ó–∞–ø—Ä–æ—Å –¢–æ–ø-30)..."):
                raw_top = get_arsenkin_urls(st.session_state.query_input, st.session_state.settings_search_engine, st.session_state.settings_region, ARSENKIN_TOKEN, depth_val=30)
                
                if not raw_top: st.stop()
                
                excl = [d.strip() for d in st.session_state.settings_excludes.split('\n') if d.strip()]
                agg_list = [
                    "avito", "ozon", "wildberries", "market.yandex", "tiu", "youtube", "vk.com", "yandex",
                    "leroymerlin", "petrovich", "satom", "pulscen", "blizko", "deal.by", "satu.kz", "prom.ua",
                    "wikipedia", "dzen", "rutube", "kino", "otzovik", "irecommend", "profi.ru", "zoon", "2gis",
                    "megamarket.ru", "lamoda.ru", "utkonos.ru", "vprok.ru", "allbiz.ru", "all-companies.ru",
                    "orgpage.ru", "list-org.com", "rusprofile.ru", "e-katalog.ru", "kufar.by", "wildberries.kz",
                    "ozon.kz", "kaspi.kz", "pulscen.kz", "allbiz.kz", "wildberries.uz", "olx.uz", "pulscen.uz",
                    "allbiz.uz", "wildberries.kg", "pulscen.kg", "allbiz.kg", "all.biz", "b2b-center.ru"
                ]
                excl.extend(agg_list)
                for res in raw_top:
                    dom = urlparse(res['url']).netloc.lower()
                    if my_domain and (my_domain in dom or dom in my_domain):
                        if my_serp_pos == 0 or res['pos'] < my_serp_pos: 
                            my_serp_pos = res['pos']
                    is_garbage = False
                    for x in excl:
                        if x.lower() in dom:
                            is_garbage = True
                            break
                    if is_garbage: continue
                    candidates_pool.append(res)
        else:
            raw_input_urls = st.session_state.get("persistent_urls", "")
            candidates_pool = [{'url': u.strip(), 'pos': i+1} for i, u in enumerate(raw_input_urls.split('\n')) if u.strip()]

        if not candidates_pool: st.error("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤."); st.stop()
        
        # 3. –°–ö–ê–ß–ò–í–ê–ù–ò–ï (–í—Å–µ—Ö 30)
        comp_data_valid = []
        with st.status(f"üïµÔ∏è –ì–ª—É–±–æ–∫–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (–í—Å–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidates_pool)})...", expanded=True) as status:
            with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
                futures = {
                    executor.submit(parse_page, item['url'], settings, st.session_state.query_input): item 
                    for item in candidates_pool
                }
                done_count = 0
                for f in concurrent.futures.as_completed(futures):
                    original_item = futures[f]
                    try:
                        res = f.result()
                        if res:
                            res['pos'] = original_item['pos']
                            comp_data_valid.append(res)
                    except: pass
                    done_count += 1
                    status.update(label=f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {done_count}/{len(candidates_pool)} | –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ: {len(comp_data_valid)}")

            comp_data_valid.sort(key=lambda x: x['pos'])
            # –°–Ω–∞—á–∞–ª–∞ –±–µ—Ä–µ–º –í–°–ï–•, –∫—Ç–æ —Å–∫–∞—á–∞–ª—Å—è (–¥–æ 30), –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
            data_for_graph = comp_data_valid[:download_limit]
            targets_for_graph = [{'url': d['url'], 'pos': d['pos']} for d in data_for_graph]

        # 5. –†–ê–°–ß–ï–¢ –ú–ï–¢–†–ò–ö (–î–í–û–ô–ù–û–ô –ü–†–û–ì–û–ù)
        with st.spinner("–ê–Ω–∞–ª–∏–∑ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è..."):
            
            # --- –≠–¢–ê–ü 1: –ß–µ—Ä–Ω–æ–≤–æ–π –ø—Ä–æ–≥–æ–Ω (–ø–æ –≤—Å–µ–º 30 —Å–∞–π—Ç–∞–º) ---
            # –≠—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ –∏ –Ω–∞–π—Ç–∏ –∞–Ω–æ–º–∞–ª–∏–∏
            results_full = calculate_metrics(data_for_graph, my_data, settings, my_serp_pos, targets_for_graph)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ü–û–õ–ù–´–ï –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ (—á—Ç–æ–±—ã –Ω–∞ –Ω–µ–º –±—ã–ª–∏ –≤—Å–µ)
            st.session_state['full_graph_data'] = results_full['relevance_top']
            
            # --- –≠–¢–ê–ü 2: –û—Ç–±–æ—Ä —á–∏—Å—Ç–æ–≤—ã—Ö (–¢–æ–ø-10/20 –±–µ–∑ –º—É—Å–æ—Ä–∞) ---
            
            # === –£–ú–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø (Smart Filter Logic) ===
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ï—Å–ª–∏ —ç—Ç–æ API-–ø—Ä–æ–≥–æ–Ω, –±–µ—Ä–µ–º –ü–û–õ–ù–´–ï –¥–∞–Ω–Ω—ã–µ (30 —Å–∞–π—Ç–æ–≤) –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π.
            # –ò–Ω–∞—á–µ –±–µ—Ä–µ–º —Ç–µ–∫—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–¥–ª—è —Ä—É—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞).
            if "API" in current_source_val and 'full_graph_data' in st.session_state:
                df_rel_check = st.session_state['full_graph_data']
            else:
                df_rel_check = st.session_state['full_graph_data'] if 'full_graph_data' in st.session_state else results_full['relevance_top']
            
            # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–∏–º–µ–Ω—è—Ç—å –∞–≤—Ç–æ-—Ñ–∏–ª—å—Ç—Ä
            # –§–∏–ª—å—Ç—Ä—É–µ–º, –µ—Å–ª–∏:
            # –ê) –ò—Å—Ç–æ—á–Ω–∏–∫ API (–≤—Å–µ–≥–¥–∞ —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ)
            # –ë) –ò—Å—Ç–æ—á–Ω–∏–∫ –†—É—á–Ω–æ–π, –ù–û —ç—Ç–æ –ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ (–Ω–µ—Ç —Å–ø–∏—Å–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö)
            should_auto_filter = True
            
            is_manual_mode = "–†—É—á–Ω–æ–π" in current_source_val
            has_previous_exclusions = 'excluded_urls_auto' in st.session_state and len(st.session_state.get('excluded_urls_auto', '')) > 5
            
            # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–µ—Ä–Ω—É–ª —Å—Å—ã–ª–∫–∏ (—Ä—É—á–Ω–æ–π —Ä–µ–∂–∏–º + –µ—Å—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏—è) -> –Ω–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–Ω–æ–≤–æ
            if is_manual_mode and has_previous_exclusions:
                is_filter_on = st.session_state.get("cb_use_smart_filter", True)
                if not is_filter_on: # –ï—Å–ª–∏ –≥–∞–ª–æ—á–∫–∞ —Å–Ω—è—Ç–∞
                    should_auto_filter = False 
            
            # 2. –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ (–æ–Ω –Ω—É–∂–µ–Ω –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ –∏ —Ç—Ä–µ–Ω–¥–æ–≤ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ)
            good_urls, bad_urls_dicts, trend = analyze_serp_anomalies(df_rel_check)
            st.session_state['serp_trend_info'] = trend
            
            # 3. –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–∫–∏
            if should_auto_filter and bad_urls_dicts:
                # –†–ï–ñ–ò–ú 1: –ê–í–¢–û-–§–ò–õ–¨–¢–† -> –£–±–∏—Ä–∞–µ–º –ø–ª–æ—Ö–∏—Ö
                st.session_state['detected_anomalies'] = bad_urls_dicts
                excluded_list = [item['url'] for item in bad_urls_dicts]
                st.session_state['excluded_urls_auto'] = "\n".join(excluded_list)
                
                bad_urls_set = set(excluded_list)
                clean_data_pool = [d for d in data_for_graph if d['url'] not in bad_urls_set]
                
                st.toast(f"üßπ –ê–≤—Ç–æ-—Ñ–∏–ª—å—Ç—Ä: –ò—Å–∫–ª—é—á–µ–Ω–æ {len(bad_urls_dicts)} —Å–ª–∞–±—ã—Ö —Å–∞–π—Ç–æ–≤.", icon="üóëÔ∏è")
            else:
                # –†–ï–ñ–ò–ú 2: –í–°–ï –ü–û–î–†–Ø–î (–†—É—á–Ω–æ–π –∏–ª–∏ –Ω–µ—Ç –∞–Ω–æ–º–∞–ª–∏–π)
                clean_data_pool = data_for_graph
                if bad_urls_dicts:
                     st.toast(f"üõ°Ô∏è –†—É—á–Ω–æ–π —Ä–µ–∂–∏–º: –°–ª–∞–±—ã–µ —Å–∞–π—Ç—ã ({len(bad_urls_dicts)} —à—Ç.) –æ—Å—Ç–∞–≤–ª–µ–Ω—ã.", icon="üîì")
                else:
                     if 'excluded_urls_auto' in st.session_state: del st.session_state['excluded_urls_auto']
                     if 'detected_anomalies' in st.session_state: del st.session_state['detected_anomalies']

            # 4. –û—Ç—Ä–µ–∑–∞–µ–º —Ä–æ–≤–Ω–æ —Å—Ç–æ–ª—å–∫–æ, —Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Å–∏–ª —é–∑–µ—Ä (10 –∏–ª–∏ 20)
            # –õ–û–ì–ò–ö–ê: 
            # –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ API -> –±–µ—Ä–µ–º –¢–æ–ø-10/20 (–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ).
            # –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –†–£–ß–ù–û–ô -> –±–µ—Ä–µ–º –í–°–ï, —á—Ç–æ –≤–≤–µ–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å (—Ö–æ—Ç—å 11, —Ö–æ—Ç—å 25).
            if "API" in current_source_val:
                final_clean_data = clean_data_pool[:user_target_top_n]
            else:
                final_clean_data = clean_data_pool # –ë–µ—Ä–µ–º –≤—Å–µ—Ö –≤—ã–∂–∏–≤—à–∏—Ö
            
            final_clean_targets = [{'url': d['url'], 'pos': d['pos']} for d in final_clean_data]
            
            # 5. –§–ò–ù–ê–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ (–¢–æ–ª—å–∫–æ –ø–æ —ç–ª–∏—Ç–µ)
            results_final = calculate_metrics(final_clean_data, my_data, settings, my_serp_pos, final_clean_targets)
            st.session_state.analysis_results = results_final
            
            # --- –û—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ (–Ω–µ–π–º–∏–Ω–≥, —Å–µ–º–∞–Ω—Ç–∏–∫–∞) ---
            naming_df = calculate_naming_metrics(final_clean_data, my_data, settings)
            st.session_state.naming_table_df = naming_df 
            st.session_state.ideal_h1_result = analyze_ideal_name(final_clean_data)
            st.session_state.analysis_done = True
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ (–ø–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º)
            res = st.session_state.analysis_results
            words_to_check = [x['word'] for x in res.get('missing_semantics_high', [])]
            if not words_to_check:
                st.session_state.categorized_products = []; st.session_state.categorized_services = []
                st.session_state.categorized_commercial = []; st.session_state.categorized_dimensions = []
            else:
                with st.spinner("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏..."):
                    categorized = classify_semantics_with_api(words_to_check, YANDEX_DICT_KEY)
                
                st.session_state.categorized_products = categorized['products']
                st.session_state.categorized_services = categorized['services']
                st.session_state.categorized_commercial = categorized['commercial']
                st.session_state.categorized_geo = categorized['geo']
                st.session_state.categorized_dimensions = categorized['dimensions']
                st.session_state.categorized_general = categorized['general']
                st.session_state.categorized_sensitive = categorized['sensitive']

                st.session_state.orig_products = categorized['products'] + categorized['sensitive']
                st.session_state.orig_services = categorized['services'] + categorized['sensitive']
                st.session_state.orig_commercial = categorized['commercial'] + categorized['sensitive']
                st.session_state.orig_geo = categorized['geo'] + categorized['sensitive']
                st.session_state.orig_dimensions = categorized['dimensions'] + categorized['sensitive']
                st.session_state.orig_general = categorized['general'] + categorized['sensitive']
                
                st.session_state['sensitive_words_input_final'] = "\n".join(categorized['sensitive'])

            all_found_products = st.session_state.categorized_products
            count_prods = len(all_found_products)
            if count_prods < 20:
                st.session_state.auto_tags_words = all_found_products
                st.session_state.auto_promo_words = []
            else:
                half_count = int(math.ceil(count_prods / 2))
                st.session_state.auto_tags_words = all_found_products[:half_count]
                st.session_state.auto_promo_words = all_found_products[half_count:]
            
            st.session_state['tags_products_edit_final'] = "\n".join(st.session_state.auto_tags_words)
            st.session_state['promo_keywords_area_final'] = "\n".join(st.session_state.auto_promo_words)

            # === –§–ò–ù–ê–õ–¨–ù–´–ô –®–¢–†–ò–•: –ê–í–¢–û-–ü–ï–†–ï–ö–õ–Æ–ß–ï–ù–ò–ï ===
            # 1. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—è–µ–º —Ä–∞–¥–∏–æ-–∫–Ω–æ–ø–∫—É –Ω–∞ "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"
            st.session_state['force_radio_switch'] = True
            
            # 2. –ï—Å–ª–∏ –º—ã –∑–∞–ø—É—Å–∫–∞–ª–∏ —á–µ—Ä–µ–∑ API, —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤ –ø–æ–ª–µ "–†—É—á–Ω–æ–π –≤–≤–æ–¥" –ø–æ–ø–∞–ª–∏ —Å—Å—ã–ª–∫–∏
            # (good_urls –º—ã –ø–æ–ª—É—á–∏–ª–∏ —á—É—Ç—å –≤—ã—à–µ –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞)
            if "API" in current_source_val:
                 clean_urls_final = [d['url'] for d in final_clean_data]
                 st.session_state['persistent_urls'] = "\n".join(clean_urls_final)
                 # –ï—Å–ª–∏ –±—ã–ª–∏ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ, –æ–Ω–∏ —É–∂–µ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ 'excluded_urls_auto' –≤—ã—à–µ
            
            st.rerun()

# ------------------------------------------
# TAB 2: WHOLESALE GENERATOR (COMBINED)
# ------------------------------------------
with tab_wholesale_main:
    st.header("üè≠ –ï–¥–∏–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    
    # ... (–û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –≤–∫–ª–∞–¥–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
    # ==========================================
    # 0. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• (–ò–ó –¢–ï–ö–£–©–ï–ì–û –°–û–°–¢–û–Ø–ù–ò–Ø)
    # ==========================================
    cat_products = st.session_state.get('categorized_products', [])
    cat_services = st.session_state.get('categorized_services', [])
    
    # 1. –î–ª—è –¢–µ–≥–æ–≤ –∏ –ü—Ä–æ–º–æ
    structure_keywords = cat_products + cat_services
    count_struct = len(structure_keywords)

    if 'auto_tags_words' in st.session_state and st.session_state.auto_tags_words:
         tags_list_source = st.session_state.auto_tags_words
         promo_list_source = st.session_state.auto_promo_words
    else:
         if count_struct > 0:
            if count_struct < 10:
                tags_list_source = structure_keywords
                promo_list_source = []
            elif count_struct < 30:
                mid = math.ceil(count_struct / 2)
                tags_list_source = structure_keywords[:mid]
                promo_list_source = structure_keywords[mid:]
            else:
                part = math.ceil(count_struct / 3)
                tags_list_source = structure_keywords[:part]
                promo_list_source = structure_keywords[part:part*2]
         else:
             tags_list_source = []
             promo_list_source = []
    
    # –î–µ—Ñ–æ–ª—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å–∞–π–¥–±–∞—Ä–∞
    sidebar_default_text = ""
    if count_struct >= 30 and 'auto_tags_words' not in st.session_state:
         part = math.ceil(count_struct / 3)
         sidebar_default_text = "\n".join(structure_keywords[part*2:])

    tags_default_text = ", ".join(tags_list_source)
    promo_default_text = ", ".join(promo_list_source)

    # 2. –î–ª—è –¢–∞–±–ª–∏—Ü (–†–∞–∑–º–µ—Ä—ã/–ì–û–°–¢)
    cat_dimensions = st.session_state.get('categorized_dimensions', [])
    tech_context_default = ", ".join(cat_dimensions) if cat_dimensions else ""

    # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ö–æ–º–º–µ—Ä—Ü–∏–∏/–û–±—â–∏—Ö –∏ –ì–ï–û
    cat_commercial = st.session_state.get('categorized_commercial', [])
    cat_general = st.session_state.get('categorized_general', [])
    cat_geo = st.session_state.get('categorized_geo', [])
    
    # –ò–°–ö–õ–Æ–ß–ê–ï–ú –ì–ï–û –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    text_context_list_raw = cat_commercial + cat_general
    text_context_default = ", ".join(text_context_list_raw)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ñ–æ–ª—Ç –¥–ª—è –ì–ï–û –±–ª–æ–∫–∞
    geo_context_default = ", ".join(cat_geo)

    # --- –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ê–ö–¢–ò–í–ù–û–°–¢–ò –ú–û–î–£–õ–ï–ô ---
    # –ï—Å–ª–∏ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –Ω–µ –ø—É—Å—Ç -> —Å—Ç–∞–≤–∏–º –≥–∞–ª–æ—á–∫—É True, –∏–Ω–∞—á–µ False
    auto_check_text = bool(text_context_list_raw)
    auto_check_tags = bool(tags_list_source)
    auto_check_tables = bool(cat_dimensions)
    auto_check_promo = bool(promo_list_source)
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–∞–π–¥–±–∞—Ä –≤–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–ª—è –Ω–µ–≥–æ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç
    auto_check_sidebar = bool(sidebar_default_text.strip())
    
    auto_check_geo = bool(cat_geo)

    # ==========================================
    # 1. –í–í–û–î–ù–´–ï –î–ê–ù–ù–´–ï
    # ==========================================
    with st.container(border=True):
        st.subheader("1. –ò—Å—Ç–æ—á–Ω–∏–∫ –∏ –î–æ—Å—Ç—É–ø—ã")
        
        col_source, col_key = st.columns([3, 1])
        
        use_manual_html = st.checkbox("üìù –í—Å—Ç–∞–≤–∏—Ç—å HTML –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", key="cb_manual_html_mode", value=False)
        
        with col_source:
            if use_manual_html:
                manual_html_source = st.text_area(
                    "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (HTML)", 
                    height=200, 
                    placeholder="<html>...</html>", 
                    help="–°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Å—é–¥–∞ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."
                )
                main_category_url = None
            else:
                main_category_url = st.text_input(
                    "URL –ö–∞—Ç–µ–≥–æ—Ä–∏–∏", 
                    placeholder="https://site.ru/catalog/...", 
                    help="–°–∫—Ä–∏–ø—Ç —Å–æ–±–µ—Ä–µ—Ç —Ç–æ–≤–∞—Ä—ã —Å —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
                )
                manual_html_source = None

        with col_key:
            default_key = st.session_state.get('pplx_key_cache', "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")
            pplx_api_key = st.text_input("AI API Key", value=default_key, type="password")
            if pplx_api_key: st.session_state.pplx_key_cache = pplx_api_key

    # ==========================================
    # 2. –í–´–ë–û–† –ú–û–î–£–õ–ï–ô
    # ==========================================
    st.subheader("2. –ö–∞–∫–∏–µ –±–ª–æ–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º?")
    st.info("‚ÑπÔ∏è **–ê–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞:** –ì–∞–ª–æ—á–∫–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ç–∞–º, –≥–¥–µ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—à–ª–∏—Å—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–ª–æ–≤–∞. –í—ã –º–æ–∂–µ—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –≤—ã–±–æ—Ä –≤—Ä—É—á–Ω—É—é.")
    col_ch1, col_ch2, col_ch3, col_ch4, col_ch5, col_ch6 = st.columns(6)
    
    # –í—Å—Ç–∞–≤–ª—è–µ–º –∞–≤—Ç–æ-–∑–Ω–∞—á–µ–Ω–∏—è –≤ value=...
    with col_ch1: use_text = st.checkbox("ü§ñ AI –¢–µ–∫—Å—Ç—ã", value=auto_check_text)
    with col_ch2: use_tags = st.checkbox("üè∑Ô∏è –¢–µ–≥–∏", value=auto_check_tags)
    with col_ch3: use_tables = st.checkbox("üß© –¢–∞–±–ª–∏—Ü—ã", value=auto_check_tables)
    with col_ch4: use_promo = st.checkbox("üî• –ü—Ä–æ–º–æ", value=auto_check_promo)
    with col_ch5: use_sidebar = st.checkbox("üìë –°–∞–π–¥–±–∞—Ä", value=auto_check_sidebar)
    with col_ch6: use_geo = st.checkbox("üåç –ì–µ–æ-–±–ª–æ–∫", value=auto_check_geo)

    # ==========================================
    # 3. –ù–ê–°–¢–†–û–ô–ö–ò –ú–û–î–£–õ–ï–ô
    # ==========================================
    global_tags_list = []
    global_promo_list = []
    global_sidebar_list = []
    global_geo_list = []
    tags_file_content = ""
    table_prompts = []
    df_db_promo = None
    promo_title = "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º"
    sidebar_content = ""
    text_context_final_list = []
    tech_context_final_str = ""
    
    # –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–ª–æ–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞ (–ø–æ –¥–µ—Ñ–æ–ª—Ç—É 5)
    num_text_blocks_val = 5 

    if any([use_text, use_tags, use_tables, use_promo, use_sidebar, use_geo]):
        st.subheader("3. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥—É–ª–µ–π")

        # --- AI TEXT ---
        if use_text:
            with st.container(border=True):
                st.markdown("#### ü§ñ 1. AI –¢–µ–∫—Å—Ç—ã")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–±–æ—Ä –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–ª–æ–∫–æ–≤
                col_txt1, col_txt2 = st.columns([1, 4])
                with col_txt1:
                    num_text_blocks_val = st.selectbox("–ö–æ–ª-–≤–æ –±–ª–æ–∫–æ–≤", [1, 2, 3, 4, 5], index=4, key="sb_num_blocks")
                
                with col_txt2:
                    ai_words_input = st.text_area(
                        "–°–ª–æ–≤–∞ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è (–ö–æ–º–º–µ—Ä—Ü–∏—è + –û–±—â–∏–µ)", 
                        value=text_context_default, 
                        height=100, 
                        key="ai_text_context_editable",
                        help="–≠—Ç–∏ —Å–ª–æ–≤–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –ø–æ—Å—Ç–∞—Ä–∞–µ—Ç—Å—è –≤–Ω–µ–¥—Ä–∏—Ç—å –≤ —Ç–µ–∫—Å—Ç."
                    )
                
                text_context_final_list = [x.strip() for x in re.split(r'[,\n]+', ai_words_input) if x.strip()]

        # --- TAGS ---
        if use_tags:
            with st.container(border=True):
                st.markdown("#### üè∑Ô∏è 2. –¢–µ–≥–∏")
                kws_input_tags = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=tags_default_text, 
                    height=100, 
                    key="kws_tags_auto"
                )
                global_tags_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_tags) if x.strip()]
                if not global_tags_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                col_t1, col_t2 = st.columns([1, 2])
                with col_t1: u_manual = st.checkbox("–°–≤–æ—è –±–∞–∑–∞ —Å—Å—ã–ª–æ–∫ (.txt)", key="cb_tags_vert")
                with col_t2:
                    default_tags_path = "data/links_base.txt"
                    if not u_manual and os.path.exists(default_tags_path):
                        st.success(f"‚úÖ –ë–∞–∑–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (`links_base.txt`)")
                        with open(default_tags_path, "r", encoding="utf-8") as f: tags_file_content = f.read()
                    elif u_manual:
                        up_t = st.file_uploader("–§–∞–π–ª .txt", type=["txt"], key="up_tags_vert", label_visibility="collapsed")
                        if up_t: tags_file_content = up_t.getvalue().decode("utf-8")
                    else: st.error("‚ùå –§–∞–π–ª –±–∞–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω!")

# --- –§–£–ù–ö–¶–ò–Ø –ì–õ–£–ë–û–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê –ö–û–ù–¢–ï–ö–°–¢–ê –î–õ–Ø –¢–ê–ë–õ–ò–¶ ---
        def generate_context_aware_headers(count, query, dimensions_list, general_list):
            """
            –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ò –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (—Ä–∞–∑–º–µ—Ä—ã, –æ–±—â–∏–µ), 
            —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —Ç–∏–ø—ã —Ç–∞–±–ª–∏—Ü –Ω—É–∂–Ω—ã.
            """
            query_lower = query.lower()
            
            # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–∫–∏ —Å–ª–æ–≤ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            dims_str = " ".join(dimensions_list).lower()
            gen_str = " ".join(general_list).lower()
            full_context = f"{dims_str} {gen_str} {query_lower}"
            
            # --- 1. –î–ï–¢–ï–ö–¢–û–†–´ –°–ò–ì–ù–ê–õ–û–í (–ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Å–µ–º–∞–Ω—Ç–∏–∫–µ) ---
            
            # –ü—Ä–∏–∑–Ω–∞–∫ —Ä–∞–∑–º–µ—Ä–æ–≤: –µ—Å—Ç—å —Ü–∏—Ñ—Ä—ã —Å '—Ö' (10—Ö20), —Å–ª–æ–≤–∞ –º–º, –∫–≥, —Ç–æ–Ω–Ω–∞, —Ä–∞–∑–º–µ—Ä
            has_sizes_signal = (
                len(dimensions_list) > 0 or 
                bool(re.search(r'\d+[x—Ö*]\d+', full_context)) or 
                any(x in full_context for x in ['—Ä–∞–∑–º–µ—Ä', '–≥–∞–±–∞—Ä–∏—Ç', '—Ç–æ–ª—â–∏–Ω', '–¥–∏–∞–º–µ—Ç—Ä', '—Ä–∞—Å–∫—Ä–æ–π', '–≤–µ—Å', '–º–∞—Å—Å'])
            )
            
            # –ü—Ä–∏–∑–Ω–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤: –ì–û–°–¢, –û–°–¢, –¢–£, DIN, AISI
            has_gost_signal = any(x in full_context for x in ['–≥–æ—Å—Ç', 'din', 'aisi', 'astm', '—Ç—É ', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç'])
            
            # –ü—Ä–∏–∑–Ω–∞–∫ –º–∞—Ä–æ–∫/–º–∞—Ç–µ—Ä–∏–∞–ª–∞: —Å—Ç–∞–ª—å, —Å–ø–ª–∞–≤, –º–∞—Ä–∫–∞, —Å—Ç.3, 09–≥2—Å
            has_grade_signal = any(x in full_context for x in ['–º–∞—Ä–∫', '—Å–ø–ª–∞–≤', '—Å—Ç–∞–ª—å', '—Å—Ç.', '–º–∞—Ç–µ—Ä–∏–∞–ª', '—Ö–∏–º–∏—á', '—Å–æ—Å—Ç–∞–≤'])
            
            # –ü—Ä–∏–∑–Ω–∞–∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è: –¥–ª—è —á–µ–≥–æ, —Å—Ñ–µ—Ä–∞, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            has_usage_signal = any(x in full_context for x in ['–ø—Ä–∏–º–µ–Ω–µ–Ω', '—Å—Ñ–µ—Ä', '–Ω–∞–∑–Ω–∞—á–µ–Ω', '–∏—Å–ø–æ–ª—å–∑'])

            # --- 2. –°–ë–û–†–ö–ê –û–ß–ï–†–ï–î–ò (PRIORITY QUEUE) ---
            # –ú—ã —Ä–∞—Å—Å—Ç–∞–≤–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—ã –≤ –ø–æ—Ä—è–¥–∫–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
            priority_stack = []
            
            # –ï—Å–ª–∏ —ç—Ç–æ –º–µ—Ç–∞–ª–ª–æ–ø—Ä–æ–∫–∞—Ç (–µ—Å—Ç—å –º–∞—Ä–∫–∏/—Å–ø–ª–∞–≤—ã), –æ–±—ã—á–Ω–æ —Å–Ω–∞—á–∞–ª–∞ —Å—Ç–∞–≤—è—Ç –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–ª–∏ –ú–∞—Ä–∫–∏
            if has_grade_signal:
                priority_stack.append("–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã")
                
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã - —ç—Ç–æ —Å—É–ø–µ—Ä –≤–∞–∂–Ω–æ, —Å—Ç–∞–≤–∏–º –≤ –Ω–∞—á–∞–ª–æ
            if has_sizes_signal:
                # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –ú–∞—Ä–∫–∏, —Ç–æ –†–∞–∑–º–µ—Ä—ã –≤—Ç–æ—Ä—ã–º–∏. –ï—Å–ª–∏ –Ω–µ—Ç - –ø–µ—Ä–≤—ã–º–∏.
                priority_stack.append("–¢–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤")
                
            # –ï—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ì–û–°–¢–æ–≤
            if has_gost_signal:
                priority_stack.append("–ì–û–°–¢—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã")
                
            # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—Ä–æ —Ö–∏–º —Å–æ—Å—Ç–∞–≤ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ)
            if "—Ö–∏–º" in full_context and "—Å–æ—Å—Ç–∞–≤" in full_context:
                 # –í—Å—Ç–∞–≤–ª—è–µ–º "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤" –≤–º–µ—Å—Ç–æ "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã" –∏–ª–∏ —Ä—è–¥–æ–º
                 if "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã" in priority_stack:
                     idx = priority_stack.index("–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã")
                     priority_stack.insert(idx+1, "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤")
                 else:
                     priority_stack.append("–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤")

            # --- 3. –ó–ê–ü–û–õ–ù–ï–ù–ò–ï –ü–£–°–¢–û–¢ (DEFAULTS) ---
            # –ï—Å–ª–∏ –º—ã –≤—ã–±—Ä–∞–ª–∏ 5 —Ç–∞–±–ª–∏—Ü, –∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞—à–ª–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ 2, –Ω—É–∂–Ω–æ –¥–æ–±–∏—Ç—å –æ—Å—Ç–∞–ª—å–Ω–æ–µ
            defaults = [
                "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞
                "–°–≤–æ–π—Å—Ç–≤–∞",
                "–°—Ñ–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è",
                "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–¥–µ–ª–∏—è",
                "–ê–Ω–∞–ª–æ–≥–∏"
            ]
            
            final_headers = []
            # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ, —á—Ç–æ –Ω–∞—à–ª–∏ —É–º–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
            for p in priority_stack:
                if p not in final_headers: final_headers.append(p)
            
            # –î–æ–±–∏–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏
            for d in defaults:
                if d not in final_headers: final_headers.append(d)
                
            # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –≤—Å—ë —Ä–∞–≤–Ω–æ –º–∞–ª–æ (—Ä–µ–¥–∫–∏–π —Å–ª—É—á–∞–π)
            while len(final_headers) < count:
                final_headers.append("–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏")
                
            return final_headers[:count]

        # --- –ë–õ–û–ö –ò–ù–¢–ï–†–§–ï–ô–°–ê TABLES ---
        if use_tables:
            with st.container(border=True):
                st.markdown("#### üß© 3. –¢–∞–±–ª–∏—Ü—ã")
                
                # –î–ê–ù–ù–´–ï –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê (–ë–µ—Ä–µ–º –∏–∑ session_state, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞)
                raw_query = st.session_state.get('query_input', '')
                found_dims = st.session_state.get('categorized_dimensions', []) # –°–ª–æ–≤–∞—Ä—å —Ä–∞–∑–º–µ—Ä–æ–≤
                found_general = st.session_state.get('categorized_general', []) # –°–ª–æ–≤–∞—Ä—å –æ–±—â–∏—Ö —Å–ª–æ–≤
                
                col_ctx, col_cnt = st.columns([3, 1]) 
                
                with col_ctx:
                    tech_context_final_str = st.text_area(
                        "–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ç–∞–±–ª–∏—Ü (–ú–∞—Ä–∫–∏, –ì–û–°–¢, –†–∞–∑–º–µ—Ä—ã)", 
                        value=tech_context_default, # –ó–¥–µ—Å—å –ª–µ–∂–∞—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
                        height=68, 
                        key="table_context_editable",
                        help="–≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–º–æ–≥—É—Ç AI —Å–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É."
                    )
                
                with col_cnt:
                    cnt_options = [1, 2, 3, 4, 5]
                    cnt = st.selectbox("–ö–æ–ª-–≤–æ —Ç–∞–±–ª–∏—Ü", cnt_options, index=1, key="num_tbl_vert_select")

                # --- –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê ---
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢–û–ì–û, –ß–¢–û –ù–ê–®–õ–ò –í –°–ï–ú–ê–ù–¢–ò–ö–ï
                smart_headers_list = generate_context_aware_headers(cnt, raw_query, found_dims, found_general)

                table_presets = [
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–°–≤–æ–π—Å—Ç–≤–∞", "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–¥–µ–ª–∏—è",
                    "–û–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ", "–¢–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤", "–°–æ—Ä—Ç–∞–º–µ–Ω—Ç",
                    "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤", "–§–∏–∑–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞", "–ú–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞",
                    "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã", "–°–æ—Å—Ç–∞–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞", "–ì–û–°–¢—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã",
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –ì–û–°–¢", "–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ",
                    "–°—Ñ–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", "–£—Å–ª–æ–≤–∏—è —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏", "–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è",
                    "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", "–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏", "–ê–Ω–∞–ª–æ–≥–∏",
                    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π", "–†–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç–∏"
                ]
                
                table_prompts = []
                st.write("") 
                
                cols = st.columns(cnt)
                
                for i, col in enumerate(cols):
                    with col:
                        st.caption(f"**–¢–∞–±–ª–∏—Ü–∞ {i+1}**")
                        
                        # –ê–≤—Ç–æ-–≤—ã–±–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
                        suggested_topic = smart_headers_list[i]
                        
                        try: default_idx = table_presets.index(suggested_topic)
                        except: default_idx = 0
                        
                        is_manual = st.checkbox("–°–≤–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫", key=f"cb_tbl_manual_{i}")
                        
                        if is_manual:
                            selected_topic = st.text_input(
                                f"–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª. {i+1}", value="", 
                                key=f"tbl_topic_custom_{i}", label_visibility="collapsed"
                            )
                            if not selected_topic.strip(): selected_topic = "–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏" 
                        else:
                            selected_topic = st.selectbox(
                                f"–¢–µ–º–∞ —Ç–∞–±–ª. {i+1}", 
                                table_presets, 
                                index=default_idx, # <--- –£–ú–ù–´–ô –ò–ù–î–ï–ö–°
                                key=f"tbl_topic_select_{i}",
                                label_visibility="collapsed"
                            )
                        
                        table_prompts.append(selected_topic)

# --- PROMO (–° –ê–ù–ê–õ–ò–ó–û–ú –ö–û–ú–ú–ï–†–ß–ï–°–ö–ò–• –§–ê–ö–¢–û–†–û–í) ---
        if use_promo:
            with st.container(border=True):
                st.markdown("#### üî• 4. –ü—Ä–æ–º–æ-–±–ª–æ–∫")
                
                kws_input_promo = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=promo_default_text, 
                    height=100, 
                    key="kws_promo_auto"
                )
                global_promo_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_promo) if x.strip()]
                if not global_promo_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                
                col_p1, col_p2 = st.columns([1, 2])
                with col_p1:
                    promo_presets = [
                        "–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ", "–ü–æ—Ö–æ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã", "–í–∞—Å –º–æ–∂–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å",
                        "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º", "–î—Ä—É–≥–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", "–í–∞–º –º–æ–∂–µ—Ç –ø—Ä–∏–≥–æ–¥–∏—Ç—å—Å—è",
                        "–¢–∞–∫–∂–µ –≤ —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ", "–° —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º –ø–æ–∫—É–ø–∞—é—Ç", "–ß–∞—Å—Ç–æ –ø–æ–∫—É–ø–∞—é—Ç –≤–º–µ—Å—Ç–µ",
                        "–°–æ–ø—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–≤–∞—Ä—ã", "–•–∏—Ç—ã –ø—Ä–æ–¥–∞–∂", "–í—ã–±–æ—Ä –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π",
                        "–õ–∏–¥–µ—Ä—ã —Å–ø—Ä–æ—Å–∞", "–ü–æ–ø—É–ª—è—Ä–Ω–æ–µ —Å–µ–π—á–∞—Å", "–¢–æ–ø —Ç–æ–≤–∞—Ä–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏",
                        "–õ—É—á—à–∞—è —Ü–µ–Ω–∞", "–°–ø–µ—Ü–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", "–£—Å–ø–µ–π—Ç–µ –∑–∞–∫–∞–∑–∞—Ç—å",
                        "–ù–µ –∑–∞–±—É–¥—å—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å", "–í—ã –Ω–µ–¥–∞–≤–Ω–æ —Å–º–æ—Ç—Ä–µ–ª–∏"
                    ]

                    # --- –õ–û–ì–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê –ö–û–ú–ú–ï–†–¶–ò–ò ---
                    # –ë–µ—Ä–µ–º –∑–∞–ø—Ä–æ—Å + —Å–ø–∏—Å–æ–∫ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ (Tab 1)
                    raw_query = st.session_state.get('query_input', '').lower()
                    comm_words = st.session_state.get('categorized_commercial', [])
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                    comm_context = f"{raw_query} {' '.join(comm_words)}".lower()
                    
                    target_header = "–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ" # –î–µ—Ñ–æ–ª—Ç (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π)

                    # 1. –Ø–≤–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è (–µ—Å—Ç—å —Å–ª–æ–≤–∞ '—Ü–µ–Ω–∞', '–∫—É–ø–∏—Ç—å' –≤ —Å–µ–º–∞–Ω—Ç–∏–∫–µ –∏–ª–∏ –∑–∞–ø—Ä–æ—Å–µ)
                    is_commercial = any(x in comm_context for x in ["–∫—É–ø–∏—Ç—å", "—Ü–µ–Ω–∞", "–∑–∞–∫–∞–∑", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–ø—Ä–∞–π—Å", "–º–∞–≥–∞–∑–∏–Ω", "–∫–æ—Ä–∑–∏–Ω–∞"])
                    
                    # 2. –ê–∫—Ü–∏–æ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
                    is_promo = any(x in comm_context for x in ["–∞–∫—Ü–∏—è", "—Å–∫–∏–¥–∫", "—Ä–∞—Å–ø—Ä–æ–¥–∞–∂", "–≤—ã–≥–æ–¥–Ω"])
                    
                    # 3. –†–µ–π—Ç–∏–Ω–≥–æ–≤—ã–µ —Å–ª–æ–≤–∞
                    is_top = any(x in comm_context for x in ["—Ç–æ–ø", "–ª—É—á—à", "—Ä–µ–π—Ç–∏–Ω–≥", "–ø–æ–ø—É–ª—è—Ä–Ω"])

                    if is_promo:
                        target_header = "–°–ø–µ—Ü–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"
                    elif is_top:
                        target_header = "–õ–∏–¥–µ—Ä—ã —Å–ø—Ä–æ—Å–∞"
                    elif is_commercial:
                        # –ï—Å–ª–∏ —ç—Ç–æ —è–≤–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è, –ª—É—á—à–µ "–ü–æ–∫—É–ø–∞—é—Ç –≤–º–µ—Å—Ç–µ" –∏–ª–∏ "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º"
                        target_header = "–° —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º –ø–æ–∫—É–ø–∞—é—Ç"
                    
                    try: promo_smart_idx = promo_presets.index(target_header)
                    except: promo_smart_idx = 0

                    use_custom_header = st.checkbox("–í–≤–µ—Å—Ç–∏ —Å–≤–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫", key="cb_custom_header")
                    
                    if use_custom_header:
                        promo_title = st.text_input("–í–∞—à –∑–∞–≥–æ–ª–æ–≤–æ–∫", placeholder="–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ", key="pr_tit_vert")
                    else:
                        promo_title = st.selectbox(
                            "–í–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–≥–æ–ª–æ–≤–∫–∞", 
                            promo_presets, 
                            index=promo_smart_idx, # <--- –£–ú–ù–´–ô –í–´–ë–û–†
                            key="promo_header_select"
                        )

                    st.markdown("<br>", unsafe_allow_html=True)
                    u_img_man = st.checkbox("–°–≤–æ—è –±–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫", key="cb_img_vert")

                with col_p2:
                    default_img_db = "data/images_db.xlsx"
                    if not u_img_man and os.path.exists(default_img_db):
                        st.success("‚úÖ –ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ (`images_db.xlsx`)")
                        try: df_db_promo = pd.read_excel(default_img_db)
                        except: pass
                    elif u_img_man:
                        up_i = st.file_uploader("–§–∞–π–ª .xlsx", type=['xlsx'], key="up_img_vert", label_visibility="collapsed")
                        if up_i: df_db_promo = pd.read_excel(up_i)
                    else: st.error("‚ùå –ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")

        # --- SIDEBAR ---
        if use_sidebar:
            with st.container(border=True):
                st.markdown("#### üìë 5. –°–∞–π–¥–±–∞—Ä")
                kws_input_sidebar = st.text_area(
                    "–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏", 
                    value=sidebar_default_text, 
                    height=100, 
                    key="kws_sidebar_auto"
                )
                global_sidebar_list = [x.strip() for x in kws_input_sidebar.split('\n') if x.strip()]
                if not global_sidebar_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                
                st.markdown("---")
                col_s1, col_s2 = st.columns([1, 2])
                with col_s1: u_sb_man = st.checkbox("–°–≤–æ–π —Ñ–∞–π–ª –º–µ–Ω—é (.txt)", key="cb_sb_vert")
                with col_s2:
                    def_menu = "data/menu_structure.txt"
                    if not u_sb_man and os.path.exists(def_menu):
                        st.success("‚úÖ –ú–µ–Ω—é —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (`menu_structure.txt`)")
                        with open(def_menu, "r", encoding="utf-8") as f: sidebar_content = f.read()
                    elif u_sb_man:
                        up_s = st.file_uploader("–§–∞–π–ª .txt", type=['txt'], key="up_sb_vert", label_visibility="collapsed")
                        if up_s: sidebar_content = up_s.getvalue().decode("utf-8")
                    else: st.error("‚ùå –§–∞–π–ª –º–µ–Ω—é –Ω–µ –Ω–∞–π–¥–µ–Ω!")

        # --- GEO BLOCK ---
        if use_geo:
            with st.container(border=True):
                st.markdown("#### üåç 6. –ì–µ–æ-–±–ª–æ–∫")
                kws_input_geo = st.text_area(
                    "–°–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤/—Ä–µ–≥–∏–æ–Ω–æ–≤ (–∏–∑ –≤–∫–ª–∞–¥–∫–∏ –ê–Ω–∞–ª–∏–∑) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", 
                    value=geo_context_default, 
                    height=100, 
                    key="kws_geo_auto"
                )
                global_geo_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_geo) if x.strip()]
                
                if not global_geo_list:
                    st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤ –ø—É—Å—Ç!")
                else:
                    st.info(f"–ë—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ç–µ–∫—Å—Ç –¥–æ—Å—Ç–∞–≤–∫–∏ –¥–ª—è –ø–æ–ª—è IP_PROP4819 —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º —ç—Ç–∏—Ö –≥–æ—Ä–æ–¥–æ–≤.")

    st.markdown("---")
    
# ==========================================
    # 4. –ó–ê–ü–£–°–ö –ì–ï–ù–ï–†–ê–¶–ò–ò (–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–û–ò–°–ö –°–°–´–õ–û–ö)
    # ==========================================
    
    ready_to_go = True
    
    if use_manual_html:
        if not manual_html_source: ready_to_go = False
    else:
        if not main_category_url: ready_to_go = False

    if (use_text or use_tables) and not pplx_api_key: ready_to_go = False
    # –£–±–∏—Ä–∞–µ–º –∂–µ—Å—Ç–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∑–¥–µ—Å—å, —Ç–∞–∫ –∫–∞–∫ –ø–æ–¥–≥—Ä—É–∑–∏–º —Ñ–∞–π–ª—ã –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –Ω–∏–∂–µ
    # if use_tags and not tags_file_content: ready_to_go = False 
    if use_promo and df_db_promo is None: ready_to_go = False
    if use_geo and not pplx_api_key: ready_to_go = False
    
    if st.button("üöÄ –ó–ê–ü–£–°–¢–ò–¢–¨ –ì–ï–ù–ï–†–ê–¶–ò–Æ", type="primary", disabled=not ready_to_go, use_container_width=True):
        # === –û–ß–ò–°–¢–ö–ê –ü–†–ï–î–´–î–£–©–ò–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
        st.session_state.gen_result_df = None
        st.session_state.unified_excel_data = None
        # ======================================
        
        status_box = st.status("üõ†Ô∏è –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...", expanded=True)
        final_data = [] 
        
        # 1. –°–ë–û–† –ò–°–•–û–î–ù–´–• –°–°–´–õ–û–ö –ò –ë–ê–ó (–ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê)
        
        # --- –ë–∞–∑–∞ –¢–µ–≥–æ–≤ (links_base.txt) ---
        tags_map = {}
        all_tags_links = []
        if use_tags:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –∏–∑ UI (–µ—Å–ª–∏ –∑–∞–≥—Ä—É–∂–∞–ª–∏ –≤—Ä—É—á–Ω—É—é)
            if tags_file_content:
                s_io = io.StringIO(tags_file_content)
                all_tags_links = [l.strip() for l in s_io.readlines() if l.strip()]
            # –ï—Å–ª–∏ –ø—É—Å—Ç–æ, –ø—Ä–æ–±—É–µ–º —á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª —Å –¥–∏—Å–∫–∞ –Ω–∞–ø—Ä—è–º—É—é
            elif os.path.exists("data/links_base.txt"):
                with open("data/links_base.txt", "r", encoding="utf-8") as f:
                    all_tags_links = [l.strip() for l in f.readlines() if l.strip()]
            
            # --- –£–ú–ù–´–ô –ü–û–ò–°–ö (Smart Matching) ---
            for kw in global_tags_list:
                # 1. –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                
                # 2. –í—ã–¥–µ–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω—è (–æ–±—Ä–µ–∑–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ 'gibkiy' –≤ 'gibkaya')
                search_roots = {tr} # –ò—Å—Ö–æ–¥–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
                if len(tr) > 5: 
                    search_roots.add(tr[:-1]) # –º–∏–Ω—É—Å 1 –±—É–∫–≤–∞
                    search_roots.add(tr[:-2]) # –º–∏–Ω—É—Å 2 –±—É–∫–≤—ã (iy, yy, ay)
                elif len(tr) > 4:
                    search_roots.add(tr[:-1])

                # 3. –ò—â–µ–º –ª—é–±–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω—è –≤ —Å—Å—ã–ª–∫–∞—Ö
                matches = []
                for u in all_tags_links:
                    u_lower = u.lower()
                    for root in search_roots:
                        if root in u_lower:
                            matches.append(u)
                            break # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –ø–æ –æ–¥–Ω–æ–º—É –∫–æ—Ä–Ω—é, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥. —Å—Å—ã–ª–∫–µ
                
                if matches: tags_map[kw] = matches

        # --- –ë–∞–∑–∞ –ü—Ä–æ–º–æ (images_db.xlsx) ---
        p_img_map = {}
        if use_promo and df_db_promo is not None:
            for _, row in df_db_promo.iterrows():
                u = str(row.iloc[0]).strip(); img = str(row.iloc[1]).strip()
                if u and u != 'nan' and img and img != 'nan': p_img_map[u.rstrip('/')] = img
        
        # --- –ë–∞–∑–∞ –°–∞–π–¥–±–∞—Ä–∞ (menu_structure.txt) ---
        all_menu_urls = []
        if use_sidebar:
            # –°–Ω–∞—á–∞–ª–∞ –∏–∑ UI
            if sidebar_content:
                s_io = io.StringIO(sidebar_content)
                all_menu_urls = [l.strip() for l in s_io.readlines() if l.strip()]
            # –ò–Ω–∞—á–µ —Å –¥–∏—Å–∫–∞
            elif os.path.exists("data/menu_structure.txt"):
                with open("data/menu_structure.txt", "r", encoding="utf-8") as f:
                    all_menu_urls = [l.strip() for l in f.readlines() if l.strip()]

        # =========================================================
        # üî• –õ–û–ì–ò–ö–ê –ü–û–ò–°–ö–ê –ü–û–¢–ï–†–Ø–ù–ù–´–• –°–õ–û–í (–û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø)
        # =========================================================
        missing_words_log = set()
        
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¢–ï–ì–ò
        if use_tags:
            for kw in global_tags_list:
                if kw not in tags_map: 
                    missing_words_log.add(kw)
        
        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ü–†–û–ú–û (–¢–æ–∂–µ —É–º–Ω—ã–π –ø–æ–∏—Å–∫)
        if use_promo:
            for kw in global_promo_list:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ—Ä–Ω–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
                roots = [tr]
                if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                
                has_match = False
                for u in p_img_map.keys():
                    if any(r in u for r in roots):
                        has_match = True
                        break
                
                if not has_match:
                    missing_words_log.add(kw)
                    
        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –°–ê–ô–î–ë–ê–† (–¢–æ–∂–µ —É–º–Ω—ã–π –ø–æ–∏—Å–∫)
        if use_sidebar and global_sidebar_list:
            for kw in global_sidebar_list:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                roots = [tr]
                if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                
                has_match = False
                for u in all_menu_urls:
                    if any(r in u for r in roots):
                        has_match = True
                        break
                
                if not has_match:
                    missing_words_log.add(kw)

        # 4. –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –ü–ï–†–ï–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï
        if missing_words_log:
            missing_list = sorted(list(missing_words_log))
            
            # –ê. –î–æ–±–∞–≤–ª—è–µ–º –≤ –¢–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            for w in missing_list:
                if w not in text_context_final_list:
                    text_context_final_list.append(w)
            
            # –ë. –î–æ–±–∞–≤–ª—è–µ–º –≤ –¢–∞–±–ª–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            tech_additions = []
            for w in missing_list:
                # –ï—Å–ª–∏ —Ü–∏—Ñ—Ä–∞, –ì–û–°–¢ –∏–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞
                if any(char.isdigit() for char in w) or any(x in w.lower() for x in ['–≥–æ—Å—Ç', '—Ç–∏–ø', '—Ñ–æ—Ä–º–∞', '–º–º', '–∫–≥']):
                    tech_additions.append(w)
            
            if tech_additions:
                tech_context_final_str += "\n" + ", ".join(tech_additions)

            # –í. –ü–õ–ê–®–ö–ê
            status_box.markdown(f"""
                <div style="background-color: #FFF4E5; border-left: 5px solid #FF9800; padding: 15px; border-radius: 4px; margin-bottom: 15px; color: #663C00;">
                    <strong>‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –ß–∞—Å—Ç—å —Å—Å—ã–ª–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞</strong><br>
                    <span style="font-size: 0.9em;">
                    –ú—ã –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–ª—è: <b>{', '.join(missing_list)}</b>.<br>
                    ‚úÖ <u>–û–Ω–∏ –±—ã–ª–∏ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã –≤ –¢–ó –¥–ª—è –ù–µ–π—Ä–æ—Å–µ—Ç–∏ (–±—É–¥—É—Ç –≤ —Ç–µ–∫—Å—Ç–µ/—Ç–∞–±–ª–∏—Ü–∞—Ö).</u>
                    </span>
                </div>
            """, unsafe_allow_html=True)
            time.sleep(2)

        # =========================================================
        # –î–ê–õ–ï–ï –°–¢–ê–ù–î–ê–†–¢–ù–ê–Ø –õ–û–ì–ò–ö–ê (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô –í –°–¢–†–£–ö–¢–£–†–ï)
        # =========================================================

        target_pages = []
        soup = None
        current_base_url = main_category_url if main_category_url else "http://localhost"

        try:
            if use_manual_html:
                status_box.write("üìÇ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º HTML –∫–æ–¥...")
                soup = BeautifulSoup(manual_html_source, 'html.parser')
            else:
                status_box.write(f"üïµÔ∏è –°–∫–∞–Ω–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {main_category_url}")
                session = requests.Session()
                retry = Retry(connect=3, read=3, redirect=3, backoff_factor=0.5)
                adapter = HTTPAdapter(max_retries=retry)
                session.mount('http://', adapter)
                session.mount('https://', adapter)
                
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
                r = session.get(main_category_url, headers=headers, timeout=30, verify=False)
                
                if r.status_code == 200:
                    soup = BeautifulSoup(r.text, 'html.parser')
                else: 
                    status_box.error(f"–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞: {r.status_code}")
                    st.stop()
            
            if soup:
                tags_container = soup.find(class_='popular-tags-inner')
                if tags_container:
                    for link in tags_container.find_all('a'):
                        href = link.get('href')
                        name = link.get_text(strip=True)
                        if href and name:
                            full_url = urljoin(current_base_url, href)
                            target_pages.append({'url': full_url, 'name': name})
                
                if not target_pages:
                    status_box.warning("–¢–µ–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–∫–ª–∞—Å—Å .popular-tags-inner). –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.")
                    h1 = soup.find('h1')
                    name = h1.get_text(strip=True) if h1 else "–¢–æ–≤–∞—Ä"
                    target_pages.append({'url': current_base_url, 'name': name})
                    
        except Exception as e: 
            status_box.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            st.stop()
            
        # –°–±–æ—Ä –∏–º–µ–Ω –¥–ª—è —Å—Å—ã–ª–æ–∫
        urls_to_fetch_names = set()
        promo_items_pool = []  # <--- –î–û–ë–ê–í–õ–ï–ù–ê –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø
        
        if use_tags:
            for kw, matches in tags_map.items():
                urls_to_fetch_names.update(matches)

        if use_promo:
            used_urls = set()
            for kw in global_promo_list:
                if kw in missing_words_log: continue
                
                # –ü–æ–≤—Ç–æ—Ä—è–µ–º —É–º–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                roots = [tr]
                if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                
                matches = []
                # –ò—â–µ–º –≤ keys() –∫–∞—Ä—Ç—ã –∫–∞—Ä—Ç–∏–Ω–æ–∫
                for u in p_img_map.keys():
                    if any(r in u for r in roots): matches.append(u)

                for m in matches:
                    if m not in used_urls:
                        urls_to_fetch_names.add(m)
                        # –¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ—à–∏–±–∫–∏ –Ω–µ –±—É–¥–µ—Ç
                        promo_items_pool.append({'url': m, 'img': p_img_map[m]})
                        used_urls.add(m)

        sidebar_matched_urls = []
        if use_sidebar:
            if global_sidebar_list:
                for kw in global_sidebar_list:
                    if kw in missing_words_log: continue
                    
                    tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                    roots = [tr]
                    if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                    
                    found = []
                    for u in all_menu_urls:
                        if any(r in u for r in roots): found.append(u)
                    
                    sidebar_matched_urls.extend(found)
                sidebar_matched_urls = list(set(sidebar_matched_urls))
            else:
                sidebar_matched_urls = all_menu_urls
            
            urls_to_fetch_names.update(sidebar_matched_urls)

        # --- –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –ò–ú–ï–ù ---
        url_name_cache = {}
        if urls_to_fetch_names:
            status_box.write(f"üåç –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è {len(urls_to_fetch_names)} —Å—Å—ã–ª–æ–∫...")
            
            def fetch_name_worker(u): 
                return u, get_breadcrumb_only(u) 
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
                future_to_url = {executor.submit(fetch_name_worker, u): u for u in urls_to_fetch_names}
                done_cnt = 0
                prog_fetch = status_box.progress(0)
                for future in concurrent.futures.as_completed(future_to_url):
                    u_res, name_res = future.result()
                    norm_key = u_res.rstrip('/')
                    
                    if name_res:
                        url_name_cache[norm_key] = name_res
                    else:
                        slug = norm_key.split('/')[-1]
                        url_name_cache[norm_key] = force_cyrillic_name_global(slug)
                    
                    done_cnt += 1
                    prog_fetch.progress(done_cnt / len(urls_to_fetch_names))
            
            status_box.write("‚úÖ –ù–∞–∑–≤–∞–Ω–∏—è —Å–æ–±—Ä–∞–Ω—ã!")

        # ------------------------------------------------------------------
        # –°–ë–û–†–ö–ê –ö–û–ù–¢–ï–ù–¢–ê
        # ------------------------------------------------------------------
        
        full_sidebar_code = ""
        if use_sidebar:
            status_box.write("üî® –°–±–æ—Ä–∫–∞ –º–µ–Ω—é...")
            tree = {}
            for url in sidebar_matched_urls:
                path = urlparse(url).path.strip('/')
                parts = [p for p in path.split('/') if p]
                idx_start = 0
                if 'catalog' in parts: idx_start = parts.index('catalog') + 1
                rel_parts = parts[idx_start:] if parts[idx_start:] else parts
                
                curr = tree
                for i, part in enumerate(rel_parts):
                    if part not in curr: curr[part] = {}
                    if i == len(rel_parts) - 1:
                        curr[part]['__url__'] = url
                        cache_key = url.rstrip('/')
                        curr[part]['__name__'] = url_name_cache.get(cache_key, force_cyrillic_name_global(part))
                    curr = curr[part]
            
            def render_tree_internal(node, level=1):
                html = ""
                keys = sorted([k for k in node.keys() if not k.startswith('__')])
                for key in keys:
                    child = node[key]
                    name = child.get('__name__', force_cyrillic_name_global(key))
                    url = child.get('__url__')
                    has_children = any(k for k in child.keys() if not k.startswith('__'))
                    
                    if level == 1:
                        html += '<li class="level-1-header">\n'
                        if has_children:
                            html += f'    <span class="dropdown-toggle">{name}</span>\n'
                            html += '    <ul class="collapse-menu list-unstyled">\n'
                            html += render_tree_internal(child, level=2)
                            html += '    </ul>\n'
                        else:
                            target = url if url else "#"
                            html += f'    <a href="{target}">{name}</a>\n'
                        html += '</li>\n'
                    elif level == 2:
                        if has_children:
                            html += '<li class="level-2-header">\n'
                            html += f'    <span class="dropdown-toggle">{name}</span>\n'
                            html += '    <ul class="collapse-menu list-unstyled">\n'
                            html += render_tree_internal(child, level=3)
                            html += '    </ul>\n'
                        else:
                            target = url if url else "#"
                            html += f'<li class="level-2-link-special"><a href="{target}">{name}</a></li>\n'
                    elif level >= 3:
                        target = url if url else "#"
                        html += f'<li class="level-3-link"><a href="{target}">{name}</a></li>\n'
                return html

            inner_html = render_tree_internal(tree, level=1)
            full_sidebar_code = f"""<div class="page-content-with-sidebar"><button id="mobile-menu-toggle" class="menu-toggle-button">‚ò∞</button><div class="sidebar-wrapper"><nav id="sidebar-menu"><ul class="list-unstyled components">{inner_html}</ul></nav></div></div>"""

        client = None
        if openai and (use_text or use_tables or use_geo):
            client = openai.OpenAI(api_key=pplx_api_key, base_url="https://api.perplexity.ai")

# --- –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú ---
        progress_bar = status_box.progress(0)
        total_steps = len(target_pages)
        
        for idx, page in enumerate(target_pages):
            base_text_raw, tags_on_page, real_header_h2, err = get_page_data_for_gen(page['url'])
            header_for_ai = real_header_h2 if real_header_h2 else page['name']
            
            row_data = {'Page URL': page['url'], 'Product Name': header_for_ai}
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç–∏–∫—É
            for k, v in STATIC_DATA_GEN.items(): row_data[k] = v
            
            # ========================================================
            # 1. –°–ù–ê–ß–ê–õ–ê –ì–ï–ù–ï–†–ò–†–£–ï–ú –í–ò–ó–£–ê–õ–¨–ù–´–ï –ë–õ–û–ö–ò (TAGS / PROMO)
            # –ß—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —á—Ç–æ –Ω–µ –≤–ª–µ–∑–ª–æ –∏ –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ —ç—Ç–æ –≤ —Ç–µ–∫—Å—Ç
            # ========================================================
            
            # –ö–æ–ø–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≠–¢–û–ô —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            # –ú—ã –±—É–¥–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å —Å—é–¥–∞ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–ª—É—á–∏–ª–∏—Å—å –≤ —Ç–µ–≥–∞—Ö
            current_page_seo_words = list(text_context_final_list)
            
            # --- TAGS GENERATION (–ë–ï–ó –õ–ò–ú–ò–¢–û–í + FALLBACK) ---
            tags_html_parts = []
            if use_tags:
                html_collector = []
                for kw in global_tags_list:
                    # 1. –ï—Å–ª–∏ —Å–ª–æ–≤–∞ –≤–æ–æ–±—â–µ –Ω–µ—Ç –≤ –±–∞–∑–µ - –æ–Ω–æ —É–∂–µ –≤ current_page_seo_words (–±–ª–∞–≥–æ–¥–∞—Ä—è –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ)
                    if kw not in tags_map:
                        continue 
                        
                    urls = tags_map[kw]
                    # 2. –ò—â–µ–º —Å—Å—ã–ª–∫—É, –∫–æ—Ç–æ—Ä–∞—è –ù–ï –≤–µ–¥–µ—Ç –Ω–∞ —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                    valid_urls = [u for u in urls if u.rstrip('/') != page['url'].rstrip('/')]
                    
                    if valid_urls:
                        # –£–°–ü–ï–•: –î–µ–ª–∞–µ–º —Ç–µ–≥
                        selected_url = random.choice(valid_urls)
                        cache_key = selected_url.rstrip('/')
                        nm = url_name_cache.get(cache_key, kw) # –ï—Å–ª–∏ –∏–º–µ–Ω–∏ –Ω–µ—Ç, –±–µ—Ä–µ–º –∫–µ–π–≤–æ—Ä–¥
                        html_collector.append(f'<a href="{selected_url}" class="tag-link">{nm}</a>')
                    else:
                        # –ù–ï–£–î–ê–ß–ê: –°—Å—ã–ª–∫–∞ –µ—Å—Ç—å, –Ω–æ –æ–Ω–∞ –≤–µ–¥–µ—Ç –Ω–∞ —Å–∞–º—É —Å–µ–±—è (valid_urls –ø—É—Å—Ç)
                        # –ó–Ω–∞—á–∏—Ç, —Ç–µ–≥ –º—ã –Ω–µ –ø–æ—Å—Ç–∞–≤–∏–ª–∏. –ß—Ç–æ–±—ã —Å–ª–æ–≤–æ –Ω–µ –ø—Ä–æ–ø–∞–ª–æ -> –∫–∏–¥–∞–µ–º –≤ –¢–ï–ö–°–¢
                        if kw not in current_page_seo_words:
                            current_page_seo_words.append(kw)

                if html_collector:
                    tags_html_parts = ['<div class="popular-tags">'] + html_collector + ['</div>']
                    row_data['Tags HTML'] = "\n".join(tags_html_parts)
                else:
                    row_data['Tags HTML'] = ""

# --- PROMO GENERATION (–ì–û–†–ò–ó–û–ù–¢–ê–õ–¨–ù–´–ô –°–ö–†–û–õ–õ) ---
            if use_promo:
                candidates = [p for p in promo_items_pool if p['url'].rstrip('/') != page['url'].rstrip('/')]
                
                # –ë–µ—Ä–µ–º –í–°–ï –Ω–∞–π–¥–µ–Ω–Ω—ã–µ (–±–µ–∑ –ª–∏–º–∏—Ç–æ–≤)
                random.shuffle(candidates)
                selected_promo = candidates
                
                if selected_promo:
                    # –ö–û–ù–¢–ï–ô–ù–ï–†:
                    # flex-wrap: nowrap -> –∑–∞–ø—Ä–µ—â–∞–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å –Ω–∞ –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
                    # overflow-x: auto -> –≤–∫–ª—é—á–∞–µ—Ç —Å–∫—Ä–æ–ª–ª
                    promo_html = f'<div class="promo-section"><h3>{promo_title}</h3><div class="promo-grid" style="display: flex; flex-wrap: nowrap; gap: 15px; overflow-x: auto; padding-bottom: 15px; scrollbar-width: thin;">'
                    
                    for item in selected_promo:
                        p_url = item['url']
                        p_img = item['img']
                        cache_key = p_url.rstrip('/')
                        p_name = url_name_cache.get(cache_key, "–¢–æ–≤–∞—Ä")
                        
                        # –ö–ê–†–¢–û–ß–ö–ê: 
                        # flex-shrink: 0 -> –∑–∞–ø—Ä–µ—â–∞–µ—Ç –∫–∞—Ä—Ç–æ—á–∫–µ —Å–∂–∏–º–∞—Ç—å—Å—è, –∑–∞—Å—Ç–∞–≤–ª—è—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å–∫—Ä–æ–ª–ª–∏—Ç—å—Å—è
                        promo_html += f'<div class="promo-card" style="min-width: 220px; width: 220px; flex-shrink: 0; border: 1px solid #eee; padding: 10px; border-radius: 5px; text-align: center;">'
                        promo_html += f'<a href="{p_url}" style="text-decoration: none; color: #333;">'
                        promo_html += f'<div style="height: 150px; overflow: hidden; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;">'
                        promo_html += f'<img src="{p_img}" alt="{p_name}" style="max-height: 100%; max-width: 100%; object-fit: contain;">'
                        promo_html += f'</div>'
                        promo_html += f'<div style="font-size: 13px; font-weight: bold; line-height: 1.3;">{p_name}</div>'
                        promo_html += f'</a></div>'

                    promo_html += '</div></div>'
                    row_data['Promo HTML'] = promo_html
                else:
                    row_data['Promo HTML'] = ""

            # ========================================================
            # 2. –ì–ï–ù–ï–†–ò–†–£–ï–ú –¢–ï–ö–°–¢ (–° –£–ß–ï–¢–û–ú –í–°–ï–• "–ü–û–¢–ï–†–Ø–®–ï–ö")
            # ========================================================
            if use_text and client:
                try:
                    # current_page_seo_words —Ç–µ–ø–µ—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç:
                    # 1. –¢–æ, —á—Ç–æ –≤–≤–µ–ª —é–∑–µ—Ä —Ä—É–∫–∞–º–∏
                    # 2. –¢–æ, —á—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω–æ –Ω–µ –Ω–∞—à–ª–æ—Å—å –≤ –±–∞–∑–µ
                    # 3. –¢–æ, —á—Ç–æ –ª–æ–∫–∞–ª—å–Ω–æ –Ω–µ —Å–º–æ–≥–ª–æ —Å—Ç–∞—Ç—å —Ç–µ–≥–æ–º (—Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–µ–±—è)
                    blocks = generate_ai_content_blocks(
                        client, base_text=base_text_raw if base_text_raw else "", 
                        tag_name=page['name'], forced_header=header_for_ai,
                        num_blocks=num_text_blocks_val, 
                        seo_words=current_page_seo_words # <-- –ü–û–õ–ù–´–ô –°–ü–ò–°–û–ö
                    )
                    row_data['Text_Block_1'] = blocks[0]
                    row_data['Text_Block_2'] = blocks[1]
                    row_data['Text_Block_3'] = blocks[2]
                    row_data['Text_Block_4'] = blocks[3]
                    row_data['Text_Block_5'] = blocks[4]
                except Exception as e: row_data['Text_Error'] = str(e)

            # --- AI TABLES (–ö–æ–Ω—Ç–µ–∫—Å—Ç —Ç–æ—Ç –∂–µ, –≥–ª–æ–±–∞–ª—å–Ω—ã–π) ---
            if use_tables and client:
                for t_i, t_topic in enumerate(table_prompts):
                    sys_p_table = "You are an expert metallurgist and data analyst. Output ONLY raw HTML <table>. No markdown."
                    context_hint = ""
                    if tech_context_final_str:
                        context_hint = f"–ò—Å–ø–æ–ª—å–∑—É–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (–º–∞—Ä–∫–∏, –ì–û–°–¢—ã): {tech_context_final_str}."
                    
                    usr_p_table = f"""
                    –ó–∞–¥–∞—á–∞: –°–æ—Å—Ç–∞–≤—å –ø–æ–¥—Ä–æ–±–Ω—É—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é —Ç–∞–±–ª–∏—Ü—É –¥–ª—è —Ç–æ–≤–∞—Ä–∞ "{header_for_ai}".
                    –¢–µ–º–∞ —Ç–∞–±–ª–∏—Ü—ã: {t_topic}.
                    {context_hint}
                    
                    –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
                    1. –¢–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ.
                    2. HTML <table>...</table>.
                    3. –ë–µ–∑ Markdown.
                    """
                    try:
                        resp = client.chat.completions.create(
                            model="sonar-pro", 
                            messages=[
                                {"role": "system", "content": sys_p_table},
                                {"role": "user", "content": usr_p_table}
                            ], 
                            temperature=0.4
                        )
                        raw_html = resp.choices[0].message.content
                        clean_html = raw_html.replace("```html", "").replace("```", "").strip()
                        clean_html = re.sub(r'\[\d+\]', '', clean_html)
                        
                        soup_table = BeautifulSoup(clean_html, 'html.parser')
                        table_tag = soup_table.find('table')
                        if table_tag:
                            table_tag['style'] = "border-collapse: collapse; width: 100%; border: 2px solid black;"
                            for cell in table_tag.find_all(['th', 'td']):
                                cell['style'] = "border: 2px solid black; padding: 5px;"
                            final_table_html = str(table_tag)
                        else: final_table_html = clean_html
                        row_data[f'Table_{t_i+1}_HTML'] = final_table_html
                    except Exception as e:
                        row_data[f'Table_{t_i+1}_HTML'] = f"Error: {e}"

            # --- SIDEBAR ---
            if use_sidebar:
                row_data['Sidebar HTML'] = full_sidebar_code

            # --- GEO ---
            if use_geo and client and global_geo_list:
                selected_cities = global_geo_list
                if len(selected_cities) > 20: selected_cities = random.sample(global_geo_list, 20)
                cities_str = ", ".join(selected_cities)
                geo_prompt = f"""Task: Write a short paragraph <p> about delivery options for "{header_for_ai}" to {cities_str}. Output HTML <p> only."""
                try:
                    resp_geo = client.chat.completions.create(
                        model="sonar-pro", 
                        messages=[{"role": "system", "content": "You are a logistic summary generator."}, {"role": "user", "content": geo_prompt}],
                        temperature=0.4
                    )
                    clean_geo = resp_geo.choices[0].message.content.replace("```html", "").replace("```", "").strip()
                    row_data['IP_PROP4819'] = clean_geo
                except Exception as e: row_data['IP_PROP4819'] = f"Error: {e}"

            final_data.append(row_data)
            progress_bar.progress((idx + 1) / total_steps)

        # --- –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ---
        df_result = pd.DataFrame(final_data)
        st.session_state.gen_result_df = df_result 
        
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            df_result.to_excel(writer, index=False)
        
        st.session_state.unified_excel_data = buffer.getvalue()
        
        status_box.update(label="‚úÖ –ö–æ–Ω–≤–µ–π–µ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω! –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã.", state="complete", expanded=False)

    if st.session_state.get('unified_excel_data') is not None:
        st.success("–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω!")
        st.download_button(
            label="üì• –°–ö–ê–ß–ê–¢–¨ –ï–î–ò–ù–´–ô EXCEL",
            data=st.session_state.unified_excel_data,
            file_name="unified_content_gen.xlsx",
            mime="application/vnd.ms-excel",
            key="btn_dl_unified"
        )
# ==========================================
# 5. –ë–õ–û–ö –ü–†–ï–î–ü–†–û–°–ú–û–¢–†–ê (PREVIEW) - –§–ò–ù–ê–õ–¨–ù–´–ô
# ==========================================
with tab_wholesale_main: 
    if 'gen_result_df' in st.session_state and st.session_state.gen_result_df is not None:
        st.markdown("---")
        st.header("üëÄ –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
        
        df = st.session_state.gen_result_df
        
        # 1. –í—ã–±–æ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        page_options = df['Product Name'].tolist()
        selected_page_name = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞:", page_options, key="preview_selector")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–æ–∫—É –¥–∞–Ω–Ω—ã—Ö
        row = df[df['Product Name'] == selected_page_name].iloc[0]
        
        # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
        has_text = any(
            (f'Text_Block_{i}' in row and pd.notna(row[f'Text_Block_{i}']) and str(row[f'Text_Block_{i}']).strip())
            for i in range(1, 6)
        )
        
        table_cols = [c for c in df.columns if 'Table_' in c and '_HTML' in c and pd.notna(row[c]) and str(row[c]).strip()]
        has_tables = len(table_cols) > 0
        
        has_tags = 'Tags HTML' in row and pd.notna(row['Tags HTML']) and str(row['Tags HTML']).strip()
        has_sidebar = 'Sidebar HTML' in row and pd.notna(row['Sidebar HTML']) and str(row['Sidebar HTML']).strip()
        has_geo = 'IP_PROP4819' in row and pd.notna(row['IP_PROP4819']) and str(row['IP_PROP4819']).strip()
        
        # --- –ü–†–û–í–ï–†–ö–ê –ü–†–û–ú–û ---
        has_promo = 'Promo HTML' in row and pd.notna(row['Promo HTML']) and str(row['Promo HTML']).strip()
        
        has_visual = has_tags or has_sidebar or has_geo or has_promo # <-- –î–æ–±–∞–≤–∏–ª–∏ –ø—Ä–æ–º–æ –≤ —É—Å–ª–æ–≤–∏–µ

        # 3. –ê–∫—Ç–∏–≤–Ω—ã–µ –≤–∫–ª–∞–¥–∫–∏
        active_tabs = []
        if has_text: active_tabs.append("üìù –¢–µ–∫—Å—Ç")
        if has_tables: active_tabs.append("üß© –¢–∞–±–ª–∏—Ü—ã")
        if has_visual: active_tabs.append("üé® –í–∏–∑—É–∞–ª")

        # –°—Ç–∏–ª–∏
        st.markdown("""
        <style>
            .preview-box { border: 1px solid #e0e0e0; padding: 20px; border-radius: 8px; background: #fff; margin-bottom: 20px; }
            .preview-label { font-size: 12px; font-weight: bold; color: #888; text-transform: uppercase; margin-bottom: 5px; }
            .popular-tags { display: flex; flex-wrap: wrap; gap: 8px; }
            .tag-link { background: #f0f2f5; color: #333; padding: 5px 10px; border-radius: 4px; text-decoration: none; font-size: 13px; }
            table { width: 100%; border-collapse: collapse; font-size: 14px; }
            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
            th { background-color: #f2f2f2; font-weight: bold; }
            .sidebar-wrapper ul { list-style-type: none; padding-left: 10px; }
            .level-1-header { font-weight: bold; margin-top: 10px; color: #277EFF; }
            /* –°—Ç–∏–ª–∏ –¥–ª—è –∫–∞—Ä—Ç–æ—á–µ–∫ –ü—Ä–æ–º–æ */
            .promo-grid { display: flex !important; flex-wrap: wrap; gap: 10px; }
            .promo-card { width: 23%; box-sizing: border-box; }
            .promo-card img { max-width: 100%; height: auto; }
        </style>
        """, unsafe_allow_html=True)

        if not active_tabs:
            st.warning("‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç –ø—É—Å—Ç.")
        else:
            tabs_objects = st.tabs(active_tabs)
            tabs_map = dict(zip(active_tabs, tabs_objects))
            
            # --- –¢–ï–ö–°–¢ ---
            if "üìù –¢–µ–∫—Å—Ç" in tabs_map:
                with tabs_map["üìù –¢–µ–∫—Å—Ç"]:
                    st.subheader(row['Product Name'])
                    for i in range(1, 6):
                        col_key = f'Text_Block_{i}'
                        if col_key in row and pd.notna(row[col_key]):
                            content = str(row[col_key]).strip()
                            if content:
                                with st.container():
                                    st.caption(f"–ë–ª–æ–∫ {i}")
                                    st.markdown(f"<div class='preview-box'>{content}</div>", unsafe_allow_html=True)

            # --- –¢–ê–ë–õ–ò–¶–´ ---
            if "üß© –¢–∞–±–ª–∏—Ü—ã" in tabs_map:
                with tabs_map["üß© –¢–∞–±–ª–∏—Ü—ã"]:
                    for t_col in table_cols:
                        content = row[t_col]
                        clean_title = t_col.replace('_HTML', '').replace('_', ' ')
                        st.caption(clean_title)
                        st.markdown(content, unsafe_allow_html=True)

            # --- –í–ò–ó–£–ê–õ ---
            if "üé® –í–∏–∑—É–∞–ª" in tabs_map:
                with tabs_map["üé® –í–∏–∑—É–∞–ª"]:
                    # –í—ã–≤–æ–¥ –ü—Ä–æ–º–æ
                    if has_promo:
                         st.markdown('<div class="preview-label">–ü—Ä–æ–º–æ-–±–ª–æ–∫ (–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏)</div>', unsafe_allow_html=True)
                         st.markdown(f"<div class='preview-box'>{row['Promo HTML']}</div>", unsafe_allow_html=True)
                    
                    c1, c2 = st.columns(2)
                    with c1:
                        if has_tags:
                            st.markdown('<div class="preview-label">–¢–µ–≥–∏</div>', unsafe_allow_html=True)
                            st.markdown(f"<div class='preview-box'>{row['Tags HTML']}</div>", unsafe_allow_html=True)
                        if has_geo:
                            st.markdown('<div class="preview-label">–ì–µ–æ-–±–ª–æ–∫</div>', unsafe_allow_html=True)
                            st.markdown(f"<div class='preview-box'>{row['IP_PROP4819']}</div>", unsafe_allow_html=True)
                    with c2:
                        if has_sidebar:
                            st.markdown('<div class="preview-label">–°–∞–π–¥–±–∞—Ä</div>', unsafe_allow_html=True)
                            st.markdown(f"<div class='preview-box' style='max-height: 400px; overflow-y: auto;'>{row['Sidebar HTML']}</div>", unsafe_allow_html=True)

