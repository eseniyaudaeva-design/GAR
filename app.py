import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
from collections import Counter
import math
import inspect
import concurrent.futures
from urllib.parse import urlparse

# ==========================================
# 1. –°–¢–ò–õ–ò–ó–ê–¶–ò–Ø (–°–≤–µ—Ç–ª–æ-–≥–æ–ª—É–±–∞—è —Ç–µ–º–∞)
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO: SEO Analysis", page_icon="üíé")

st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap');
        
        /* 1. –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ñ–æ–Ω –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (–°–≤–µ—Ç–ª–æ-–≥–æ–ª—É–±–æ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç) */
        .stApp {
            background: linear-gradient(135deg, #E0F7FA 0%, #E3F2FD 100%);
            font-family: 'Roboto', sans-serif;
            color: #0F172A; /* –¢–µ–º–Ω–æ-—Å–∏–Ω–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞ */
        }
        
        /* 2. –ó–∞–≥–æ–ª–æ–≤–∫–∏ */
        h1, h2, h3, h4 {
            color: #0277BD !important; /* –ù–∞—Å—ã—â–µ–Ω–Ω—ã–π –≥–æ–ª—É–±–æ–π */
            font-weight: 700;
        }
        
        /* 3. –ë–µ–ª—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (—á—Ç–æ–±—ã —á–∏—Ç–∞–ª–æ—Å—å) */
        .block-container {
            padding-top: 2rem;
        }
        
        /* –°—Ç–∏–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª–µ–π –≤–≤–æ–¥–∞ (–±–µ–ª—ã–π —Ñ–æ–Ω, –≥–æ–ª—É–±–∞—è —Ä–∞–º–∫–∞) */
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] {
            background-color: #FFFFFF !important;
            color: #333333 !important;
            border: 1px solid #81D4FA !important;
            border-radius: 6px;
        }
        
        /* –õ–µ–π–±–ª—ã –Ω–∞–¥ –ø–æ–ª—è–º–∏ */
        .stTextInput label, .stTextArea label, .stSelectbox label, .stRadio label {
            color: #01579B !important;
            font-weight: 600;
        }
        
        /* 4. –ö–Ω–æ–ø–∫–∞ (–Ø—Ä–∫–∞—è, –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–∞—è) */
        div.stButton > button {
            background: linear-gradient(90deg, #0288D1 0%, #01579B 100%);
            color: white !important;
            border: none;
            border-radius: 8px;
            padding: 12px 24px;
            font-weight: bold;
            font-size: 16px;
            box-shadow: 0 4px 10px rgba(2, 136, 209, 0.3);
            transition: 0.3s;
            width: 100%;
        }
        div.stButton > button:hover {
            background: linear-gradient(90deg, #039BE5 0%, #0277BD 100%);
            box-shadow: 0 6px 14px rgba(2, 136, 209, 0.5);
            transform: translateY(-2px);
        }
        
        /* 5. –¢–∞–±–ª–∏—Ü—ã (–ë–µ–ª—ã–π —Ñ–æ–Ω, —á–µ—Ç–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã) */
        div[data-testid="stDataFrame"] {
            background-color: white;
            border: 1px solid #B3E5FC;
            border-radius: 10px;
            padding: 10px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        }
        
        /* 6. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ (Expander) */
        .streamlit-expanderHeader {
            background-color: #E1F5FE !important;
            color: #0277BD !important;
            border: 1px solid #81D4FA;
            border-radius: 8px;
        }
        div[data-testid="stExpander"] {
            background-color: rgba(255,255,255,0.6);
            border-radius: 8px;
        }
        
        /* –£–±–∏—Ä–∞–µ–º –≤–µ—Ä—Ö–Ω—é—é –ø–æ–ª–æ—Å—É –¥–µ–∫–æ—Ä–∞ */
        header[data-testid="stHeader"] {
            background-color: transparent;
        }
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 2. –ë–≠–ö–ï–ù–î (–õ–û–ì–ò–ö–ê - –¢–ê –ñ–ï –°–ê–ú–ê–Ø)
# ==========================================

# --- –ü–∞—Ç—á NLP ---
try:
    if not hasattr(inspect, 'getargspec'):
        def getargspec(func):
            spec = inspect.getfullargspec(func)
            return spec.args, spec.varargs, spec.varkw, spec.defaults
        inspect.getargspec = getargspec
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except:
    morph = None
    USE_NLP = False

# --- –ü–æ–∏—Å–∫ ---
try:
    from googlesearch import search
    USE_SEARCH = True
except:
    USE_SEARCH = False

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
DEFAULT_EXCLUDE = ["yandex.ru", "avito.ru", "ozon.ru", "wildberries.ru", "youtube.com", "dzen.ru", "hh.ru", "t.me", "tiu.ru", "pulscen.ru", "satu.kz"]
DEFAULT_STOPS = ["—Ä—É–±–ª–µ–π", "—Ä—É–±", "–∫—É–ø–∏—Ç—å", "—Ü–µ–Ω–∞", "—à—Ç", "—Å–º", "–º–º", "–∫–≥", "–∫–≤", "–º2", "—Å—Ç—Ä", "—É–ª", "–¥–æ—Å—Ç–∞–≤–∫–∞", "–∑–≤–æ–Ω–∏—Ç–µ", "–∑–∞–∫–∞–∑–∞—Ç—å"]

# --- –§—É–Ω–∫—Ü–∏–∏ –ü–∞—Ä—Å–∏–Ω–≥–∞ ---

def process_text(text, settings, n_gram=1):
    pattern = r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+' if settings['numbers'] else r'[–∞-—è–ê-–Ø—ë–Åa-zA-Z]+'
    words = re.findall(pattern, text.lower())
    stops = set(w.lower() for w in settings['custom_stops'])
    clean_words = []
    
    for w in words:
        if len(w) < 2 or w in stops: continue
        lemma = w
        if USE_NLP and n_gram == 1: 
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            lemma = p.normal_form
        clean_words.append(lemma)
    
    if n_gram > 1:
        ngrams = []
        for i in range(len(clean_words) - n_gram + 1):
            phrase = " ".join(clean_words[i:i+n_gram])
            ngrams.append(phrase)
        return ngrams
    return clean_words

def parse_page(url, settings):
    headers = {'User-Agent': settings['ua']}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code != 200: return None
        soup = BeautifulSoup(r.text, 'html.parser')
        
        title = soup.title.string.strip() if soup.title and soup.title.string else ""
        desc = ""
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc: desc = meta_desc.get("content", "").strip()
        h1 = soup.find("h1").get_text(strip=True) if soup.find("h1") else ""
        
        if settings['noindex']:
            for t in soup.find_all(['noindex', 'script', 'style', 'head', 'footer', 'nav']): t.decompose()
        else:
            for t in soup(['script', 'style', 'head']): t.decompose()
            
        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)
        
        extra_text = []
        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])
        body_text = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        
        return {
            'url': url, 'domain': urlparse(url).netloc, 'title': title, 'desc': desc, 'h1': h1,
            'body_text': body_text, 'anchor_text': anchor_text
        }
    except: return None

# --- –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ ---
def calculate_advanced_metrics(corpus_pages, my_page, settings):
    my_lemmas = process_text(my_page['body_text'], settings)
    my_anchors = process_text(my_page['anchor_text'], settings)
    
    comp_docs = []
    for p in corpus_pages:
        body = process_text(p['body_text'], settings)
        anchor = process_text(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor})
        
    avg_len = np.mean([len(d['body']) for d in comp_docs])
    my_len = len(my_lemmas)
    norm_k = (my_len / avg_len) if (settings['norm'] and avg_len > 0) else 1.0
    
    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    
    N = len(comp_docs)
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
        
    k1, b = 1.2, 0.75
    table_depth, table_hybrid = [], []
    
    for word in vocab:
        df = doc_freqs[word]
        if df < 2 and word not in my_lemmas: continue 
        
        my_tf = my_lemmas.count(word)
        my_anch_tf = my_anchors.count(word)
        comp_tfs = [d['body'].count(word) for d in comp_docs]
        comp_anch_tfs = [d['anchor'].count(word) for d in comp_docs]
        
        med_tf = np.median(comp_tfs)
        mean_tf = np.mean(comp_tfs)
        max_tf = np.max(comp_tfs)
        med_anch = np.median(comp_anch_tfs)
        
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        
        bm25_scores = []
        for i, d in enumerate(comp_docs):
            tf = comp_tfs[i]
            dl = len(d['body'])
            score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (dl / avg_len)))
            bm25_scores.append(score)
        bm25_top = np.median(bm25_scores)
        bm25_my = idf * (my_tf * (k1 + 1)) / (my_tf + k1 * (1 - b + b * (my_len / avg_len)))
        
        target_body = int(med_tf * 1.3 * norm_k)
        diff_body = target_body - my_tf
        
        if med_tf > 0.5 or my_tf > 0:
            table_depth.append({
                "–°–ª–æ–≤–æ": word, "–ü–æ–≤—Ç–æ—Ä—ã —É –≤–∞—Å": my_tf, "–û–±—â–µ–µ –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": diff_body,
                "–¢–µ–≥ A —É –≤–∞—Å": my_anch_tf, "–¢–µ–≥ A —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏": int(med_anch * norm_k),
                "–¢–µ–≥ A –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": int(med_anch * norm_k) - my_anch_tf,
                "–¢–µ–∫—Å—Ç —É –≤–∞—Å": my_tf, "–¢–µ–∫—Å—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏": target_body, "–¢–µ–∫—Å—Ç –î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": diff_body,
                "–ü–µ—Ä–µ—Å–ø–∞–º": int(max_tf * norm_k), "–ü–µ—Ä–µ—Å–ø–∞–º*IDF": round(max_tf * norm_k * idf, 1),
                "diff_abs": abs(diff_body)
            })
            table_hybrid.append({
                "–°–ª–æ–≤–æ": word, "TF-IDF –¢–û–ü": round(med_tf * idf, 2), "TF-IDF –≤–∞—à —Å–∞–π—Ç": round(my_tf * idf, 2),
                "BM25 –¢–û–ü": round(bm25_top, 2), "BM25 –≤–∞—à —Å–∞–π—Ç": round(bm25_my, 2), "IDF": round(idf, 2),
                "–ö–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤": df, "–ú–µ–¥–∏–∞–Ω–∞": round(med_tf, 1), "–ü–µ—Ä–µ—Å–ø–∞–º": max_tf,
                "–°—Ä–µ–¥–Ω–µ–µ –ø–æ –¢–û–ü—É": round(mean_tf, 1), "–í–∞—à —Å–∞–π—Ç": my_tf
            })

    # N-–≥—Ä–∞–º–º—ã
    my_bigrams = process_text(my_page['body_text'], settings, n_gram=2)
    comp_bigrams_list = [process_text(p['body_text'], settings, n_gram=2) for p in corpus_pages]
    all_bigrams = set(my_bigrams)
    for cb in comp_bigrams_list: all_bigrams.update(cb)
    
    bg_freqs = Counter()
    for cb in comp_bigrams_list:
        for bg in set(cb): bg_freqs[bg] += 1
        
    table_ngrams = []
    for bg in all_bigrams:
        df = bg_freqs[bg]
        if df < 2 and bg not in my_bigrams: continue
        my_cnt = my_bigrams.count(bg)
        comp_cnts = [cb.count(bg) for cb in comp_bigrams_list]
        med_cnt = np.median(comp_cnts)
        if med_cnt > 0 or my_cnt > 0:
            table_ngrams.append({
                "N-–≥—Ä–∞–º–º–∞": bg, "–ö–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤": df, "–ú–µ–¥–∏–∞–Ω–∞": med_cnt,
                "–°—Ä–µ–¥–Ω–µ–µ": round(np.mean(comp_cnts), 1), "–ù–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ": my_cnt,
                "TF-IDF": round(my_cnt * math.log(N/df if df>0 else 1), 3)
            })

    table_relevance = []
    for i, p in enumerate(corpus_pages):
        p_lemmas = process_text(p['body_text'], settings)
        common = set(p_lemmas).intersection(vocab)
        table_relevance.append({
            "–î–æ–º–µ–Ω": p['domain'], "–ü–æ–∑–∏—Ü–∏—è": i+1, "–®–∏—Ä–∏–Ω–∞": len(common), "–ì–ª—É–±–∏–Ω–∞": len(p_lemmas)
        })
        
    return {
        "depth": pd.DataFrame(table_depth), "hybrid": pd.DataFrame(table_hybrid),
        "ngrams": pd.DataFrame(table_ngrams), "relevance_top": pd.DataFrame(table_relevance),
        "my_score": {"width": len(set(my_lemmas).intersection(vocab)), "depth": len(my_lemmas)}
    }

# ==========================================
# 3. –ò–ù–¢–ï–†–§–ï–ô–° (UI)
# ==========================================

st.markdown("<h1 style='text-align: center;'>SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏</h1>", unsafe_allow_html=True)

# –ë–ª–æ–∫ –≤–≤–æ–¥–∞ URL –∏ –ó–∞–ø—Ä–æ—Å–∞
st.markdown("#### üìù –í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö")
c1, c2 = st.columns(2)
with c1:
    my_url = st.text_input("–í–∞—à URL", placeholder="https://site.ru")
with c2:
    query = st.text_input("–ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞")

st.markdown("#### üïµÔ∏è –ò—Å—Ç–æ—á–Ω–∏–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
source_type = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫:", ["Google –ü–æ–∏—Å–∫ (–ê–≤—Ç–æ)", "–°–ø–∏—Å–æ–∫ URL –≤—Ä—É—á–Ω—É—é"], horizontal=True, label_visibility="collapsed")

if source_type == "Google –ü–æ–∏—Å–∫ (–ê–≤—Ç–æ)":
    cl1, cl2 = st.columns(2)
    with cl1: top_n = st.selectbox("–ì–ª—É–±–∏–Ω–∞ –¢–û–ü–∞:", [5, 10, 20], index=1)
    with cl2: excludes = st.text_input("–ò—Å–∫–ª—é—á–∏—Ç—å:", " ".join(DEFAULT_EXCLUDE))
else:
    manual_urls = st.text_area("–°–ø–∏—Å–æ–∫ URL (–ø–æ—Å—Ç—Ä–æ—á–Ω–æ):", height=100)

st.markdown("#### ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
with st.expander("–û—Ç–∫—Ä—ã—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏", expanded=False):
    col1, col2, col3 = st.columns(3)
    with col1:
        s_noindex = st.toggle("–ò—Å–∫–ª—é—á–∞—Ç—å noindex", True)
        s_alt = st.toggle("–í–∫–ª—é—á–∞—Ç—å alt/title", False)
    with col2:
        s_norm = st.toggle("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True)
        s_num = st.toggle("–£—á–∏—Ç—ã–≤–∞—Ç—å —Ü–∏—Ñ—Ä—ã", False)
    with col3:
        s_agg = st.toggle("–ë–µ–∑ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–æ–≤", True)
    
    st.markdown("---")
    ua = st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0", "Googlebot/2.1"])
