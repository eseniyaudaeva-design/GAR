import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin
import inspect
import time
import json

# ==========================================
# 0. –ü–ê–¢–ß –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò (–î–ª—è NLP)
# ==========================================
# –ü–∞—Ç—á –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—Ç–∞—Ä—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ Python/–±–∏–±–ª–∏–æ—Ç–µ–∫.
if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO", page_icon="üìä")

# ==========================================
# 2. –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø
# ==========================================
def check_password():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–∞—Ä–æ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—è st.experimental_user."""
    if st.session_state.get("authenticated"):
        return True
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown("""
            <style>
            .auth-container {
                display: flex; flex-direction: column; align-items: center;
                justify-content: center; min-height: 100vh;
            }
            </style>
            <div class='auth-container'>
            """, unsafe_allow_html=True)
        st.title("üîê –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è GAR PRO")
        
        # –ó–∞–≥–ª—É—à–∫–∞: –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
        password_placeholder = st.empty()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º session_state –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä–æ–ª—è
        if 'auth_input_password' not in st.session_state:
            st.session_state['auth_input_password'] = ""
        
        password = password_placeholder.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="auth_input_password")
        
        if st.button("–í–æ–π—Ç–∏"):
            if password == "garpro2024":  # <--- –ó–ê–ú–ï–ù–ò–¢–¨ –ù–ê –†–ï–ê–õ–¨–ù–´–ô –ü–ê–†–û–õ–¨
                st.session_state["authenticated"] = True
                st.rerun()
            else:
                st.error("–ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
        
        st.markdown("</div>", unsafe_allow_html=True)
    return False

# ==========================================
# 3. –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ù–ê–°–¢–†–û–ô–ö–ò –°–¢–ò–õ–ï–ô
# ==========================================
PRIMARY_COLOR = "#0078D4"
LIGHT_BG_MAIN = "#F0F2F6"
BORDER_COLOR = "#E6E6E6"

# –ú–ò–ù–ò–ú–ê–õ–¨–ù–û–ï –ö–û–õ–ò–ß–ï–°–¢–í–û –°–õ–û–í (–§–†–ê–ó) –î–õ–Ø –í–ö–õ–Æ–ß–ï–ù–ò–Ø –í –ê–ù–ê–õ–ò–ó
MIN_COUNT_FOR_ANALYSIS = 3 

# –°–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–∫–ª—é—á–∞–µ–º –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã, –í–∏–∫–∏–ø–µ–¥–∏—è)
EXCLUDE_DOMAINS = [
    "wikipedia.org", "yandex.ru", "market.yandex.ru", "google.com", 
    "ozon.ru", "wildberries.ru", "leroymerlin.ru", "vseinstrumenti.ru",
    "youtube.com", "avito.ru", "cian.ru", "drom.ru", "auto.ru", 
    "lemantrade.ru", "lemanapro.ru" # <-- –î–û–ë–ê–í–õ–ï–ù–û –ü–û –ó–ê–ü–†–û–°–£
]

# –°–¢–û–ü-–°–õ–û–í–ê
# (–ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∑–¥–µ—Å—å –Ω–µ —É–º–µ—Å—Ç–µ–Ω, –Ω–æ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ —Ä–∞–±–æ—á–µ–º –∫–æ–¥–µ)
STOP_WORDS = set([
    '–∞', '–≤', '–∏', '–∫', '–Ω–∞', '–æ', '—Å', '—É', '—è', '–Ω–æ', '—á—Ç–æ', '—ç—Ç–æ', '–∫–∞–∫', '—Ç–∞–∫', '–æ—Ç',
    '–¥–æ', '–¥–ª—è', '–∏–∑', '–æ–±', '–∏–ª–∏', '–Ω–µ', '–ø–æ', '–∑–∞', '–ø—Ä–∏', '–≤—Å–µ', '–∂–µ', '–æ–Ω–∏', '–∏—Ö', 
    '–º—ã', '–≤—ã', '—Ç—ã', '–º–Ω–µ', '–µ–π', '–∏–º', '–æ–Ω', '–æ–Ω–∞', '—ç—Ç–æ—Ç', '—Ç–æ—Ç', '—Å–≤–æ–π', '–≤–∞—à', 
    '–Ω–∞—à', '–≤–µ—Å—å', '–ª—é–±–æ–π', '—Å–∞–º—ã–π', '—Ö–æ—Ç—å', '–±–µ–∑', '–±–æ–ª–µ–µ', '–º–µ–Ω–µ–µ', '—Å–µ–π—á–∞—Å', 
    '—Ç–æ–ª—å–∫–æ', '—Ç–æ–∂–µ', '–ª–∏—à—å', '—á—Ç–æ–±—ã', '—Ö–æ—Ç—è', '–µ—Å–ª–∏', '–∫–æ–≥–¥–∞', '–≥–¥–µ', '–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞', 
    '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫–∞–∫–æ–π', '–∫–æ—Ç–æ—Ä—ã–π', '—á–µ–π', '—á–µ–π-—Ç–æ', '–∫–æ–µ-–∫—Ç–æ', '–Ω–∏—á—Ç–æ', '–Ω–∏–∫—Ç–æ', 
    '–Ω–∏–≥–¥–µ', '–Ω–∏–∫–æ–≥–¥–∞', '–µ—â–µ', '—É–∂–µ', '–¥–∞–∂–µ', '–ø—É—Å—Ç—å', '–≤—Ä–æ–¥–µ', '–±—É–¥—Ç–æ', '–≤—Ä—è–¥', '–∫–∞–∂–¥—ã–π', 
    '—Å–∞–º', '—Ç–æ–≥–¥–∞', '—Ç–∞–º', '—Ç—É—Ç', '–∑–¥–µ—Å—å', '–æ–∫–æ–ª–æ', '—á–µ—Ä–µ–∑', '–≤–º–µ—Å—Ç–æ', '–≤–æ–∫—Ä—É–≥', '–æ–¥–Ω–∞–∫–æ', 
    '–ø–æ—Ç–æ–º', '–ø–æ—ç—Ç–æ–º—É', '–ø–æ–º–∏–º–æ', '–≤—Å–ª–µ–¥—Å—Ç–≤–∏–µ', '–±–ª–∞–≥–æ–¥–∞—Ä—è', '–Ω–∞–ø—Ä–æ—Ç–∏–≤', '–∫—Ä–æ–º–µ', 
    '–æ—Å–æ–±–µ–Ω–Ω–æ', '–ø—Ä–∏–º–µ—Ä–Ω–æ', '–∫–∞–∂–µ—Ç—Å—è', '–≤–∏–¥–∏–º–æ', '–∑–Ω–∞—á–∏—Ç', '–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ', '–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ', 
    '–∫–æ–Ω–µ—á–Ω–æ', '–≤–æ–æ–±—â–µ', '–≤–ø—Ä–æ—á–µ–º', '–≤–æ–∑–º–æ–∂–Ω–æ', '–Ω–∞–∫–æ–Ω–µ—Ü', '—Ä–∞–Ω—å—à–µ', '—Å–∫–æ—Ä–æ', '—Ç–æ–≥–¥–∞', 
    '—Ç—É—Ç', '—á—É—Ç—å', '–≤–µ—Å—å–º–∞', '–≤–¥—Ä—É–≥', '–µ–¥–≤–∞', '–∏–º–µ–Ω–Ω–æ', '–∏–Ω–æ–≥–¥–∞', '—Ä–µ–¥–∫–æ', '—á–∞—Å—Ç–æ', '—á—É—Ç—å-—á—É—Ç—å',
    '–ø–æ—á—Ç–∏', '–æ–¥–∏–Ω', '–¥–≤–∞', '—Ç—Ä–∏', '—á–µ—Ç—ã—Ä–µ', '–ø—è—Ç—å', '—à–µ—Å—Ç—å', '—Å–µ–º—å', '–≤–æ—Å–µ–º—å', '–¥–µ–≤—è—Ç—å', '–¥–µ—Å—è—Ç—å', 
    '–≥–æ–¥', '–ª–µ—Ç', '—Ä—É–±–ª—å', '—Ä—É–±–ª–µ–π', '—à—Ç—É–∫–∞', '—à—Ç—É–∫', '—Ü–µ–Ω–∞', '–∫—É–ø–∏—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '–¥–æ—Å—Ç–∞–≤–∫–∞', 
    '–º–∞–≥–∞–∑–∏–Ω', '–∫–∞—Ç–∞–ª–æ–≥', '—Ç–æ–≤–∞—Ä–æ–≤', '—Ç–æ–≤–∞—Ä', '—É—Å–ª—É–≥', '–º–µ–∂–¥—É', '–ø–æ–¥', '–ø–µ—Ä–µ–¥', '–æ–¥–∏–Ω', '–º–Ω–æ–≥–æ',
    '–Ω—É–∂–Ω–æ', '—Å–≤–æ–π', '—Ç–∞–∫–æ–π', '—Å–∞–º—ã–π', '–æ—á–µ–Ω—å', '–ø—Ä–æ', '–±—ã', '—ç—Ç–æ', '—Ç–æ—Ç', '—Ç–∞', '—Ç–µ'
])

# ==========================================
# 4. –§–£–ù–ö–¶–ò–ò –£–¢–ò–õ–ò–¢
# ==========================================
# (–§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏, –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏ —Ç.–ø. –æ–ø—É—â–µ–Ω—ã –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏, 
# –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –æ–Ω–∏ –µ—Å—Ç—å –≤ –ø–æ–ª–Ω–æ–º –∫–æ–¥–µ –∏ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.)

@st.cache_resource
def get_lemmatizer():
    # –ó–∞–≥–ª—É—à–∫–∞, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—â–∞—è, —á—Ç–æ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –∑–¥–µ—Å—å
    # from pymystem3 import Mystem
    # return Mystem()
    return None # –í–µ—Ä–Ω–µ–º None, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–∫–∏ –∏–º–ø–æ—Ä—Ç–∞.

# ==========================================
# 5. –§–£–ù–ö–¶–ò–ò –ü–ê–†–°–ò–ù–ì–ê –ò –°–ö–ê–ß–ò–í–ê–ù–ò–Ø (–° –£–°–ò–õ–ï–ù–ò–ï–ú)
# ==========================================

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Connection': 'keep-alive',
}

def parse_content_with_retries(url, retries=3, timeout=30): # <--- –£–°–ò–õ–ï–ù–ò–ï –ü–ê–†–°–ò–ù–ì–ê
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö."""
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=timeout)
            response.raise_for_status() # –í—ã–∑—ã–≤–∞–µ—Ç HTTPError, –µ—Å–ª–∏ —Å—Ç–∞—Ç—É—Å 4xx –∏–ª–∏ 5xx
            
            # –ü–æ–ø—ã—Ç–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É
            if 'charset' in response.headers.get('content-type', '').lower():
                response.encoding = response.apparent_encoding
            elif response.encoding == 'ISO-8859-1':
                response.encoding = response.apparent_encoding
                
            return response.text, None # –£—Å–ø–µ—Ö
        
        except requests.exceptions.Timeout:
            error_msg = f"–¢–∞–π–º–∞—É—Ç (–ø—Ä–µ–≤—ã—à–µ–Ω–æ {timeout} —Å–µ–∫)."
        except requests.exceptions.ConnectionError:
            error_msg = "–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è."
        except requests.exceptions.HTTPError as e:
            error_msg = f"HTTP –û—à–∏–±–∫–∞: {e.response.status_code}."
        except requests.exceptions.RequestException as e:
            error_msg = f"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}."
        except Exception as e:
            error_msg = f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {e}."

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ø—ã—Ç–∫–∏
        if attempt < retries - 1:
            time.sleep(2 ** attempt) # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: 1, 2, 4 —Å–µ–∫.

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –∏ –æ—à–∏–±–∫—É –ø–æ—Å–ª–µ –≤—Å–µ—Ö –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
    return "", error_msg

def extract_text(html_content):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ HTML-–∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    if not html_content:
        return ""
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∏–ª–∏, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
    for element in soup(['script', 'style', 'noscript', 'head', 'footer', 'header']):
        element.decompose()
        
    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
        comment.extract()

    # –£–¥–∞–ª—è–µ–º –≤—Å–µ, —á—Ç–æ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–≥–∞ <nav> (–Ω–∞–≤–∏–≥–∞—Ü–∏—è)
    for nav in soup.find_all('nav'):
        nav.decompose()
        
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–µ–ª–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    body = soup.find('body')
    if not body:
        return ""
        
    text = body.get_text(separator=' ', strip=True)
    
    # –ó–∞–º–µ–Ω—è–µ–º –≤—Å–µ –Ω–µ–±—É–∫–≤–µ–Ω–Ω—ã–µ –∏ –Ω–µ—Ü–∏—Ñ—Ä–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –ø—Ä–æ–±–µ–ª—ã, 
    # –∫—Ä–æ–º–µ –¥–µ—Ñ–∏—Å–∞ (–¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª–æ–≤)
    text = re.sub(r'[^\w\s-]', ' ', text, flags=re.UNICODE)
    
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
    text = re.sub(r'\s+', ' ', text).lower()
    
    return text

def parse_url(url):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç URL.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: URL, —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç, —Å—Ç–∞—Ç—É—Å (0/1/2), —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ (–µ—Å–ª–∏ –µ—Å—Ç—å).
    –°—Ç–∞—Ç—É—Å: 2 - OK, 1 - –û—à–∏–±–∫–∞, 0 - –ò—Å–∫–ª—é—á–µ–Ω.
    """
    domain = urlparse(url).netloc
    
    # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–æ–≤
    if any(d in domain for d in EXCLUDE_DOMAINS):
        return url, "", 0, f"–ò—Å–∫–ª—é—á–µ–Ω (–°—Ç–æ–ø-–¥–æ–º–µ–Ω: {domain})"

    html_content, error_msg = parse_content_with_retries(url)
    
    if error_msg:
        return url, "", 1, error_msg # –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏
    
    text = extract_text(html_content)
    
    if not text or len(text.split()) < 50: # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –æ—á–µ–Ω—å –º–∞–ª–æ, –≤–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –æ—à–∏–±–∫–∞
        return url, "", 1, "–û—à–∏–±–∫–∞: –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"
        
    return url, text, 2, "OK"

# ==========================================
# 6. –§–£–ù–ö–¶–ò–ò –ê–ù–ê–õ–ò–ó–ê
# ==========================================

def preprocess_text_and_get_terms(text, lemmatizer):
    """–õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤."""
    # –ó–∞–≥–ª—É—à–∫–∞: –≤ —Ä–∞–±–æ—á–µ–º –∫–æ–¥–µ –∑–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
    tokens = [word for word in text.split() if word and word not in STOP_WORDS and len(word) > 2]
    return tokens

def calculate_tf_idf_scores(documents):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç TF-IDF –¥–ª—è —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Å–ø–∏—Å–∫–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤)."""
    if not documents:
        return defaultdict(lambda: (0, 0))

    # 1. –°–±–æ—Ä —á–∞—Å—Ç–æ—Ç (TF) –∏ —á–∞—Å—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (DF)
    tf_scores = []
    df = defaultdict(int)
    N = len(documents)

    for doc_tokens in documents:
        doc_tf = Counter(doc_tokens)
        tf_scores.append(doc_tf)
        for word in doc_tf:
            df[word] += 1

    # 2. –†–∞—Å—á–µ—Ç IDF
    idf_scores = {word: math.log(N / df[word]) for word, count in df.items()}

    # 3. –†–∞—Å—á–µ—Ç TF-IDF (—Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º)
    tf_idf_sums = defaultdict(float)
    word_counts = defaultdict(int)

    for i, doc_tf in enumerate(tf_scores):
        for word, tf in doc_tf.items():
            tf_idf_sums[word] += tf * idf_scores.get(word, 0)
            word_counts[word] += tf

    # 4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è (TF-IDF —Å—É–º–º–∞, Count)
    final_scores = {word: (tf_idf_sums[word], word_counts[word]) 
                    for word in tf_idf_sums if word_counts[word] >= MIN_COUNT_FOR_ANALYSIS}
    
    return final_scores

def calculate_semantics(my_text, competitors_texts):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –≥–ª—É–±–∏–Ω—ã, —à–∏—Ä–∏–Ω—ã –∏ TF-IDF."""
    
    lemmatizer = get_lemmatizer()
    
    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
    my_tokens = preprocess_text_and_get_terms(my_text, lemmatizer)
    
    # –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã (—Ç–æ–∫–µ–Ω—ã)
    competitors_token_docs = [preprocess_text_and_get_terms(text, lemmatizer) 
                             for text in competitors_texts if text]

    # 2. –†–∞—Å—á–µ—Ç TF-IDF –¥–ª—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    comp_tf_idf_results = calculate_tf_idf_scores(competitors_token_docs)
    
    # 3. –ì–∏–±—Ä–∏–¥–Ω—ã–π –¢–û–ü (TF-IDF)
    # –≠—Ç–æ —Ç–∞–±–ª–∏—Ü–∞, —Å–æ–¥–µ—Ä–∂–∞—â–∞—è TF-IDF —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è N-–≥—Ä–∞–º–º–∞–º–∏ (–æ–¥–Ω–æ—Å–ª–æ–∂–Ω—ã–µ –∏ —Ñ—Ä–∞–∑—ã)
    hybrid_data = []
    
    # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ N-–≥—Ä–∞–º–º, –¥–æ–±–∞–≤–∏–º —Å—é–¥–∞ —Ä–∞—Å—á–µ—Ç 2-–≥—Ä–∞–º–º 
    # (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–¥–µ —ç—Ç–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ, –Ω–æ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞)
    
    all_comp_tokens = [token for doc in competitors_token_docs for token in doc]
    all_comp_counter = Counter(all_comp_tokens)
    
    comp_phrases_counter = Counter()
    for doc in competitors_token_docs:
        bigrams = [f"{doc[i]} {doc[i+1]}" for i in range(len(doc) - 1)]
        comp_phrases_counter.update(bigrams)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º TF-IDF —Å–ª–æ–≤–∞ (–æ–¥–Ω–æ—Å–ª–æ–∂–Ω—ã–µ) –∏ —Ñ—Ä–∞–∑—ã (2-–≥—Ä–∞–º–º—ã)
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥ MIN_COUNT_FOR_ANALYSIS –¥–ª—è —Ñ—Ä–∞–∑
    top_phrases = {phrase: count for phrase, count in comp_phrases_counter.items() 
                   if count >= MIN_COUNT_FOR_ANALYSIS}
    
    # –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è: TF-IDF –¥–ª—è —Ñ—Ä–∞–∑ –Ω–µ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º, –∞ –±–µ—Ä–µ–º –ø—Ä–æ—Å—Ç–æ —á–∞—Å—Ç–æ—Ç—É
    # (–í —Ä–∞–±–æ—á–µ–º –∫–æ–¥–µ —Ç—É—Ç –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞)
    
    # ... –õ–æ–≥–∏–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ "–ú–∏–Ω–∏–º—É–º", "–ú–∞–∫—Å–∏–º—É–º", "–ü–µ—Ä–µ—Å–ø–∞–º" ...
    # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏, —á—Ç–æ —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –ø—É—Å—Ç–∞—è:
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ–¥–Ω–æ—Å–ª–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞
    for word, (tf_idf, count) in comp_tf_idf_results.items():
        # ... —Ä–∞—Å—á–µ—Ç Min/Max/–ü–µ—Ä–µ—Å–ø–∞–º–∞ ...
        is_in_my_text = word in my_tokens
        hybrid_data.append({
            "–°–ª–æ–≤–æ/–§—Ä–∞–∑–∞": word,
            "–ß–∞—Å—Ç–æ—Ç–∞ (–°—É–º–º–∞)": count,
            "TF-IDF (–°—É–º–º–∞)": f"{tf_idf:.2f}",
            "–ú–∏–Ω–∏–º—É–º": 0, "–ú–∞–∫—Å–∏–º—É–º": 0, 
            "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –º–µ–Ω—è": my_tokens.count(word),
            "–ï—Å—Ç—å —É –º–µ–Ω—è": "–î–∞" if is_in_my_text else "<span class='text-red'>–ù–µ—Ç</span>",
            "–î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": "–£–±—Ä–∞—Ç—å" if is_in_my_text else "–î–æ–±–∞–≤–∏—Ç—å",
        })

    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ—Ä–∞–∑—ã (N-–≥—Ä–∞–º–º—ã)
    for phrase, count in top_phrases.items():
        is_in_my_text = phrase in " ".join(my_tokens) # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã
        hybrid_data.append({
            "–°–ª–æ–≤–æ/–§—Ä–∞–∑–∞": phrase,
            "–ß–∞—Å—Ç–æ—Ç–∞ (–°—É–º–º–∞)": count,
            "TF-IDF (–°—É–º–º–∞)": "N-–≥—Ä–∞–º–º–∞",
            "–ú–∏–Ω–∏–º—É–º": 0, "–ú–∞–∫—Å–∏–º—É–º": 0,
            "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –º–µ–Ω—è": my_text.count(phrase),
            "–ï—Å—Ç—å —É –º–µ–Ω—è": "–î–∞" if is_in_my_text else "<span class='text-red'>–ù–µ—Ç</span>",
            "–î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å": "–£–±—Ä–∞—Ç—å" if is_in_my_text else "–î–æ–±–∞–≤–∏—Ç—å",
        })

    # 4. –†–∞—Å—á–µ—Ç –®–∏—Ä–∏–Ω—ã –∏ –ì–ª—É–±–∏–Ω—ã (–ó–∞–≥–ª—É—à–∫–∞)
    total_relevant_words = len(comp_tf_idf_results)
    my_relevant_words = len([w for w in comp_tf_idf_results if w in my_tokens])
    
    width_score = round((my_relevant_words / total_relevant_words) * 100) if total_relevant_words else 0
    depth_score = 50 # –ó–∞–≥–ª—É—à–∫–∞

    # 5. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results = {
        'my_score': {'width': min(100, width_score), 'depth': depth_score},
        'competitors': [], # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ
        'depth': [], # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –≥–ª—É–±–∏–Ω–µ
        'hybrid': hybrid_data, # –¢–∞–±–ª–∏—Ü–∞ –ì–∏–±—Ä–∏–¥–Ω—ã–π –¢–û–ü (TF-IDF)
        'width': [], # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —à–∏—Ä–∏–Ω–µ
        'comp_tf_idf': comp_tf_idf_results,
    }

    return results

# ==========================================
# 7. –§–£–ù–ö–¶–ò–ò –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø –ò–ù–¢–ï–†–§–ï–ô–°–ê
# ==========================================

def render_paginated_table(data, title, table_id, default_sort_col=None, use_abs_sort_default=False):
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É Streamlit —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º."""
    st.subheader(f"## {title}")
    
    if not data:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")
        return
    
    df = pd.DataFrame(data)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    if default_sort_col and default_sort_col in df.columns:
        df = df.sort_values(by=default_sort_col, ascending=use_abs_sort_default, key=lambda x: np.abs(x) if use_abs_sort_default and np.issubdtype(x.dtype, np.number) else x)

    # Streamlit –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–æ–±—Ä–∞–∑–∏—Ç DataFrame
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º unsafe_allow_html –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è HTML –≤ –∫–æ–ª–æ–Ω–∫–µ "–ï—Å—Ç—å —É –º–µ–Ω—è"
    st.markdown(f'<div id="{table_id}">', unsafe_allow_html=True)
    
    # –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Ç–∞–±–ª–∏—Ü—ã —Å HTML-–∫–æ–ª–æ–Ω–∫–∞–º–∏
    st.markdown(df.to_html(escape=False), unsafe_allow_html=True)
    
    st.markdown('</div>', unsafe_allow_html=True)

def render_competitor_table(competitor_data):
    """
    –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.
    –î–æ–º–µ–Ω—ã —Ç–µ–ø–µ—Ä—å —è–≤–ª—è—é—Ç—Å—è –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π URL.
    """
    st.subheader("## 2. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (—Å—Ç–∞—Ç—É—Å)")
    
    if not competitor_data:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")
        return

    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = pd.DataFrame(competitor_data)
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    def make_clickable_domain(row):
        url = row['URL']
        domain = row['–î–æ–º–µ–Ω']
        # –°–æ–∑–¥–∞–µ–º HTML-—Å—Å—ã–ª–∫—É
        return f'<a href="{url}" target="_blank">{domain}</a>'
        
    df['–î–æ–º–µ–Ω'] = df.apply(make_clickable_domain, axis=1)
    
    # –í—ã–±–∏—Ä–∞–µ–º –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    display_df = df[['URL', '–î–æ–º–µ–Ω', '–°—Ç–∞—Ç—É—Å', '–û—à–∏–±–∫–∞']]
    
    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å HTML-–∫–æ–ª–æ–Ω–∫–∞–º–∏
    st.markdown(display_df.to_html(escape=False, index=False), unsafe_allow_html=True)

def save_analysis_to_history(my_url, competitors_urls, results, comp_data):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é —Å–µ—Å—Å–∏–∏."""
    if 'history' not in st.session_state:
        st.session_state['history'] = []
    
    # –ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç
    history_entry = {
        'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
        'my_url': my_url,
        'competitors_urls': competitors_urls,
        'width': results['my_score']['width'],
        'depth': results['my_score']['depth'],
        'full_results': {
            'results': results,
            'comp_data': comp_data
        }
    }
    st.session_state['history'].insert(0, history_entry) # –î–æ–±–∞–≤–ª—è–µ–º –≤ –Ω–∞—á–∞–ª–æ

def load_analysis_from_history(entry):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –≤ —Ç–µ–∫—É—â—É—é —Å–µ—Å—Å–∏—é –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    st.session_state['last_results'] = entry['full_results']['results']
    st.session_state['competitor_data'] = entry['full_results']['comp_data']
    st.session_state['my_url_input'] = entry['my_url']
    st.session_state['competitors_input'] = "\n".join(entry['competitors_urls'])
    st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω –∞–Ω–∞–ª–∏–∑ –æ—Ç {entry['timestamp']}.")
    st.rerun()

# ==========================================
# 8. –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø
# ==========================================

if not check_password():
    st.stop()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–µ–π—Ç–æ–≤
if 'my_url_input' not in st.session_state:
    st.session_state['my_url_input'] = ""
if 'competitors_input' not in st.session_state:
    st.session_state['competitors_input'] = ""
if 'last_results' not in st.session_state:
    st.session_state['last_results'] = None
if 'competitor_data' not in st.session_state:
    st.session_state['competitor_data'] = []
if 'history' not in st.session_state:
    st.session_state['history'] = []

st.markdown(f"""
    <style>
    .reportview-container .main .block-container{{
        padding-top: 1rem;
        padding-right: 1rem;
        padding-left: 1rem;
        padding-bottom: 1rem;
    }}
    h1 {{ color: {PRIMARY_COLOR}; }}
    h4 {{ color: {PRIMARY_COLOR}; }}
    .text-red {{ color: #FF4B4B; font-weight: bold; }}
    .text-bold {{ font-weight: bold; }}
    .legend-box {{ 
        background-color: {LIGHT_BG_MAIN}; 
        padding: 10px; 
        border-radius: 5px; 
        border: 1px solid {BORDER_COLOR};
        margin-bottom: 20px;
        font-size: 0.9em;
    }}
    /* –°—Ç–∏–ª—å –¥–ª—è –∞–∫—Ç–∏–≤–Ω–æ–π –≤–∫–ª–∞–¥–∫–∏ */
    .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {{
        border-bottom: 3px solid {PRIMARY_COLOR}; /* –ê–∫—Ü–µ–Ω—Ç–Ω—ã–π —Ü–≤–µ—Ç */
        color: {PRIMARY_COLOR};
    }}
    /* –°—Ç–∏–ª—å –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ –ø—Ä–æ–≤–µ—Ä–æ–∫ - –≤—ã–¥–µ–ª–µ–Ω–∏–µ */
    .stTabs [data-baseweb="tab-list"] button:last-child {{
        background-color: #ffe0b2; /* –°–≤–µ—Ç–ª–æ-–æ—Ä–∞–Ω–∂–µ–≤—ã–π —Ñ–æ–Ω */
        font-weight: bold;
    }}
    </style>
""", unsafe_allow_html=True)

st.title("GAR PRO - SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")

# --- –í–∫–ª–∞–¥–∫–∏ ---
tab_analysis, tab_history = st.tabs(["üìä –ê–Ω–∞–ª–∏–∑ –°–µ–º–∞–Ω—Ç–∏–∫–∏", "üìö –ò–°–¢–û–†–ò–Ø –ü–†–û–í–ï–†–û–ö"]) # <-- –ù–û–í–´–ï –í–ö–õ–ê–î–ö–ò

with tab_analysis:
    
    st.subheader("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
    
    col_my, col_comp, col_btn = st.columns([1, 1, 0.5])

    with col_my:
        my_url = st.text_input(
            "–í–∞—à URL (–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞):",
            key='my_url_input',
            placeholder="https://mysite.ru/page/"
        )

    with col_comp:
        competitors_urls_str = st.text_area(
            "URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–∫–∞–∂–¥—ã–π —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏):",
            key='competitors_input',
            height=100,
            placeholder="https://comp1.ru/page/\nhttps://comp2.com/item/"
        )
        
    with col_btn:
        st.markdown("<div style='height: 2.7rem;'></div>", unsafe_allow_html=True) # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ
        start_analysis = st.button("üöÄ –ù–∞—á–∞—Ç—å –ê–Ω–∞–ª–∏–∑", use_container_width=True)

    if start_analysis:
        
        if not my_url.strip() or not competitors_urls_str.strip():
            st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –í–∞—à URL –∏ URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.")
            st.stop()

        # –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–∏—Å–∫–æ–≤ URL
        competitors_list = [url.strip() for url in competitors_urls_str.split('\n') if url.strip()]
        
        all_urls_to_parse = [my_url] + competitors_list
        parsed_data = []
        
        with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç..."):
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏ –Ω–∞ –ø–∞—Ä—Å–∏–Ω–≥
                future_to_url = {executor.submit(parse_url, url): url for url in all_urls_to_parse}
                
                for future in concurrent.futures.as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        parsed_data.append(future.result())
                    except Exception as exc:
                        parsed_data.append((url, "", 1, f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {exc}"))

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        my_page_data = parsed_data[0]
        competitors_data = parsed_data[1:]
        
        my_url_parsed, my_text, my_status, my_error = my_page_data
        
        if my_status != 2:
            st.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –í–∞—à URL ({my_url}): {my_error}")
            st.stop()

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        successful_competitors = [(url, text, status, error) for url, text, status, error in competitors_data if status == 2]
        competitors_texts = [text for url, text, status, error in successful_competitors]
        
        if not competitors_texts:
            st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞.")
            st.stop()

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        comp_data = []
        for url, text, status, error in competitors_data:
            domain = urlparse(url).netloc
            comp_data.append({
                'URL': url,
                '–î–æ–º–µ–Ω': domain,
                '–°—Ç–∞—Ç—É—Å': "OK" if status == 2 else ("–û—à–∏–±–∫–∞" if status == 1 else "–ò—Å–∫–ª—é—á–µ–Ω"),
                '–û—à–∏–±–∫–∞': error if status != 2 else ""
            })

        st.session_state['competitor_data'] = comp_data
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª—è –≤–≤–æ–¥–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –ø–æ–ª–Ω—ã–º–∏ URL-–∞–¥—Ä–µ—Å–∞–º–∏
        successful_comp_urls = [url for url, text, status, error in successful_competitors]
        st.session_state['competitors_input'] = "\n".join(successful_comp_urls)
        
        # --- –ê–Ω–∞–ª–∏–∑ –°–µ–º–∞–Ω—Ç–∏–∫–∏ ---
        with st.spinner("–ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏–∫–∏..."):
            results = calculate_semantics(my_text, competitors_texts)

        st.session_state['last_results'] = results
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é
        save_analysis_to_history(my_url, successful_comp_urls, results, comp_data)

    # --- –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å) ---
    if st.session_state['last_results']:
        results = st.session_state['last_results']
        comp_data = st.session_state['competitor_data']
        
        # 0. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        st.markdown(f"""
            <div style='background-color: {LIGHT_BG_MAIN}; padding: 15px; border-radius: 8px; border: 1px solid {BORDER_COLOR}; margin-bottom: 20px;'>
                <h4 style='margin:0; color: {PRIMARY_COLOR};'>–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ (–≤ –±–∞–ª–ª–∞—Ö –æ—Ç 0 –¥–æ 100)</h4>
                <p style='margin:5px 0 0 0;'>–®–∏—Ä–∏–Ω–∞ (–æ—Ö–≤–∞—Ç —Å–µ–º–∞–Ω—Ç–∏–∫–∏): <b>{results['my_score']['width']}</b> | –ì–ª—É–±–∏–Ω–∞ (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è): <b>{results['my_score']['depth']}</b></p>
            </div>
            <div class="legend-box">
                <span class="text-red">–ö—Ä–∞—Å–Ω—ã–π</span>: —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç —É –≤–∞—Å. <span class="text-bold">–ñ–∏—Ä–Ω—ã–π</span>: —Å–ª–æ–≤–∞, —É—á–∞—Å—Ç–≤—É—é—â–∏–µ –≤ –∞–Ω–∞–ª–∏–∑–µ.<br>
                –ú–∏–Ω–∏–º—É–º: min(—Å—Ä–µ–¥–Ω–µ–µ, –º–µ–¥–∏–∞–Ω–∞). –ü–µ—Ä–µ—Å–ø–∞–º: % –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –º–∞–∫—Å. –¥–∏–∞–ø–∞–∑–æ–Ω–∞. <br>
                ‚ÑπÔ∏è –î–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –≤—Å–µ–≥–æ —Å–ø–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—é –Ω–∞–¥ —Ç–∞–±–ª–∏—Ü–µ–π.
            </div>
        """, unsafe_allow_html=True)
        
        # 1. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≥–ª—É–±–∏–Ω–µ
        render_paginated_table(results['depth'], "1. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≥–ª—É–±–∏–Ω–µ", "tbl_depth_1", default_sort_col="–î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å", use_abs_sort_default=True)
        
        # 2. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (—Å –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏)
        render_competitor_table(comp_data)
        
        # 3. –ì–∏–±—Ä–∏–¥–Ω—ã–π –¢–û–ü (TF-IDF)
        render_paginated_table(results['hybrid'], "3. –ì–∏–±—Ä–∏–¥–Ω—ã–π –¢–û–ü (TF-IDF)", "tbl_hybrid", default_sort_col="–ß–∞—Å—Ç–æ—Ç–∞ (–°—É–º–º–∞)", use_abs_sort_default=False)
        
        # 4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —à–∏—Ä–∏–Ω–µ
        render_paginated_table(results['width'], "4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —à–∏—Ä–∏–Ω–µ", "tbl_width", default_sort_col="–î–æ–±–∞–≤–∏—Ç—å/–£–±—Ä–∞—Ç—å", use_abs_sort_default=True)
        
        st.success("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")

with tab_history: # <-- –ù–û–í–ê–Ø –í–ö–õ–ê–î–ö–ê
    
    st.header("üìö –ò—Å—Ç–æ—Ä–∏—è –ü—Ä–æ–≤–µ—Ä–æ–∫")
    
    if not st.session_state['history']:
        st.info("–ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–µ—Ä–æ–∫ –ø—É—Å—Ç–∞. –ù–∞—á–Ω–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–ê–Ω–∞–ª–∏–∑ –°–µ–º–∞–Ω—Ç–∏–∫–∏'.")
    else:
        for i, entry in enumerate(st.session_state['history']):
            st.markdown(f"""
                <div style='background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 5px; border: 1px solid {BORDER_COLOR}; margin-bottom: 10px;'>
                    <div style="display: flex; justify-content: space-between; align-items: center;">
                        <div>
                            <p style='margin:0; font-size: 1.1em; color: {PRIMARY_COLOR};'>
                                <b>{entry['timestamp']}</b>
                            </p>
                            <p style='margin:5px 0 0 0;'>
                                üîó URL: <b>{entry['my_url']}</b>
                            </p>
                            <p style='margin:5px 0 0 0;'>
                                –®–∏—Ä–∏–Ω–∞: <b>{entry['width']}</b> | –ì–ª—É–±–∏–Ω–∞: <b>{entry['depth']}</b>
                            </p>
                        </div>
                    </div>
                </div>
            """, unsafe_allow_html=True)
            
            # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            if st.button(f"–ü–µ—Ä–µ–π—Ç–∏ –∫ –ø–æ–ª–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É", key=f"load_history_{i}"):
                load_analysis_from_history(entry)
