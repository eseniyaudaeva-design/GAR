import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
from collections import Counter
import math
import inspect
import concurrent.futures

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò –°–¢–ò–õ–ò
# ==========================================

st.set_page_config(
    page_title="–ì–ê–† PRO: –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# CSS –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap');
        
        html, body, [class*="css"] {
            font-family: 'Inter', sans-serif;
            color: #171717;
        }
        
        /* –ó–∞–≥–æ–ª–æ–≤–∫–∏ */
        h1, h2, h3 { font-weight: 700; color: #0F172A; }

        /* –ü–æ–ª—è –≤–≤–æ–¥–∞ –∏ –∫–Ω–æ–ø–∫–∏ */
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] {
            border-radius: 6px;
            border: 1px solid #CBD5E1;
        }
        
        /* –ê–∫—Ü–µ–Ω—Ç–Ω–∞—è –∫–Ω–æ–ø–∫–∞ */
        div.stButton > button {
            background-color: #F97316; /* –û—Ä–∞–Ω–∂–µ–≤—ã–π –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ */
            color: white;
            border-radius: 6px;
            font-weight: 600;
            border: none;
            padding: 0.5rem 1rem;
            transition: 0.2s;
        }
        div.stButton > button:hover {
            background-color: #EA580C;
            color: white;
        }

        /* –¢–∞–±–ª–∏—Ü–∞ */
        div[data-testid="stDataFrame"] {
            font-size: 14px;
        }
        
        /* –°–∫—Ä—ã—Ç–∏–µ –º–µ–Ω—é */
        #MainMenu {visibility: hidden;}
        footer {visibility: hidden;}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 2. –ë–õ–û–ö –ê–í–¢–û–†–ò–ó–ê–¶–ò–ò
# ==========================================
def check_password():
    if "password_correct" not in st.session_state:
        st.session_state["password_correct"] = False

    if st.session_state["password_correct"]:
        return True

    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown("<h3 style='text-align: center;'>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3>", unsafe_allow_html=True)
        pwd = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", label_visibility="collapsed")
        if st.button("–í–æ–π—Ç–∏"):
            if pwd == "admin123":  
                st.session_state["password_correct"] = True
                st.rerun()
            else:
                st.error("–ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

# ==========================================
# 3. –õ–û–ì–ò–ö–ê –ì–ê–† (BACKEND)
# ==========================================

# --- –ü–∞—Ç—á Pymorphy2 ---
try:
    if not hasattr(inspect, 'getargspec'):
        def getargspec(func):
            spec = inspect.getfullargspec(func)
            return spec.args, spec.varargs, spec.varkw, spec.defaults
        inspect.getargspec = getargspec
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except Exception:
    morph = None
    USE_NLP = False

# --- –ü–æ–∏—Å–∫ Google ---
try:
    from googlesearch import search
    USE_SEARCH = True
except ImportError:
    USE_SEARCH = False

# --- –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ---
DEFAULT_EXCLUDE = """yandex.ru
avito.ru
ozon.ru
wildberries.ru
wikipedia.org
youtube.com
dzen.ru
rutube.ru
hh.ru
t.me"""

DEFAULT_STOPS = """—Ä—É–±–ª–µ–π
—Ä—É–±
–∫—É–ø–∏—Ç—å
—Ü–µ–Ω–∞
—à—Ç
—Å–º
–º–º
–∫–≥
–∫–≤
–º2
—Å—Ç—Ä
—É–ª"""

STANDARD_STOP_WORDS = {
    '–∏', '–≤', '–Ω–∞', '—Å', '–∫', '–ø–æ', '–∑–∞', '–æ—Ç', '–¥–æ', '—ç—Ç–æ', '–º—ã', '–≤—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏', '–∏—Ö', '–µ–µ', '–µ–≥–æ', '–º–Ω–µ',
    '—Ç–µ–±–µ', '—Å–µ–±–µ', '–¥–ª—è', '—á—Ç–æ', '–∫–∞–∫', '—Ç–∞–∫', '–Ω–æ', '–∏–ª–∏', '–∞', '—á—Ç–æ–±—ã', '–∂–µ', '–±—ã', '–¥–∞', '–Ω–µ—Ç', '—É', '–±–µ–∑', '–ø–æ–¥',
    '–Ω–∞–¥', '–ø–µ—Ä–µ–¥', '–ø—Ä–∏', '—á–µ—Ä–µ–∑', '–º–µ–∂–¥—É', '—Å—Ä–µ–¥–∏', '–ø–æ—Å–ª–µ', '–≤–º–µ—Å—Ç–æ', '–æ–∫–æ–ª–æ', '–≤–æ–∫—Ä—É–≥', '—Å–æ', '–∏–∑', '–∏–∑-–∑–∞', '–∏–∑-–ø–æ–¥'
}

# --- –§—É–Ω–∫—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ ---

def get_lemmas(text, settings):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ª–µ–º–º"""
    # 1. –û—á–∏—Å—Ç–∫–∞ –æ—Ç HTML —Ç–µ–≥–æ–≤ (–≥—Ä—É–±–∞—è, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç —É–∂–µ –Ω–µ —á–∏—Å—Ç)
    text = re.sub(r'<[^>]+>', ' ', text)
    
    # 2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    pattern = r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+' if settings['numbers'] else r'[–∞-—è–ê-–Ø—ë–Åa-zA-Z]+'
    words = re.findall(pattern, text)
    
    # 3. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
    clean_lemmas = []
    custom_stops = set(w.lower() for w in settings['custom_stops'])
    
    for w in words:
        w_lower = w.lower()
        if len(w) < 2 or w_lower in custom_stops: continue
        
        lemma = w_lower
        if USE_NLP:
            p = morph.parse(w_lower)[0]
            if settings['std_stops']:
                if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag:
                    continue
            lemma = p.normal_form
        
        clean_lemmas.append(lemma)
        
    return clean_lemmas

def parse_html(html, settings):
    """–ü–∞—Ä—Å–∏—Ç HTML –∏ –æ—Ç–¥–∞–µ—Ç –¢–µ–∫—Å—Ç –∏ –¢–µ–∫—Å—Ç –°—Å—ã–ª–æ–∫ —Ä–∞–∑–¥–µ–ª—å–Ω–æ"""
    if not html: return "", ""
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –º—É—Å–æ—Ä–∞
    if settings['noindex']:
        for tag in soup.find_all(['noindex', 'script', 'style', 'head', 'footer', 'nav', 'header', 'aside']):
            tag.decompose()
    else:
        for tag in soup(['script', 'style', 'head']):
            tag.decompose()

    # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–æ–∫ (Anchor)
    anchors = []
    for a in soup.find_all('a'):
        txt = a.get_text(strip=True)
        if txt:
            anchors.append(txt)
    anchor_text = " ".join(anchors)

    # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç (Body)
    # –î–æ–±–∞–≤–ª—è–µ–º Alt –∏ Title, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    extra_text = []
    if settings['alt_title']:
        for img in soup.find_all('img', alt=True):
            extra_text.append(img['alt'])
        for t in soup.find_all(title=True):
            extra_text.append(t['title'])
            
    body_text = soup.get_text(separator=' ') + " " + " ".join(extra_text)
    
    return body_text, anchor_text

def get_page_data(url, settings):
    headers = {'User-Agent': settings['ua']}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code == 200:
            return parse_html(r.text, settings)
    except:
        return "", ""
    return "", ""

# --- –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ (TF-IDF, BM25) ---

def calculate_metrics(corpus_data, my_data, settings):
    """
    corpus_data: list of dicts {'body': [lemmas], 'anchor': [lemmas]}
    my_data: dict {'body': [lemmas], 'anchor': [lemmas]}
    """
    
    # 1. –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞—Ä—å (–≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞)
    vocab = set(my_data['body'])
    for doc in corpus_data:
        vocab.update(doc['body'])
    vocab = sorted(list(vocab))
    
    N = len(corpus_data)
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    stats = []
    
    # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∫–æ—Ä–ø—É—Å–µ (–¥–ª—è BM25)
    avgdl = np.mean([len(doc['body']) for doc in corpus_data]) if N > 0 else 1
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã BM25
    k1 = 1.2
    b = 0.75

    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç DF (Document Frequency)
    doc_freqs = Counter()
    for doc in corpus_data:
        unique_words = set(doc['body'])
        for w in unique_words:
            doc_freqs[w] += 1

    # –ü–æ–¥—Å—á–µ—Ç –≤–µ–∫—Ç–æ—Ä–æ–≤
    for word in vocab:
        # --- –ë–∞–∑–æ–≤—ã–µ —Å—á–µ—Ç—á–∏–∫–∏ ---
        # –ú–æ–π —Å–∞–π—Ç
        my_tf = my_data['body'].count(word)
        my_anchor_tf = my_data['anchor'].count(word)
        
        # –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã (–º–∞—Å—Å–∏–≤—ã –∑–Ω–∞—á–µ–Ω–∏–π)
        comp_tfs = [doc['body'].count(word) for doc in corpus_data]
        comp_anchor_tfs = [doc['anchor'].count(word) for doc in corpus_data]
        
        # --- –ú–µ—Ç—Ä–∏–∫–∏ –∫–æ—Ä–ø—É—Å–∞ ---
        df = doc_freqs[word] # –ö–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤
        
        # IDF (Standard: log(N/df))
        idf = math.log((N / (df if df > 0 else 1)) + 1)
        
        # –ú–µ–¥–∏–∞–Ω–∞, –ú–∞–∫—Å–∏–º—É–º, –°—Ä–µ–¥–Ω–µ–µ
        median_tf = np.median(comp_tfs)
        max_tf = np.max(comp_tfs)
        mean_tf = np.mean(comp_tfs)
        
        median_anchor = np.median(comp_anchor_tfs)
        
        # --- TF-IDF ---
        # –î–ª—è —Ç–æ–ø–∞ –±–µ—Ä–µ–º –º–µ–¥–∏–∞–Ω–Ω—ã–π TF
        tfidf_top = median_tf * idf # –£–ø—Ä–æ—â–µ–Ω–Ω–æ, –∫–∞–∫ —á–∞—Å—Ç–æ –¥–µ–ª–∞—é—Ç –≤ SEO —Ç—É–ª–∑–∞—Ö
        tfidf_my = my_tf * idf
        
        # --- BM25 ---
        # Score = IDF * (TF * (k1 + 1)) / (TF + k1 * (1 - b + b * (|D| / avgdl)))
        # –°—á–∏—Ç–∞–µ–º BM25 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞ –∏ –±–µ—Ä–µ–º –º–µ–¥–∏–∞–Ω—É
        bm25_scores = []
        for i, doc in enumerate(corpus_data):
            tf = comp_tfs[i]
            dl = len(doc['body'])
            score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (dl / avgdl)))
            bm25_scores.append(score)
        
        bm25_top = np.median(bm25_scores)
        
        my_dl = len(my_data['body'])
        bm25_my = idf * (my_tf * (k1 + 1)) / (my_tf + k1 * (1 - b + b * (my_dl / avgdl)))

        # --- –ù–æ—Ä–º–∏—Ä–æ–≤–∫–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞) ---
        # –ï—Å–ª–∏ –º–æ–π —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω–µ–µ/–∫–æ—Ä–æ—á–µ —Å—Ä–µ–¥–Ω–µ–≥–æ, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        norm_factor = 1.0
        if settings['norm'] and avgdl > 0:
            norm_factor = my_dl / avgdl
        
        # –§–∏–ª—å—Ç—Ä "–∑–Ω–∞—á–∏–º–æ—Å—Ç–∏" (—á—Ç–æ–±—ã –Ω–µ –≤—ã–≤–æ–¥–∏—Ç—å –º—É—Å–æ—Ä)
        if (median_tf > 0 or my_tf > 0):
            stats.append({
                "–°–ª–æ–≤–∞": word,
                "TF-IDF –¢–û–ü": round(tfidf_top, 2),
                "TF-IDF –≤–∞—à —Å–∞–π—Ç": round(tfidf_my, 2),
                "BM25 –¢–û–ü": round(bm25_top, 2),
                "BM25 –≤–∞—à —Å–∞–π—Ç": round(bm25_my, 2),
                "IDF": round(idf, 2),
                "–ö–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤": df,
                "–ú–µ–¥–∏–∞–Ω–∞": median_tf * norm_factor, # –° —É—á–µ—Ç–æ–º –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∏
                "–ü–µ—Ä–µ—Å–ø–∞–º": max_tf * norm_factor,
                "–°—Ä–µ–¥–Ω–µ–µ –ø–æ –¢–û–ü—É (–ø–æ–≤—Ç.)": round(mean_tf * norm_factor, 1),
                "–í–∞—à —Å–∞–π—Ç (–ø–æ–≤—Ç.)": my_tf,
                "<a> –ø–æ –¢–û–ü—É (–ø–æ–≤—Ç.)": round(median_anchor * norm_factor, 1),
                "<a> –≤–∞—à —Å–∞–π—Ç (–ø–æ–≤—Ç.)": my_anchor_tf,
                # –°–∫—Ä—ã—Ç–æ–µ –ø–æ–ª–µ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (—Ä–∞–∑–Ω–∏—Ü–∞ —Å –º–µ–¥–∏–∞–Ω–æ–π)
                "diff": abs((median_tf * norm_factor) - my_tf) 
            })
            
    return pd.DataFrame(stats)

# ==========================================
# 4. –§–£–ù–ö–¶–ò–Ø –û–¢–†–ò–°–û–í–ö–ò –ù–ê–°–¢–†–û–ï–ö
# ==========================================
def render_settings_block(key_suffix):
    """–†–∏—Å—É–µ—Ç –±–ª–æ–∫ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    st.markdown("---")
    st.markdown("### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞")
    
    with st.expander("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏", expanded=True):
        c1, c2 = st.columns(2)
        with c1:
            noindex = st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å noindex", True, key=f"noindex_{key_suffix}")
            alt = st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", False, key=f"alt_{key_suffix}")
            num = st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", False, key=f"num_{key_suffix}")
        with c2:
            norm = st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True, key=f"norm_{key_suffix}", help="–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–Ω–∏—Ü—ã –≤ –æ–±—ä–µ–º–µ —Ç–µ–∫—Å—Ç–∞")
            std_stops = st.checkbox("–£–±–∏—Ä–∞—Ç—å –ø—Ä–µ–¥–ª–æ–≥–∏/—Å–æ—é–∑—ã", True, key=f"std_{key_suffix}")
            
    with st.expander("üõë –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ User-Agent", expanded=False):
        c_stops = st.text_area("–°–≤–æ–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (—Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", DEFAULT_STOPS, height=100, key=f"stops_{key_suffix}")
        ua = st.text_input("User-Agent", "Mozilla/5.0 (compatible; Hybrid-Analyzer/1.0;)", key=f"ua_{key_suffix}")

    return {
        'noindex': noindex, 'alt_title': alt, 'numbers': num, 
        'norm': norm, 'std_stops': std_stops, 
        'custom_stops': c_stops.split(), 'ua': ua
    }

# ==========================================
# 5. –ò–ù–¢–ï–†–§–ï–ô–°
# ==========================================

st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")

# –í–∫–ª–∞–¥–∫–∏
tab_task, tab_comp = st.tabs(["üìÑ –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏", "üïµÔ∏è –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã"])

# --- –í–ö–õ–ê–î–ö–ê 1: –ó–ê–î–ê–ß–ê ---
with tab_task:
    col1, col2 = st.columns(2)
    with col1:
        my_url = st.text_input("–í–∞—à URL", placeholder="https://site.ru/page")
    with col2:
        query = st.text_input("–ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞")
    
    st.info("–í–≤–µ–¥–∏—Ç–µ URL —Å–≤–æ–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è. –ï—Å–ª–∏ –æ—Å—Ç–∞–≤–∏—Ç—å –ø—É—Å—Ç—ã–º, –±—É–¥–µ—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Ç–æ–ª—å–∫–æ –¢–û–ü.")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–Ω–∏–∑—É –±–ª–æ–∫–∞
    settings_task = render_settings_block("task")
    
    if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó üöÄ", key="btn_task"):
        # –õ–æ–≥–∏–∫–∞ –∑–∞–ø—É—Å–∫–∞ (–¥–ª—è –¢–∞–±–∞ 1 –Ω—É–∂–µ–Ω –∞–≤—Ç–æ–ø–æ–∏—Å–∫)
        if not query:
            st.error("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å!")
            st.stop()
            
        with st.spinner("–°–±–æ—Ä –ø–æ–∑–∏—Ü–∏–π –≤ Google..."):
            try:
                # –≠–º—É–ª—è—Ü–∏—è –ø–æ–∏—Å–∫–∞ (–∏—Å–∫–ª—é—á–∞–µ–º –¥–æ–º–µ–Ω—ã)
                excl_list = DEFAULT_EXCLUDE.split()
                found_urls = search(query, num_results=20, lang="ru")
                competitors = []
                count = 0
                for u in found_urls:
                    if my_url and u in my_url: continue # –ü—Ä–æ–ø—É—Å–∫ —Å–≤–æ–µ–≥–æ —Å–∞–π—Ç–∞
                    if any(x in u for x in excl_list): continue
                    competitors.append(u)
                    count += 1
                    if count >= 10: break # –ë–µ—Ä–µ–º –¢–û–ü-10
                
                if not competitors:
                    st.error("–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä—É—á–Ω–æ–π —Ä–µ–∂–∏–º.")
                else:
                    st.session_state['run_data'] = {
                        'my_url': my_url, 
                        'competitors': competitors,
                        'settings': settings_task
                    }
                    st.rerun() # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")

# --- –í–ö–õ–ê–î–ö–ê 2: –ö–û–ù–ö–£–†–ï–ù–¢–´ ---
with tab_comp:
    manual_urls_text = st.text_area("–°–ø–∏—Å–æ–∫ URL –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–∫–∞–∂–¥—ã–π —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", height=150, placeholder="https://site1.ru\nhttps://site2.ru")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–Ω–∏–∑—É –±–ª–æ–∫–∞
    settings_comp = render_settings_block("comp")
    
    if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó (–ü–æ —Å–ø–∏—Å–∫—É) üöÄ", key="btn_comp"):
        comps = [u.strip() for u in manual_urls_text.split('\n') if u.strip()]
        if not comps:
            st.error("–°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
        else:
            st.session_state['run_data'] = {
                'my_url': my_url, # –ë–µ—Ä–µ–º –∏–∑ –ø–µ—Ä–≤–æ–π –≤–∫–ª–∞–¥–∫–∏, –µ—Å–ª–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω
                'competitors': comps,
                'settings': settings_comp
            }
            st.rerun()

# ==========================================
# 6. –í–´–ü–û–õ–ù–ï–ù–ò–ï –ò –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
# ==========================================

if 'run_data' in st.session_state:
    data = st.session_state['run_data']
    st.divider()
    st.header("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞")
    
    status = st.empty()
    bar = st.progress(0)
    
    # 1. –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
    my_body_lemmas = []
    my_anchor_lemmas = []
    
    # –ï—Å–ª–∏ URL —Å–≤–æ–µ–≥–æ —Å–∞–π—Ç–∞ –∑–∞–¥–∞–Ω - –∫–∞—á–∞–µ–º
    if data['my_url']:
        status.info(f"–°–∫–∞—á–∏–≤–∞–µ–º –≤–∞—à —Å–∞–π—Ç: {data['my_url']}")
        b_txt, a_txt = get_page_data(data['my_url'], data['settings'])
        if b_txt:
            my_body_lemmas = get_lemmas(b_txt, data['settings'])
            my_anchor_lemmas = get_lemmas(a_txt, data['settings'])
        else:
            st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –≤–∞—à —Å–∞–π—Ç. –¢–∞–±–ª–∏—Ü–∞ –±—É–¥–µ—Ç –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –ø–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º.")
    
    # –ö–∞—á–∞–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    corpus_data = []
    comps = data['competitors']
    
    status.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(comps)} –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_url = {executor.submit(get_page_data, url, data['settings']): url for url in comps}
        completed = 0
        for future in concurrent.futures.as_completed(future_to_url):
            try:
                b_txt, a_txt = future.result()
                if len(b_txt) > 100:
                    corpus_data.append({
                        'body': get_lemmas(b_txt, data['settings']),
                        'anchor': get_lemmas(a_txt, data['settings'])
                    })
            except: pass
            completed += 1
            bar.progress(completed / len(comps))
            
    if len(corpus_data) < 2:
        st.error("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–º–µ–Ω—å—à–µ 2 —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫).")
    else:
        status.success("–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫...")
        bar.empty()
        
        # 2. –†–∞—Å—á–µ—Ç —Ç–∞–±–ª–∏—Ü—ã
        df_result = calculate_metrics(
            corpus_data, 
            {'body': my_body_lemmas, 'anchor': my_anchor_lemmas}, 
            data['settings']
        )
        
        # 3. –í—ã–≤–æ–¥
        if not df_result.empty:
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ TF-IDF –¢–û–ü –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            df_result = df_result.sort_values(by="TF-IDF –¢–û–ü", ascending=False)
            
            # –ü–æ–¥—Å–≤–µ—Ç–∫–∞ (—É—Å–ª–æ–≤–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)
            # –ï—Å–ª–∏ "–í–∞—à —Å–∞–π—Ç" —Å–∏–ª—å–Ω–æ –º–µ–Ω—å—à–µ "–ú–µ–¥–∏–∞–Ω—ã" - –∫—Ä–∞—Å–Ω—ã–π, –µ—Å–ª–∏ –±–æ–ª—å—à–µ –ü–µ—Ä–µ—Å–ø–∞–º–∞ - –∂–µ–ª—Ç—ã–π
            st.dataframe(
                df_result,
                column_config={
                    "diff": None # –°–∫—Ä—ã–≤–∞–µ–º —Ç–µ—Ö. –∫–æ–ª–æ–Ω–∫—É
                },
                use_container_width=True,
                height=800
            )
            
            # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            csv = df_result.to_csv(index=False).encode('utf-8')
            st.download_button(
                "üì• –°–∫–∞—á–∞—Ç—å –≤ Excel (CSV)",
                csv,
                "gar_analysis.csv",
                "text/csv",
                key='download-csv'
            )
        else:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
