import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin, unquote
import inspect
import time
import json
import io
import os
import random
import streamlit.components.v1 as components

# ==========================================
# 0. –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò (–¢—Ä–∞–Ω—Å–ª–∏—Ç –∏ –°–ø–µ–ª–ª–µ—Ä)
# ==========================================

def transliterate_text(text):
    """
    –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ -> –õ–∞—Ç–∏–Ω–∏—Ü–∞ (–¥–ª—è –ø–æ–∏—Å–∫–∞)
    """
    mapping = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }
    result = []
    for char in text.lower():
        if char in mapping:
            result.append(mapping[char])
        elif char.isalnum() or char == '-':
            result.append(char)
    return "".join(result)

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ openai
try:
    import openai
except ImportError:
    openai = None

# ==========================================
# 0.5 –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–û–°–¢–û–Ø–ù–ò–Ø (SESSION STATE)
# ==========================================
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False

if 'ai_generated_df' not in st.session_state:
    st.session_state.ai_generated_df = None
if 'ai_excel_bytes' not in st.session_state:
    st.session_state.ai_excel_bytes = None

if 'tags_html_result' not in st.session_state:
    st.session_state.tags_html_result = None
if 'table_html_result' not in st.session_state:
    st.session_state.table_html_result = None

if 'categorized_products' not in st.session_state:
    st.session_state.categorized_products = []
if 'categorized_services' not in st.session_state:
    st.session_state.categorized_services = []
if 'categorized_commercial' not in st.session_state:
    st.session_state.categorized_commercial = []
if 'categorized_dimensions' not in st.session_state:
    st.session_state.categorized_dimensions = []

if 'persistent_urls' not in st.session_state:
    st.session_state['persistent_urls'] = ""

if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´ –ò –°–ü–ò–°–ö–ò
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO v2.6 (Mass Promo)", page_icon="üìä")

GARBAGE_LATIN_STOPLIST = {
    'whatsapp', 'viber', 'telegram', 'skype', 'vk', 'instagram', 'facebook', 'youtube', 'twitter',
    'cookie', 'cookies', 'policy', 'privacy', 'agreement', 'terms',
    'click', 'submit', 'send', 'zakaz', 'basket', 'cart', 'order', 'call', 'back', 'callback',
    'login', 'logout', 'sign', 'register', 'auth', 'account', 'profile',
    'search', 'menu', 'nav', 'navigation', 'footer', 'header', 'sidebar',
    'img', 'jpg', 'png', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'svg',
    'ok', 'error', 'undefined', 'null', 'true', 'false', 'var', 'let', 'const', 'function', 'return',
    'ru', 'en', 'com', 'net', 'org', 'biz', 'shop', 'store',
    'phone', 'email', 'tel', 'fax', 'mob', 'address', 'copyright', 'all', 'rights', 'reserved',
    'div', 'span', 'class', 'id', 'style', 'script', 'body', 'html', 'head', 'meta', 'link'
}

# ==========================================
# 2. –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø
# ==========================================
def check_password():
    if st.session_state.get("authenticated"):
        return True

    st.markdown("""
        <style>
        .main { display: flex; flex-direction: column; justify-content: center; align-items: center; }
        .auth-logo-box { text-align: center; margin-bottom: 1rem; padding-top: 0; }
        </style>
    """, unsafe_allow_html=True)

    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="auth-logo-box"><h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3></div>', unsafe_allow_html=True)
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "jfV6Xel-Q7vp-_s2UYPO":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

# ==========================================
# 3. –ü–û–õ–£–ß–ï–ù–ò–ï API –ö–õ–Æ–ß–ï–ô
# ==========================================
if "arsenkin_token" in st.session_state:
    ARSENKIN_TOKEN = st.session_state.arsenkin_token
else:
    try: ARSENKIN_TOKEN = st.secrets["api"]["arsenkin_token"]
    except (FileNotFoundError, KeyError): ARSENKIN_TOKEN = None

if "yandex_dict_key" in st.session_state:
    YANDEX_DICT_KEY = st.session_state.yandex_dict_key
else:
    try: YANDEX_DICT_KEY = st.secrets["api"]["yandex_dict_key"]
    except (FileNotFoundError, KeyError): YANDEX_DICT_KEY = None


REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956},
    "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931},
    "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–ö–∏–µ–≤ (UA)": {"ya": 143, "go": 1012852},
    "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493},
    "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601}
}

DEFAULT_EXCLUDE_DOMAINS = [
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "ebay.com",
    "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", "pandao.ru",
    "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", "banki.ru",
    "regmarkets.ru", "zoon.ru", "pulscen.ru", "prodoctorov.ru", "blizko.ru",
    "domclick.ru", "satom.ru", "quto.ru", "edadeal.ru", "cataloxy.ru",
    "irr.ru", "onliner.by", "shop.by", "deal.by", "yell.ru",
    "profi.ru", "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by",
    "market.yandex.ru", "youtube.com", "gosuslugi.ru", "dzen.ru",
    "2gis.by", "wildberries.ru", "rutube.ru", "vk.com", "facebook.com"
]
DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n–∫—É–ø–∏—Ç—å\n—Ü–µ–Ω–∞\n—à—Ç\n—Å–º\n–º–º\n–∫–≥\n–∫–≤\n–º2\n—Å—Ç—Ä\n—É–ª"

# –¶–≤–µ—Ç–∞ –∏ —Å—Ç–∏–ª–∏
PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE"

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif; color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important; }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important; color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] div[role="gridcell"] {{
            background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; border-bottom: 1px solid {ROW_BORDER_COLOR} !important;
        }}
        .legend-box {{ padding: 10px; background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px; }}
        .text-red {{ color: #D32F2F; font-weight: bold; }}
        .text-green {{ color: #2E7D32; font-weight: bold; }}
        .text-bold {{ font-weight: 600; }}
        .sort-container {{ background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 8px; margin-bottom: 10px; border: 1px solid {BORDER_COLOR}; }}
        
        .stApp > header {{ background-color: transparent !important; }}
        .stTextInput input:disabled, .stTextArea textarea:disabled, .stSelectbox div[aria-disabled="true"] {{
            opacity: 1 !important; background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; cursor: text !important; -webkit-text-fill-color: {TEXT_COLOR} !important; border-color: {BORDER_COLOR} !important;
        }}
        .stButton button:disabled {{ opacity: 1 !important; background-color: {PRIMARY_COLOR} !important; color: white !important; cursor: progress !important; }}
        div[data-testid="stAppViewContainer"] {{ filter: none !important; opacity: 1 !important; transition: none !important; }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 4. –õ–û–ì–ò–ö–ê (–ë–≠–ö–ï–ù–î)
# ==========================================

try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except Exception as e:
    morph = None
    USE_NLP = False
    st.sidebar.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ NLP: {e}")

def get_yandex_dict_info(text, api_key):
    if not api_key: return {'lemma': text, 'pos': 'unknown'}
    url = "https://dictionary.yandex.net/api/v1/dicservice.json/lookup"
    params = {'key': api_key, 'lang': 'ru-ru', 'text': text, 'ui': 'ru'}
    try:
        r = requests.get(url, params=params, timeout=2)
        if r.status_code == 200:
            data = r.json()
            if data.get('def'):
                first_def = data['def'][0]
                return {'lemma': first_def.get('text', text), 'pos': first_def.get('pos', 'unknown')}
    except: pass
    return {'lemma': text, 'pos': 'unknown'}

def classify_semantics_with_api(words_list, yandex_key):
    dim_pattern = re.compile(r'\d+(?:[\.\,]\d+)?\s?[—Öx\*√ó]\s?\d+', re.IGNORECASE)
    grade_pattern = re.compile(r'^([–∞-—èa-z]{1,4}\-?\d+[–∞-—èa-z0-9]*)$', re.IGNORECASE)
    gost_pattern = re.compile(r'(–≥–æ—Å—Ç|din|—Ç—É|iso|—Å—Ç|—Å–ø)\s?\d+', re.IGNORECASE)
    SITE_UI_GARBAGE = {'–º–µ–Ω—é', '–ø–æ–∏—Å–∫', '–≥–ª–∞–≤–Ω–∞—è', '–∫–∞—Ä—Ç–∞', '—Å–∞–π—Ç', '–ª–∏—á–Ω—ã–π', '–∫–∞–±–∏–Ω–µ—Ç', '–≤—Ö–æ–¥', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è', '–∫–æ—Ä–∑–∏–Ω–∞', '–∏–∑–±—Ä–∞–Ω–Ω–æ–µ', '—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ', '–ø—Ä–æ—Ñ–∏–ª—å', '—Ç–µ–ª–µ—Ñ–æ–Ω', '–∞–¥—Ä–µ—Å', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', 'email', '–∑–≤–æ–Ω–æ–∫', 'callback', '–æ—Ç–∑—ã–≤', '–æ—Ç–∑—ã–≤—ã', '–≤–æ–ø—Ä–æ—Å', '–æ—Ç–≤–µ—Ç', '–º–µ–Ω–µ–¥–∂–µ—Ä', '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è', '–ø–æ–ª–∏—Ç–∏–∫–∞', '–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ', '–æ—Ñ–µ—Ä—Ç–∞', 'cookie', '—Å–æ–≥–ª–∞—à–∞—Ç—å—Å—è', '—Å–æ–≥–ª–∞—Å–∏–µ', '–ø—Ä–∏–Ω–∏–º–∞—Ç—å', '–æ—Ç–ø—Ä–∞–≤–∏—Ç—å', '–æ—à–∏–±–∫–∞', '—É—Å–ø–µ—à–Ω–æ', '–∫–Ω–æ–ø–∫–∞', '—Ñ–æ—Ä–º–∞', '–ø–æ–ª–µ', '–æ–±–∑–æ—Ä', '–Ω–æ–≤–æ—Å—Ç–∏', '—Å—Ç–∞—Ç—å–∏', '—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞', '–æ–ø–∏—Å–∞–Ω–∏–µ', '–ø–∞—Ä–∞–º–µ—Ç—Ä', '—Å–≤–æ–π—Å—Ç–≤–æ', '–∞—Ä—Ç–∏–∫—É–ª', '–∫–æ–¥', '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä', '—Ñ–∏–ª—å—Ç—Ä', '—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞', '–ø–æ–∫–∞–∑–∞—Ç—å', '—Å–±—Ä–æ—Å–∏—Ç—å', '–∏–º—è', '—Ñ–∞–º–∏–ª–∏—è', '—Å–æ–æ–±—â–µ–Ω–∏–µ', '—Ñ–∞–π–ª', '–¥–æ–∫—É–º–µ–Ω—Ç', '—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç', '–∫–∞—Ç–µ–≥–æ—Ä–∏—è', '—Ä–∞–∑–¥–µ–ª', '—Å–ø–∏—Å–æ–∫', '–≤–∏–¥', '—Ç–∏–ø', '–∫–ª–∞—Å—Å', '—Å–µ—Ä–∏—è', '—Ä–µ–π—Ç–∏–Ω–≥', '–Ω–∞–ª–∏—á–∏–µ', '—Å–∫–ª–∞–¥', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', '–±—Ä–µ–Ω–¥', '–º–∞—Ä–∫–∞', '–≤–µ—Å', '–¥–ª–∏–Ω–∞', '—à–∏—Ä–∏–Ω–∞', '–≤—ã—Å–æ—Ç–∞', '—Ç–æ–ª—â–∏–Ω–∞', '–¥–∏–∞–º–µ—Ç—Ä', '—Ä–∞–∑–º–µ—Ä', '–æ–±—ä–µ–º', '–º–∞—Å—Å–∞', '—Ç–æ–Ω–Ω–∞', '–º–µ—Ç—Ä', '—à—Ç', '–∫–≥', '—É–ø–∞–∫–æ–≤–∫–∞', '—Ü–µ–Ω–∞', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '–º–∞–≥–∞–∑–∏–Ω', '–∫–∞—Ç–∞–ª–æ–≥', '–≥–æ–¥'}
    COMMERCIAL_WORDS = {'–∫—É–ø–∏—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Ü–µ–Ω–∞', '—Ü–µ–Ω—ã', '–ø—Ä–∞–π—Å', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø—Ä–æ–¥–∞–∂–∞', '–Ω–µ–¥–æ—Ä–æ–≥–æ', '–¥–µ—à–µ–≤–æ', '–¥–æ—Ä–æ–≥–æ', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞', '–æ–ø—Ç–æ–º', '—Ä–æ–∑–Ω–∏—Ü–∞', '—Ä—É–±', '—Ä—É–±–ª–µ–π', '—É–µ', '–∑–∞–∫–∞–∑', '–æ–ø–ª–∞—Ç–∞', '–ø–ª–∞—Ç–µ–∂', '—Ä–∞—Å—Å—Ä–æ—á–∫–∞', '–∫—Ä–µ–¥–∏—Ç', '–ª–∏–∑–∏–Ω–≥', '–¥–æ—Å—Ç–∞–≤–∫–∞', '—Å–∞–º–æ–≤—ã–≤–æ–∑', '–æ—Ç–≥—Ä—É–∑–∫–∞', '–ø–æ—Å—Ç–∞–≤–∫–∞', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '–ª–æ–≥–∏—Å—Ç–∏–∫–∞', '–≥–∞—Ä–∞–Ω—Ç–∏—è', '–≤–æ–∑–≤—Ä–∞—Ç', '–æ–±–º–µ–Ω', '–≤—ã–≥–æ–¥–Ω—ã–π', '–Ω–∏–∑–∫–∏–π', '–≤—ã—Å–æ–∫–∏–π', '–ª—É—á—à–∏–π', '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π', '–Ω–∞–¥–µ–∂–Ω—ã–π', '–±–æ–ª—å—à–æ–π', '–º–∞–ª—ã–π', '—É–¥–æ–±–Ω—ã–π', '–±—ã—Å—Ç—Ä—ã–π', '–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π', '—Ö–æ—Ä–æ—à–∏–π', '–¥–æ—Å—Ç—É–ø–Ω—ã–π', '–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π', '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π', '–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π', '—É–Ω–∏–∫–∞–ª—å–Ω—ã–π', '—à–∏—Ä–æ–∫–∏–π', '–æ–≥—Ä–æ–º–Ω—ã–π', '—Ä–∞–∑–ª–∏—á–Ω—ã–π'}
    GEO_ROOTS = ['–º–æ—Å–∫–≤', '–ø–∏—Ç–µ—Ä', '—Å–ø–±', '–µ–∫–±', '–µ–∫–∞—Ç–µ—Ä–∏–Ω', '—Ä–æ—Å—Å–∏', '—Ä—Ñ', '–≥–æ—Ä–æ–¥', '–æ–±–ª–∞—Å—Ç', '–Ω–æ–≤–≥–æ—Ä–æ–¥', '–∫–∞–∑–∞–Ω', '–∫–∏–µ–≤', '–º–∏–Ω—Å–∫', '–∞–ª–º–∞—Ç—ã', '—Å–∞–º–∞—Ä–∞', '–æ–º—Å–∫', '—á–µ–ª—è–±–∏–Ω', '—Ä–æ—Å—Ç–æ–≤', '—É—Ñ–∞', '–≤–æ–ª–≥–æ–≥—Ä–∞–¥', '–ø–µ—Ä–º', '–∫—Ä–∞—Å–Ω–æ—è—Ä', '–≤–æ—Ä–æ–Ω–µ–∂', '—Å–∞—Ä–∞—Ç–æ–≤', '–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä', '—Ç—é–º–µ–Ω', '–∏–∂–µ–≤—Å–∫', '—Ç–æ–ª—å—è—Ç—Ç–∏', '–±–∞—Ä–Ω–∞—É–ª', '–∏—Ä–∫—É—Ç—Å–∫', '—É–ª—å—è–Ω–æ–≤—Å–∫', '—Ö–∞–±–∞—Ä–æ–≤—Å–∫']
    SERVICE_KEYWORDS = {'—Ä–µ–∑–∫–∞', '–≥–∏–±–∫–∞', '—Å–≤–∞—Ä–∫–∞', '–æ—Ü–∏–Ω–∫–æ–≤–∫–∞', '—Ä—É–±–∫–∞', '–º–æ–Ω—Ç–∞–∂', '—É–∫–ª–∞–¥–∫–∞', '–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∏–∑–æ–ª—è—Ü–∏—è', '—Å–≤–µ—Ä–ª–µ–Ω–∏–µ', '–≥—Ä—É–Ω—Ç–æ–≤–∫–∞', '–ø–æ–∫—Ä–∞—Å–∫–∞', '—É—Å–ª—É–≥–∞', '–º–µ—Ç–∞–ª–ª–æ–æ–±—Ä–∞–±–æ—Ç–∫–∞', '–æ–±—Ä–∞–±–æ—Ç–∫–∞', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '—Ä–µ–º–æ–Ω—Ç', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ', '–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ'}

    categories = {'products': set(), 'services': set(), 'commercial': set(), 'dimensions': set()}
    api_candidates = []

    for word in words_list:
        word_lower = word.lower()
        if dim_pattern.search(word_lower) or grade_pattern.match(word_lower) or gost_pattern.search(word_lower) or word_lower.isdigit():
            categories['dimensions'].add(word_lower); continue
        
        lemma = morph.parse(word_lower)[0].normal_form if morph else word_lower
        if lemma in SITE_UI_GARBAGE or lemma in COMMERCIAL_WORDS: categories['commercial'].add(lemma); continue
        if any(root in lemma for root in GEO_ROOTS): categories['commercial'].add(lemma); continue
        if lemma in SERVICE_KEYWORDS or lemma.endswith('–æ–±—Ä–∞–±–æ—Ç–∫–∞'): categories['services'].add(lemma); continue
        api_candidates.append(word_lower)

    yandex_results = {} 
    if api_candidates and yandex_key:
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            future_to_word = {executor.submit(get_yandex_dict_info, w, yandex_key): w for w in api_candidates}
            for future in concurrent.futures.as_completed(future_to_word):
                orig_word = future_to_word[future]
                try: yandex_results[orig_word] = future.result()
                except: yandex_results[orig_word] = {'lemma': orig_word, 'pos': 'unknown'}
    else:
        for w in api_candidates: yandex_results[w] = {'lemma': w, 'pos': 'unknown'}

    for word in api_candidates:
        info = yandex_results.get(word, {'lemma': word, 'pos': 'unknown'})
        lemma = info['lemma']
        pos = info['pos']
        
        if lemma in SITE_UI_GARBAGE or lemma in COMMERCIAL_WORDS: categories['commercial'].add(lemma); continue
        is_service = False
        if lemma.endswith('–Ω–∏–µ') or lemma.endswith('–µ–Ω–∏–µ') or lemma.endswith('–æ–±—Ä–∞–±–æ—Ç–∫–∞') or lemma in SERVICE_KEYWORDS: is_service = True
        
        if is_service: categories['services'].add(lemma); continue

        if pos == 'noun' or pos == 'adjective' or pos == 'participle' or pos == 'unknown':
            if len(lemma) > 2: categories['products'].add(lemma)
        else: categories['commercial'].add(lemma)

    return {k: sorted(list(v)) for k, v in categories.items()}

def get_arsenkin_urls(query, engine_type, region_name, api_token, depth_val=10):
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check"
    url_get = "https://arsenkin.ru/api/tools/get"
    headers = {"Authorization": f"Bearer {api_token}", "Content-type": "application/json"}
    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []
    if "–Ø–Ω–¥–µ–∫—Å" in engine_type: se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type: se_params.append({"type": 11, "region": reg_ids['go']})

    payload = {"tools_name": "check-top", "data": {"queries": [query], "is_snippet": False, "noreask": True, "se": se_params, "depth": depth_val}}
    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        resp_json = r.json()
        if "error" in resp_json or "task_id" not in resp_json: st.error(f"‚ùå –û—à–∏–±–∫–∞ API: {resp_json}"); return []
        task_id = resp_json["task_id"]
        st.toast(f"–ó–∞–¥–∞—á–∞ ID {task_id} –∑–∞–ø—É—â–µ–Ω–∞")
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}"); return []

    status = "process"
    attempts = 0
    while status == "process" and attempts < 40:
        time.sleep(5); attempts += 1
        try:
            r_check = requests.post(url_check, headers=headers, json={"task_id": task_id})
            res_check_data = r_check.json()
            if res_check_data.get("status") == "finish": status = "done"; break
        except: pass

    if status != "done": st.error(f"‚è≥ –¢–∞–π–º-–∞—É—Ç API"); return []

    try:
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id}, timeout=30)
        res_data = r_final.json()
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}"); return []

    results_list = []
    try:
        collect = res_data.get('result', {}).get('result', {}).get('collect')
        if not collect: return []
        final_url_list = []
        if isinstance(collect, list) and len(collect) > 0 and isinstance(collect[0], list): final_url_list = collect[0][0]
        else:
             unique_urls = set()
             for engine_data in collect:
                 if isinstance(engine_data, dict):
                     for _, serps in engine_data.items():
                         for item in serps:
                             if item.get('url') and item.get('url') not in unique_urls:
                                 results_list.append({'url': item['url'], 'pos': item['pos']})
                                 unique_urls.add(item['url'])
             return results_list

        if final_url_list:
            for index, url in enumerate(final_url_list): results_list.append({'url': url, 'pos': index + 1})
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}"); return []
    return results_list

def process_text_detailed(text, settings, n_gram=1):
    text = text.lower().replace('—ë', '–µ')
    words = re.findall(r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+', text)
    stops = set(w.lower().replace('—ë', '–µ') for w in settings['custom_stops'])
    lemmas = []
    forms_map = defaultdict(set)
    for w in words:
        if len(w) < 2: continue
        if not settings['numbers'] and w.isdigit(): continue
        if w in stops: continue
        lemma = w
        if USE_NLP and n_gram == 1:
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            lemma = p.normal_form.replace('—ë', '–µ')
        lemmas.append(lemma)
        forms_map[lemma].add(w)
    return lemmas, forms_map

def parse_page(url, settings):
    headers = {'User-Agent': settings['ua']}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code != 200: return None
        soup = BeautifulSoup(r.text, 'html.parser')
        tags_to_remove = []
        if settings['noindex']: tags_to_remove.append('noindex')
        for c in soup.find_all(string=lambda text: isinstance(text, Comment)): c.extract()
        if tags_to_remove:
            for t in soup.find_all(tags_to_remove): t.decompose()
        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)
        extra_text = []
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'): extra_text.append(meta_desc['content'])
        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])
        body_text_raw = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text = re.sub(r'\s+', ' ', body_text_raw).strip()
        if not body_text: return None
        return {'url': url, 'domain': urlparse(url).netloc, 'body_text': body_text, 'anchor_text': anchor_text}
    except: return None

def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    all_forms_map = defaultdict(set)
    if not my_data or not my_data.get('body_text'): my_lemmas, my_forms, my_anchors, my_len = [], {}, [], 0
    else:
        my_lemmas, my_forms = process_text_detailed(my_data['body_text'], settings)
        my_anchors, _ = process_text_detailed(my_data['anchor_text'], settings)
        my_len = len(my_lemmas)
        for k, v in my_forms.items(): all_forms_map[k].update(v)

    comp_data_parsed = [d for d in comp_data_full if d.get('body_text')]
    comp_docs = []
    for p in comp_data_parsed:
        body, c_forms = process_text_detailed(p['body_text'], settings)
        anchor, _ = process_text_detailed(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor, 'url': p['url'], 'domain': p['domain']})
        for k, v in c_forms.items(): all_forms_map[k].update(v)

    if not comp_docs:
        return { "depth": pd.DataFrame(), "hybrid": pd.DataFrame(), "relevance_top": pd.DataFrame(), "my_score": {"width": 0, "depth": 0}, "missing_semantics_high": [], "missing_semantics_low": [] }

    c_lens = [len(d['body']) for d in comp_docs]
    avg_dl = np.mean(c_lens) if c_lens else 1
    median_len = np.median(c_lens) if c_lens else 0
    norm_k_recs = (my_len / median_len) if (median_len > 0 and my_len > 0 and settings['norm']) else 1.0

    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    N = len(comp_docs)
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
    word_counts_per_doc = [Counter(d['body']) for d in comp_docs]

    word_idf_map = {}
    for lemma in vocab:
        df = doc_freqs[lemma]
        if df == 0: continue
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        word_idf_map[lemma] = max(idf, 0.01)

    S_WIDTH_CORE = set()
    missing_semantics_high = []
    missing_semantics_low = []
    my_full_lemmas_set = set(my_lemmas) | set(my_anchors)
    lsi_candidates_weighted = []

    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        c_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        med_val = np.median(c_counts)
        percent = int((doc_freqs[lemma] / N) * 100)
        weight_simple = word_idf_map.get(lemma, 0) * med_val
        if med_val > 0: lsi_candidates_weighted.append((lemma, weight_simple))
        is_width_word = False
        if med_val >= 1: S_WIDTH_CORE.add(lemma); is_width_word = True

        if lemma not in my_full_lemmas_set:
            if len(lemma) < 2 or lemma.isdigit(): continue
            item = {'word': lemma, 'percent': percent, 'weight': weight_simple}
            if is_width_word: missing_semantics_high.append(item)
            elif percent >= 30: missing_semantics_low.append(item)

    missing_semantics_high.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low.sort(key=lambda x: x['percent'], reverse=True)
    lsi_candidates_weighted.sort(key=lambda x: x[1], reverse=True)
    S_DEPTH_TOP70 = set([x[0] for x in lsi_candidates_weighted[:70]])
    total_width_core_count = len(S_WIDTH_CORE)

    def calculate_bm25_okapi(doc_tokens, doc_len):
        if avg_dl == 0 or doc_len == 0: return 0
        score = 0
        counts = Counter(doc_tokens)
        k1 = 1.2; b = 0.75
        target_words = S_WIDTH_CORE if S_WIDTH_CORE else S_DEPTH_TOP70
        for word in target_words:
            if word not in counts: continue
            tf = counts[word]
            idf = word_idf_map.get(word, 0)
            score += idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avg_dl)))
        return score

    def calculate_width_score_val(lemmas_set):
        if total_width_core_count == 0: return 0
        ratio = len(lemmas_set.intersection(S_WIDTH_CORE)) / total_width_core_count
        return 100 if ratio >= 0.9 else int(round((ratio / 0.9) * 100))

    competitor_scores_map = {}
    comp_bm25_list = []
    for i, doc in enumerate(comp_docs):
        raw_bm25 = calculate_bm25_okapi(doc['body'], c_lens[i])
        comp_bm25_list.append(raw_bm25)
        width_val = calculate_width_score_val(set(doc['body']))
        competitor_scores_map[doc['url']] = {'width_final': min(100, width_val), 'bm25_val': raw_bm25}

    median_bm25_top = np.median(comp_bm25_list) if comp_bm25_list else 0
    spam_limit = median_bm25_top * 1.25 if median_bm25_top > 0 else 1

    for url, scores in competitor_scores_map.items():
        depth_val = int(round((scores['bm25_val'] / spam_limit) * 100))
        scores['depth_final'] = min(100, depth_val)

    my_bm25 = calculate_bm25_okapi(my_lemmas, my_len)
    my_depth_score_final = min(100, int(round((my_bm25 / spam_limit) * 100)))
    my_width_score_final = min(100, calculate_width_score_val(my_full_lemmas_set))

    table_depth, table_hybrid = [], []
    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        df = doc_freqs[lemma]
        if df < 2 and lemma not in my_lemmas: continue
        my_tf_count = my_lemmas.count(lemma)
        forms_str = ", ".join(sorted(list(all_forms_map.get(lemma, set())))) if all_forms_map.get(lemma) else lemma
        c_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        med_total = np.median(c_counts); max_total = np.max(c_counts)
        base_min = min(np.mean(c_counts), med_total)
        rec_min = int(math.ceil(base_min * norm_k_recs))
        rec_max = int(round(max_total * norm_k_recs))
        if rec_max < rec_min: rec_max = rec_min
        rec_median = med_total * norm_k_recs
        
        status = "–ù–æ—Ä–º–∞"; action_diff = 0; action_text = "‚úÖ"
        if my_tf_count < rec_min:
            status = "–ù–µ–¥–æ—Å–ø–∞–º"; action_diff = int(round(rec_min - my_tf_count))
            if action_diff == 0: action_diff = 1
            action_text = f"+{action_diff}"
        elif my_tf_count > rec_max:
            status = "–ü–µ—Ä–µ—Å–ø–∞–º"; action_diff = int(round(my_tf_count - rec_max))
            if action_diff == 0: action_diff = 1
            action_text = f"-{action_diff}"

        depth_percent = int(round((my_tf_count / rec_median) * 100)) if rec_median > 0.1 else (0 if my_tf_count == 0 else 100)
        weight_hybrid = word_idf_map.get(lemma, 0) * (my_tf_count / my_len if my_len > 0 else 0)
        table_depth.append({
            "–°–ª–æ–≤–æ": lemma, "–°–ª–æ–≤–æ—Ñ–æ—Ä–º—ã": forms_str, "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_tf_count,
            "–ú–µ–¥–∏–∞–Ω–∞": round(med_total, 1), "–ú–∏–Ω–∏–º—É–º (—Ä–µ–∫)": rec_min, "–ú–∞–∫—Å–∏–º—É–º (—Ä–µ–∫)": rec_max,
            "–ì–ª—É–±–∏–Ω–∞ %": min(100, depth_percent), "–°—Ç–∞—Ç—É—Å": status, "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": action_text,
            "is_missing": (status == "–ù–µ–¥–æ—Å–ø–∞–º" and my_tf_count == 0), "sort_val": abs(action_diff) if status != "–ù–æ—Ä–º–∞" else 0
        })
        table_hybrid.append({
            "–°–ª–æ–≤–æ": lemma, "TF-IDF –¢–û–ü": round(word_idf_map.get(lemma, 0) * (med_total / avg_dl if avg_dl > 0 else 0), 4),
            "TF-IDF —É –≤–∞—Å": round(weight_hybrid, 4), "–°–∞–π—Ç–æ–≤": df, "–ü–µ—Ä–µ—Å–ø–∞–º": max_total
        })

    table_rel = []
    for item in original_results:
        url = item['url']
        scores = competitor_scores_map.get(url, {'width_final':0, 'depth_final':0})
        table_rel.append({ "–î–æ–º–µ–Ω": urlparse(url).netloc, "–ü–æ–∑–∏—Ü–∏—è": item['pos'], "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": scores['width_final'], "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": scores['depth_final'] })
    my_label = f"{my_data['domain']} (–í—ã)" if (my_data and my_data.get('domain')) else "–í–∞—à —Å–∞–π—Ç"
    table_rel.append({ "–î–æ–º–µ–Ω": my_label, "–ü–æ–∑–∏—Ü–∏—è": my_serp_pos if my_serp_pos > 0 else len(original_results) + 1, "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": my_width_score_final, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": my_depth_score_final })

    return { "depth": pd.DataFrame(table_depth), "hybrid": pd.DataFrame(table_hybrid), "relevance_top": pd.DataFrame(table_rel).sort_values(by='–ü–æ–∑–∏—Ü–∏—è', ascending=True).reset_index(drop=True), "my_score": {"width": my_width_score_final, "depth": my_depth_score_final}, "missing_semantics_high": missing_semantics_high, "missing_semantics_low": missing_semantics_low }

# ==========================================
# 5. –§–£–ù–ö–¶–ò–Ø –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø (PAGINATION)
# ==========================================
def render_paginated_table(df, title_text, key_prefix, default_sort_col=None, use_abs_sort_default=False):
    if df.empty: st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö."); return
    col_t1, col_t2 = st.columns([7, 3])
    with col_t1: st.markdown(f"### {title_text}")
    if f'{key_prefix}_sort_col' not in st.session_state: st.session_state[f'{key_prefix}_sort_col'] = default_sort_col if (default_sort_col and default_sort_col in df.columns) else df.columns[0]
    if f'{key_prefix}_sort_order' not in st.session_state: st.session_state[f'{key_prefix}_sort_order'] = "–£–±—ã–≤–∞–Ω–∏–µ"

    search_query = st.text_input(f"üîç –ü–æ–∏—Å–∫ ({title_text})", key=f"{key_prefix}_search")
    if search_query:
        mask = df.astype(str).apply(lambda x: x.str.contains(search_query, case=False, na=False)).any(axis=1)
        df_filtered = df[mask].copy()
    else: df_filtered = df.copy()

    if df_filtered.empty: st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."); return

    with st.container():
        st.markdown("<div class='sort-container'>", unsafe_allow_html=True)
        col_s1, col_s2, col_sp = st.columns([2, 2, 4])
        with col_s1:
            current_sort = st.session_state[f'{key_prefix}_sort_col']
            if current_sort not in df_filtered.columns: current_sort = df_filtered.columns[0]
            sort_col = st.selectbox("üóÇ –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ:", df_filtered.columns, key=f"{key_prefix}_sort_box", index=list(df_filtered.columns).index(current_sort))
            st.session_state[f'{key_prefix}_sort_col'] = sort_col
        with col_s2:
            sort_order = st.radio("–ü–æ—Ä—è–¥–æ–∫:", ["–£–±—ã–≤–∞–Ω–∏–µ", "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ"], horizontal=True, key=f"{key_prefix}_order_box", index=0 if st.session_state[f'{key_prefix}_sort_order'] == "–£–±—ã–≤–∞–Ω–∏–µ" else 1)
            st.session_state[f'{key_prefix}_sort_order'] = sort_order
        st.markdown("</div>", unsafe_allow_html=True)

    ascending = (sort_order == "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ")
    if use_abs_sort_default and sort_col == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è" and "sort_val" in df_filtered.columns: df_filtered = df_filtered.sort_values(by="sort_val", ascending=ascending)
    elif ("–î–æ–±–∞–≤–∏—Ç—å" in sort_col or "+/-" in sort_col) and df_filtered[sort_col].dtype == object:
        try:
            df_filtered['_temp_sort'] = df_filtered[sort_col].astype(str).str.replace(r'[^\d]', '', regex=True)
            df_filtered['_temp_sort'] = pd.to_numeric(df_filtered['_temp_sort'], errors='coerce').fillna(0)
            df_filtered = df_filtered.sort_values(by='_temp_sort', ascending=ascending).drop(columns=['_temp_sort'])
        except: df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)
    else: df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)

    df_filtered = df_filtered.reset_index(drop=True); df_filtered.index = df_filtered.index + 1
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        export_df = df_filtered.copy()
        if "is_missing" in export_df.columns: del export_df["is_missing"]
        if "sort_val" in export_df.columns: del export_df["sort_val"]
        export_df.to_excel(writer, index=False, sheet_name='Data')
    excel_data = buffer.getvalue()
    with col_t2: st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel", data=excel_data, file_name=f"{key_prefix}_export.xlsx", mime="application/vnd.ms-excel", key=f"{key_prefix}_down")

    ROWS_PER_PAGE = 20
    if f'{key_prefix}_page' not in st.session_state: st.session_state[f'{key_prefix}_page'] = 1
    total_rows = len(df_filtered); total_pages = math.ceil(total_rows / ROWS_PER_PAGE)
    if total_pages == 0: total_pages = 1
    current_page = st.session_state[f'{key_prefix}_page']
    if current_page > total_pages: current_page = total_pages
    if current_page < 1: current_page = 1
    st.session_state[f'{key_prefix}_page'] = current_page
    start_idx = (current_page - 1) * ROWS_PER_PAGE
    end_idx = start_idx + ROWS_PER_PAGE
    df_view = df_filtered.iloc[start_idx:end_idx]

    def highlight_rows(row):
        base_style = 'background-color: #FFFFFF; color: #3D4858; border-bottom: 1px solid #DBEAFE;'
        styles = []
        status = row.get("–°—Ç–∞—Ç—É—Å", "")
        for col_name in row.index:
            cell_style = base_style
            if col_name == "–°—Ç–∞—Ç—É—Å":
                if status == "–ù–µ–¥–æ—Å–ø–∞–º": cell_style += "color: #D32F2F; font-weight: bold;"
                elif status == "–ü–µ—Ä–µ—Å–ø–∞–º": cell_style += "color: #E65100; font-weight: bold;"
                elif status == "–ù–æ—Ä–º–∞": cell_style += "color: #2E7D32; font-weight: bold;"
            styles.append(cell_style)
        return styles

    cols_to_hide = [c for c in ["is_missing", "sort_val"] if c in df_view.columns]
    try: styled_df = df_view.style.apply(highlight_rows, axis=1)
    except: styled_df = df_view
    st.dataframe(styled_df, use_container_width=True, height=(len(df_view) * 35) + 40, column_config={c: None for c in cols_to_hide})
    c_spacer, c_btn_prev, c_info, c_btn_next = st.columns([6, 1, 1, 1])
    with c_btn_prev:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_prev", disabled=(current_page <= 1), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] -= 1
            st.rerun()
    with c_info: st.markdown(f"<div style='text-align: center; margin-top: 10px;'><b>{current_page}</b> / {total_pages}</div>", unsafe_allow_html=True)
    with c_btn_next:
        if st.button("‚û°Ô∏è", key=f"{key_prefix}_next", disabled=(current_page >= total_pages), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] += 1
            st.rerun()
    st.markdown("---")

# ==========================================
# 6. –õ–û–ì–ò–ö–ê –î–õ–Ø PERPLEXITY (AI GEN)
# ==========================================
STATIC_DATA_GEN = {
    'IP_PROP4817': "–£—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏",
    'IP_PROP4818': "–û–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –≤ —Ä–µ–≥–∏–æ–Ω—ã —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4819': """<p>–ù–∞–¥–µ–∂–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫–∞–∑–∞ –≤ –ª—é–±—É—é —Ç–æ—á–∫—É —Å—Ç—Ä–∞–Ω—ã: "–°—Ç–∞–ª—å–º–µ—Ç—É—Ä–∞–ª" –æ—Ç–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–≤–∞—Ä 24 —á–∞—Å–∞ –≤ —Å—É—Ç–∫–∏, 7 –¥–Ω–µ–π –≤ –Ω–µ–¥–µ–ª—é. –ë–æ–ª–µ–µ 4 000 –æ—Ç–≥—Ä—É–∑–æ–∫ –≤ –≥–æ–¥. –ü—Ä–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–∏ –∑–∞–∫–∞–∑–∞ –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –≤–∞–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ä—à—Ä—É—Ç.</p>""",
    'IP_PROP4820': """<p>–ù–∞—à–∏ –∏–∑–¥–µ–ª–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è—Ö –£—Ä–∞–ª–∞, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞, –ü–æ–≤–æ–ª–∂—å—è, –°–∏–±–∏—Ä–∏. –ü–∞—Ä—Ç–Ω–µ—Ä—ã –ø–æ –ª–æ–≥–∏—Å—Ç–∏–∫–µ –ø—Ä–µ–¥–ª–æ–∂–∞—Ç –¥–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–∫–∞–∑ —Å–∞–º—ã–º —É–¥–æ–±–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º ‚Äì –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–º, –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω—ã–º, –¥–∞–∂–µ –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º. –î–ª—è –≤–∞—Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—É—é —Å—Ö–µ–º—É –ø–æ–¥ —É–¥–æ–±–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è. –ü–æ–≥—Ä—É–∑–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ç–µ—Ö–Ω–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.</p><div class="h4"><h4>–°–∞–º–æ–≤—ã–≤–æ–∑</h4></div><p>–ï—Å–ª–∏ –æ–±—ã—á–Ω–æ —Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∑–∞–±—Ä–∞—Ç—å —Ç–æ–≤–∞—Ä –∏–ª–∏ –¥–∞–µ—Ç–µ —ç—Ç–æ –ø—Ä–∞–≤–æ —É–ø–æ–ª–Ω–æ–º–æ—á–µ–Ω–Ω—ã–º, –∞–¥—Ä–µ—Å –∏ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã —Å–∫–ª–∞–¥–∞ –≤ —Å–≤–æ–µ–º –≥–æ—Ä–æ–¥–µ —É—Ç–æ—á–Ω—è–π—Ç–µ —É –º–µ–Ω–µ–¥–∂–µ—Ä–∞.</p><div class="h4"><h4>–ì—Ä—É–∑–æ–≤–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏</h4></div><p>–û—Ç–ø—Ä–∞–≤–∏–º –ø—Ä–æ–∫–∞—Ç –Ω–∞ –≤–∞—à –æ–±—ä–µ–∫—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –∞–≤—Ç–æ–ø–∞—Ä–∫–æ–º. –ü–æ–ª—É—á–µ–Ω–∏–µ –≤ —É–ø–∞–∫–æ–≤–∫–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏, –∞ –∏–º–µ–Ω–Ω–æ –Ω–∞ –¥–µ—Ä–µ–≤—è–Ω–Ω–æ–º –ø–æ–¥–¥–æ–Ω–µ.</p><div class="h4"><h4>–°–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å –¢–ö</h4></div><p>–î–æ—Å—Ç–∞–≤–∫–∞ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –†–æ—Å—Å–∏–∏ –∏ –°–ù–ì. –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å—Å—è, —Ç–∞–∫ –∫–∞–∫ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç, –æ–¥–Ω–∞–∫–æ, —Å—Ä–∞–≤–Ω–∏–º —Å—Ç–æ–∏–º–æ—Å—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª—É–∂–± –∏ –≤—ã–±–µ—Ä–µ–º –ª—É—á—à—É—é.</p>""",
    'IP_PROP4821': "–û–ø–ª–∞—Ç–∞ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤:",
    'IP_PROP4822': """<p>–ù–∞—à–∞ –∫–æ–º–ø–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞ –ø—Ä–∏–Ω—è—Ç—å –ª—é–±—ã–µ –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–µ –≤–∏–¥—ã –æ–ø–ª–∞—Ç—ã –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏—Ü: –ø–æ —Å—á–µ—Ç—É, –Ω–∞–ª–∏—á–Ω–∞—è –∏ –±–µ–∑–Ω–∞–ª–∏—á–Ω–∞—è, –Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–π –ø–ª–∞—Ç–µ–∂, —Ç–∞–∫–∂–µ –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞ –∏ –æ—Ç—Å—Ä–æ—á–∫–∞ –ø–ª–∞—Ç–µ–∂–∞.</p>""",
    'IP_PROP4823': """<div class="h4"><h3>–ü—Ä–∏–º–µ—Ä—ã –≤–æ–∑–º–æ–∂–Ω–æ–π –æ–ø–ª–∞—Ç—ã</h3></div><div class="an-col-12"><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–° –ø–æ–º–æ—â—å—é –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤ —Ü–µ–Ω—Ç—Ä–∞—Ö –ø—Ä–æ–¥–∞–∂</span></p></li></ul><p>–í–∞–∂–Ω–æ! –¶–µ–Ω–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—É–±–ª–∏—á–Ω–æ–π –æ—Ñ–µ—Ä—Ç–æ–π. –ü—Ä–∏—Ö–æ–¥–∏—Ç–µ –≤ –Ω–∞—à –æ—Ñ–∏—Å, —á—Ç–æ–±—ã —É—Ç–æ—á–Ω–∏—Ç—å –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ, –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ—á—Ç–∏ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å, —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –≤–æ–∑–≤—Ä–∞—Ç, —Å—á–µ—Ç, —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ª–æ–≥–∏—Å—Ç–∏–∫—É.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞ —Ä–∞—Å—á–µ—Ç–Ω—ã–π —Å—á–µ—Ç</span></p></li></ul><p>–ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É —Å—á–µ—Ç—É –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏ –±–∞–Ω–∫–∞ –∏–ª–∏ –ø—É—Ç–µ–º –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥—Å—Ç–≤ —á–µ—Ä–µ–∑ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç (—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∑–∞—â–∏—â–µ–Ω—ã, —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –æ—Ç–¥–µ–ª–µ–Ω–∏—è). –î–ª—è –ø—Ä–∞–≤–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å —Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –ø–ª–∞—Ç–µ–∂–Ω–æ–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ —Å –æ—Ç–º–µ—Ç–∫–æ–π –±–∞–Ω–∫–∞.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞–ª–∏—á–Ω—ã–º–∏ –∏–ª–∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∫–∞—Ä—Ç–æ–π –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏</span></p></li></ul><p><span style="font-weight: 400;">–ü–æ–º–æ–∂–µ–º —Å –æ–ø–ª–∞—Ç–æ–π: –æ–±—ä–µ–º –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ. –ö—Ä—É–ø–Ω—ã–º –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º ‚Äì –¥–µ–Ω—å–≥–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–µ–º–∫–∏ —Ç–æ–≤–∞—Ä–∞.</span></p><p>–ú–µ–Ω–µ–¥–∂–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.</p><p>–ó–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ —á–µ—Ä–µ–∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç:</p><p><a class="btn btn-blue" href="/catalog/">–ö–∞—Ç–∞–ª–æ–≥ (–º–∞–≥–∞–∑–∏–Ω-–º–µ–Ω—é):</a></p></div></div><br>""",
    'IP_PROP4824': "–û–ø–∏—Å–∞–Ω–∏–µ, —Å—Ç–∞—Ç—å–∏, –ø–æ–∏—Å–∫, –æ—Ç–∑—ã–≤—ã, –Ω–æ–≤–æ—Å—Ç–∏, –∞–∫—Ü–∏–∏, –∂—É—Ä–Ω–∞–ª, info:",
    'IP_PROP4825': "–ú–æ–∂–µ–º –º–µ—Ç–∞–ª–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –æ—Ü–∏–Ω–∫–æ–≤–∞—Ç—å, –Ω–∏–∫–µ–ª–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–æ–≤–æ–ª–æ—á—å",
    'IP_PROP4826': "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥",
    'IP_PROP4834': "–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –±–µ–∑ –ø—Ä–∏–º–µ—Å–µ–π",
    'IP_PROP4835': "–ü–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫",
    'IP_PROP4836': "–ö–∞—á–µ—Å—Ç–≤–æ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä",
    'IP_PROP4837': "–ü–æ—Ä—è–¥–æ–∫ –≤ –ì–û–°–¢–∞—Ö"
}

def get_page_data_for_gen(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
    except Exception as e: return None, None, f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}"
    if response.status_code != 200: return None, None, f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—É—Å–∞: {response.status_code}"
    soup = BeautifulSoup(response.text, 'html.parser')
    description_div = soup.find('div', class_='description-container')
    base_text = description_div.get_text(separator="\n", strip=True) if description_div else soup.body.get_text(separator="\n", strip=True)[:5000]
    tags_container = soup.find(class_='popular-tags-inner')
    tags_data = []
    if tags_container:
        links = tags_container.find_all('a')
        for link in links:
            tag_url = urljoin(url, link.get('href')) if link.get('href') else None
            if tag_url: tags_data.append({'name': link.get_text(strip=True), 'url': tag_url})
    return base_text, tags_data, None

def generate_five_blocks(client, base_text, tag_name, seo_words=None):
    if not base_text: return ["Error: No base text"] * 5
    system_instruction = "–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä. –ù–∞–ø–∏—à–∏ 5 HTML –±–ª–æ–∫–æ–≤. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π markdown."
    keywords_instruction = ""
    if seo_words and len(seo_words) > 0:
        keywords_str = ", ".join(seo_words)
        keywords_instruction = f"–í–∫–ª—é—á–∏ —ç—Ç–∏ —Å–ª–æ–≤–∞ (—Å–∫–ª–æ–Ω—è—è –∏—Ö) –∏ –≤—ã–¥–µ–ª–∏ <b>: {keywords_str}"

    user_prompt = f"""–í–í–û–î–ù–´–ï: –¢–µ–≥ "{tag_name}". –ë–∞–∑–∞: \"\"\"{base_text[:3000]}\"\"\" {keywords_instruction}
    –ó–ê–î–ê–ß–ê: 5 –±–ª–æ–∫–æ–≤. –°—Ç—Ä—É–∫—Ç—É—Ä–∞: h2/h3, –∞–±–∑–∞—Ü, –≤–≤–æ–¥–Ω–∞—è —Ñ—Ä–∞–∑–∞:, —Å–ø–∏—Å–æ–∫, –∑–∞–∫–ª—é—á–µ–Ω–∏–µ. –ë–µ–∑ [1] —Å—Å—ã–ª–æ–∫. –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: |||BLOCK_SEP|||"""

    try:
        response = client.chat.completions.create(model="sonar-pro", messages=[{"role": "system", "content": system_instruction}, {"role": "user", "content": user_prompt}], temperature=0.7)
        content = response.choices[0].message.content
        content = re.sub(r'\[\d+\]', '', content).replace("```html", "").replace("```", "")
        blocks = [b.strip() for b in content.split("|||BLOCK_SEP|||") if b.strip()]
        while len(blocks) < 5: blocks.append("")
        return blocks[:5]
    except Exception as e: return [f"API Error: {str(e)}"] * 5

def generate_html_table(client, user_prompt, seo_keywords_data=None):
    seo_instruction = ""
    if seo_keywords_data:
        words_desc = [f"- '{item['word']}': {item['count']} times" for item in seo_keywords_data]
        seo_instruction = f"MANDATORY SEO: Use these words ({', '.join(words_desc)}). Wrap in <b>."
    system_instruction = f"Generate HTML tables. Inline CSS: table border 2px solid black, th bg #f0f0f0. {seo_instruction} No markdown."
    try:
        response = client.chat.completions.create(model="sonar-pro", messages=[{"role": "system", "content": system_instruction}, {"role": "user", "content": user_prompt}], temperature=0.7)
        return re.sub(r'\[\d+\]', '', response.choices[0].message.content).replace("```html", "").replace("```", "").strip()
    except Exception as e: return f"Error: {e}"

# ==========================================
# 7. –ò–ù–¢–ï–†–§–ï–ô–° (TABS)
# ==========================================
tab_seo, tab_ai, tab_tags, tab_tables, tab_promo = st.tabs(["üìä SEO –ê–Ω–∞–ª–∏–∑", "ü§ñ AI –ì–µ–Ω–µ—Ä–∞—Ü–∏—è", "üè∑Ô∏è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–≥–æ–≤", "üß© –¢–∞–±–ª–∏—Ü—ã", "üî• –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ê–∫—Ü–∏–∏"])

# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 1: SEO
# ------------------------------------------
with tab_seo:
    col_main, col_sidebar = st.columns([65, 35])
    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
        my_input_type = st.radio("–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], horizontal=True, label_visibility="collapsed", key="my_page_source_radio")
        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            st.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã", placeholder="https://site.ru/catalog/tovar", label_visibility="collapsed", key="my_url_input")
        elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏–ª–∏ —Ç–µ–∫—Å—Ç", height=200, label_visibility="collapsed", placeholder="–í—Å—Ç–∞–≤—å—Ç–µ HTML", key="my_content_input")

        st.markdown("### –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å")
        st.text_input("–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞", label_visibility="collapsed", key="query_input")
        st.markdown("### –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        source_type_new = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫", ["–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)", "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"], horizontal=True, label_visibility="collapsed", key="competitor_source_radio")
        source_type = "API" if "API" in source_type_new else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"
        if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
            def update_manual_urls(): st.session_state['persistent_urls'] = st.session_state.manual_urls_widget
            st.text_area("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", height=200, key="manual_urls_widget", value=st.session_state['persistent_urls'], on_change=update_manual_urls)
        st.markdown("### –°–ø–∏—Å–∫–∏ (Stop / Exclude)")
        st.text_area("–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–æ–º–µ–Ω—ã", DEFAULT_EXCLUDE, height=100, key="settings_excludes")
        st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞", DEFAULT_STOPS, height=100, key="settings_stops")
        if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", type="primary", use_container_width=True, key="start_analysis_btn"):
            for key in list(st.session_state.keys()):
                if key.endswith('_page'): st.session_state[key] = 1
            st.session_state.start_analysis_flag = True

    with col_sidebar:
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API")
        if not ARSENKIN_TOKEN:
             new_arsenkin = st.text_input("Arsenkin Token", type="password", key="input_arsenkin")
             if new_arsenkin: st.session_state.arsenkin_token = new_arsenkin; ARSENKIN_TOKEN = new_arsenkin 
        if not YANDEX_DICT_KEY:
             new_yandex = st.text_input("Yandex Dict Key", type="password", key="input_yandex")
             if new_yandex: st.session_state.yandex_dict_key = new_yandex; YANDEX_DICT_KEY = new_yandex
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
        st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
        st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
        st.selectbox("–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞", list(REGION_MAP.keys()), key="settings_region")
        st.selectbox("–ì–ª—É–±–∏–Ω–∞ —Å–±–æ—Ä–∞ (–¢–û–ü)", [10, 20, 30], index=0, key="settings_top_n")
        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å <noindex>", True, key="settings_noindex")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", False, key="settings_alt")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", False, key="settings_numbers")
        st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True, key="settings_norm")
        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã", True, key="settings_agg")

    if st.session_state.get('start_analysis_flag'):
        st.session_state.start_analysis_flag = False
        settings = {'noindex': st.session_state.settings_noindex, 'alt_title': st.session_state.settings_alt, 'numbers': st.session_state.settings_numbers, 'norm': st.session_state.settings_norm, 'ua': st.session_state.settings_ua, 'custom_stops': st.session_state.settings_stops.split()}
        my_data, my_domain, my_serp_pos = None, "", 0
        current_input_type = st.session_state.get("my_page_source_radio")
        if current_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
                my_data = parse_page(st.session_state.my_url_input, settings)
                if not my_data: st.error("–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."); st.stop()
                my_domain = urlparse(st.session_state.my_url_input).netloc
        elif current_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            my_data = {'url': 'Local', 'domain': 'local', 'body_text': st.session_state.my_content_input, 'anchor_text': ''}
        target_urls_raw = []
        current_source_val = st.session_state.get("competitor_source_radio")
        current_source_type = "API" if "API" in current_source_val else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"
        if current_source_type == "API":
            if not ARSENKIN_TOKEN: st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç API —Ç–æ–∫–µ–Ω Arsenkin."); st.stop()
            with st.spinner("API Arsenkin..."):
                found = get_arsenkin_urls(st.session_state.query_input, st.session_state.settings_search_engine, st.session_state.settings_region, ARSENKIN_TOKEN)
                if not found: st.stop()
                excl = [d.strip() for d in st.session_state.settings_excludes.split('\n') if d.strip()]
                if st.session_state.settings_agg: excl.extend(["avito", "ozon", "wildberries", "market.yandex", "tiu", "youtube", "vk.com", "yandex", "leroymerlin", "petrovich"])
                filtered = []
                for res in found:
                    dom = urlparse(res['url']).netloc
                    if my_domain and my_domain == dom:
                        if my_serp_pos == 0 or res['pos'] < my_serp_pos: my_serp_pos = res['pos']
                        continue
                    if any(x in dom for x in excl): continue
                    filtered.append(res)
                target_urls_raw = filtered[:st.session_state.settings_top_n]
                st.session_state['persistent_urls'] = "\n".join([i['url'] for i in target_urls_raw])
        else:
            raw_urls = st.session_state.get("persistent_urls", "")
            target_urls_raw = [{'url': u.strip(), 'pos': i+1} for i, u in enumerate(raw_urls.split('\n')) if u.strip()]
        if not target_urls_raw: st.error("–ù–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤."); st.stop()
        comp_data_full = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = {executor.submit(parse_page, u['url'], settings): u['url'] for u in target_urls_raw}
            done, total = 0, len(target_urls_raw)
            prog = st.progress(0)
            for f in concurrent.futures.as_completed(futures):
                if res := f.result(): comp_data_full.append(res)
                done += 1; prog.progress(done / total)
        prog.empty()
        with st.spinner("–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫..."):
            st.session_state.analysis_results = calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, target_urls_raw)
            st.session_state.analysis_done = True
            res = st.session_state.analysis_results
            words_to_check = [x['word'] for x in res.get('missing_semantics_high', [])]
            if not words_to_check:
                st.session_state.categorized_products = []; st.session_state.categorized_services = []; st.session_state.categorized_commercial = []; st.session_state.categorized_dimensions = []
            else:
                with st.spinner("–£—Ç–æ—á–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å –°–ª–æ–≤–∞—Ä—å..."):
                    categorized = classify_semantics_with_api(words_to_check, YANDEX_DICT_KEY)
                st.session_state.categorized_products = categorized['products']
                st.session_state.categorized_services = categorized['services']
                st.session_state.categorized_commercial = categorized['commercial']
                st.session_state.categorized_dimensions = categorized['dimensions']
            st.rerun()

    if st.session_state.analysis_done and st.session_state.analysis_results:
        results = st.session_state.analysis_results
        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        st.markdown(f"<div style='background:{LIGHT_BG_MAIN};padding:15px;border-radius:8px;'><b>–†–µ–∑—É–ª—å—Ç–∞—Ç:</b> –®–∏—Ä–∏–Ω–∞: {results['my_score']['width']} | –ì–ª—É–±–∏–Ω–∞: {results['my_score']['depth']}</div>", unsafe_allow_html=True)
        with st.expander("üõí –†–µ–∑—É–ª—å—Ç–∞—Ç –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å–ª–æ–≤", expanded=True):
            c1, c2, c3, c4 = st.columns(4)
            with c1: st.info(f"üß± –¢–æ–≤–∞—Ä—ã ({len(st.session_state.categorized_products)})"); st.caption(", ".join(st.session_state.categorized_products))
            with c2: st.error(f"üõ†Ô∏è –£—Å–ª—É–≥–∏ ({len(st.session_state.categorized_services)})"); st.caption(", ".join(st.session_state.categorized_services))
            with c3: st.warning(f"üí∞ –ö–æ–º–º–µ—Ä—Ü–∏—è ({len(st.session_state.categorized_commercial)})"); st.caption(", ".join(st.session_state.categorized_commercial))
            with c4: dims = st.session_state.get('categorized_dimensions', []); st.success(f"üìè –†–∞–∑–º–µ—Ä—ã ({len(dims)})"); st.caption(", ".join(dims))
        high = results.get('missing_semantics_high', [])
        low = results.get('missing_semantics_low', [])
        if high or low:
            with st.expander(f"üß© –£–ø—É—â–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ ({len(high)+len(low)})", expanded=False):
                if high: st.markdown(f"<div style='background:#EBF5FF;padding:10px;border-radius:5px;'><b>–í–∞–∂–Ω—ã–µ:</b> {', '.join([x['word'] for x in high])}</div>", unsafe_allow_html=True)
                if low: st.markdown(f"<div style='background:#F7FAFC;padding:10px;border-radius:5px;margin-top:5px;'><b>–î–æ–ø:</b> {', '.join([x['word'] for x in low])}</div>", unsafe_allow_html=True)
        render_paginated_table(results['depth'], "1. –ì–ª—É–±–∏–Ω–∞", "tbl_depth_1", default_sort_col="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è", use_abs_sort_default=True)
        render_paginated_table(results['hybrid'], "3. TF-IDF", "tbl_hybrid", default_sort_col="TF-IDF –¢–û–ü")
        render_paginated_table(results['relevance_top'], "4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", "tbl_rel", default_sort_col="–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)")

# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 2: AI
# ------------------------------------------
with tab_ai:
    st.title("AI –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä (Perplexity)")
    pplx_key = st.text_input("Perplexity API Key", type="password", key="pplx_key_input")
    target_url_gen = st.text_input("URL –°—Ç—Ä–∞–Ω–∏—Ü—ã (–¥–æ–Ω–æ—Ä —Ç–µ–≥–æ–≤)", key="pplx_url_input")
    if st.button("üöÄ –ù–∞—á–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é", key="btn_start_gen", disabled=not pplx_key):
        st.session_state.ai_generated_df = None
        if not openai: st.error("–ù–µ—Ç openai"); st.stop()
        client = openai.OpenAI(api_key=pplx_key, base_url="https://api.perplexity.ai")
        with st.status("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è...", expanded=True) as status:
            base_text, tags, err = get_page_data_for_gen(target_url_gen)
            if err or not tags: st.error(err or "–ù–µ—Ç —Ç–µ–≥–æ–≤"); st.stop()
            seo_list = [x['word'] for x in st.session_state.analysis_results.get('missing_semantics_high', []) if x['word'] not in GARBAGE_LATIN_STOPLIST][:15] if st.session_state.analysis_results else []
            all_rows = []
            bar = st.progress(0)
            for i, tag in enumerate(tags):
                blocks = generate_five_blocks(client, base_text, tag['name'], seo_list)
                all_rows.append({'TagName': tag['name'], 'URL': tag['url'], 'IP_PROP4839': blocks[0], 'IP_PROP4816': blocks[1], 'IP_PROP4838': blocks[2], 'IP_PROP4829': blocks[3], 'IP_PROP4831': blocks[4], **STATIC_DATA_GEN})
                bar.progress((i+1)/len(tags))
            df = pd.DataFrame(all_rows)
            st.session_state.ai_generated_df = df
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer: df.to_excel(writer, index=False)
            st.session_state.ai_excel_bytes = buffer.getvalue()
            st.rerun()
    if st.session_state.ai_generated_df is not None:
        st.download_button("üì• –°–∫–∞—á–∞—Ç—å Excel", st.session_state.ai_excel_bytes, "seo_texts.xlsx", "application/vnd.ms-excel")
        st.dataframe(st.session_state.ai_generated_df.head())

# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 3: –¢–ï–ì–ò (V12)
# ------------------------------------------
with tab_tags:
    st.title("üè∑Ô∏è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–ª–∏—Ç–∫–∏ —Ç–µ–≥–æ–≤ (Mass Production + AI Fix)")

    col_t1, col_t2 = st.columns([1, 1])
    with col_t1:
        st.markdown("##### üîó –ò—Å—Ç–æ—á–Ω–∏–∫")
        category_url = st.text_input("URL –ö–∞—Ç–µ–≥–æ—Ä–∏–∏", placeholder="https://site.ru/catalog/truba/")
        st.markdown("##### üìÇ –ë–∞–∑–∞ —Å—Å—ã–ª–æ–∫")
        uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å—Å—ã–ª–æ–∫ (.txt)", type=["txt"], key="urls_uploader_mass_v4")
    with col_t2:
        st.markdown("##### üìù –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–¢–æ–≤–∞—Ä—ã)")
        raw_products = st.session_state.get('categorized_products', [])
        default_text = "\n".join(raw_products) if raw_products else ""
        products_input = st.text_area("–°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤:", value=default_text, height=200, key="tags_products_edit_v12")
        products = [line.strip() for line in products_input.split('\n') if line.strip()]

    st.markdown("---")
    if st.button("üöÄ –°–ø–∞—Ä—Å–∏—Ç—å –∏ —Å–æ–±—Ä–∞—Ç—å Excel (Smart)", key="btn_tags_smart_gen", disabled=(not products or not uploaded_file or not category_url)):
        status_box = st.status("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞...", expanded=True)
        status_box.write(f"üïµÔ∏è –ü–∞—Ä—Å–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {category_url}")
        target_urls_list = []
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            r = requests.get(category_url, headers=headers, timeout=10)
            if r.status_code == 200:
                soup = BeautifulSoup(r.text, 'html.parser')
                tags_container = soup.find(class_='popular-tags-inner')
                if tags_container:
                    for link in tags_container.find_all('a'):
                        href = link.get('href')
                        if href: target_urls_list.append(urljoin(category_url, href))
        except Exception as e: status_box.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}"); st.stop()
            
        if not target_urls_list: status_box.error("–¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–ª–∞—Å—Å .popular-tags-inner)"); st.stop()
        status_box.write(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ü–µ–ª–µ–π: {len(target_urls_list)}")
        status_box.write("üìÇ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞–∑—ã —Å—Å—ã–ª–æ–∫...")
        stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
        all_txt_links = [line.strip() for line in stringio.readlines() if line.strip()]
        product_candidates_map = {}
        for p in products:
            tr = transliterate_text(p)
            if len(tr) >= 3:
                matches = [u for u in all_txt_links if tr in u]
                if matches: product_candidates_map[p] = matches
        status_box.write(f"‚úÖ –¢–æ–≤–∞—Ä—ã —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω—ã ({len(product_candidates_map)} —à—Ç.)")
        status_box.write("üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è '—É–º–Ω—ã—Ö' –∞–Ω–∫–æ—Ä–æ–≤ —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å.–°–ø–µ–ª–ª–µ—Ä...")
        
        final_rows = []
        prog_bar = st.progress(0)
        for i, target_url in enumerate(target_urls_list):
            current_page_tags = []
            for prod_name, candidates in product_candidates_map.items():
                valid = [u for u in candidates if u.rstrip('/') != target_url.rstrip('/')]
                if valid:
                    chosen_url = random.choice(valid)
                    current_page_tags.append({'name': prod_name.capitalize(), 'url': chosen_url})
            if current_page_tags:
                random.shuffle(current_page_tags)
                html_block = '<div class="popular-tags">\n' + "\n".join([f'    <a href="{item["url"]}" class="tag-link">{item["name"]}</a>' for item in current_page_tags]) + '\n</div>'
            else: html_block = ""
            final_rows.append({'Page URL': target_url, 'Tags HTML': html_block})
            prog_bar.progress((i + 1) / len(target_urls_list))
        
        df_tags_result = pd.DataFrame(final_rows)
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer: df_tags_result.to_excel(writer, index=False)
        st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel", data=buffer.getvalue(), file_name="tags_tiles_smart.xlsx")

# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 4: –¢–ê–ë–õ–ò–¶–´ (Smart V3.1 - No Citations)
# ------------------------------------------
with tab_tables:
    st.header("üß© –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä HTML —Ç–∞–±–ª–∏—Ü (Smart Style)")
    st.caption("–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–æ–≤–∞—Ä –ø–æ URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ + –¢–µ–≥—É. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—ã —Å –∂–µ—Å—Ç–∫–∏–º —Å—Ç–∏–ª–µ–º (—á–µ—Ä–Ω—ã–µ —Ä–∞–º–∫–∏). –£–¥–∞–ª—è–µ—Ç —Å–Ω–æ—Å–∫–∏ [1].")

    # -- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è --
    if 'tables_generated_df' not in st.session_state:
        st.session_state.tables_generated_df = None
    if 'tables_excel_data' not in st.session_state:
        st.session_state.tables_excel_data = None

    # -- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ --
    col_tbl_1, col_tbl_2 = st.columns([2, 1])
    with col_tbl_1:
        pplx_key_tbl = st.text_input("Perplexity API Key", type="password", key="pplx_key_tbl_v3")
        parent_cat_url = st.text_input("URL –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (–∏—Å—Ç–æ—á–Ω–∏–∫ —Ç–µ–≥–æ–≤)", placeholder="https://stalmetural.ru/catalog/nikel/")
    
    with col_tbl_2:
        num_tables = st.selectbox("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∞–±–ª–∏—Ü –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É", options=[1, 2, 3, 4, 5], index=1, key="num_tables_select_v3")
        
    # -- –¢–µ–º—ã —Ç–∞–±–ª–∏—Ü --
    if num_tables > 0:
        st.markdown(f"### üìù –¢–µ–º—ã —Ç–∞–±–ª–∏—Ü")
        st.caption("–ù–µ–π—Ä–æ—Å–µ—Ç—å —Å–∞–º–∞ –ø–æ–π–º–µ—Ç, –æ –∫–∞–∫–æ–º —Ç–æ–≤–∞—Ä–µ —Ä–µ—á—å. –ó–¥–µ—Å—å —É–∫–∞–∂–∏—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–ª–æ–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: '–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏').")
        
        table_prompts = []
        defaults = ["–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–†–∞–∑–º–µ—Ä—ã –∏ –≤–µ—Å", "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤", "–£—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏", "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ"]
        
        for i in range(num_tables):
            def_val = defaults[i] if i < len(defaults) else f"–¢–∞–±–ª–∏—Ü–∞ {i+1}"
            t_title = st.text_input(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫ {i+1}", value=def_val, key=f"tbl_title_v3_{i}")
            table_prompts.append(t_title)

    st.markdown("---")

    # -- –õ–æ–≥–∏–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ --
    if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é", key="btn_gen_tbl_smart", disabled=(not pplx_key_tbl or not parent_cat_url)):
        if not openai:
            st.error("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ OpenAI –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
            st.stop()
        
        client = openai.OpenAI(api_key=pplx_key_tbl, base_url="https://api.perplexity.ai")
        status_box = st.status("‚öôÔ∏è –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏...", expanded=True)

        # 1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ URL
        try:
            path = urlparse(parent_cat_url).path.strip('/')
            slug = path.split('/')[-1]
            decoded_slug = unquote(slug)
            parent_name = decoded_slug.replace('-', ' ').replace('_', ' ').capitalize()
            if not parent_name: parent_name = "–¢–æ–≤–∞—Ä"
        except:
            parent_name = "–¢–æ–≤–∞—Ä"
        
        status_box.write(f"üß† –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞: **{parent_name}**")
        
        # 2. –ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–≥–æ–≤
        tags_found = []
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            r = requests.get(parent_cat_url, headers=headers, timeout=15)
            if r.status_code == 200:
                soup = BeautifulSoup(r.text, 'html.parser')
                tags_container = soup.find(class_='popular-tags-inner')
                if tags_container:
                    links = tags_container.find_all('a')
                    for link in links:
                        href = link.get('href')
                        name = link.get_text(strip=True)
                        if href and name:
                            full_url = urljoin(parent_cat_url, href)
                            tags_found.append({'name': name, 'url': full_url})
            else:
                status_box.error(f"–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞: {r.status_code}")
                st.stop()
        except Exception as e:
            status_box.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            st.stop()

        if not tags_found:
            status_box.error("–¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ .popular-tags-inner)")
            st.stop()
            
        status_box.write(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–µ–≥–æ–≤: {len(tags_found)}")
        status_box.write("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü (Perplexity)...")

        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        results_rows = []
        progress_bar = st.progress(0)
        total_steps = len(tags_found)
        
        # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –°—Ç–∏–ª—å + –ó–∞–ø—Ä–µ—Ç —Å–Ω–æ—Å–æ–∫
        style_instruction = """
        STRICT RULES:
        1. Create a <table> with style="border-collapse: collapse; width: 100%; border: 2px solid black;"
        2. Every <th> and <td> must have style="border: 2px solid black; padding: 5px;"
        3. Do NOT use <style> tags or classes. ONLY inline styles.
        4. Do NOT include citation markers like [1], [2] in the text.
        5. Output ONLY the HTML code.
        """
        
        for idx, tag in enumerate(tags_found):
            row_data = {
                'Tag Name': tag['name'],
                'Tag URL': tag['url']
            }
            
            full_product_name = f"{parent_name} {tag['name']}"
            
            for t_i, t_topic in enumerate(table_prompts):
                system_prompt = f"You are a strict HTML generator. {style_instruction}"
                user_prompt = f"""
                Task: Create a technical HTML table.
                Product: "{full_product_name}".
                Table Topic: "{t_topic}".
                Content: Generate realistic technical data (dimensions, grades, properties) relevant to '{full_product_name}' and the topic '{t_topic}'.
                """
                
                try:
                    response = client.chat.completions.create(
                        model="sonar-pro",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        temperature=0.5
                    )
                    content = response.choices[0].message.content
                    
                    # === –ó–ê–©–ò–¢–ê –û–¢ –°–ù–û–°–û–ö ===
                    # –£–¥–∞–ª—è–µ–º –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–∏–¥–∞ [1], [10], [1][2]
                    content = re.sub(r'\[\d+\]', '', content)
                    
                    clean_html = content.replace("```html", "").replace("```", "").strip()
                    row_data[f'Table_{t_i+1}_HTML'] = clean_html
                except Exception as e:
                    row_data[f'Table_{t_i+1}_HTML'] = f"Error: {e}"
            
            results_rows.append(row_data)
            progress_bar.progress((idx + 1) / total_steps)
        
        status_box.update(label="‚úÖ –ì–æ—Ç–æ–≤–æ!", state="complete", expanded=False)
        
        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        df_final = pd.DataFrame(results_rows)
        st.session_state.tables_generated_df = df_final
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            df_final.to_excel(writer, index=False)
        st.session_state.tables_excel_data = buffer.getvalue()
        st.rerun()

    # -- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã --
    if st.session_state.tables_generated_df is not None:
        st.success(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Ç–∞–±–ª–∏—Ü –¥–ª—è {len(st.session_state.tables_generated_df)} —Ç–æ–≤–∞—Ä–æ–≤.")
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å Excel",
            data=st.session_state.tables_excel_data,
            file_name="smart_tables.xlsx",
            mime="application/vnd.ms-excel",
            key="btn_down_tbl_smart"
        )
        st.dataframe(st.session_state.tables_generated_df.head(), use_container_width=True)
        
        with st.expander("üëÅÔ∏è –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –ø–µ—Ä–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã (–±–µ–∑ —Å–Ω–æ—Å–æ–∫)", expanded=False):
            first_html = st.session_state.tables_generated_df.iloc[0].get('Table_1_HTML', '')
            st.markdown(first_html, unsafe_allow_html=True)
            st.text_area("HTML –∫–æ–¥:", value=first_html, height=200)

# ------------------------------------------
# –í–∫–ª–∞–¥–∫–∞ 5: –ì–ï–ù–ï–†–ê–¢–û–† –ê–ö–¶–ò–ò (PRO V3.0 - Positional Mapping)
# ------------------------------------------
with tab_promo:
    st.header("–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –±–ª–æ–∫–∞ \"–ê–∫—Ü–∏–∏\" (Mass Production)")
    
    st.info("""
    **–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è (–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–∞—è –ø—Ä–∏–≤—è–∑–∫–∞):**
    1. **–°–∫–∞–Ω–Ω–µ—Ä:** –£–∫–∞–∂–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—é, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ —Ç–µ–≥–∏ –¥–ª—è Excel.
    2. **–¢–æ–≤–∞—Ä—ã:** –í—Å—Ç–∞–≤—å—Ç–µ —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
    3. **–ö–∞—Ä—Ç–∏–Ω–∫–∏:** –ó–∞–≥—Ä—É–∑–∏—Ç–µ .txt —Ñ–∞–π–ª, –≥–¥–µ –ª–µ–∂–∞—Ç **—Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏** –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ? —á—Ç–æ –∏ –≤ –ø–æ–ª–µ **–°—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏**.
    
    *–°–∫—Ä–∏–ø—Ç —Å–æ–µ–¥–∏–Ω–∏—Ç –∏—Ö –ø–æ –Ω–æ–º–µ—Ä–∞–º —Å—Ç—Ä–æ–∫: 1-—è —Å—Å—ã–ª–∫–∞ + 1-—è –∫–∞—Ä—Ç–∏–Ω–∫–∞ –∏ —Ç.–¥.*
    """)

    # -- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è --
    if 'promo_generated_df' not in st.session_state:
        st.session_state.promo_generated_df = None
    if 'promo_excel_data' not in st.session_state:
        st.session_state.promo_excel_data = None
    if 'promo_html_preview' not in st.session_state:
        st.session_state.promo_html_preview = None
    
    col_p1, col_p2 = st.columns([1, 1])
    
    with col_p1:
        st.markdown("–°—Å—ã–ª–∫–∞ –Ω–∞ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å —Ç–µ–≥–∞–º–∏")
        parent_cat_url = st.text_input("URL –†–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–∏—Å—Ç–æ—á–Ω–∏–∫ —Ç–µ–≥–æ–≤)", placeholder="https://stalmetural.ru/catalog/alyuminievaya-truba/", key="promo_parent_url")
        
        st.markdown("–ó–∞–≥–æ–ª–æ–≤–æ–∫ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –±–ª–æ–∫–∞")
        promo_title = st.text_input("–ó–∞–≥–æ–ª–æ–≤–æ–∫ –±–ª–æ–∫–∞ (h3)", value="–í–∞—Å –º–æ–∂–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å", key="promo_title_input")
        
        st.markdown("–°—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏ (–°—Ç—Ä–æ–≥–æ –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ —á—Ç–æ –∏ —Å—Å—ã–ª–∫–∏)")
        st.caption("–§–∞–π–ª .txt, –≥–¥–µ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî —ç—Ç–æ —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫—É.")
        promo_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–∞—Ä—Ç–∏–Ω–æ–∫ (.txt)", type=["txt"], key="promo_img_loader")
        
        img_lines = []
        if promo_file:
            stringio = io.StringIO(promo_file.getvalue().decode("utf-8"))
            # –ß–∏—Ç–∞–µ–º –ª–∏–Ω–∏–∏, —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            img_lines = [line.strip() for line in stringio.readlines() if line.strip()]
            st.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–∞—Ä—Ç–∏–Ω–æ–∫: {len(img_lines)} —à—Ç.")

    with col_p2:
        st.markdown("–°—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
        st.caption("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫–∏. –ü–æ—Ä—è–¥–æ–∫ –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏.")
        promo_links_text = st.text_area("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", height=300, key="promo_links_area", placeholder="https://stalmetural.ru/catalog/truba-al-profilnaya/\nhttps://stalmetural.ru/catalog/list-riflenyy/")
        
        # –ü–æ–¥—Å—á–µ—Ç —Å—Å—ã–ª–æ–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        link_lines = [line.strip() for line in promo_links_text.split('\n') if line.strip()]
        if link_lines:
            st.caption(f"üìù –í–≤–µ–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(link_lines)} —à—Ç.")
            if promo_file and len(link_lines) != len(img_lines):
                st.warning(f"‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ! –°—Å—ã–ª–æ–∫: {len(link_lines)}, –ö–∞—Ä—Ç–∏–Ω–æ–∫: {len(img_lines)}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ.")

    # --- –ö–ù–û–ü–ö–ê –ì–ï–ù–ï–†–ê–¶–ò–ò ---
    if st.button("üõ†Ô∏è –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å Excel", use_container_width=True, type="primary"):
        # –ü–†–û–í–ï–†–ö–ò
        if not parent_cat_url:
            st.error("–í–≤–µ–¥–∏—Ç–µ URL —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–°–∫–∞–Ω–Ω–µ—Ä)!")
            st.stop()
        if not link_lines:
            st.error("–í–≤–µ–¥–∏—Ç–µ —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –¥–ª—è –±–ª–æ–∫–∞ –ê–∫—Ü–∏–∏!")
            st.stop()
            
        status = st.status("–ó–∞–ø—É—Å–∫ —É–º–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏...", expanded=True)
        
        # --- –§–£–ù–ö–¶–ò–Ø –¢–†–ê–ù–°–õ–ò–¢–ê (–°–ª–æ–≤–∞—Ä–∏ + –Ø–Ω–¥–µ–∫—Å) ---
        def force_cyrillic_name(slug_text):
            raw = unquote(slug_text).lower()
            raw = raw.replace('.html', '').replace('.php', '')
            if re.search(r'[–∞-—è]', raw):
                return raw.replace('-', ' ').replace('_', ' ').capitalize()

            words = re.split(r'[-_]', raw)
            rus_words = []
            
            # –°–ª–æ–≤–∞—Ä—å –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
            exact_map = {
                'nikel': '–Ω–∏–∫–µ–ª—å', 'stal': '—Å—Ç–∞–ª—å', 'med': '–º–µ–¥—å', 'latun': '–ª–∞—Ç—É–Ω—å',
                'bronza': '–±—Ä–æ–Ω–∑–∞', 'svinec': '—Å–≤–∏–Ω–µ—Ü', 'titan': '—Ç–∏—Ç–∞–Ω',
                'alyuminiy': '–∞–ª—é–º–∏–Ω–∏–π', 'al': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è', 'alyuminievaya': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è',
                'nerzhaveyushchiy': '–Ω–µ—Ä–∂–∞–≤–µ—é—â–∏–π', 'nerzhaveyka': '–Ω–µ—Ä–∂–∞–≤–µ–π–∫–∞',
                'profil': '–ø—Ä–æ—Ñ–∏–ª—å', 'shveller': '—à–≤–µ–ª–ª–µ—Ä', 'ugolok': '—É–≥–æ–ª–æ–∫',
                'polosa': '–ø–æ–ª–æ—Å–∞', 'krug': '–∫—Ä—É–≥', 'kvadrat': '–∫–≤–∞–¥—Ä–∞—Ç',
                'list': '–ª–∏—Å—Ç', 'truba': '—Ç—Ä—É–±–∞', 'setka': '—Å–µ—Ç–∫–∞',
                'provoloka': '–ø—Ä–æ–≤–æ–ª–æ–∫–∞', 'armatura': '–∞—Ä–º–∞—Ç—É—Ä–∞', 'balka': '–±–∞–ª–∫–∞',
                'katanka': '–∫–∞—Ç–∞–Ω–∫–∞', 'otvod': '–æ—Ç–≤–æ–¥', 'perehod': '–ø–µ—Ä–µ—Ö–æ–¥',
                'flanec': '—Ñ–ª–∞–Ω–µ—Ü', 'zaglushka': '–∑–∞–≥–ª—É—à–∫–∞', 'metiz': '–º–µ—Ç–∏–∑—ã',
                'profnastil': '–ø—Ä–æ—Ñ–Ω–∞—Å—Ç–∏–ª', 'shtrips': '—à—Ç—Ä–∏–ø—Å',
                'polipropilenovye': '–ø–æ–ª–∏–ø—Ä–æ–ø–∏–ª–µ–Ω–æ–≤—ã–µ', 'truby': '—Ç—Ä—É–±—ã'
            }

            for w in words:
                if not w: continue
                if w in exact_map:
                    rus_words.append(exact_map[w])
                    continue
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏–π
                processed_w = w
                if processed_w.endswith('yy'): processed_w = processed_w[:-2] + '—ã–π'
                elif processed_w.endswith('iy'): processed_w = processed_w[:-2] + '–∏–π'
                elif processed_w.endswith('ij'): processed_w = processed_w[:-2] + '–∏–π'
                elif processed_w.endswith('yi'): processed_w = processed_w[:-2] + '–∏–π'
                elif processed_w.endswith('aya'): processed_w = processed_w[:-3] + '–∞—è'
                elif processed_w.endswith('oye'): processed_w = processed_w[:-3] + '–æ–µ'

                # –¢—Ä–∞–Ω—Å–ª–∏—Ç –∫–æ—Ä–Ω—è
                replacements = [
                    ('shch', '—â'), ('sch', '—â'), ('yo', '—ë'), ('zh', '–∂'), ('ch', '—á'), ('sh', '—à'), 
                    ('yu', '—é'), ('ya', '—è'), ('kh', '—Ö'), ('ts', '—Ü'), ('ph', '—Ñ'),
                    ('a', '–∞'), ('b', '–±'), ('v', '–≤'), ('g', '–≥'), ('d', '–¥'), ('e', '–µ'), 
                    ('z', '–∑'), ('i', '–∏'), ('j', '–π'), ('k', '–∫'), ('l', '–ª'), ('m', '–º'), 
                    ('n', '–Ω'), ('o', '–æ'), ('p', '–ø'), ('r', '—Ä'), ('s', '—Å'), ('t', '—Ç'), 
                    ('u', '—É'), ('f', '—Ñ'), ('h', '—Ö'), ('c', '–∫'), ('w', '–≤'), ('y', '—ã'), ('x', '–∫—Å')
                ]
                
                temp_res = processed_w
                for eng, rus in replacements:
                    temp_res = temp_res.replace(eng, rus)
                
                rus_words.append(temp_res)

            draft_phrase = " ".join(rus_words)
            try: final_phrase = spell_check_yandex(draft_phrase)
            except: final_phrase = draft_phrase
            return final_phrase.capitalize()

        # --- –≠–¢–ê–ü 1: –°–ë–û–†–ö–ê HTML –ë–õ–û–ö–ê (–ü–û–ó–ò–¶–ò–û–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê) ---
        status.write("üî® –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫...")
        items_html = ""
        
        # –¶–∏–∫–ª –ø–æ —Å—Å—ã–ª–∫–∞–º –Ω–∞ —Ç–æ–≤–∞—Ä—ã
        for index, line in enumerate(link_lines):
            url = ""
            name = ""
            if '|' in line:
                parts = line.split('|')
                url = parts[0].strip()
                name = parts[1].strip()
            else:
                url = line
                clean_url = url.rstrip('/')
                slug = clean_url.split('/')[-1]
                name = force_cyrillic_name(slug)
            
            # --- –ì–õ–ê–í–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ë–ï–†–ï–ú –ö–ê–†–¢–ò–ù–ö–£ –ü–û –ò–ù–î–ï–ö–°–£ ---
            # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–≤–∞—Ä–∞ –º–µ–Ω—å—à–µ, —á–µ–º –¥–ª–∏–Ω–∞ —Å–ø–∏—Å–∫–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫, –±–µ—Ä–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É.
            # –ò–Ω–∞—á–µ - –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–π src.
            img_src = ""
            if index < len(img_lines):
                # –ë–µ—Ä–µ–º –≤—Å—é —Å—Ç—Ä–æ–∫—É —Ü–µ–ª–∏–∫–æ–º, —Ç–∞–∫ –∫–∞–∫ –¥–æ–≥–æ–≤–æ—Ä–∏–ª–∏—Å—å, —á—Ç–æ –≤ —Ñ–∞–π–ª–µ —Ç–æ–ª—å–∫–æ URL
                img_src = img_lines[index]
            
            items_html += f"""            <div class="gallery-item">
                <h3><a href="{url}" target="_blank">{name}</a></h3>
                <figure>
                    <a href="{url}" target="_blank">
                        <picture>
                            <img src="{img_src}" 
                                 alt="{name}" 
                                 title="{name}" 
                                 loading="lazy">
                        </picture>
                    </a>
                </figure>
            </div>\n"""

        css_styles = """<style>
.outer-full-width-section { padding: 25px 0; width: 100%; }
.gallery-content-wrapper { max-width: 1400px; margin: 0 auto; padding: 25px 15px; box-sizing: border-box; border-radius: 10px; overflow: hidden; background-color: #F6F7FC; }
h3.gallery-title { color: #3D4858; font-size: 1.8em; font-weight: normal; padding: 0; margin-top: 0; margin-bottom: 15px; text-align: left; }
.five-col-gallery { display: flex; justify-content: flex-start; align-items: flex-start; gap: 20px; margin-bottom: 0; padding: 0; list-style: none; flex-wrap: nowrap !important; overflow-x: auto !important; padding-bottom: 15px; }
.gallery-item { flex: 0 0 260px !important; box-sizing: border-box; text-align: center; scroll-snap-align: start; }
.gallery-item h3 { font-size: 1.1em; margin-bottom: 8px; font-weight: normal; text-align: center; line-height: 1.1em; display: block; min-height: 40px; }
.gallery-item h3 a { text-decoration: none; color: #333; display: block; height: 100%; display: flex; align-items: center; justify-content: center; transition: color 0.2s ease; }
.gallery-item h3 a:hover { color: #007bff; }
.gallery-item figure { width: 100%; margin: 0; float: none !important; height: 260px; overflow: hidden; margin-bottom: 5px; border-radius: 8px; }
.gallery-item figure a { display: block; height: 100%; text-decoration: none; }
.gallery-item img { width: 100%; height: 100%; display: block; margin: 0 auto; object-fit: cover; transition: transform 0.3s ease; border-radius: 8px; }
.gallery-item figure a:hover img { transform: scale(1.05); }
</style>"""

        full_block_html = f"""{css_styles}
<div class="outer-full-width-section">
    <div class="gallery-content-wrapper"> 
        <h3 class="gallery-title">{promo_title}</h3>
        <div class="five-col-gallery">
{items_html}        </div>
    </div>
</div>"""

        # --- –≠–¢–ê–ü 2: –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ï –¢–ï–ì–û–í ---
        status.write(f"üïµÔ∏è –°–∫–∞–Ω–∏—Ä—É–µ–º —Ç–µ–≥–∏: {parent_cat_url}")
        found_tags = []
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            r = requests.get(parent_cat_url, headers=headers, timeout=10)
            if r.status_code == 200:
                soup = BeautifulSoup(r.text, 'html.parser')
                tags_container = soup.find(class_='popular-tags-inner')
                if tags_container:
                    for link in tags_container.find_all('a'):
                        href = link.get('href')
                        if href:
                            full_url = urljoin(parent_cat_url, href)
                            found_tags.append(full_url)
        except Exception as e:
            status.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}"); st.stop()
            
        if not found_tags: status.error("–¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ .popular-tags-inner)"); st.stop()
        
        status.write(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–µ–≥–æ–≤ –¥–ª—è Excel: {len(found_tags)}")
        
        # --- –≠–¢–ê–ü 3: –°–û–•–†–ê–ù–ï–ù–ò–ï ---
        excel_rows = []
        for tag_url in found_tags:
            excel_rows.append({
                'Page URL': tag_url,
                'HTML Block': full_block_html
            })
            
        df_promo = pd.DataFrame(excel_rows)
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            df_promo.to_excel(writer, index=False)
            
        st.session_state.promo_generated_df = df_promo
        st.session_state.promo_excel_data = buffer.getvalue()
        st.session_state.promo_html_preview = full_block_html
        
        status.update(label="–ì–æ—Ç–æ–≤–æ!", state="complete", expanded=False)
        st.rerun()

    # --- –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–ê ---
    if st.session_state.promo_generated_df is not None:
        st.success("üéâ –§–∞–π–ª –≥–æ—Ç–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å Excel",
            data=st.session_state.promo_excel_data,
            file_name="promo_blocks_final.xlsx",
            mime="application/vnd.ms-excel",
            key="btn_down_promo_final"
        )
        
        with st.expander("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –±–ª–æ–∫–∞ (–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏)", expanded=True):
            components.html(st.session_state.promo_html_preview, height=450, scrolling=True)




