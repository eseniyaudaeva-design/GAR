import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
import math
import concurrent.futures
from urllib.parse import urlparse
import inspect
import time
import json
import os # –î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π

# ==========================================
# 0. –ü–ê–¢–ß –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò (–î–ª—è NLP)
# ==========================================
if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´
# ==========================================
st.set_page_config(layout="wide", page_title="GAR PRO", page_icon="üìä")

# ==========================================
# 2. –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø
# ==========================================
def check_password():
    if st.session_state.get("authenticated"):
        return True
   
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown("""
            <style>
            .auth-container {
                display: flex; flex-direction: column; align-items: center;
                justify-content: center; padding: 2rem; background-color: white;
                border-radius: 10px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                margin-top: 5rem;
            }
            </style>
            <div class="auth-container">
                <h3>üìä GAR PRO</h3>
                <h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3>
            </div>
        """, unsafe_allow_html=True)
        
        
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "jfV6Xel-Q7vp-_s2UYPO":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password():
    st.stop()

# ==========================================
# 3. –ù–ê–°–¢–†–û–ô–ö–ò API –ò –†–ï–ì–ò–û–ù–û–í
# ==========================================
# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω –∞–∫—Ç—É–∞–ª–µ–Ω!
ARSENKIN_TOKEN = "43acbbb60cb7989c05914ff21be45379"

# –°–ª–æ–≤–∞—Ä—å —Ä–µ–≥–∏–æ–Ω–æ–≤ (–ù–∞–∑–≤–∞–Ω–∏–µ -> {yandex_id, google_id})
REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956},
    "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931},
    "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–ö–∏–µ–≤ (UA)": {"ya": 143, "go": 1012852},
    "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493},
    "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601}
}

DEFAULT_EXCLUDE_DOMAINS = [
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "ebay.com",
    "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", "pandao.ru",
    "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", "banki.ru", 
    "regmarkets.ru", "zoon.ru", "pulscen.ru", "prodoctorov.ru", "blizko.ru", 
    "domclick.ru", "satom.ru", "quto.ru", "edadeal.ru", "cataloxy.ru", 
    "irr.ru", "onliner.by", "shop.by", "deal.by", "yell.ru", 
    "profi.ru", 
    "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by", "market.yandex.ru", 
    "youtube.com", "gosuslugi.ru", "dzen.ru", "2gis.by", "wildberries.ru", 
    "rutube.ru", "vk.com", "facebook.com"
]
DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n–∫—É–ø–∏—Ç—å\n—Ü–µ–Ω–∞\n—à—Ç\n—Å–º\n–º–º\n–∫–≥\n–∫–≤\n–º2\n—Å—Ç—Ä\n—É–ª"

# –¶–≤–µ—Ç–∞
PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE" 

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif;
        color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important;
        border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important;
        }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important;
        color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important;
        border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important;
        color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] div[role="gridcell"] {{
            background-color: #FFFFFF !important;
        color: {TEXT_COLOR} !important; border-bottom: 1px solid {ROW_BORDER_COLOR} !important;
        }}
        .legend-box {{ padding: 10px;
        background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px;
        }}
        .text-red {{ color: #D32F2F; font-weight: bold;
        }}
        .text-bold {{ font-weight: 600;
        }}
        .sort-container {{ background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 8px; margin-bottom: 10px;
        border: 1px solid {BORDER_COLOR}; }}
        section[data-testid="stSidebar"] {{ background-color: #FFFFFF !important;
        border-left: 1px solid {BORDER_COLOR} !important; }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 4. –õ–û–ì–ò–ö–ê (–ë–≠–ö–ï–ù–î)
# ==========================================

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP
try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except Exception as e:
    morph = None
    USE_NLP = False
    st.sidebar.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ NLP: {e}")

if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False
if 'app_page' not in st.session_state:
    st.session_state.app_page = "–ê–Ω–∞–ª–∏–∑" # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏

# --- –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ò–°–¢–û–†–ò–ò –ó–ê–î–ê–ß ---
RESULTS_FILE = "gar_pro_results.json" # –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏

def load_results():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞."""
    if not os.path.path.exists(RESULTS_FILE):
        return []
    try:
        with open(RESULTS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return []

def save_results(data):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON —Ñ–∞–π–ª."""
    try:
        with open(RESULTS_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        return True
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
        return False
        
# --- –§–£–ù–ö–¶–ò–ò –î–õ–Ø –°–ö–ê–ß–ò–í–ê–ù–ò–Ø ---
def convert_df_to_csv(df):
    """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ DataFrame –≤ CSV —Å—Ç—Ä–æ–∫—É (—Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º ';')."""
    return df.to_csv(index=False, sep=';', encoding='utf-8')

def convert_df_to_xml(df, root_name="Results", row_name="Item"):
    """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ DataFrame –≤ –ø—Ä–æ—Å—Ç—É—é XML —Å—Ç—Ä–æ–∫—É."""
    data = df.to_dict(orient='records')
    xml_string = f'<?xml version="1.0" encoding="utf8"?>\n<{root_name}>\n'
    
    for record in data:
        xml_string += f'  <{row_name}>\n'
        for key, value in record.items():
            # –ó–∞–º–µ–Ω–∞ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ –∏–º–µ–Ω–∞—Ö —Ç–µ–≥–æ–≤
            tag_name = re.sub(r'[^a-zA-Z0-9_]', '', key.replace(' ', '_'))
            # –≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
            safe_value = str(value).replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('"', '&quot;').replace("'", '&apos;')
            xml_string += f'    <{tag_name}>{safe_value}</{tag_name}>\n'
        xml_string += f'  </{row_name}>\n'
    
    xml_string += f'</{root_name}>'
    return xml_string


# --- –§–£–ù–ö–¶–ò–Ø –†–ê–ë–û–¢–´ –° API ARSENKIN ---
def get_arsenkin_urls(query, engine_type, region_name, depth_val=10):
# ... (–æ—Å—Ç–∞–≤—à–∞—è—Å—è —á–∞—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ get_arsenkin_urls –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...

# --- –§–£–ù–ö–¶–ò–Ø –ê–ù–ê–õ–ò–ó–ê –ú–ï–¢–†–ò–ö ---
def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    all_forms_map = defaultdict(set)
    
    # 1. –í–∞—à —Å–∞–π—Ç
    if not my_data or not my_data.get('body_text'):
        my_lemmas, my_forms, my_anchors, my_len = [], {}, [], 0
    else:
        my_lemmas, my_forms = process_text_detailed(my_data['body_text'], settings)
        my_anchors, _ = process_text_detailed(my_data['anchor_text'], settings)
        my_len = len(my_lemmas)
        for k, v in my_forms.items():
            all_forms_map[k].update(v)

    # –†–∞–∑–¥–µ–ª—è–µ–º —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–µ–º–º –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    comp_data_parsed = [d for d in comp_data_full if d.get('body_text')]
    
    # 2. –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã (—Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã–µ)
    comp_docs = []
    for p in comp_data_parsed:
        body, c_forms = process_text_detailed(p['body_text'], settings)
        anchor, _ = process_text_detailed(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor})
        for k, v in c_forms.items():
            all_forms_map[k].update(v)
    
    # –ï—Å–ª–∏ –Ω–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤, –º—ã –Ω–µ –º–æ–∂–µ–º —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
    if not comp_docs:
        # –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –Ω–∞–º –Ω—É–∂–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å, –∫—Ç–æ –±—ã–ª –≤ –¢–û–ü–µ
        
        table_rel_fallback = []
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ URL, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏—à–ª–∏ –∏–∑ API/—Ä—É—á–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞, —á—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å –∏—Ö –ø–æ–∑–∏—Ü–∏–∏
        for item in original_results:
            domain = urlparse(item['url']).netloc
            table_rel_fallback.append({
                "–î–æ–º–µ–Ω": domain, 
                "–ü–æ–∑–∏—Ü–∏—è": item['pos'],
                "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": 0, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": 0
            })
        
        if my_data and my_data.get('domain'):
            my_label = f"{my_data['domain']} (–í—ã)"
        else:
            my_label = "–í–∞—à —Å–∞–π—Ç"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –í–∞—à —Å–∞–π—Ç
        table_rel_fallback.append({
            "–î–æ–º–µ–Ω": my_label, 
            "–ü–æ–∑–∏—Ü–∏—è": my_serp_pos if my_serp_pos > 0 else len(original_results) + 1,
            "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": 0, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": 0
        })
        
        table_rel_df = pd.DataFrame(table_rel_fallback).sort_values(by='–ü–æ–∑–∏—Ü–∏—è', ascending=True).reset_index(drop=True)
        # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï 1: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–≥–∞ (‚Ññ) –≤ –Ω–∞—á–∞–ª–æ ---
        table_rel_df.insert(0, '‚Ññ', table_rel_df.index + 1)
        # -----------------------------------------------------------------
        
        return {"depth": pd.DataFrame(), "hybrid": pd.DataFrame(), "ngrams": pd.DataFrame(), "relevance_top": table_rel_df, "my_score": {"width": 0, "depth": 0}}


    # –î–∞–ª—å—à–µ —Ä–∞—Å—á–µ—Ç—ã –∏–¥—É—Ç —Ç–æ–ª—å–∫–æ –ø–æ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã–º comp_docs
    avg_len = np.mean([len(d['body']) for d in comp_docs])
    norm_k = (my_len / avg_len) if (settings['norm'] and my_len > 0 and avg_len > 0) else 1.0
    
    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    N = len(comp_docs) # N - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
        
    # ... (–û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å —Ä–∞—Å—á–µ—Ç–∞ table_depth, table_hybrid, table_ngrams) ...
    
    # –†–∞—Å—á–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã TOP
    # 2. –ë–∞–ª–ª—ã –¥–ª—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–ø–æ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã–º)
    competitor_stats_raw = []
    # ... (–†–∞—Å—á–µ—Ç raw_width, raw_depth) ...
    # ... (–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ max_width_top, max_depth_top) ...
    # 3. –ë–∞–ª–ª—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (—Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ –≤—Å–µ–º, –∫—Ç–æ –±—ã–ª –≤ original_results)
    table_rel = []
    for c in competitor_stats_raw:
        score_w = int(round((c['raw_w'] / max_width_top) * 100))
        score_d = int(round((c['raw_d'] / max_depth_top) * 100))
        table_rel.append({
            "–î–æ–º–µ–Ω": c['domain'],
            "–ü–æ–∑–∏—Ü–∏—è": c['pos'], # –≠—Ç–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–∑–∏—Ü–∏—è –≤ SERP
            "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": score_w,
            "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": score_d
        })
        
    # 4. –ë–∞–ª–ª—ã –¥–ª—è –í–ê–®–ï–ì–û —Å–∞–π—Ç–∞
    # ... (–†–∞—Å—á–µ—Ç my_score_w, my_score_d) ...
    
    # –î–æ–±–∞–≤–ª—è–µ–º –í–ê–® —Å–∞–π—Ç –≤ —Ç–∞–±–ª–∏—Ü—É
    if my_data and my_data.get('domain'):
        my_label = f"{my_data['domain']} (–í—ã)"
    else:
        my_label = "–í–∞—à —Å–∞–π—Ç"
        
    table_rel.append({
        "–î–æ–º–µ–Ω": my_label,
        "–ü–æ–∑–∏—Ü–∏—è": my_serp_pos if my_serp_pos > 0 else len(original_results) + 1, # –°—Ç–∞–≤–∏–º –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞
        "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": my_score_w,
        "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": my_score_d
    })

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ –ø–æ–∑–∏—Ü–∏–∏
    table_rel_df = pd.DataFrame(table_rel)
    table_rel_df = table_rel_df.sort_values(by='–ü–æ–∑–∏—Ü–∏—è', ascending=True).reset_index(drop=True)

    # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï 1: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–≥–∞ (‚Ññ) –≤ –Ω–∞—á–∞–ª–æ ---
    table_rel_df.insert(0, '‚Ññ', table_rel_df.index + 1)
    # -----------------------------------------------------------------
    
    return {
        "depth": pd.DataFrame(table_depth),
        "hybrid": pd.DataFrame(table_hybrid),
        "ngrams": pd.DataFrame(table_ngrams),
        "relevance_top": table_rel_df,
        "my_score": {"width": my_score_w, "depth": my_score_d}
    }

# ==========================================
# 5. –§–£–ù–ö–¶–ò–Ø –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø (FINAL)
# ==========================================

def render_paginated_table(df, title_text, key_prefix, default_sort_col=None, use_abs_sort_default=False):
    # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï 3: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–Ω–æ–ø–æ–∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è CSV/XML ---
    st.markdown(f"#### {title_text}")

    df_for_download = df.copy() 
    # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø–µ—Ä–µ–¥ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ–º
    if 'diff_abs' in df_for_download.columns:
        df_for_download = df_for_download.drop(columns=['diff_abs'])
    if 'is_missing' in df_for_download.columns:
        df_for_download = df_for_download.drop(columns=['is_missing'])
        
    csv_data = convert_df_to_csv(df_for_download)
    xml_data = convert_df_to_xml(df_for_download, root_name=key_prefix, row_name="item")

    c_dl1, c_dl2, c_dl_spacer = st.columns([1, 1, 8])

    with c_dl1:
        st.download_button(
            label="‚¨áÔ∏è CSV",
            data=csv_data,
            file_name=f"{key_prefix}.csv",
            mime="text/csv",
            key=f"{key_prefix}_dl_csv",
            use_container_width=True
        )

    with c_dl2:
        st.download_button(
            label="‚¨áÔ∏è XML",
            data=xml_data,
            file_name=f"{key_prefix}.xml",
            mime="text/xml",
            key=f"{key_prefix}_dl_xml",
            use_container_width=True
        )
    # -------------------------------------------------------------------------
    
    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    if default_sort_col and default_sort_col in df.columns:
        # ... (–ª–æ–≥–∏–∫–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏)
        if use_abs_sort_default:
            df = df.sort_values(by='diff_abs', ascending=False).reset_index(drop=True)
        else:
            df = df.sort_values(by=default_sort_col, ascending=False).reset_index(drop=True)
            
    df = df.reset_index(drop=True)
    df.index = df.index + 1 # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ Streamlit (1, 2, 3...)
    ROWS_PER_PAGE = 20 
    # ... (–û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –∏ –æ—Ç—Ä–∏—Å–æ–≤–∫–∏) ...
    
    # –í–´–í–û–î –¢–ê–ë–õ–ò–¶–´
    dynamic_height = (len(df_view) * 35) + 40
    st.dataframe(
        styled_df,
        use_container_width=True,
        height=dynamic_height,
        column_config={c: None for c in cols_to_hide}
    )
    # ... (–ö–Ω–æ–ø–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü) ...
    st.markdown("---")

# --- –§–£–ù–ö–¶–ò–Ø –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø –°–¢–†–ê–ù–ò–¶–´ –ò–°–¢–û–†–ò–ò ---
def render_history_page():
    st.title("üìä –ò—Å—Ç–æ—Ä–∏—è –ê–Ω–∞–ª–∏–∑–æ–≤")
    st.markdown("–ó–¥–µ—Å—å —Ö—Ä–∞–Ω—è—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –≤–∞—à–∏—Ö –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–¥–∞—á.")
    
    all_results = load_results()
    
    if not all_results:
        st.info("–ò—Å—Ç–æ—Ä–∏—è –∑–∞–¥–∞—á –ø—É—Å—Ç–∞.")
        return
        
    for idx, task in enumerate(all_results):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º expanser –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        header = f"[{task['date_str']}] {task['query']} ({task['url']} / {task['region']})"
        with st.expander(header):
            st.markdown(f"**–ó–∞–ø—Ä–æ—Å:** {task['query']}")
            st.markdown(f"**URL:** {task['url']}")
            st.markdown(f"**–†–µ–≥–∏–æ–Ω/–ü–°:** {task['region']} / {task['engine']}")
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
            st.markdown(f""" 
                <div style='background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 6px; border: 1px solid {BORDER_COLOR};'>
                    <h5 style='margin:0; color: {PRIMARY_COLOR};'>–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ (–≤ –±–∞–ª–ª–∞—Ö)</h5>
                    <p style='margin:5px 0 0 0;'>–®–∏—Ä–∏–Ω–∞ (–æ—Ö–≤–∞—Ç): <b>{task['my_score']['width']}</b> | –ì–ª—É–±–∏–Ω–∞ (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è): <b>{task['my_score']['depth']}</b></p>
                </div>
            """, unsafe_allow_html=True)
            
            # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –≤ —Ç–µ–∫—É—â–∏–π –∞–Ω–∞–ª–∏–∑
            if st.button(f"–ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã", key=f"show_details_{idx}"):
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ–±—Ä–∞—Ç–Ω–æ –≤ DataFrames
                st.session_state.analysis_results = {
                    'depth': pd.DataFrame.from_records(task['depth']),
                    'hybrid': pd.DataFrame.from_records(task['hybrid']),
                    'ngrams': pd.DataFrame.from_records(task['ngrams']),
                    'relevance_top': pd.DataFrame.from_records(task['relevance_top']),
                    'my_score': task['my_score']
                }
                st.session_state.analysis_done = True
                st.session_state.app_page = "–ê–Ω–∞–ª–∏–∑" # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∞–Ω–∞–ª–∏–∑–∞
                st.rerun()

# ==========================================
# 6. –ò–ù–¢–ï–†–§–ï–ô–°
# ==========================================

col_main, col_sidebar = st.columns([65, 35])

with col_sidebar:
    st.session_state.app_page = st.radio(
        "–ù–∞–≤–∏–≥–∞—Ü–∏—è",
        ["–ê–Ω–∞–ª–∏–∑", "–ò—Å—Ç–æ—Ä–∏—è"],
        index=0 if st.session_state.app_page == "–ê–Ω–∞–ª–∏–∑" else 1,
        key="app_page_select"
    )
    
if st.session_state.app_page == "–ò—Å—Ç–æ—Ä–∏—è":
    render_history_page()
    
elif st.session_state.app_page == "–ê–Ω–∞–ª–∏–∑":
    
    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")
        # ... (–û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞: –≤–≤–æ–¥ URL, –∑–∞–ø—Ä–æ—Å–∞, –Ω–∞—Å—Ç—Ä–æ–µ–∫) ...

        # ... (–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ —Å–∞–π–¥–±–∞—Ä–µ - –æ—Å—Ç–∞—é—Ç—Å—è —Ç–∞–º –∂–µ, –Ω–æ –ø–æ—Å–ª–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏) ...
        
        with col_sidebar:
            st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
            ua = st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
            search_engine = st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
            region = st.selectbox("–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞", list(REGION_MAP.keys()), key="settings_region")
            device = st.selectbox("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", ["Desktop", "Mobile"], key="settings_device")
            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞, –∫–æ—Ç–æ—Ä—É—é –ø–æ–∑–≤–æ–ª—è–µ—Ç API - 30. 
            top_n = st.selectbox("–ì–ª—É–±–∏–Ω–∞ —Å–±–æ—Ä–∞ (–¢–û–ü)", [10, 20, 30], index=0, key="settings_top_n")
            # ... (–û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–∫) ...
            
        # ... (–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Å–µ–∫—Ü–∏—è 7. –í–´–ü–û–õ–ù–ï–ù–ò–ï) ...
        # ==========================================
        # 7. –í–´–ü–û–õ–ù–ï–ù–ò–ï (–°–ö–û–†–†–ï–ö–¢–ò–†–û–í–ê–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –°–ë–û–†–ê)
        # ==========================================
        if st.session_state.get('start_analysis_flag'):
            st.session_state.start_analysis_flag = False
            # ... (–ü—Ä–æ–≤–µ—Ä–∫–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö) ... 
            
            # ... (–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –ø–∞—Ä—Å–∏–Ω–≥, –∏ —Ç.–¥.) ...
            
            # ... (–í–´–ó–û–í calculate_metrics) ...
            with st.spinner("–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö..."):
                results = calculate_metrics(
                    comp_data_full, my_data, settings, my_serp_pos, target_urls_raw # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–∏—Å–æ–∫ URL:pos, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –æ—Ç–æ–±—Ä–∞–ª–∏
                )
                
            st.session_state.analysis_results = results
            st.session_state.analysis_done = True
            
            # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï 2: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é ---
            if st.session_state.analysis_results:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º DataFrames –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç (—Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π)
                new_result = {
                    "timestamp": time.time(),
                    "date_str": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "query": st.session_state.get('query_input', 'N/A'),
                    "url": st.session_state.get('my_url_input', 'N/A'),
                    "region": st.session_state.settings_region,
                    "engine": st.session_state.settings_search_engine,
                    "depth": results['depth'].to_dict(orient='records'),
                    "hybrid": results['hybrid'].to_dict(orient='records'),
                    "ngrams": results['ngrams'].to_dict(orient='records'),
                    "relevance_top": results['relevance_top'].to_dict(orient='records'),
                    "my_score": results['my_score']
                }
                
                all_results = load_results()
                all_results.insert(0, new_result) # –î–æ–±–∞–≤–ª—è–µ–º –≤ –Ω–∞—á–∞–ª–æ
                save_results(all_results)
            # ---------------------------------------------------
            
            st.rerun()

        if st.session_state.analysis_done and st.session_state.analysis_results:
            results = st.session_state.analysis_results
            st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
            # ... (–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤) ...
            
            # ... (–í—ã–∑–æ–≤—ã render_paginated_table - –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –Ω–æ —Ç–µ–ø–µ—Ä—å –æ–Ω–∏ –≤–∫–ª—é—á–∞—é—Ç –∫–Ω–æ–ø–∫–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è) ...
