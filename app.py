import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin, unquote
import inspect
import time
import json
import io
import os
import random
import streamlit.components.v1 as components

# --- NLP Libraries ---
try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except ImportError:
    morph = None
    USE_NLP = False

try:
    import openai
except ImportError:
    openai = None

# ==========================================
# 0. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò –°–¢–ò–õ–ò
# ==========================================

st.set_page_config(layout="wide", page_title="GAR PRO v3.5 (Pipeline)", page_icon="üè≠")

# –¶–≤–µ—Ç–∞
PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE"

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif; color: {TEXT_COLOR} !important; }}
        
        /* –ö–Ω–æ–ø–∫–∏ */
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important; }}
        
        /* –ü–æ–ª—è –≤–≤–æ–¥–∞ */
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        
        /* –¢–∞–±–ª–∏—Ü—ã */
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important; color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        
        /* –ö–∞—Ä—Ç–æ—á–∫–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ (–í–∫–ª–∞–¥–∫–∞ 2) */
        .tool-card {{ 
            padding: 20px; 
            border: 1px solid #E2E8F0; 
            border-radius: 10px; 
            background-color: #F8FAFC; 
            margin-bottom: 20px; 
        }}
        .block-title {{ 
            color: {PRIMARY_COLOR}; 
            font-size: 1.2em; 
            font-weight: bold; 
            margin-bottom: 10px; 
            display: flex; 
            align-items: center; 
        }}
        .block-icon {{ margin-right: 10px; font-size: 1.2em; }}
        
        /* –õ–µ–≥–µ–Ω–¥–∞ (–í–∫–ª–∞–¥–∫–∞ 1) */
        .legend-box {{ padding: 10px; background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px; }}
        .text-red {{ color: #D32F2F; font-weight: bold; }}
        .text-green {{ color: #2E7D32; font-weight: bold; }}
        
        .stApp > header {{ background-color: transparent !important; }}
    </style>
""", unsafe_allow_html=True)

# –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
def check_password():
    if st.session_state.get("authenticated"): return True
    st.markdown("""<style>.main { display: flex; flex-direction: column; justify-content: center; align-items: center; } .auth-logo-box { text-align: center; margin-bottom: 1rem; padding-top: 0; }</style>""", unsafe_allow_html=True)
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="auth-logo-box"><h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3></div>', unsafe_allow_html=True)
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "jfV6Xel-Q7vp-_s2UYPO":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password(): st.stop()

# ==========================================
# 1. –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –ò –ü–ï–†–ï–ú–ï–ù–ù–´–ï
# ==========================================

# --- API Keys ---
if "arsenkin_token" in st.session_state: ARSENKIN_TOKEN = st.session_state.arsenkin_token
else:
    try: ARSENKIN_TOKEN = st.secrets["api"]["arsenkin_token"]
    except: ARSENKIN_TOKEN = None

if "yandex_dict_key" in st.session_state: YANDEX_DICT_KEY = st.session_state.yandex_dict_key
else:
    try: YANDEX_DICT_KEY = st.secrets["api"]["yandex_dict_key"]
    except: YANDEX_DICT_KEY = None

REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    # ... –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ—Å—Ç–∞–ª—å–Ω—ã–µ
}

DEFAULT_EXCLUDE = "avito.ru\nyandex.ru\nozon.ru\nwildberries.ru\nmarket.yandex.ru\ntiu.ru"
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n–∫—É–ø–∏—Ç—å\n—Ü–µ–Ω–∞\n—à—Ç\n—Å–º\n–º–º\n–∫–≥"
GARBAGE_LATIN_STOPLIST = {'whatsapp', 'viber', 'telegram', 'vk', 'instagram', 'facebook', 'youtube', 'twitter', 'cookie', 'policy', 'privacy', 'terms', 'cart', 'order', 'call', 'back', 'login', 'sign', 'search', 'menu', 'nav', 'footer', 'header', 'sidebar', 'img', 'png', 'jpg'}

# --- Helpers ---
def transliterate_text(text):
    mapping = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }
    result = []
    for char in text.lower():
        if char in mapping: result.append(mapping[char])
        elif char.isalnum() or char == '-': result.append(char)
    return "".join(result)

def force_cyrillic_name_global(slug_text):
    raw = unquote(slug_text).lower().replace('.html', '').replace('.php', '')
    if re.search(r'[–∞-—è]', raw):
        return raw.replace('-', ' ').replace('_', ' ').capitalize()
    
    words = re.split(r'[-_]', raw)
    rus_words = []
    # –°–æ–∫—Ä–∞—â–µ–Ω–Ω–∞—è –∫–∞—Ä—Ç–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞, –≤ –ø–æ–ª–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –æ–Ω–∞ –±–æ–ª—å—à–µ
    exact_map = {'nikel': '–Ω–∏–∫–µ–ª—å', 'stal': '—Å—Ç–∞–ª—å', 'med': '–º–µ–¥—å', 'truba': '—Ç—Ä—É–±–∞', 'list': '–ª–∏—Å—Ç', 'krug': '–∫—Ä—É–≥'}
    for w in words:
        if not w: continue
        if w in exact_map: rus_words.append(exact_map[w])
        else: rus_words.append(w)
    return " ".join(rus_words).capitalize()

@st.cache_data
def load_lemmatized_dictionaries():
    # –ó–∞–≥–ª—É—à–∫–∞. –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ç—É—Ç —á—Ç–µ–Ω–∏–µ JSON —Ñ–∞–π–ª–æ–≤
    return set(), set(), set(), set(), set()

def classify_semantics_with_api(words_list, yandex_key):
    PRODUCTS_SET, COMM_SET, SPECS_SET, GEO_SET, SERVICES_SET = load_lemmatized_dictionaries()
    DEFAULT_COMMERCIAL = {'—Ü–µ–Ω–∞', '–∫—É–ø–∏—Ç—å', '–ø—Ä–∞–π—Å', '–∫–æ—Ä–∑–∏–Ω–∞', '–∑–∞–∫–∞–∑', '—Ä—É–±', '–Ω–∞–ª–∏—á–∏–µ', '—Å–∫–ª–∞–¥', '–º–∞–≥–∞–∑–∏–Ω', '–∞–∫—Ü–∏—è', '—Å–∫–∏–¥–∫–∞', '–æ–ø—Ç', '—Ä–æ–∑–Ω–∏—Ü–∞', '–∫–∞—Ç–∞–ª–æ–≥', '—Ç–µ–ª–µ—Ñ–æ–Ω'}
    categories = {'products': set(), 'services': set(), 'commercial': set(), 'dimensions': set(), 'geo': set(), 'general': set()}
    
    for word in words_list:
        word_lower = word.lower()
        if morph:
            p = morph.parse(word_lower)[0]
            lemma = p.normal_form
        else:
            lemma = word_lower

        if lemma in SPECS_SET: categories['dimensions'].add(lemma); continue
        if lemma in PRODUCTS_SET: categories['products'].add(lemma); continue
        if lemma in GEO_SET: categories['geo'].add(lemma); continue
        if lemma in SERVICES_SET: categories['services'].add(lemma); continue
        if lemma in COMM_SET or lemma in DEFAULT_COMMERCIAL: categories['commercial'].add(lemma); continue
        categories['general'].add(lemma)

    return {k: sorted(list(v)) for k, v in categories.items()}

# --- Parsing & Metrics (–î–ª—è –¢–∞–±–∞ 1) ---
def get_arsenkin_urls(query, engine_type, region_name, api_token, depth_val=10):
    if not api_token: return []
    # (–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ API Arsenkin, —á—Ç–æ–±—ã –Ω–µ –∑–∞–Ω–∏–º–∞—Ç—å –º–µ—Å—Ç–æ, –Ω–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è)
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check"
    url_get = "https://arsenkin.ru/api/tools/get"
    headers = {"Authorization": f"Bearer {api_token}", "Content-type": "application/json"}
    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []
    if "–Ø–Ω–¥–µ–∫—Å" in engine_type: se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type: se_params.append({"type": 11, "region": reg_ids['go']})
    
    payload = {"tools_name": "check-top", "data": {"queries": [query], "is_snippet": False, "noreask": True, "se": se_params, "depth": depth_val}}
    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        task_id = r.json().get("task_id")
        if not task_id: return []
    except: return []

    status = "process"
    attempts = 0
    while status == "process" and attempts < 40:
        time.sleep(5); attempts += 1
        try:
            if requests.post(url_check, headers=headers, json={"task_id": task_id}).json().get("status") == "finish": status = "done"; break
        except: pass
    
    if status != "done": return []
    try:
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id}, timeout=30)
        collect = r_final.json().get('result', {}).get('result', {}).get('collect')
        res = []
        if collect and isinstance(collect, list) and len(collect)>0 and isinstance(collect[0], list):
            for i, u in enumerate(collect[0][0]): res.append({'url': u, 'pos': i+1})
        return res
    except: return []

def process_text_detailed(text, settings, n_gram=1):
    text = text.lower().replace('—ë', '–µ')
    words = re.findall(r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+', text)
    stops = set(w.lower().replace('—ë', '–µ') for w in settings['custom_stops'])
    lemmas = []
    forms_map = defaultdict(set)
    for w in words:
        if len(w) < 2 or (not settings['numbers'] and w.isdigit()) or w in stops: continue
        lemma = w
        if USE_NLP and n_gram == 1:
            p = morph.parse(w)[0]
            if 'PREP' not in p.tag and 'CONJ' not in p.tag: lemma = p.normal_form.replace('—ë', '–µ')
        lemmas.append(lemma)
        forms_map[lemma].add(w)
    return lemmas, forms_map

def parse_page(url, settings):
    headers = {'User-Agent': settings['ua']}
    try:
        r = requests.get(url, headers=headers, timeout=10)
        if r.status_code != 200: return None
        soup = BeautifulSoup(r.text, 'html.parser')
        for c in soup.find_all(string=lambda text: isinstance(text, Comment)): c.extract()
        if settings['noindex']: 
            for t in soup.find_all('noindex'): t.decompose()
        
        anchors = " ".join([a.get_text(strip=True) for a in soup.find_all('a')])
        body_text_raw = soup.get_text(separator=' ')
        body_text = re.sub(r'\s+', ' ', body_text_raw).strip()
        if not body_text: return None
        return {'url': url, 'domain': urlparse(url).netloc, 'body_text': body_text, 'anchor_text': anchors}
    except: return None

def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    all_forms_map = defaultdict(set)
    if not my_data or not my_data.get('body_text'): my_lemmas, my_len = [], 0
    else:
        my_lemmas, my_forms = process_text_detailed(my_data['body_text'], settings)
        my_len = len(my_lemmas)
        for k, v in my_forms.items(): all_forms_map[k].update(v)

    comp_docs = []
    for p in comp_data_full:
        if not p: continue
        body, c_forms = process_text_detailed(p['body_text'], settings)
        comp_docs.append({'body': body, 'url': p['url']})
        for k, v in c_forms.items(): all_forms_map[k].update(v)

    if not comp_docs: return { "depth": pd.DataFrame(), "relevance_top": pd.DataFrame(), "my_score": {"width": 0, "depth": 0}, "missing_semantics_high": [], "missing_semantics_low": [] }

    # (–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞, –Ω–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π)
    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    N = len(comp_docs)
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
    
    missing_high, missing_low = [], []
    table_depth = []
    
    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        df = doc_freqs[lemma]
        if df < 2 and lemma not in my_lemmas: continue
        my_tf = my_lemmas.count(lemma)
        med_tf = np.median([d['body'].count(lemma) for d in comp_docs])
        
        if lemma not in my_lemmas:
            if med_tf >= 1: missing_high.append({'word': lemma})
            elif df >= N/3: missing_low.append({'word': lemma})
        
        table_depth.append({
            "–°–ª–æ–≤–æ": lemma, "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_tf, "–ú–µ–¥–∏–∞–Ω–∞": med_tf,
            "–°—Ç–∞—Ç—É—Å": "–ù–µ–¥–æ—Å–ø–∞–º" if my_tf < med_tf else "–ù–æ—Ä–º–∞"
        })

    return { 
        "depth": pd.DataFrame(table_depth), 
        "relevance_top": pd.DataFrame(original_results), # –ó–∞–≥–ª—É—à–∫–∞
        "my_score": {"width": 50, "depth": 50}, 
        "missing_semantics_high": missing_high, 
        "missing_semantics_low": missing_low 
    }

def render_paginated_table(df, title_text, key_prefix):
    if df.empty: st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö."); return
    st.markdown(f"### {title_text}")
    st.dataframe(df, use_container_width=True, height=400)

# --- AI & Generator Functions (–î–ª—è –¢–∞–±–∞ 2) ---
STATIC_DATA_GEN = {
    'IP_PROP4817': "–£—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏",
    'IP_PROP4818': "–û–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –≤ —Ä–µ–≥–∏–æ–Ω—ã —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4821': "–û–ø–ª–∞—Ç–∞ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã",
    'IP_PROP4824': "–û–ø–∏—Å–∞–Ω–∏–µ, —Å—Ç–∞—Ç—å–∏, –ø–æ–∏—Å–∫, –æ—Ç–∑—ã–≤—ã",
    'IP_PROP4825': "–ú–æ–∂–µ–º –º–µ—Ç–∞–ª–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –æ—Ü–∏–Ω–∫–æ–≤–∞—Ç—å",
    'IP_PROP4837': "–ü–æ—Ä—è–¥–æ–∫ –≤ –ì–û–°–¢–∞—Ö"
}

def generate_five_blocks(client, base_text, tag_name, seo_words=None):
    if not base_text: return ["No base text"] * 5
    system = "–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä. –ù–∞–ø–∏—à–∏ 5 HTML –±–ª–æ–∫–æ–≤."
    prompt = f"""–¢–µ–≥: {tag_name}. –ë–∞–∑–∞: {base_text[:2000]}.
    SEO: {", ".join(seo_words) if seo_words else "–Ω–µ—Ç"}.
    –ó–∞–¥–∞—á–∞: 5 –±–ª–æ–∫–æ–≤ HTML. –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: |||BLOCK_SEP|||"""
    try:
        resp = client.chat.completions.create(model="sonar-pro", messages=[{"role": "system", "content": system}, {"role": "user", "content": prompt}], temperature=0.7)
        content = resp.choices[0].message.content.replace("```html", "").replace("```", "")
        blocks = content.split("|||BLOCK_SEP|||")
        while len(blocks) < 5: blocks.append("")
        return blocks[:5]
    except Exception as e: return [f"Error: {e}"] * 5

def generate_html_table(client, prompt):
    sys = "Generate HTML table only. Inline CSS: border 2px solid black."
    try:
        resp = client.chat.completions.create(model="sonar-pro", messages=[{"role": "system", "content": sys}, {"role": "user", "content": prompt}], temperature=0.5)
        return resp.choices[0].message.content.replace("```html", "").replace("```", "").strip()
    except Exception as e: return f"Error: {e}"

# ==========================================
# STATE INIT (–° –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï–ú –û–®–ò–ë–û–ö)
# ==========================================
if 'sidebar_gen_df' not in st.session_state: st.session_state.sidebar_gen_df = None
if 'analysis_results' not in st.session_state: st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state: st.session_state.analysis_done = False
if 'categorized_products' not in st.session_state: st.session_state.categorized_products = []
if 'categorized_services' not in st.session_state: st.session_state.categorized_services = []
if 'categorized_commercial' not in st.session_state: st.session_state.categorized_commercial = []
if 'categorized_dimensions' not in st.session_state: st.session_state.categorized_dimensions = []
if 'categorized_geo' not in st.session_state: st.session_state.categorized_geo = []
if 'categorized_general' not in st.session_state: st.session_state.categorized_general = []
if 'persistent_urls' not in st.session_state: st.session_state['persistent_urls'] = ""

# ==========================================
# UI
# ==========================================
tab_seo, tab_gen = st.tabs(["üìä SEO –ê–Ω–∞–ª–∏–∑", "üè≠ –û–ø—Ç–æ–≤–∞—è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è"])

# ------------------------------------------
# TAB 1: SEO
# ------------------------------------------
with tab_seo:
    col_main, col_sidebar = st.columns([65, 35])
    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
        my_input_type = st.radio("–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], horizontal=True, label_visibility="collapsed", key="my_page_source_radio")
        
        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ —Å–∞–π—Ç–µ":
            st.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã", placeholder="https://site.ru/catalog/tovar", label_visibility="collapsed", key="my_url_input")
        elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥":
            st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥", height=200, label_visibility="collapsed", key="my_content_input")

        st.markdown("### –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å")
        st.text_input("–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞", label_visibility="collapsed", key="query_input")
        
        st.markdown("### –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        source_type_new = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫", ["API Arsenkin", "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"], horizontal=True, label_visibility="collapsed", key="competitor_source_radio")
        
        if source_type_new == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
            manual_val = st.text_area("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (—Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", height=200, key="manual_urls_widget", value=st.session_state.get('persistent_urls', ""))
            st.session_state['persistent_urls'] = manual_val

        st.markdown("### –°–ø–∏—Å–∫–∏ (Stop / Exclude)")
        st.text_area("–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–æ–º–µ–Ω—ã", DEFAULT_EXCLUDE, height=100, key="settings_excludes")
        st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞", DEFAULT_STOPS, height=100, key="settings_stops")
        
        if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", type="primary", use_container_width=True, key="start_analysis_btn"):
            st.session_state.start_analysis_flag = True

    with col_sidebar:
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API")
        if not ARSENKIN_TOKEN:
             st.text_input("Arsenkin Token", type="password", key="input_arsenkin")
        if not YANDEX_DICT_KEY:
             st.text_input("Yandex Dict Key", type="password", key="input_yandex")
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
        st.selectbox("User-Agent", ["Mozilla/5.0", "YandexBot/3.0"], key="settings_ua")
        st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google"], key="settings_search_engine")
        st.selectbox("–†–µ–≥–∏–æ–Ω", list(REGION_MAP.keys()), key="settings_region")
        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å <noindex>", True, key="settings_noindex")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", False, key="settings_alt")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", False, key="settings_numbers")
        st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True, key="settings_norm")
        st.selectbox("–ì–ª—É–±–∏–Ω–∞ (–¢–û–ü)", [10, 20, 30], index=0, key="settings_top_n")

    # –õ–æ–≥–∏–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
    if st.session_state.get('start_analysis_flag'):
        st.session_state.start_analysis_flag = False
        # ... (–ó–¥–µ—Å—å –∏–¥–µ—Ç –ª–æ–≥–∏–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, –≤—ã–∑–æ–≤–∞ parse_page –∏ calculate_metrics)
        # –î–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏: –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, –¥–∞–Ω–Ω—ã–µ —Å–æ–±—Ä–∞–Ω—ã –∏ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å—á–∏—Ç–∞–Ω—ã
        # –í —Ä–∞–±–æ—á–µ–º –∫–æ–¥–µ –∑–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–Ω—ã–π –±–ª–æ–∫ —Å ThreadPoolExecutor
        
        # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ç–∞–±–∞ –±–µ–∑ API
        # –í —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–¥–µ —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –ª–æ–≥–∏–∫—É
        st.warning("‚ö†Ô∏è –î–ª—è —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã –Ω—É–∂–µ–Ω API. –°–µ–π—á–∞—Å (–≤ –ø—Ä–∏–º–µ—Ä–µ) –ª–æ–≥–∏–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞, –Ω–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞ –º–µ—Å—Ç–µ.")
        # ...

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if st.session_state.analysis_results:
        res = st.session_state.analysis_results
        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        with st.expander("üõí –°–µ–º–∞–Ω—Ç–∏–∫–∞", expanded=True):
            st.info(f"–¢–æ–≤–∞—Ä—ã: {len(st.session_state.categorized_products)}")
            st.write(", ".join(st.session_state.categorized_products))
        
        render_paginated_table(res['depth'], "1. –ì–ª—É–±–∏–Ω–∞", "tbl_depth")

# ------------------------------------------
# TAB 2: –û–ü–¢–û–í–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø (–ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê)
# ------------------------------------------
with tab_gen:
    st.title("üè≠ –¶–µ–Ω—Ç—Ä –û–ø—Ç–æ–≤–æ–π –ì–µ–Ω–µ—Ä–∞—Ü–∏–∏ (Pipeline)")
    st.markdown("–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –≤—ã–±–µ—Ä–∏—Ç–µ –Ω—É–∂–Ω—ã–µ –º–æ–¥—É–ª–∏ –∏ –ø–æ–ª—É—á–∏—Ç–µ –µ–¥–∏–Ω—ã–π Excel-–æ—Ç—á–µ—Ç.")

    # --- 1. –ì–õ–û–ë–ê–õ–¨–ù–´–ï –í–í–û–î–ù–´–ï ---
    with st.container():
        st.markdown('<div class="tool-card" style="border-left: 5px solid #277EFF;">', unsafe_allow_html=True)
        st.markdown("### üåç –ì–ª–∞–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
        col_g1, col_g2 = st.columns([1, 1])
        with col_g1:
            if 'global_pplx_key' not in st.session_state: st.session_state.global_pplx_key = "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"
            st.session_state.global_pplx_key = st.text_input("üîë Perplexity API Key", value=st.session_state.global_pplx_key, type="password")
        with col_g2:
            if 'global_parent_url' not in st.session_state: st.session_state.global_parent_url = ""
            st.session_state.global_parent_url = st.text_input("üîó URL –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (–î–æ–Ω–æ—Ä)", value=st.session_state.global_parent_url, placeholder="https://site.ru/catalog/category/")
        st.caption("–≠—Ç–æ—Ç URL –±—É–¥–µ—Ç –ø—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω –æ–¥–∏–Ω —Ä–∞–∑ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π.")
        st.markdown('</div>', unsafe_allow_html=True)

    # --- 2. –í–´–ë–û–† –ú–û–î–£–õ–ï–ô ---
    st.subheader("üõ†Ô∏è –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥—É–ª–∏:")
    c_sel1, c_sel2, c_sel3, c_sel4, c_sel5 = st.columns(5)
    
    use_texts = c_sel1.checkbox("ü§ñ AI –¢–µ–∫—Å—Ç—ã", value=True)
    use_tags = c_sel2.checkbox("üè∑Ô∏è –ü–ª–∏—Ç–∫–∞ —Ç–µ–≥–æ–≤")
    use_sidebar = c_sel3.checkbox("üìë –ë–æ–∫–æ–≤–æ–µ –º–µ–Ω—é")
    use_tables = c_sel4.checkbox("üß© –¢–∞–±–ª–∏—Ü—ã")
    use_promo = c_sel5.checkbox("üî• –ü—Ä–æ–º–æ-–∞–∫—Ü–∏–∏")

    st.markdown("---")

    # --- 3. –ù–ê–°–¢–†–û–ô–ö–ò (–ü–û–Ø–í–õ–Ø–Æ–¢–°–Ø –ï–°–õ–ò –í–´–ë–†–ê–ù–û) ---
    
    # AI –¢–µ–∫—Å—Ç—ã
    seo_words_str = ""
    if use_texts:
        with st.container():
            st.markdown('<div class="tool-card"><div class="block-title"><span class="block-icon">ü§ñ</span> –ù–∞—Å—Ç—Ä–æ–π–∫–∏: AI –¢–µ–∫—Å—Ç—ã</div>', unsafe_allow_html=True)
            seo_words_str = st.text_input("SEO —Å–ª–æ–≤–∞", placeholder="–∫—É–ø–∏—Ç—å, —Ü–µ–Ω–∞...", key="txt_seo")
            st.markdown('</div>', unsafe_allow_html=True)

    # –ü–ª–∏—Ç–∫–∞ —Ç–µ–≥–æ–≤
    tags_file = None
    tags_products_in = ""
    if use_tags:
        with st.container():
            st.markdown('<div class="tool-card"><div class="block-title"><span class="block-icon">üè∑Ô∏è</span> –ù–∞—Å—Ç—Ä–æ–π–∫–∏: –¢–µ–≥–∏</div>', unsafe_allow_html=True)
            c1, c2 = st.columns(2)
            with c1: tags_file = st.file_uploader("–ë–∞–∑–∞ —Å—Å—ã–ª–æ–∫ (.txt)", type=["txt"], key="tags_f")
            with c2: tags_products_in = st.text_area("–°–ø–∏—Å–æ–∫ –∞–Ω–∫–æ—Ä–æ–≤", height=100, key="tags_p")
            st.markdown('</div>', unsafe_allow_html=True)

    # –ú–µ–Ω—é
    sb_file = None
    if use_sidebar:
        with st.container():
            st.markdown('<div class="tool-card"><div class="block-title"><span class="block-icon">üìë</span> –ù–∞—Å—Ç—Ä–æ–π–∫–∏: –ú–µ–Ω—é</div>', unsafe_allow_html=True)
            sb_file = st.file_uploader("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–µ–Ω—é (.txt)", type=["txt"], key="sb_f")
            st.markdown('</div>', unsafe_allow_html=True)

    # –¢–∞–±–ª–∏—Ü—ã
    table_headers = []
    if use_tables:
        with st.container():
            st.markdown('<div class="tool-card"><div class="block-title"><span class="block-icon">üß©</span> –ù–∞—Å—Ç—Ä–æ–π–∫–∏: –¢–∞–±–ª–∏—Ü—ã</div>', unsafe_allow_html=True)
            n_tabs = st.selectbox("–ö–æ–ª-–≤–æ —Ç–∞–±–ª–∏—Ü", [1, 2, 3], key="tbl_n")
            cols = st.columns(n_tabs)
            for i, col in enumerate(cols):
                th = col.text_input(f"–¢–µ–º–∞ {i+1}", value=f"–¢–∞–±–ª–∏—Ü–∞ {i+1}", key=f"tbl_h_{i}")
                table_headers.append(th)
            st.markdown('</div>', unsafe_allow_html=True)

    # –ü—Ä–æ–º–æ
    promo_db = None
    promo_links_str = ""
    promo_h3 = ""
    if use_promo:
        with st.container():
            st.markdown('<div class="tool-card"><div class="block-title"><span class="block-icon">üî•</span> –ù–∞—Å—Ç—Ä–æ–π–∫–∏: –ü—Ä–æ–º–æ</div>', unsafe_allow_html=True)
            c1, c2 = st.columns(2)
            with c1: 
                promo_db = st.file_uploader("–ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ (.xlsx)", type=['xlsx'], key="promo_f")
                promo_h3 = st.text_input("–ó–∞–≥–æ–ª–æ–≤–æ–∫", value="–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º", key="promo_h")
            with c2: 
                promo_links_str = st.text_area("–°—Å—ã–ª–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤", height=100, key="promo_l")
            st.markdown('</div>', unsafe_allow_html=True)

    # --- 4. –ï–î–ò–ù–ê–Ø –ö–ù–û–ü–ö–ê –ó–ê–ü–£–°–ö–ê ---
    st.markdown("---")
    if st.button("üöÄ –ó–ê–ü–£–°–¢–ò–¢–¨ –ì–ï–ù–ï–†–ê–¶–ò–Æ (–í–°–ï –í –û–î–ò–ù –§–ê–ô–õ)", type="primary", use_container_width=True):
        if not st.session_state.global_parent_url:
            st.error("–£–∫–∞–∂–∏—Ç–µ URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏!"); st.stop()
        
        status = st.status("‚öôÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...", expanded=True)
        
        # 1. –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–Ω–æ—Ä–∞ (–æ–±—â–∏–π)
        status.write("üïµÔ∏è –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–Ω–æ—Ä–∞...")
        parsed_items = [] # List of dicts
        base_text_context = ""
        try:
            r = requests.get(st.session_state.global_parent_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)
            if r.status_code == 200:
                soup = BeautifulSoup(r.text, 'html.parser')
                # –¢–µ–∫—Å—Ç –¥–ª—è AI
                d_div = soup.find('div', class_='description-container')
                base_text_context = d_div.get_text(separator="\n", strip=True) if d_div else soup.body.get_text()[:3000]
                # –¢–µ–≥–∏ (—Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
                tc = soup.find(class_='popular-tags-inner')
                if tc:
                    for a in tc.find_all('a'):
                        href = a.get('href')
                        if href:
                            parsed_items.append({'TagName': a.get_text(strip=True), 'Page URL': urljoin(st.session_state.global_parent_url, href)})
            else:
                status.error(f"–û—à–∏–±–∫–∞ {r.status_code}"); st.stop()
        except Exception as e: status.error(f"–û—à–∏–±–∫–∞: {e}"); st.stop()

        if not parsed_items:
            parsed_items.append({'TagName': 'Main', 'Page URL': st.session_state.global_parent_url})
            status.warning("–¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Ä–∞–±–æ—Ç–∞–µ–º —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π.")
        
        status.write(f"–ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(parsed_items)}")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –º–æ–¥—É–ª–µ–π
        client = None
        if (use_texts or use_tables) and openai:
            client = openai.OpenAI(api_key=st.session_state.global_pplx_key, base_url="https://api.perplexity.ai")
        
        seo_list = [s.strip() for s in seo_words_str.split(',')] if seo_words_str else []
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¢–µ–≥–æ–≤ (–ö–∞—Ä—Ç–∞ –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏)
        tags_map = {}
        if use_tags and tags_file and tags_products_in:
            anchors = [l.strip() for l in tags_products_in.split('\n') if l.strip()]
            s_io = io.StringIO(tags_file.getvalue().decode("utf-8"))
            links_db = [l.strip() for l in s_io.readlines() if l.strip()]
            for anch in anchors:
                tr = transliterate_text(anch)
                if len(tr) > 2:
                    matches = [u for u in links_db if tr in u]
                    if matches: tags_map[anch] = matches

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ú–µ–Ω—é
        sidebar_html = ""
        if use_sidebar and sb_file:
            s_io = io.StringIO(sb_file.getvalue().decode("utf-8"))
            menu_urls = list(dict.fromkeys([l.strip() for l in s_io.readlines() if l.strip()]))
            # (–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ä–µ–≤–∞ - —Å—Ç–∞—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞)
            sidebar_html = f"<div class='sidebar'><ul>" + "".join([f"<li><a href='{u}'>Link</a></li>" for u in menu_urls[:5]]) + "</ul></div>"

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ü—Ä–æ–º–æ
        promo_block_html = ""
        if use_promo and promo_db and promo_links_str:
            p_df = pd.read_excel(promo_db)
            p_img_map = {str(r.iloc[0]).strip().rstrip('/'): str(r.iloc[1]).strip() for _, r in p_df.iterrows() if str(r.iloc[0]) != 'nan'}
            p_links = [l.strip() for l in promo_links_str.split('\n') if l.strip()]
            inner_html = ""
            for l in p_links:
                src = p_img_map.get(l.rstrip('/'), "")
                nm = force_cyrillic_name_global(l.split('/')[-1])
                inner_html += f'<div class="gallery-item"><h3><a href="{l}">{nm}</a></h3><figure><img src="{src}"></figure></div>'
            promo_block_html = f'<div class="gallery-wrapper"><h3>{promo_h3}</h3><div class="gallery">{inner_html}</div></div>'

        # –ì–õ–ê–í–ù–´–ô –¶–ò–ö–õ
        status.write("üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        progress_bar = status.progress(0)
        total_steps = len(parsed_items)
        
        path_parent = urlparse(st.session_state.global_parent_url).path.strip('/')
        parent_name_ru = force_cyrillic_name_global(path_parent.split('/')[-1])

        for idx, item in enumerate(parsed_items):
            current_url = item['Page URL']
            current_tag_name = item['TagName']
            
            # 1. –¢–ï–ö–°–¢–´
            if use_texts and client:
                blocks = generate_five_blocks(client, base_text_context, current_tag_name, seo_list)
                item['IP_PROP4839'] = blocks[0]
                item['IP_PROP4816'] = blocks[1]
                item['IP_PROP4838'] = blocks[2]
                item['IP_PROP4829'] = blocks[3]
                item['IP_PROP4831'] = blocks[4]
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏–∫—É
                for k, v in STATIC_DATA_GEN.items():
                    item[k] = v
            
            # 2. –¢–ï–ì–ò
            if use_tags:
                my_tags = []
                for anch, urls in tags_map.items():
                    # –ò—Å–∫–ª—é—á–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–∞–º—É —Å–µ–±—è
                    valid_u = [u for u in urls if u.rstrip('/') != current_url.rstrip('/')]
                    if valid_u:
                        my_tags.append({'name': anch.capitalize(), 'url': random.choice(valid_u)})
                
                if my_tags:
                    random.shuffle(my_tags)
                    # –ì–µ–Ω–µ—Ä–∏–º HTML
                    thtml = '<div class="popular-tags">\n' + "\n".join([f'<a href="{t["url"]}" class="tag-link">{t["name"]}</a>' for t in my_tags]) + '\n</div>'
                    item['Tags_HTML'] = thtml
                else:
                    item['Tags_HTML'] = ""

            # 3. –ú–ï–ù–Æ
            if use_sidebar:
                # –í—Å—Ç–∞–≤–ª—è–µ–º –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π HTML –¥–ª—è –≤—Å–µ—Ö (–∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ –º–µ–Ω—é —Å–∫–≤–æ–∑–Ω–æ–µ)
                item['Sidebar_HTML'] = sidebar_html

            # 4. –¢–ê–ë–õ–ò–¶–´
            if use_tables and client:
                full_prod_name = f"{parent_name_ru} {current_tag_name}"
                for ti, th in enumerate(table_headers):
                    prompt = f"Make HTML table. Product: {full_prod_name}. Topic: {th}."
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–≤–æ—é —Ñ—É–Ω–∫—Ü–∏—é generate_html_table
                    t_html = generate_html_table(client, prompt)
                    item[f'Table_{ti+1}_HTML'] = t_html

            # 5. –ü–†–û–ú–û
            if use_promo:
                item['Promo_HTML'] = promo_block_html

            progress_bar.progress((idx + 1) / total_steps)

        # –§–ò–ù–ê–õ
        status.write("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Excel...")
        df_result = pd.DataFrame(parsed_items)
        
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            df_result.to_excel(writer, index=False)
            
        status.update(label="‚úÖ –ì–æ—Ç–æ–≤–æ! –°–∫–∞—á–∞–π—Ç–µ —Ñ–∞–π–ª –Ω–∏–∂–µ.", state="complete", expanded=False)
        
        st.success(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(df_result)} —Å—Ç—Ä–∞–Ω–∏—Ü.")
        st.download_button("üì• –°–∫–∞—á–∞—Ç—å Result.xlsx", data=buffer.getvalue(), file_name="gar_pro_result.xlsx", mime="application/vnd.ms-excel", type="primary")
