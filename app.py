import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
import re
from collections import Counter, defaultdict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import math
import concurrent.futures
from urllib.parse import urlparse, urljoin, unquote
import inspect
import time
import json
import io
import os
import random
import streamlit.components.v1 as components
import copy
import plotly.graph_objects as go

# ==========================================
# FIX FOR PYTHON 3.11+
# ==========================================
if not hasattr(inspect, 'getargspec'):
    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return (spec.args, spec.varargs, spec.varkw, spec.defaults)
    inspect.getargspec = getargspec

try:
    import pymorphy2
    morph = pymorphy2.MorphAnalyzer()
    USE_NLP = True
except ImportError:
    morph = None
    USE_NLP = False

try:
    import google.generativeai as genai
except ImportError:
    genai = None

# ==========================================
# 0. –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ==========================================

def transliterate_text(text):
    mapping = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
    }
    result = []
    for char in text.lower():
        if char in mapping:
            result.append(mapping[char])
        elif char.isalnum() or char == '-':
            result.append(char)
    return "".join(result)

def force_cyrillic_name_global(slug_text):
    raw = unquote(slug_text).lower()
    raw = raw.replace('.html', '').replace('.php', '')
    if re.search(r'[–∞-—è]', raw):
        return raw.replace('-', ' ').replace('_', ' ').capitalize()

    words = re.split(r'[-_]', raw)
    rus_words = []
    
    exact_map = {
        'nikel': '–Ω–∏–∫–µ–ª—å', 'stal': '—Å—Ç–∞–ª—å', 'med': '–º–µ–¥—å', 'latun': '–ª–∞—Ç—É–Ω—å',
        'bronza': '–±—Ä–æ–Ω–∑–∞', 'svinec': '—Å–≤–∏–Ω–µ—Ü', 'titan': '—Ç–∏—Ç–∞–Ω', 'tsink': '—Ü–∏–Ω–∫',
        'dural': '–¥—é—Ä–∞–ª—å', 'dyural': '–¥—é—Ä–∞–ª—å', 'chugun': '—á—É–≥—É–Ω',
        'alyuminiy': '–∞–ª—é–º–∏–Ω–∏–π', 'al': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è', 'alyuminievaya': '–∞–ª—é–º–∏–Ω–∏–µ–≤–∞—è',
        'nerzhaveyushchiy': '–Ω–µ—Ä–∂–∞–≤–µ—é—â–∏–π', 'nerzhaveyka': '–Ω–µ—Ä–∂–∞–≤–µ–π–∫–∞',
        'profil': '–ø—Ä–æ—Ñ–∏–ª—å', 'shveller': '—à–≤–µ–ª–ª–µ—Ä', 'ugolok': '—É–≥–æ–ª–æ–∫',
        'polosa': '–ø–æ–ª–æ—Å–∞', 'krug': '–∫—Ä—É–≥', 'kvadrat': '–∫–≤–∞–¥—Ä–∞—Ç',
        'list': '–ª–∏—Å—Ç', 'truba': '—Ç—Ä—É–±–∞', 'setka': '—Å–µ—Ç–∫–∞',
        'provoloka': '–ø—Ä–æ–≤–æ–ª–æ–∫–∞', 'armatura': '–∞—Ä–º–∞—Ç—É—Ä–∞', 'balka': '–±–∞–ª–∫–∞',
        'katanka': '–∫–∞—Ç–∞–Ω–∫–∞', 'otvod': '–æ—Ç–≤–æ–¥', 'perehod': '–ø–µ—Ä–µ—Ö–æ–¥',
        'flanec': '—Ñ–ª–∞–Ω–µ—Ü', 'zaglushka': '–∑–∞–≥–ª—É—à–∫–∞', 'metiz': '–º–µ—Ç–∏–∑—ã',
        'profnastil': '–ø—Ä–æ—Ñ–Ω–∞—Å—Ç–∏–ª', 'shtrips': '—à—Ç—Ä–∏–ø—Å', 'lenta': '–ª–µ–Ω—Ç–∞',
        'shina': '—à–∏–Ω–∞', 'prutok': '–ø—Ä—É—Ç–æ–∫', 'shestigrannik': '—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫',
        'vtulka': '–≤—Ç—É–ª–∫–∞', 'kabel': '–∫–∞–±–µ–ª—å', 'panel': '–ø–∞–Ω–µ–ª—å',
        'detal': '–¥–µ—Ç–∞–ª—å', 'set': '—Å–µ—Ç—å', 'cep': '—Ü–µ–ø—å', 'svyaz': '—Å–≤—è–∑—å',
        'rezba': '—Ä–µ–∑—å–±–∞', 'gost': '–ì–û–°–¢',
        'polipropilenovye': '–ø–æ–ª–∏–ø—Ä–æ–ø–∏–ª–µ–Ω–æ–≤—ã–µ', 'truby': '—Ç—Ä—É–±—ã',
        'ocinkovannaya': '–æ—Ü–∏–Ω–∫–æ–≤–∞–Ω–Ω–∞—è', 'riflenyy': '—Ä–∏—Ñ–ª–µ–Ω—ã–π'
    }

    for w in words:
        if not w: continue
        if w in exact_map:
            rus_words.append(exact_map[w])
            continue
        
        processed_w = w
        if processed_w.endswith('yy'): processed_w = processed_w[:-2] + '—ã–π'
        elif processed_w.endswith('iy'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('ij'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('yi'): processed_w = processed_w[:-2] + '–∏–π'
        elif processed_w.endswith('aya'): processed_w = processed_w[:-3] + '–∞—è'
        elif processed_w.endswith('oye'): processed_w = processed_w[:-3] + '–æ–µ'
        elif processed_w.endswith('ye'): processed_w = processed_w[:-2] + '—ã–µ'

        replacements = [
            ('shch', '—â'), ('sch', '—â'), ('yo', '—ë'), ('zh', '–∂'), ('ch', '—á'), ('sh', '—à'), 
            ('yu', '—é'), ('ya', '—è'), ('kh', '—Ö'), ('ts', '—Ü'), ('ph', '—Ñ'),
            ('a', '–∞'), ('b', '–±'), ('v', '–≤'), ('g', '–≥'), ('d', '–¥'), ('e', '–µ'), 
            ('z', '–∑'), ('i', '–∏'), ('j', '–π'), ('k', '–∫'), ('l', '–ª'), ('m', '–º'), 
            ('n', '–Ω'), ('o', '–æ'), ('p', '–ø'), ('r', '—Ä'), ('s', '—Å'), ('t', '—Ç'), 
            ('u', '—É'), ('f', '—Ñ'), ('h', '—Ö'), ('c', '–∫'), ('w', '–≤'), ('y', '—ã'), ('x', '–∫—Å')
        ]
        
        temp_res = processed_w
        for eng, rus in replacements:
            temp_res = temp_res.replace(eng, rus)
        
        rus_words.append(temp_res)

    draft_phrase = " ".join(rus_words)
    draft_phrase = draft_phrase.replace('–ø—Ä–æ—Ñ–∏–ª', '–ø—Ä–æ—Ñ–∏–ª—å').replace('–ø—Ä–æ—Ñ–∏–ª—å–Ω', '–ø—Ä–æ—Ñ–∏–ª—å–Ω')
    draft_phrase = draft_phrase.replace('–µ–ª–Ω—ã–π', '–µ–ª—å–Ω—ã–π').replace('–∞–ª–Ω—ã–π', '–∞–ª—å–Ω—ã–π')
    draft_phrase = draft_phrase.replace('–µ–ª–Ω–∞—è', '–µ–ª—å–Ω–∞—è').replace('–∞–ª–Ω–∞—è', '–∞–ª—å–Ω–∞—è')
    draft_phrase = draft_phrase.replace('—Å—Ç–∞–ª–Ω', '—Å—Ç–∞–ª—å–Ω').replace('–º–µ–¥—å–Ω', '–º–µ–¥–Ω')
    draft_phrase = draft_phrase.replace('–π–∞', '—è').replace('–π–æ', '—ë')

    return draft_phrase.capitalize()

def get_breadcrumb_only(url, ua_settings="Mozilla/5.0"):
    try:
        session = requests.Session()
        retry = Retry(connect=3, read=3, redirect=3, backoff_factor=0.5)
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        
        headers = {'User-Agent': ua_settings}
        r = session.get(url, headers=headers, timeout=25)
        if r.status_code != 200: 
            return None
        
        soup = BeautifulSoup(r.text, 'html.parser')
        
        breadcrumbs = soup.find(class_=re.compile(r'breadcrumb|breadcrumbs|nav-path|nav-chain|bx-breadcrumb', re.I))
        if not breadcrumbs:
            breadcrumbs = soup.find(id=re.compile(r'breadcrumb|breadcrumbs|nav-path', re.I))

        if breadcrumbs:
            full_text = breadcrumbs.get_text(separator='|||', strip=True)
            parts = [p.strip() for p in full_text.split('|||') if p.strip()]
            clean_parts = [p for p in parts if p not in ['/', '\\', '>', '¬ª', '‚Ä¢', '-', '|']]
            
            if clean_parts:
                last_item = clean_parts[-1]
                if len(last_item) > 2 and last_item.lower() != "–≥–ª–∞–≤–Ω–∞—è":
                    return last_item
    except:
        return None
    return None

def render_clean_block(title, icon, words_list):
    unique_words = sorted(list(set(words_list))) if words_list else []
    count = len(unique_words)
    
    if count > 0:
        content_html = ", ".join(unique_words)
        # –ö–∞—Ä—Ç–æ—á–∫–∞ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è
        html_code = f"""
        <details class="details-card">
            <summary class="card-summary">
                <div>
                    <span class="arrow-icon">‚ñ∂</span>
                    {icon} {title}
                </div>
                <span class="count-tag">{count}</span>
            </summary>
            <div class="card-content">
                {content_html}
            </div>
        </details>
        """
    else:
        # –ï—Å–ª–∏ –ø—É—Å—Ç–æ - –∫–∞—Ä—Ç–æ—á–∫–∞ –Ω–µ–∞–∫—Ç–∏–≤–Ω–∞ (–±–µ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
        html_code = f"""
        <div class="details-card">
            <div class="card-summary" style="cursor: default; color: #9ca3af;">
                <div>{icon} {title}</div>
                <span class="count-tag">0</span>
            </div>
        </div>
        """
    
    st.markdown(html_code, unsafe_allow_html=True)

def render_relevance_chart(df_rel, unique_key="default"):
    if df_rel.empty: return
    df = df_rel[df_rel['–ü–æ–∑–∏—Ü–∏—è'] > 0].copy()
    if df.empty: return

    df = df.sort_values(by='–ü–æ–∑–∏—Ü–∏—è')
    x_indices = np.arange(len(df))
    tick_links = []
    
    for _, row in df.iterrows():
        raw_name = row['–î–æ–º–µ–Ω'].replace(' (–í—ã)', '').strip()
        clean_domain = raw_name.replace('www.', '').split('/')[0]
        label_text = f"{row['–ü–æ–∑–∏—Ü–∏—è']}. {clean_domain}"
        if len(label_text) > 20: label_text = label_text[:18] + ".."
        url_target = row.get('URL', f"https://{raw_name}")
        link_html = f"<a href='{url_target}' target='_blank' class='chart-link'>{label_text}</a>"
        tick_links.append(link_html)

    df['Total_Rel'] = (df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] + df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)']) / 2
    z = np.polyfit(x_indices, df['Total_Rel'], 1)
    p = np.poly1d(z)
    df['Trend'] = p(x_indices)

    fig = go.Figure()
    COLOR_MAIN = '#4F46E5'; COLOR_WIDTH = '#0EA5E9'; COLOR_DEPTH = '#E11D48'; COLOR_TREND = '#15803d'
    COMMON_CONFIG = dict(mode='lines+markers', line=dict(width=3, shape='spline'), marker=dict(size=8, line=dict(width=2, color='white'), symbol='circle'))

    fig.add_trace(go.Scatter(x=x_indices, y=df['Total_Rel'], name='–û–±—â–∞—è', line=dict(color=COLOR_MAIN, **COMMON_CONFIG['line']), marker=dict(color=COLOR_MAIN, **COMMON_CONFIG['marker']), mode='lines+markers'))
    fig.add_trace(go.Scatter(x=x_indices, y=df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'], name='–®–∏—Ä–∏–Ω–∞', line=dict(color=COLOR_WIDTH, **COMMON_CONFIG['line']), marker=dict(color=COLOR_WIDTH, **COMMON_CONFIG['marker']), mode='lines+markers'))
    fig.add_trace(go.Scatter(x=x_indices, y=df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'], name='–ì–ª—É–±–∏–Ω–∞', line=dict(color=COLOR_DEPTH, **COMMON_CONFIG['line']), marker=dict(color=COLOR_DEPTH, **COMMON_CONFIG['marker']), mode='lines+markers'))
    fig.add_trace(go.Scatter(x=x_indices, y=df['Trend'], name='–¢—Ä–µ–Ω–¥', line=dict(color=COLOR_TREND, **COMMON_CONFIG['line']), marker=dict(color=COLOR_TREND, **COMMON_CONFIG['marker']), mode='lines+markers', opacity=0.8))

    fig.update_layout(
        template="plotly_white",
        legend=dict(orientation="h", yanchor="bottom", y=1.05, xanchor="center", x=0.5, font=dict(size=12, color="#111827", family="Inter, sans-serif")),
        xaxis=dict(showgrid=False, linecolor='#E5E7EB', tickmode='array', tickvals=x_indices, ticktext=tick_links, tickfont=dict(size=12), fixedrange=True, range=[-0.5, len(df) - 0.5], automargin=True),
        yaxis=dict(range=[0, 115], showgrid=True, gridcolor='#F3F4F6', gridwidth=1, zeroline=False, fixedrange=True),
        margin=dict(l=10, r=10, t=50, b=40), hovermode="x unified", height=380
    )
    st.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False}, key=f"rel_chart_{unique_key}")

def analyze_serp_anomalies(df_rel):
    if df_rel.empty: return [], [], {"type": "none", "msg": ""}
    df = df_rel[~df_rel['–î–æ–º–µ–Ω'].str.contains("\(–í—ã\)", na=False)].copy()
    if df.empty: return [], [], {"type": "none", "msg": ""}

    df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] = pd.to_numeric(df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'], errors='coerce').fillna(0)
    df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'] = pd.to_numeric(df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)'], errors='coerce').fillna(0)
    df['Total'] = (df['–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)'] + df['–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)']) / 2
    
    max_score = df['Total'].max()
    if max_score < 1: max_score = 1
    threshold = max(max_score * 0.75, 40) 
    
    anomalies = []
    normal_urls = []
    
    for _, row in df.iterrows():
        current_url = str(row.get('URL', '')).strip()
        if not current_url or current_url.lower() == 'nan': current_url = f"https://{row['–î–æ–º–µ–Ω']}" 
        score = row['Total']
        if score < threshold:
            reason = f"–°–∫–æ—Ä {int(score)} < {int(threshold)} (–õ–∏–¥–µ—Ä {int(max_score)})"
            anomalies.append({'url': current_url, 'reason': reason, 'score': score})
        else:
            normal_urls.append(current_url)

    if anomalies: st.toast(f"üóëÔ∏è –§–∏–ª—å—Ç—Ä (–õ–∏–¥–µ—Ä {int(max_score)} / –ü–æ—Ä–æ–≥ {int(threshold)}). –ò—Å–∫–ª—é—á–µ–Ω–æ: {len(anomalies)}", icon="‚ö†Ô∏è")
    else: st.toast(f"‚úÖ –í—Å–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –æ–∫. (–õ–∏–¥–µ—Ä {int(max_score)} / –ü–æ—Ä–æ–≥ {int(threshold)}). –ú–∏–Ω. –±–∞–ª–ª: {int(df['Total'].min())}", icon="‚ÑπÔ∏è")
    
    x = np.arange(len(df)); y = df['Total'].values
    slope = np.polyfit(x, y, 1)[0] if len(x) > 1 else 0
    trend_msg = "üìâ –ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–æ–ø" if slope < -1 else ("üìà –ü–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç—ã–π —Ç–æ–ø" if slope > 1 else "‚û°Ô∏è –†–æ–≤–Ω—ã–π —Ç–æ–ø")

    return normal_urls, anomalies, {"type": "info", "msg": trend_msg}

@st.cache_data
def load_lemmatized_dictionaries():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_path = os.path.join(script_dir, "data")
    sets = {"products": set(), "commercial": set(), "specs": set(), "geo": set(), "services": set(), "sensitive": set()}
    files_map = {"metal_products.json": "products", "commercial_triggers.json": "commercial", "geo_locations.json": "geo", "services_triggers.json": "services", "tech_specs.json": "specs", "SENSITIVE_STOPLIST.json": "sensitive"}

    for filename, set_key in files_map.items():
        full_path = os.path.join(base_path, filename)
        if not os.path.exists(full_path): continue
        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                data = json.load(f) 
                words_bucket = []
                if isinstance(data, dict):
                    for cat_list in data.values(): words_bucket.extend(cat_list)
                elif isinstance(data, list): words_bucket = data
                for phrase in words_bucket:
                    w_clean = str(phrase).lower().strip().replace('—ë', '–µ')
                    if not w_clean: continue
                    sets[set_key].add(w_clean)
                    if morph:
                        normal_form = morph.parse(w_clean)[0].normal_form.replace('—ë', '–µ')
                        sets[set_key].add(normal_form)
                    if ' ' in w_clean:
                        parts = w_clean.split()
                        for p in parts:
                            sets[set_key].add(p)
                            if morph: sets[set_key].add(morph.parse(p)[0].normal_form.replace('—ë', '–µ'))
        except Exception: pass
    return sets["products"], sets["commercial"], sets["specs"], sets["geo"], sets["services"], sets["sensitive"]

def classify_semantics_with_api(words_list, yandex_key):
    PRODUCTS_SET, COMM_SET, SPECS_SET, GEO_SET, SERVICES_SET, SENS_SET = load_lemmatized_dictionaries()
    FULL_SENSITIVE = SENS_SET.union(SENSITIVE_STOPLIST)

    if 'debug_geo_count' not in st.session_state: st.session_state.debug_geo_count = len(GEO_SET)
    st.sidebar.info(f"–°–ª–æ–≤–∞—Ä–∏ (–∏–∑ —Ñ–∞–π–ª–æ–≤):\nüì¶ –¢–æ–≤–∞—Ä—ã: {len(PRODUCTS_SET)}\nüí∞ –ö–æ–º–º–µ—Ä—Ü–∏—è: {len(COMM_SET)}\nüõ†Ô∏è –£—Å–ª—É–≥–∏: {len(SERVICES_SET)}\nüåç –ì–æ—Ä–æ–¥–∞: {len(GEO_SET)}")

    dim_pattern = re.compile(r'\d+(?:[\.\,]\d+)?\s?[—Öx\*√ó]\s?\d+', re.IGNORECASE)
    grade_pattern = re.compile(r'^([–∞-—èa-z]{1,4}\-?\d+[–∞-—èa-z0-9]*)$', re.IGNORECASE)
    categories = {'products': set(), 'services': set(), 'commercial': set(), 'dimensions': set(), 'geo': set(), 'general': set(), 'sensitive': set()}
    
    for word in words_list:
        word_lower = word.lower()
        is_sensitive = False
        if word_lower in FULL_SENSITIVE: is_sensitive = True
        else:
            for stop_w in FULL_SENSITIVE:
                if len(stop_w) > 3 and stop_w in word_lower: is_sensitive = True; break
        if is_sensitive: categories['sensitive'].add(word_lower); continue
        
        lemma = word_lower
        if morph:
            p = morph.parse(word_lower)[0]
            lemma = p.normal_form

        if word_lower in SPECS_SET or lemma in SPECS_SET: categories['dimensions'].add(word_lower); continue
        if dim_pattern.search(word_lower) or grade_pattern.match(word_lower) or word_lower.isdigit(): categories['dimensions'].add(word_lower); continue

        if word_lower in PRODUCTS_SET or lemma in PRODUCTS_SET: categories['products'].add(word_lower); continue
        is_product_root = False
        for prod in PRODUCTS_SET:
            check_root = prod[:-1] if len(prod) > 4 else prod
            if len(check_root) > 3 and check_root in word_lower:
                categories['products'].add(word_lower)
                is_product_root = True
                break
        if is_product_root: continue

        if lemma in GEO_SET or word_lower in GEO_SET: categories['geo'].add(word_lower); continue
        if lemma in SERVICES_SET or word_lower in SERVICES_SET: categories['services'].add(word_lower); continue
        if lemma.endswith('–æ–±—Ä–∞–±–æ—Ç–∫–∞') or lemma.endswith('–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ') or lemma == "—Ä–µ–∑–∫–∞": categories['services'].add(word_lower); continue
        if lemma in COMM_SET or word_lower in COMM_SET: categories['commercial'].add(word_lower); continue
        categories['general'].add(word_lower)

    return {k: sorted(list(v)) for k, v in categories.items()}

# ==========================================
# STATE INIT
# ==========================================
if 'sidebar_gen_df' not in st.session_state: st.session_state.sidebar_gen_df = None
if 'sidebar_excel_bytes' not in st.session_state: st.session_state.sidebar_excel_bytes = None
if 'analysis_results' not in st.session_state: st.session_state.analysis_results = None
if 'analysis_done' not in st.session_state: st.session_state.analysis_done = False
if 'ai_generated_df' not in st.session_state: st.session_state.ai_generated_df = None
if 'ai_excel_bytes' not in st.session_state: st.session_state.ai_excel_bytes = None
if 'tags_html_result' not in st.session_state: st.session_state.tags_html_result = None
if 'table_html_result' not in st.session_state: st.session_state.table_html_result = None
if 'tags_generated_df' not in st.session_state: st.session_state.tags_generated_df = None
if 'tags_excel_data' not in st.session_state: st.session_state.tags_excel_data = None

if 'categorized_products' not in st.session_state: st.session_state.categorized_products = []
if 'categorized_services' not in st.session_state: st.session_state.categorized_services = []
if 'categorized_commercial' not in st.session_state: st.session_state.categorized_commercial = []
if 'categorized_dimensions' not in st.session_state: st.session_state.categorized_dimensions = []
if 'categorized_geo' not in st.session_state: st.session_state.categorized_geo = []
if 'categorized_general' not in st.session_state: st.session_state.categorized_general = []
if 'categorized_sensitive' not in st.session_state: st.session_state.categorized_sensitive = []

if 'orig_products' not in st.session_state: st.session_state.orig_products = []
if 'orig_services' not in st.session_state: st.session_state.orig_services = []
if 'orig_commercial' not in st.session_state: st.session_state.orig_commercial = []
if 'orig_dimensions' not in st.session_state: st.session_state.orig_dimensions = []
if 'orig_geo' not in st.session_state: st.session_state.orig_geo = []
if 'orig_general' not in st.session_state: st.session_state.orig_general = []

if 'auto_tags_words' not in st.session_state: st.session_state.auto_tags_words = []
if 'auto_promo_words' not in st.session_state: st.session_state.auto_promo_words = []
if 'persistent_urls' not in st.session_state: st.session_state['persistent_urls'] = ""

st.set_page_config(layout="wide", page_title="GAR PRO v2.6 (Mass Promo)", page_icon="üìä")

GARBAGE_LATIN_STOPLIST = {
    'whatsapp', 'viber', 'telegram', 'skype', 'vk', 'instagram', 'facebook', 'youtube', 'twitter',
    'cookie', 'cookies', 'policy', 'privacy', 'agreement', 'terms',
    'click', 'submit', 'send', 'zakaz', 'basket', 'cart', 'order', 'call', 'back', 'callback',
    'login', 'logout', 'sign', 'register', 'auth', 'account', 'profile',
    'search', 'menu', 'nav', 'navigation', 'footer', 'header', 'sidebar',
    'img', 'jpg', 'png', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'svg',
    'ok', 'error', 'undefined', 'null', 'true', 'false', 'var', 'let', 'const', 'function', 'return',
    'ru', 'en', 'com', 'net', 'org', 'biz', 'shop', 'store',
    'phone', 'email', 'tel', 'fax', 'mob', 'address', 'copyright', 'all', 'rights', 'reserved',
    'div', 'span', 'class', 'id', 'style', 'script', 'body', 'html', 'head', 'meta', 'link'
}

SENSITIVE_STOPLIST_RAW = {
    "—É–∫—Ä–∞–∏–Ω–∞", "ukraine", "ua", "–≤—Å—É", "–∑—Å—É", "–∞—Ç–æ",
    "–∫–∏–µ–≤", "–ª—å–≤–æ–≤", "—Ö–∞—Ä—å–∫–æ–≤", "–æ–¥–µ—Å—Å–∞", "–¥–Ω–µ–ø—Ä", "–º–∞—Ä–∏—É–ø–æ–ª—å",
    "–¥–æ–Ω–µ—Ü–∫", "–ª—É–≥–∞–Ω—Å–∫", "–¥–Ω—Ä", "–ª–Ω—Ä", "–¥–æ–Ω–±–∞—Å—Å", 
    "–º–µ–ª–∏—Ç–æ–ø–æ–ª—å", "–±–µ—Ä–¥—è–Ω—Å–∫", "–±–∞—Ö–º—É—Ç", "–∑–∞–ø–æ—Ä–æ–∂—å–µ", "—Ö–µ—Ä—Å–æ–Ω",
    "–∫—Ä—ã–º", "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å", "—Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å"
}
SENSITIVE_STOPLIST = {w.lower() for w in SENSITIVE_STOPLIST_RAW}

def check_password():
    if st.session_state.get("authenticated"): return True
    st.markdown("""<style>.main { display: flex; flex-direction: column; justify-content: center; align-items: center; } .auth-logo-box { text-align: center; margin-bottom: 1rem; padding-top: 0; }</style>""", unsafe_allow_html=True)
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown('<div class="auth-logo-box"><h3>–í—Ö–æ–¥ –≤ —Å–∏—Å—Ç–µ–º—É</h3></div>', unsafe_allow_html=True)
        password = st.text_input("–ü–∞—Ä–æ–ª—å", type="password", key="password_input", label_visibility="collapsed")
        if st.button("–í–û–ô–¢–ò", type="primary", use_container_width=True):
            if password == "ZVC01w4_pIquj0bMiaAu":
                st.session_state.authenticated = True
                st.rerun()
            else: st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø–∞—Ä–æ–ª—å")
    return False

if not check_password(): st.stop()

if "arsenkin_token" in st.session_state: ARSENKIN_TOKEN = st.session_state.arsenkin_token
else:
    try: ARSENKIN_TOKEN = st.secrets["api"]["arsenkin_token"]
    except (FileNotFoundError, KeyError): ARSENKIN_TOKEN = None

if "yandex_dict_key" in st.session_state: YANDEX_DICT_KEY = st.session_state.yandex_dict_key
else:
    try: YANDEX_DICT_KEY = st.secrets["api"]["yandex_dict_key"]
    except (FileNotFoundError, KeyError): YANDEX_DICT_KEY = None

REGION_MAP = {
    "–ú–æ—Å–∫–≤–∞": {"ya": 213, "go": 1011969},
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥": {"ya": 2, "go": 1011966},
    "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": {"ya": 54, "go": 1011868},
    "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": {"ya": 65, "go": 1011928},
    "–ö–∞–∑–∞–Ω—å": {"ya": 43, "go": 1011904},
    "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥": {"ya": 47, "go": 1011918},
    "–°–∞–º–∞—Ä–∞": {"ya": 51, "go": 1011956},
    "–ß–µ–ª—è–±–∏–Ω—Å–∫": {"ya": 56, "go": 1011882},
    "–û–º—Å–∫": {"ya": 66, "go": 1011931},
    "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä": {"ya": 35, "go": 1011894},
    "–ö–∏–µ–≤ (UA)": {"ya": 143, "go": 1012852},
    "–ú–∏–Ω—Å–∫ (BY)": {"ya": 157, "go": 1001493},
    "–ê–ª–º–∞—Ç—ã (KZ)": {"ya": 162, "go": 1014601}
}

DEFAULT_EXCLUDE_DOMAINS = {
    "yandex.ru", "avito.ru", "beru.ru", "tiu.ru", "aliexpress.com", "aliexpress.ru", 
    "ebay.com", "auto.ru", "2gis.ru", "sravni.ru", "toshop.ru", "price.ru", 
    "pandao.ru", "instagram.com", "wikipedia.org", "rambler.ru", "hh.ru", 
    "banki.ru", "regmarkets.ru", "zoon.ru", "pulscen.ru", "prodoctorov.ru", 
    "blizko.ru", "domclick.ru", "satom.ru", "quto.ru", "edadeal.ru", 
    "cataloxy.ru", "irr.ru", "onliner.by", "shop.by", "deal.by", "yell.ru", 
    "profi.ru", "irecommend.ru", "otzovik.com", "ozon.ru", "ozon.by", 
    "market.yandex.ru", "youtube.com", "www.youtube.com", "gosuslugi.ru", 
    "www.gosuslugi.ru", "dzen.ru", "2gis.by", "wildberries.ru", "rutube.ru", 
    "vk.com", "facebook.com", "chipdip.ru"
    }

DEFAULT_EXCLUDE = "\n".join(DEFAULT_EXCLUDE_DOMAINS)
DEFAULT_STOPS = "—Ä—É–±–ª–µ–π\n—Ä—É–±\n—Å—Ç—Ä\n—É–ª\n—à—Ç\n—Å–º\n–º–º\n–º–ª\n–∫–≥\n–∫–≤\n–º¬≤\n—Å–º¬≤\n–º2\n—Å–º2"

PRIMARY_COLOR = "#277EFF"
PRIMARY_DARK = "#1E63C4"
TEXT_COLOR = "#3D4858"
LIGHT_BG_MAIN = "#F1F5F9"
BORDER_COLOR = "#E2E8F0"
HEADER_BG = "#F0F7FF"
ROW_BORDER_COLOR = "#DBEAFE"

st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp {{ background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; }}
        html, body, p, li, h1, h2, h3, h4 {{ font-family: 'Inter', sans-serif; color: {TEXT_COLOR} !important; }}
        .stButton button {{ background-color: {PRIMARY_COLOR} !important; color: white !important; border: none; border-radius: 6px; }}
        .stButton button:hover {{ background-color: {PRIMARY_DARK} !important; }}
        .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"] > div {{
            background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; border: 1px solid {BORDER_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] {{ border: 2px solid {PRIMARY_COLOR} !important; border-radius: 8px !important; }}
        div[data-testid="stDataFrame"] div[role="columnheader"] {{
            background-color: {HEADER_BG} !important; color: {PRIMARY_COLOR} !important; font-weight: 700 !important; border-bottom: 2px solid {PRIMARY_COLOR} !important;
        }}
        div[data-testid="stDataFrame"] div[role="gridcell"] {{
            background-color: #FFFFFF !important; color: {TEXT_COLOR} !important; border-bottom: 1px solid {ROW_BORDER_COLOR} !important;
        }}
        .legend-box {{ padding: 10px; background-color: #F8FAFC; border: 1px solid #E2E8F0; border-radius: 5px; font-size: 14px; margin-bottom: 10px; }}
        .text-red {{ color: #D32F2F; font-weight: bold; }}
        .text-green {{ color: #2E7D32; font-weight: bold; }}
        .text-bold {{ font-weight: 600; }}
        .sort-container {{ background-color: {LIGHT_BG_MAIN}; padding: 10px; border-radius: 8px; margin-bottom: 10px; border: 1px solid {BORDER_COLOR}; }}
        
        .stApp > header {{ background-color: transparent !important; }}
        .stTextInput input:disabled, .stTextArea textarea:disabled, .stSelectbox div[aria-disabled="true"] {{
            opacity: 1 !important; background-color: {LIGHT_BG_MAIN} !important; color: {TEXT_COLOR} !important; cursor: text !important; -webkit-text-fill-color: {TEXT_COLOR} !important; border-color: {BORDER_COLOR} !important;
        }}
        .stButton button:disabled {{ opacity: 1 !important; background-color: {PRIMARY_COLOR} !important; color: white !important; cursor: progress !important; }}
        div[data-testid="stAppViewContainer"] {{ filter: none !important; opacity: 1 !important; transition: none !important; }}
        /* –°—Ç–∏–ª–∏ –¥–ª—è —Å—Å—ã–ª–æ–∫ –≤–Ω—É—Ç—Ä–∏ –≥—Ä–∞—Ñ–∏–∫–∞ Plotly */
        .chart-link {{
            color: #277EFF !important;
            font-weight: 600 !important;
            text-decoration: none !important;
            border-bottom: 4px solid #CBD5E1 !important; 
            display: inline-block !important;
            transition: border-color 0.2s ease !important;
        }}
        .chart-link:hover {{
            border-bottom-color: #277EFF !important;
            cursor: pointer !important;
        }}
    </style>
""", unsafe_allow_html=True)

def get_yandex_dict_info(text, api_key):
    if not api_key: return {'lemma': text, 'pos': 'unknown'}
    url = "https://dictionary.yandex.net/api/v1/dicservice.json/lookup"
    params = {'key': api_key, 'lang': 'ru-ru', 'text': text, 'ui': 'ru'}
    try:
        r = requests.get(url, params=params, timeout=2)
        if r.status_code == 200:
            data = r.json()
            if data.get('def'):
                first_def = data['def'][0]
                return {'lemma': first_def.get('text', text), 'pos': first_def.get('pos', 'unknown')}
    except: pass
    return {'lemma': text, 'pos': 'unknown'}

def get_arsenkin_urls(query, engine_type, region_name, api_token, depth_val=10):
    url_set = "https://arsenkin.ru/api/tools/set"
    url_check = "https://arsenkin.ru/api/tools/check"
    url_get = "https://arsenkin.ru/api/tools/get"
    headers = {"Authorization": f"Bearer {api_token}", "Content-type": "application/json"}
    reg_ids = REGION_MAP.get(region_name, {"ya": 213, "go": 1011969})
    se_params = []
    if "–Ø–Ω–¥–µ–∫—Å" in engine_type: se_params.append({"type": 2, "region": reg_ids['ya']})
    if "Google" in engine_type: se_params.append({"type": 11, "region": reg_ids['go']})

    payload = {"tools_name": "check-top", "data": {"queries": [query], "is_snippet": False, "noreask": True, "se": se_params, "depth": depth_val}}
    try:
        r = requests.post(url_set, headers=headers, json=payload, timeout=15)
        resp_json = r.json()
        if "error" in resp_json or "task_id" not in resp_json: st.error(f"‚ùå –û—à–∏–±–∫–∞ API: {resp_json}"); return []
        task_id = resp_json["task_id"]
        st.toast(f"–ó–∞–¥–∞—á–∞ ID {task_id} –∑–∞–ø—É—â–µ–Ω–∞")
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}"); return []

    status = "process"
    attempts = 0
    while status == "process" and attempts < 120:
        time.sleep(5); attempts += 1
        try:
            r_check = requests.post(url_check, headers=headers, json={"task_id": task_id})
            res_check_data = r_check.json()
            if res_check_data.get("status") == "finish": status = "done"; break
        except: pass

    if status != "done": st.error(f"‚è≥ –¢–∞–π–º-–∞—É—Ç API"); return []

    try:
        r_final = requests.post(url_get, headers=headers, json={"task_id": task_id}, timeout=30)
        res_data = r_final.json()
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}"); return []

    results_list = []
    try:
        collect = res_data.get('result', {}).get('result', {}).get('collect')
        if not collect: return []
        final_url_list = []
        if isinstance(collect, list) and len(collect) > 0 and isinstance(collect[0], list): final_url_list = collect[0][0]
        else:
             unique_urls = set()
             for engine_data in collect:
                 if isinstance(engine_data, dict):
                     for _, serps in engine_data.items():
                         for item in serps:
                             if item.get('url') and item.get('url') not in unique_urls:
                                 results_list.append({'url': item['url'], 'pos': item['pos']})
                                 unique_urls.add(item['url'])
             return results_list

        if final_url_list:
            for index, url in enumerate(final_url_list): results_list.append({'url': url, 'pos': index + 1})
    except Exception as e: st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}"); return []
    return results_list

def process_text_detailed(text, settings, n_gram=1):
    text = text.lower().replace('—ë', '–µ')
    words = re.findall(r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+', text)
    stops = set(w.lower().replace('—ë', '–µ') for w in settings['custom_stops'])
    lemmas = []
    forms_map = defaultdict(set)
    for w in words:
        if len(w) < 2: continue
        if not settings['numbers'] and w.isdigit(): continue
        if w in stops: continue
        lemma = w
        if USE_NLP and n_gram == 1:
            p = morph.parse(w)[0]
            if 'PREP' in p.tag or 'CONJ' in p.tag or 'PRCL' in p.tag or 'NPRO' in p.tag: continue
            lemma = p.normal_form.replace('—ë', '–µ')
        lemmas.append(lemma)
        forms_map[lemma].add(w)
    return lemmas, forms_map

def parse_page(url, settings, query_context=""):
    try:
        from curl_cffi import requests as cffi_requests
        headers = {
            'User-Agent': settings['ua'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        }
        r = cffi_requests.get(url, headers=headers, timeout=20, impersonate="chrome110")
        if r.status_code == 403: raise Exception("CURL_CFFI –ø–æ–ª—É—á–∏–ª 403 Forbidden")
        if r.status_code != 200: return None
        content = r.content
        encoding = r.encoding if r.encoding else 'utf-8'
    except Exception:
        try:
            import requests
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            session = requests.Session()
            headers = {'User-Agent': settings['ua']}
            r = session.get(url, headers=headers, timeout=20, verify=False)
            if r.status_code != 200: return None
            content = r.content
            encoding = r.apparent_encoding
        except Exception: return None

    try:
        soup = BeautifulSoup(content, 'html.parser', from_encoding=encoding)
        product_titles = []
        search_roots = set()
        if query_context:
            clean_q = query_context.lower().replace('–∫—É–ø–∏—Ç—å', '').replace('—Ü–µ–Ω–∞', '').replace(' –≤ ', ' ')
            words = re.findall(r'[–∞-—èa-z]+', clean_q)
            for w in words:
                if len(w) > 3: search_roots.add(w[:-1])
                else: search_roots.add(w)
        
        parsed_current = urlparse(url)
        current_path_clean = parsed_current.path.rstrip('/')
        seen_titles = set()
        
        for a in soup.find_all('a', href=True):
            txt = a.get_text(strip=True)
            raw_href = a['href']
            if len(txt) < 5 or len(txt) > 300: continue
            if raw_href.startswith('#') or raw_href.startswith('javascript'): continue
            
            abs_href = urljoin(url, raw_href)
            parsed_href = urlparse(abs_href)
            href_path_clean = parsed_href.path.rstrip('/')
            
            is_child_path = href_path_clean.startswith(current_path_clean)
            is_deeper = len(href_path_clean) > len(current_path_clean)
            is_not_query_param_only = (href_path_clean != current_path_clean)

            if is_child_path and is_deeper and is_not_query_param_only:
                txt_lower = txt.lower()
                href_lower = abs_href.lower()
                has_keywords = False
                if search_roots:
                    for root in search_roots:
                        if root in txt_lower or root in href_lower:
                            has_keywords = True; break
                else:
                    if re.search(r'\d', txt): has_keywords = True

                is_buy_button = txt_lower in {'–∫—É–ø–∏—Ç—å', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ', '–≤ –∫–æ—Ä–∑–∏–Ω—É', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Ü–µ–Ω–∞'}
                if has_keywords and not is_buy_button:
                    if txt not in seen_titles:
                        product_titles.append(txt)
                        seen_titles.add(txt)
        
        h1_tag = soup.find('h1')
        h1_text = h1_tag.get_text(strip=True) if h1_tag else ""

        soup_no_grid = BeautifulSoup(content, 'html.parser', from_encoding=encoding)
        grid_div = soup_no_grid.find('div', class_='an-container-fluid an-container-xl')
        if grid_div: grid_div.decompose()
        
        tags_to_remove = []
        if settings['noindex']: tags_to_remove.append('noindex')
        
        for s in [soup, soup_no_grid]:
            for c in s.find_all(string=lambda text: isinstance(text, Comment)): c.extract()
            if tags_to_remove:
                for t in s.find_all(tags_to_remove): t.decompose()
            for script in s(["script", "style", "svg", "path", "noscript"]): script.decompose()

        anchors_list = [a.get_text(strip=True) for a in soup.find_all('a') if a.get_text(strip=True)]
        anchor_text = " ".join(anchors_list)
        
        extra_text = []
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'): extra_text.append(meta_desc['content'])

        if settings['alt_title']:
            for img in soup.find_all('img', alt=True): extra_text.append(img['alt'])
            for t in soup.find_all(title=True): extra_text.append(t['title'])

        body_text_raw = soup.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text = re.sub(r'\s+', ' ', body_text_raw).strip()
        body_text_no_grid_raw = soup_no_grid.get_text(separator=' ') + " " + " ".join(extra_text)
        body_text_no_grid = re.sub(r'\s+', ' ', body_text_no_grid_raw).strip()

        if not body_text: return None
        return {
            'url': url, 'domain': urlparse(url).netloc, 
            'body_text': body_text, 'body_text_no_grid': body_text_no_grid,
            'anchor_text': anchor_text, 'h1': h1_text, 'product_titles': product_titles
        }
    except Exception: return None
        
def calculate_metrics(comp_data_full, my_data, settings, my_serp_pos, original_results):
    def math_round(number): return int(number + 0.5)

    all_forms_map = defaultdict(set)
    global_forms_counter = defaultdict(Counter) 
    
    if not my_data or not my_data.get('body_text'): 
        my_lemmas, my_forms, my_anchors, my_len = [], {}, [], 0
        my_clean_domain = "local"
    else:
        my_lemmas, my_forms = process_text_detailed(my_data['body_text'], settings)
        my_anchors, _ = process_text_detailed(my_data['anchor_text'], settings)
        my_len = len(my_lemmas)
        for k, v in my_forms.items(): all_forms_map[k].update(v)
        my_clean_domain = my_data['domain'].lower().replace('www.', '').split(':')[0]

    comp_docs = []
    for p in comp_data_full:
        if not p.get('body_text'): continue
        p_domain = p['domain'].lower().replace('www.', '').split(':')[0]
        if my_clean_domain != "local" and p_domain == my_clean_domain: continue
        body, c_forms = process_text_detailed(p['body_text'], settings)
        raw_words_for_stats = re.findall(r'[–∞-—è–ê-–Ø—ë–Å0-9a-zA-Z]+', p['body_text'].lower())
        for rw in raw_words_for_stats:
            if len(rw) < 2: continue
            if morph:
                parsed = morph.parse(rw)[0]
                if 'PREP' not in parsed.tag and 'CONJ' not in parsed.tag:
                    rw_lemma = parsed.normal_form.replace('—ë', '–µ')
                    global_forms_counter[rw_lemma][rw] += 1
        anchor, _ = process_text_detailed(p['anchor_text'], settings)
        comp_docs.append({'body': body, 'anchor': anchor, 'url': p['url'], 'domain': p['domain']})
        for k, v in c_forms.items(): all_forms_map[k].update(v)

    if not comp_docs:
        return { "depth": pd.DataFrame(), "hybrid": pd.DataFrame(), "relevance_top": pd.DataFrame(), "my_score": {"width": 0, "depth": 0}, "missing_semantics_high": [], "missing_semantics_low": [] }

    c_lens = [len(d['body']) for d in comp_docs]
    avg_dl = np.mean(c_lens) if c_lens else 1
    N = len(comp_docs)
    vocab = set(my_lemmas)
    for d in comp_docs: vocab.update(d['body'])
    vocab = sorted(list(vocab))
    doc_freqs = Counter()
    for d in comp_docs:
        for w in set(d['body']): doc_freqs[w] += 1
    word_counts_per_doc = [Counter(d['body']) for d in comp_docs]

    word_idf_map = {}
    for lemma in vocab:
        df = doc_freqs[lemma]
        if df == 0: continue
        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)
        word_idf_map[lemma] = max(idf, 0.01)

    table_depth = []
    table_hybrid = []
    missing_semantics_high = []
    missing_semantics_low = []
    words_with_median_gt_0 = set() 
    my_found_words_from_median = set() 
    
    for lemma in vocab:
        if lemma in GARBAGE_LATIN_STOPLIST: continue
        raw_counts = [word_counts_per_doc[i][lemma] for i in range(N)]
        sorted_raw = sorted(raw_counts)
        if sorted_raw:
            rec_median_absolute = math_round(np.median(sorted_raw))
            obs_min = math_round(sorted_raw[0])
            obs_max = math_round(sorted_raw[-1])
        else:
            rec_median_absolute = 0; obs_min = 0; obs_max = 0

        if settings['norm'] and my_len > 0:
            norm_counts = []
            for i in range(N):
                raw_cnt = raw_counts[i]
                comp_len = c_lens[i]
                if comp_len > 0:
                    normalized_val = raw_cnt * (my_len / comp_len)
                    norm_counts.append(normalized_val)
                else: norm_counts.append(0)
            if norm_counts: rec_median_target = math_round(np.median(sorted(norm_counts)))
            else: rec_median_target = 0
        else: rec_median_target = rec_median_absolute

        my_tf_count = my_lemmas.count(lemma)
        if obs_max == 0 and my_tf_count == 0: continue

        if rec_median_target >= 1:
            words_with_median_gt_0.add(lemma)
            if my_tf_count > 0: my_found_words_from_median.add(lemma)

        display_word = lemma
        if global_forms_counter[lemma]: display_word = global_forms_counter[lemma].most_common(1)[0][0]

        if my_tf_count == 0:
            weight = word_idf_map.get(lemma, 0) * (rec_median_target if rec_median_target > 0 else 1)
            item = {'word': display_word, 'weight': weight}
            if rec_median_target >= 1: missing_semantics_high.append(item)
            else: missing_semantics_low.append(item)

        diff = rec_median_target - my_tf_count
        if diff == 0: status = "–ù–æ—Ä–º–∞"; action_text = "‚úÖ"; sort_val = 0
        elif diff > 0: status = "–ù–µ–¥–æ—Å–ø–∞–º"; action_text = f"+{diff}"; sort_val = diff
        else: status = "–ü–µ—Ä–µ—Å–ø–∞–º"; action_text = f"{diff}"; sort_val = abs(diff)

        forms_str = ", ".join(sorted(list(all_forms_map.get(lemma, set())))) if all_forms_map.get(lemma) else lemma
        table_depth.append({
            "–°–ª–æ–≤–æ": display_word, "–°–ª–æ–≤–æ—Ñ–æ—Ä–º—ã": forms_str, "–í—Ö–æ–∂–¥–µ–Ω–∏–π —É –≤–∞—Å": my_tf_count,
            "–ú–µ–¥–∏–∞–Ω–∞": rec_median_absolute, "–ú–∏–Ω–∏–º—É–º (–∫–æ–Ω–∫—É—Ä.)": obs_min, "–ú–∞–∫—Å–∏–º—É–º (–∫–æ–Ω–∫—É—Ä.)": obs_max,
            "–°—Ç–∞—Ç—É—Å": status, "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": action_text, "is_missing": (my_tf_count == 0), "sort_val": sort_val
        })
        table_hybrid.append({
            "–°–ª–æ–≤–æ": display_word,
            "TF-IDF –¢–û–ü": round(word_idf_map.get(lemma, 0) * (rec_median_absolute / avg_dl if avg_dl > 0 else 0), 4),
            "TF-IDF —É –≤–∞—Å": round(word_idf_map.get(lemma, 0) * (my_tf_count / my_len if my_len > 0 else 0), 4),
            "–°–∞–π—Ç–æ–≤": doc_freqs[lemma], "–ü–µ—Ä–µ—Å–ø–∞–º": obs_max
        })
    
    total_needed = len(words_with_median_gt_0)
    total_found = len(my_found_words_from_median)
    if total_needed > 0:
        ratio = total_found / total_needed
        my_width_score_final = int(min(100, ratio * 120))
    else: my_width_score_final = 0

    S_WIDTH_CORE = words_with_median_gt_0 
    
    def calculate_raw_power(doc_tokens, doc_len):
        if avg_dl == 0 or doc_len == 0: return 0
        score = 0
        counts = Counter(doc_tokens)
        k1 = 1.2; b = 0.75
        for word in S_WIDTH_CORE:
            if word not in counts: continue
            tf = counts[word]
            idf = word_idf_map.get(word, 0)
            term_weight = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avg_dl)))
            score += term_weight
        return score

    comp_raw_scores = []
    competitor_scores_map = {}
    for i, doc in enumerate(comp_docs):
        c_found = len(set(doc['body']).intersection(S_WIDTH_CORE))
        if total_needed > 0: c_width_val = int(min(100, (c_found / total_needed) * 120))
        else: c_width_val = 0
        raw_val = calculate_raw_power(doc['body'], c_lens[i])
        comp_raw_scores.append(raw_val)
        competitor_scores_map[doc['url']] = {'width_final': c_width_val, 'raw_depth': raw_val}

    if comp_raw_scores:
        median_raw = np.median(comp_raw_scores)
        ref_val = median_raw if median_raw > 0.1 else 1.0
    else: ref_val = 1.0
    
    k_norm = 80.0 / ref_val
    my_raw_bm25 = calculate_raw_power(my_lemmas, my_len)
    my_depth_score_final = int(round(min(100, my_raw_bm25 * k_norm)))

    for url, data in competitor_scores_map.items(): data['depth_final'] = int(round(min(100, data['raw_depth'] * k_norm)))

    missing_semantics_high.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low.sort(key=lambda x: x['weight'], reverse=True)
    missing_semantics_low = missing_semantics_low[:500]

    table_rel = []
    my_site_found_in_selection = False
    for item in original_results:
        url = item['url']
        if url not in competitor_scores_map: continue
        row_domain = urlparse(url).netloc.lower().replace('www.', '')
        is_my_site = False
        if my_clean_domain and my_clean_domain != "local" and my_clean_domain in row_domain:
            is_my_site = True
            my_site_found_in_selection = True
            display_name = f"{urlparse(url).netloc} (–í—ã)"
        else: display_name = urlparse(url).netloc
        scores = competitor_scores_map[url]
        table_rel.append({ "–î–æ–º–µ–Ω": display_name, "URL": url, "–ü–æ–∑–∏—Ü–∏—è": item['pos'], "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": scores['width_final'], "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": scores['depth_final'] })
        
    if not my_site_found_in_selection:
        pos_to_show = my_serp_pos if my_serp_pos > 0 else 0
        my_label = f"{my_data['domain']} (–í—ã)" if (my_data and my_data.get('domain')) else "–í–∞—à —Å–∞–π—Ç"
        my_full_url = my_data['url'] if (my_data and 'url' in my_data) else "#"
        table_rel.append({ "–î–æ–º–µ–Ω": my_label, "URL": my_full_url, "–ü–æ–∑–∏—Ü–∏—è": pos_to_show, "–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)": my_width_score_final, "–ì–ª—É–±–∏–Ω–∞ (–±–∞–ª–ª)": my_depth_score_final })

    df_rel_for_analysis = pd.DataFrame(table_rel)
    good_urls, bad_urls_dicts, trend_info = analyze_serp_anomalies(df_rel_for_analysis)
    
    st.session_state['detected_anomalies'] = bad_urls_dicts
    st.session_state['serp_trend_info'] = trend_info
    
    if bad_urls_dicts:
        st.session_state['persistent_urls'] = "\n".join(good_urls)
        st.session_state['excluded_urls_auto'] = "\n".join([item['url'] for item in bad_urls_dicts])
    else:
        st.session_state['persistent_urls'] = "\n".join([r.get('URL', r['–î–æ–º–µ–Ω']) for r in table_rel if "(–í—ã)" not in r['–î–æ–º–µ–Ω']])
        st.session_state['excluded_urls_auto'] = ""

    return { 
        "depth": pd.DataFrame(table_depth), "hybrid": pd.DataFrame(table_hybrid), 
        "relevance_top": pd.DataFrame(table_rel).sort_values(by='–ü–æ–∑–∏—Ü–∏—è', ascending=True).reset_index(drop=True), 
        "my_score": {"width": my_width_score_final, "depth": my_depth_score_final}, 
        "missing_semantics_high": missing_semantics_high, "missing_semantics_low": missing_semantics_low 
    }
    
def get_hybrid_word_type(word, main_marker_root, specs_dict=None):
    w = word.lower()
    specs_dict = specs_dict or set()
    if w == main_marker_root: return "1. üíé –ú–∞—Ä–∫–µ—Ä (–¢–æ–≤–∞—Ä)"
    if morph:
        norm = morph.parse(w)[0].normal_form
        if norm == main_marker_root: return "1. üíé –ú–∞—Ä–∫–µ—Ä (–¢–æ–≤–∞—Ä)"
    if re.search(r'(gost|din|iso|en|tu|astm|aisi|–≥–æ—Å—Ç|–æ—Å—Ç|—Ç—É|–¥–∏–Ω)', w): return "6. üìú –°—Ç–∞–Ω–¥–∞—Ä—Ç"
    if re.fullmatch(r'\d+([.,]\d+)?', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    if re.search(r'^\d+[x—Ö*\-/]\d+', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    if re.search(r'\d+(–º–º|mm|–º|m|kg|–∫–≥|bar|–±–∞—Ä|–∞—Ç–º)$', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    if re.match(r'^(d|dn|pn|sn|sdr|–¥—É|—Ä—É|√∏)\d+', w): return "5. üî¢ –†–∞–∑–º–µ—Ä—ã/–ü—Ä–æ—á–µ–µ"
    if w in specs_dict: return "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"
    if re.search(r'\d', w): return "3. üèóÔ∏è –ú–∞—Ä–∫–∞/–°–ø–ª–∞–≤"
    if re.search(r'^[a-z\-]+$', w): return "7. üî† –õ–∞—Ç–∏–Ω–∏—Ü–∞/–ë—Ä–µ–Ω–¥"
    if morph:
        p = morph.parse(w)[0]
        tag = p.tag
        if {'PREP'} in tag or {'CONJ'} in tag: return "SKIP"
        if {'ADJF'} in tag or {'PRTF'} in tag or {'ADJS'} in tag: return "2. üé® –°–≤–æ–π—Å—Ç–≤–∞"
        if {'NOUN'} in tag: return "4. üîó –î–æ–ø–æ–ª–Ω–µ–Ω–∏—è"
    if w.endswith(('–∏–π', '—ã–π', '–∞—è', '–æ–µ', '—ã–µ', '–∞—è')): return "2. üé® –°–≤–æ–π—Å—Ç–≤–∞"
    return "4. üîó –î–æ–ø–æ–ª–Ω–µ–Ω–∏—è"
    
def calculate_naming_metrics(comp_data_full, my_data, settings):
    SPECS_SET = st.session_state.get('categorized_dimensions', set())
    if not SPECS_SET: _, _, SPECS_SET, _, _, _ = load_lemmatized_dictionaries()
    my_tokens = []
    if my_data and my_data.get('body_text_no_grid'):
        raw_w = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', my_data['body_text_no_grid'].lower())
        for w in raw_w:
            if not re.search(r'\d', w) and morph: my_tokens.append(morph.parse(w)[0].normal_form)
            else: my_tokens.append(w)
    all_words_flat = []
    site_vocab_map = []
    for p in comp_data_full:
        titles = p.get('product_titles', [])
        valid_titles = [t for t in titles if 5 < len(t) < 150]
        if not valid_titles: site_vocab_map.append(set()); continue
        curr_site_tokens = set()
        for t in valid_titles:
            words = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower())
            for w in words:
                if len(w) < 2: continue
                if re.search(r'\d', w): token = w
                elif re.search(r'^[a-z]+$', w): token = w
                elif morph: token = morph.parse(w)[0].normal_form
                else: token = w
                all_words_flat.append(token)
                curr_site_tokens.add(token)
        site_vocab_map.append(curr_site_tokens)
    if not all_words_flat: return pd.DataFrame()
    N_sites = len(site_vocab_map)
    counts = Counter([w for w in all_words_flat if not re.search(r'\d', w)])
    main_marker_root = ""
    for w, c in counts.most_common(10):
        if morph and 'NOUN' in morph.parse(w)[0].tag: main_marker_root = w; break
    if not main_marker_root and counts: main_marker_root = counts.most_common(1)[0][0]
    vocab = sorted(list(set(all_words_flat)))
    table_rows = []
    for token in vocab:
        if token in GARBAGE_LATIN_STOPLIST: continue
        sites_with_word = sum(1 for s_set in site_vocab_map if token in s_set)
        freq_percent = int((sites_with_word / N_sites) * 100)
        cat = get_hybrid_word_type(token, main_marker_root, SPECS_SET)
        if cat == "SKIP": continue
        is_spec = "–ú–∞—Ä–∫–∞" in cat or "–°—Ç–∞–Ω–¥–∞—Ä—Ç" in cat
        if is_spec and freq_percent < 5: continue
        if not is_spec and "–†–∞–∑–º–µ—Ä—ã" not in cat and freq_percent < 15: continue
        if "–†–∞–∑–º–µ—Ä—ã" in cat and freq_percent < 15: continue
        rec_median = 1 if freq_percent > 30 else 0
        my_tf = my_tokens.count(token)
        diff = rec_median - my_tf
        action_text = f"+{diff}" if diff > 0 else ("‚úÖ" if diff == 0 else f"{diff}")
        table_rows.append({
            "–¢–∏–ø —Ö–∞—Ä-–∫–∏": cat[3:], "–°–ª–æ–≤–æ": token, "–ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å (%)": f"{freq_percent}%",
            "–£ –í–∞—Å": my_tf, "–ú–µ–¥–∏–∞–Ω–∞": rec_median, "–î–æ–±–∞–≤–∏—Ç—å": action_text, "raw_freq": freq_percent, "cat_sort": int(cat[0])
        })
    df = pd.DataFrame(table_rows)
    if not df.empty: df = df.sort_values(by=["cat_sort", "raw_freq"], ascending=[True, False])
    return df

def analyze_ideal_name(comp_data_full):
    SPECS_SET = st.session_state.get('categorized_dimensions', set())
    if not SPECS_SET: _, _, SPECS_SET, _, _, _ = load_lemmatized_dictionaries()
    titles = []
    for d in comp_data_full:
        ts = d.get('product_titles', [])
        titles.extend([t for t in ts if 5 < len(t) < 150])
    if not titles: return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö", []
    all_w = []
    for t in titles: all_w.extend(re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower()))
    c = Counter(all_w)
    main_marker_root = ""
    for w, _ in c.most_common(5):
        if not re.search(r'\d', w):
             if morph and 'NOUN' in morph.parse(w)[0].tag: main_marker_root = w; break
             elif not morph: main_marker_root = w; break
    if not main_marker_root and c: main_marker_root = c.most_common(1)[0][0]
    structure_counter = Counter()
    vocab_by_type = defaultdict(Counter)
    sample = titles[:500]
    for t in sample:
        words = re.findall(r'[–∞-—è–ê-–Øa-zA-Z0-9\-]+', t.lower())
        pattern = []
        for w in words:
            if len(w) < 2: continue
            cat_full = get_hybrid_word_type(w, main_marker_root, SPECS_SET)
            if cat_full == "SKIP": continue
            try: cat_short = cat_full.split('.', 1)[1].strip().split(' ', 1)[1]
            except: cat_short = cat_full 
            vocab_by_type[cat_short][w] += 1
            if not pattern or pattern[-1] != cat_short: pattern.append(cat_short)
        if pattern:
            structure_str = " + ".join(pattern)
            structure_counter[structure_str] += 1
    if not structure_counter: return "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞", []
    best_struct_str, _ = structure_counter.most_common(1)[0]
    best_struct_list = best_struct_str.split(" + ")
    final_parts = []
    used_words = set()
    for block in best_struct_list:
        if "–†–∞–∑–º–µ—Ä—ã" in block or "–°—Ç–∞–Ω–¥–∞—Ä—Ç" in block or "–ú–∞—Ä–∫–∞" in block:
            top_cand = vocab_by_type[block].most_common(1)
            if top_cand and top_cand[0][1] > (len(sample) * 0.3): final_parts.append(top_cand[0][0])
            else: final_parts.append(f"[{block.upper()}]")
            continue
        candidates = vocab_by_type[block].most_common(3)
        for w, cnt in candidates:
            if w not in used_words:
                if "–ú–∞—Ä–∫–µ—Ä" in block: w = w.capitalize()
                final_parts.append(w)
                used_words.add(w)
                break
    ideal_name = " ".join(final_parts)
    report = []
    report.append(f"**–°—Ö–µ–º–∞:** {best_struct_str}")
    report.append("")
    report.append("**–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**")
    for block in best_struct_list:
        if "–†–∞–∑–º–µ—Ä—ã" in block: continue
        top = [f"{w}" for w, c in vocab_by_type[block].most_common(3)]
        report.append(f"- **{block}**: {', '.join(top)}")
    return ideal_name, report

def render_paginated_table(df, title_text, key_prefix, default_sort_col=None, use_abs_sort_default=False):
    if df.empty: st.info(f"{title_text}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö."); return
    col_t1, col_t2 = st.columns([7, 3])
    with col_t1: st.markdown(f"### {title_text}")
    if f'{key_prefix}_sort_col' not in st.session_state: st.session_state[f'{key_prefix}_sort_col'] = default_sort_col if (default_sort_col and default_sort_col in df.columns) else df.columns[0]
    if f'{key_prefix}_sort_order' not in st.session_state: st.session_state[f'{key_prefix}_sort_order'] = "–£–±—ã–≤–∞–Ω–∏–µ"
    search_query = st.text_input(f"üîç –ü–æ–∏—Å–∫ ({title_text})", key=f"{key_prefix}_search")
    if search_query:
        mask = df.astype(str).apply(lambda x: x.str.contains(search_query, case=False, na=False)).any(axis=1)
        df_filtered = df[mask].copy()
    else: df_filtered = df.copy()
    if df_filtered.empty: st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."); return
    with st.container():
        st.markdown("<div class='sort-container'>", unsafe_allow_html=True)
        col_s1, col_s2, col_sp = st.columns([2, 2, 4])
        with col_s1:
            current_sort = st.session_state[f'{key_prefix}_sort_col']
            if current_sort not in df_filtered.columns: current_sort = df_filtered.columns[0]
            sort_col = st.selectbox("üóÇ –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ:", df_filtered.columns, key=f"{key_prefix}_sort_box", index=list(df_filtered.columns).index(current_sort))
            st.session_state[f'{key_prefix}_sort_col'] = sort_col
        with col_s2:
            sort_order = st.radio("–ü–æ—Ä—è–¥–æ–∫:", ["–£–±—ã–≤–∞–Ω–∏–µ", "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ"], horizontal=True, key=f"{key_prefix}_order_box", index=0 if st.session_state[f'{key_prefix}_sort_order'] == "–£–±—ã–≤–∞–Ω–∏–µ" else 1)
            st.session_state[f'{key_prefix}_sort_order'] = sort_order
        st.markdown("</div>", unsafe_allow_html=True)
    ascending = (sort_order == "–í–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ")
    if use_abs_sort_default and sort_col == "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è" and "sort_val" in df_filtered.columns: df_filtered = df_filtered.sort_values(by="sort_val", ascending=ascending)
    elif ("–î–æ–±–∞–≤–∏—Ç—å" in sort_col or "+/-" in sort_col) and df_filtered[sort_col].dtype == object:
        try:
            df_filtered['_temp_sort'] = df_filtered[sort_col].astype(str).str.replace(r'[^\d]', '', regex=True)
            df_filtered['_temp_sort'] = pd.to_numeric(df_filtered['_temp_sort'], errors='coerce').fillna(0)
            df_filtered = df_filtered.sort_values(by='_temp_sort', ascending=ascending).drop(columns=['_temp_sort'])
        except: df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)
    else: df_filtered = df_filtered.sort_values(by=sort_col, ascending=ascending)
    df_filtered = df_filtered.reset_index(drop=True); df_filtered.index = df_filtered.index + 1
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
        export_df = df_filtered.copy()
        if "is_missing" in export_df.columns: del export_df["is_missing"]
        if "sort_val" in export_df.columns: del export_df["sort_val"]
        export_df.to_excel(writer, index=False, sheet_name='Data')
    excel_data = buffer.getvalue()
    with col_t2: st.download_button(label="üì• –°–∫–∞—á–∞—Ç—å Excel", data=excel_data, file_name=f"{key_prefix}_export.xlsx", mime="application/vnd.ms-excel", key=f"{key_prefix}_down")
    ROWS_PER_PAGE = 20
    if f'{key_prefix}_page' not in st.session_state: st.session_state[f'{key_prefix}_page'] = 1
    total_rows = len(df_filtered); total_pages = math.ceil(total_rows / ROWS_PER_PAGE)
    if total_pages == 0: total_pages = 1
    current_page = st.session_state[f'{key_prefix}_page']
    if current_page > total_pages: current_page = total_pages
    if current_page < 1: current_page = 1
    st.session_state[f'{key_prefix}_page'] = current_page
    start_idx = (current_page - 1) * ROWS_PER_PAGE
    end_idx = start_idx + ROWS_PER_PAGE
    df_view = df_filtered.iloc[start_idx:end_idx]
    def highlight_rows(row):
        base_style = 'background-color: #FFFFFF; color: #3D4858; border-bottom: 1px solid #DBEAFE;'
        styles = []
        status = row.get("–°—Ç–∞—Ç—É—Å", "")
        for col_name in row.index:
            cell_style = base_style
            if col_name == "–°—Ç–∞—Ç—É—Å":
                if status == "–ù–µ–¥–æ—Å–ø–∞–º": cell_style += "color: #D32F2F; font-weight: bold;"
                elif status == "–ü–µ—Ä–µ—Å–ø–∞–º": cell_style += "color: #E65100; font-weight: bold;"
                elif status == "–ù–æ—Ä–º–∞": cell_style += "color: #2E7D32; font-weight: bold;"
            styles.append(cell_style)
        return styles
    cols_to_hide = [c for c in ["is_missing", "sort_val"] if c in df_view.columns]
    try: styled_df = df_view.style.apply(highlight_rows, axis=1)
    except: styled_df = df_view
    st.dataframe(styled_df, use_container_width=True, height=(len(df_view) * 35) + 40, column_config={c: None for c in cols_to_hide})
    c_spacer, c_btn_prev, c_info, c_btn_next = st.columns([6, 1, 1, 1])
    with c_btn_prev:
        if st.button("‚¨ÖÔ∏è", key=f"{key_prefix}_prev", disabled=(current_page <= 1), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] -= 1
            st.rerun()
    with c_info: st.markdown(f"<div style='text-align: center; margin-top: 10px;'><b>{current_page}</b> / {total_pages}</div>", unsafe_allow_html=True)
    with c_btn_next:
        if st.button("‚û°Ô∏è", key=f"{key_prefix}_next", disabled=(current_page >= total_pages), use_container_width=True):
            st.session_state[f'{key_prefix}_page'] += 1
            st.rerun()
    st.markdown("---")

STATIC_DATA_GEN = {
    'IP_PROP4817': "–£—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–∫–∏",
    'IP_PROP4818': "–û–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –≤ —Ä–µ–≥–∏–æ–Ω—ã —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫",
    'IP_PROP4819': """<p>–ù–∞–¥–µ–∂–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫–∞–∑–∞ –≤ –ª—é–±—É—é —Ç–æ—á–∫—É —Å—Ç—Ä–∞–Ω—ã: "–°—Ç–∞–ª—å–º–µ—Ç—É—Ä–∞–ª" –æ—Ç–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–≤–∞—Ä 24 —á–∞—Å–∞ –≤ —Å—É—Ç–∫–∏, 7 –¥–Ω–µ–π –≤ –Ω–µ–¥–µ–ª—é. –ë–æ–ª–µ–µ 4 000 –æ—Ç–≥—Ä—É–∑–æ–∫ –≤ –≥–æ–¥. –ü—Ä–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–∏ –∑–∞–∫–∞–∑–∞ –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –≤–∞–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ä—à—Ä—É—Ç.</p>""",
    'IP_PROP4820': """<p>–ù–∞—à–∏ –∏–∑–¥–µ–ª–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è—Ö –£—Ä–∞–ª–∞, —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞, –ü–æ–≤–æ–ª–∂—å—è, –°–∏–±–∏—Ä–∏. –ü–∞—Ä—Ç–Ω–µ—Ä—ã –ø–æ –ª–æ–≥–∏—Å—Ç–∏–∫–µ –ø—Ä–µ–¥–ª–æ–∂–∞—Ç –¥–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–∫–∞–∑ —Å–∞–º—ã–º —É–¥–æ–±–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º ‚Äì –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–º, –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω—ã–º, –¥–∞–∂–µ –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º. –î–ª—è –≤–∞—Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—é—Ç —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—É—é —Å—Ö–µ–º—É –ø–æ–¥ —É–¥–æ–±–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è. –ü–æ–≥—Ä—É–∑–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ç–µ—Ö–Ω–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.</p><div class="h4"><h4>–°–∞–º–æ–≤—ã–≤–æ–∑</h4></div><p>–ï—Å–ª–∏ –æ–±—ã—á–Ω–æ —Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∑–∞–±—Ä–∞—Ç—å —Ç–æ–≤–∞—Ä –∏–ª–∏ –¥–∞–µ—Ç–µ —ç—Ç–æ –ø—Ä–∞–≤–æ —É–ø–æ–ª–Ω–æ–º–æ—á–µ–Ω–Ω—ã–º, –∞–¥—Ä–µ—Å –∏ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã —Å–∫–ª–∞–¥–∞ –≤ —Å–≤–æ–µ–º –≥–æ—Ä–æ–¥–µ —É—Ç–æ—á–Ω—è–π—Ç–µ —É –º–µ–Ω–µ–¥–∂–µ—Ä–∞.</p><div class="h4"><h4>–ì—Ä—É–∑–æ–≤–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏</h4></div><p>–û—Ç–ø—Ä–∞–≤–∏–º –ø—Ä–æ–∫–∞—Ç –Ω–∞ –≤–∞—à –æ–±—ä–µ–∫—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –∞–≤—Ç–æ–ø–∞—Ä–∫–æ–º. –ü–æ–ª—É—á–µ–Ω–∏–µ –≤ —É–ø–∞–∫–æ–≤–∫–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏, –∞ –∏–º–µ–Ω–Ω–æ –Ω–∞ –¥–µ—Ä–µ–≤—è–Ω–Ω–æ–º –ø–æ–¥–¥–æ–Ω–µ.</p><div class="h4"><h4>–°–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å –¢–ö</h4></div><p>–î–æ—Å—Ç–∞–≤–∫–∞ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –†–æ—Å—Å–∏–∏ –∏ –°–ù–ì. –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å—Å—è, —Ç–∞–∫ –∫–∞–∫ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç, –æ–¥–Ω–∞–∫–æ, —Å—Ä–∞–≤–Ω–∏–º —Å—Ç–æ–∏–º–æ—Å—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª—É–∂–± –∏ –≤—ã–±–µ—Ä–µ–º –ª—É—á—à—É—é.</p>""",
    'IP_PROP4821': "–û–ø–ª–∞—Ç–∞ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤:",
    'IP_PROP4822': """<p>–ù–∞—à–∞ –∫–æ–º–ø–∞–Ω–∏—è –≥–æ—Ç–æ–≤–∞ –ø—Ä–∏–Ω—è—Ç—å –ª—é–±—ã–µ –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–µ –≤–∏–¥—ã –æ–ø–ª–∞—Ç—ã –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏—Ü: –ø–æ —Å—á–µ—Ç—É, –Ω–∞–ª–∏—á–Ω–∞—è –∏ –±–µ–∑–Ω–∞–ª–∏—á–Ω–∞—è, –Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–π –ø–ª–∞—Ç–µ–∂, —Ç–∞–∫–∂–µ –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞ –∏ –æ—Ç—Å—Ä–æ—á–∫–∞ –ø–ª–∞—Ç–µ–∂–∞.</p>""",
    'IP_PROP4823': """<div class="h4"><h3>–ü—Ä–∏–º–µ—Ä—ã –≤–æ–∑–º–æ–∂–Ω–æ–π –æ–ø–ª–∞—Ç—ã</h3></div><div class="an-col-12"><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–° –ø–æ–º–æ—â—å—é –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤ —Ü–µ–Ω—Ç—Ä–∞—Ö –ø—Ä–æ–¥–∞–∂</span></p></li></ul><p>–í–∞–∂–Ω–æ! –¶–µ–Ω–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—É–±–ª–∏—á–Ω–æ–π –æ—Ñ–µ—Ä—Ç–æ–π. –ü—Ä–∏—Ö–æ–¥–∏—Ç–µ –≤ –Ω–∞—à –æ—Ñ–∏—Å, —á—Ç–æ–±—ã —É—Ç–æ—á–Ω–∏—Ç—å –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ, –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ—á—Ç–∏ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å, —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –≤–æ–∑–≤—Ä–∞—Ç, —Å—á–µ—Ç, —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ª–æ–≥–∏—Å—Ç–∏–∫—É.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞ —Ä–∞—Å—á–µ—Ç–Ω—ã–π —Å—á–µ—Ç</span></p></li></ul><p>–ü–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É —Å—á–µ—Ç—É –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏ –±–∞–Ω–∫–∞ –∏–ª–∏ –ø—É—Ç–µ–º –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥—Å—Ç–≤ —á–µ—Ä–µ–∑ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç (—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∑–∞—â–∏—â–µ–Ω—ã, —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –æ—Ç–¥–µ–ª–µ–Ω–∏—è). –î–ª—è –ø—Ä–∞–≤–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å —Å–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –ø–ª–∞—Ç–µ–∂–Ω–æ–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ —Å –æ—Ç–º–µ—Ç–∫–æ–π –±–∞–Ω–∫–∞.</p><ul><li style="font-weight: 400;"><p><span style="font-weight: 400;">–ù–∞–ª–∏—á–Ω—ã–º–∏ –∏–ª–∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∫–∞—Ä—Ç–æ–π –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏</span></p></li></ul><p><span style="font-weight: 400;">–ü–æ–º–æ–∂–µ–º —Å –æ–ø–ª–∞—Ç–æ–π: –æ–±—ä–µ–º –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ. –ö—Ä—É–ø–Ω—ã–º –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º ‚Äì –¥–µ–Ω—å–≥–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –ø–æ—Å–ª–µ –ø—Ä–∏–µ–º–∫–∏ —Ç–æ–≤–∞—Ä–∞.</span></p><p>–ú–µ–Ω–µ–¥–∂–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.</p><p>–ó–∞–∫–∞–∑—ã–≤–∞–π—Ç–µ —á–µ—Ä–µ–∑ –ø—Ä–∞–π—Å-–ª–∏—Å—Ç:</p><p><a class="btn btn-blue" href="/catalog/">–ö–∞—Ç–∞–ª–æ–≥ (–º–∞–≥–∞–∑–∏–Ω-–º–µ–Ω—é):</a></p></div></div><br>""",
    'IP_PROP4824': "–û–ø–∏—Å–∞–Ω–∏–µ, —Å—Ç–∞—Ç—å–∏, –ø–æ–∏—Å–∫, –æ—Ç–∑—ã–≤—ã, –Ω–æ–≤–æ—Å—Ç–∏, –∞–∫—Ü–∏–∏, –∂—É—Ä–Ω–∞–ª, info:",
    'IP_PROP4825': "–ú–æ–∂–µ–º –º–µ—Ç–∞–ª–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –æ—Ü–∏–Ω–∫–æ–≤–∞—Ç—å, –Ω–∏–∫–µ–ª–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–æ–≤–æ–ª–æ—á—å",
    'IP_PROP4826': "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥",
    'IP_PROP4834': "–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –±–µ–∑ –ø—Ä–∏–º–µ—Å–µ–π",
    'IP_PROP4835': "–ü–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ—Å—Ç–∞–≤—â–∏–∫",
    'IP_PROP4836': "–ö–∞—á–µ—Å—Ç–≤–æ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä",
    'IP_PROP4837': "–ü–æ—Ä—è–¥–æ–∫ –≤ –ì–û–°–¢–∞—Ö"
}

def get_page_data_for_gen(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.encoding = 'utf-8'
    except Exception as e: return None, None, None, f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}"
    if response.status_code != 200: return None, None, None, f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—É—Å–∞: {response.status_code}"
    soup = BeautifulSoup(response.text, 'html.parser')
    description_div = soup.find('div', class_='description-container')
    target_h2 = None
    if description_div: target_h2 = description_div.find('h2')
    if not target_h2: target_h2 = soup.find('h2')
    page_header = target_h2.get_text(strip=True) if target_h2 else "–û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞"
    base_text = description_div.get_text(separator="\n", strip=True) if description_div else soup.body.get_text(separator="\n", strip=True)[:5000]
    tags_container = soup.find(class_='popular-tags-inner')
    tags_data = []
    if tags_container:
        links = tags_container.find_all('a')
        for link in links:
            tag_url = urljoin(url, link.get('href')) if link.get('href') else None
            if tag_url: tags_data.append({'name': link.get_text(strip=True), 'url': tag_url})
    return base_text, tags_data, page_header, None

def call_gemini_safe(model, prompt, retries=3):
    base_wait = 5 # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    for attempt in range(retries):
        try:
            time.sleep(base_wait) 
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            err_str = str(e)
            if "429" in err_str:
                wait_time = 60
                st.warning(f"‚è≥ –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç Google API (429). –ñ–¥–µ–º {wait_time} —Å–µ–∫...")
                time.sleep(wait_time)
            else: return f"Error: {err_str}"
    return "Error: Timeout (Too many 429s)"

def generate_ai_content_blocks(api_key, base_text, tag_name, forced_header, num_blocks=5, seo_words=None):
    if not base_text: return ["Error: No base text"] * 5
    if not genai: return ["Error: google-generativeai lib not installed"] * 5
    genai.configure(api_key=api_key)
    try: model = genai.GenerativeModel('gemini-2.0-flash-exp') 
    except: model = genai.GenerativeModel('gemini-1.5-flash')
    seo_words = seo_words or []
    seo_instruction_block = ""
    if seo_words:
        seo_list_str = ", ".join(seo_words)
        seo_instruction_block = f"""
        –í–Ω–µ–¥—Ä–∏—Ç—å —Å–ª–æ–≤–∞: {seo_list_str}. 
        –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–¥–µ–ª–∏ –∏—Ö —Ç–µ–≥–æ–º <b>.
        –†–∞—Å–∫–∏–¥–∞–π –ø–æ –≤—Å–µ–º –±–ª–æ–∫–∞–º.
        """
    system_instruction = "–¢—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä. –í—ã–¥–∞–µ—à—å –¢–û–õ–¨–ö–û HTML. –ë–µ–∑ Markdown. –ë–µ–∑ —Å—Å—ã–ª–æ–∫ [1]."
    user_prompt = f"""
    –¢–æ–≤–∞—Ä: "{tag_name}"
    –§–∞–∫—Ç—É—Ä–∞: \"\"\"{base_text[:4000]}\"\"\"
    {seo_instruction_block}
    –ù–∞–ø–∏—à–∏ {num_blocks} –±–ª–æ–∫–æ–≤ HTML, —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: |||BLOCK_SEP|||
    –¢–µ–º—ã:
    1. <h2>{forced_header}</h2> (–í–≤–µ–¥–µ–Ω–∏–µ).
    2-{num_blocks}. <h3> (–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ).
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –±–ª–æ–∫–∞: –ó–∞–≥–æ–ª–æ–≤–æ–∫ -> –¢–µ–∫—Å—Ç -> –°–ø–∏—Å–æ–∫ -> –¢–µ–∫—Å—Ç.
    """
    full_prompt = f"{system_instruction}\n\n{user_prompt}"
    content = call_gemini_safe(model, full_prompt)
    if "Error:" in content and "BLOCK_SEP" not in content: return [content] * num_blocks
    content = re.sub(r'\[\d+\]', '', content)
    content = content.replace("```html", "").replace("```", "").strip()
    blocks = [b.strip() for b in content.split("|||BLOCK_SEP|||") if b.strip()]
    while len(blocks) < num_blocks: blocks.append("")
    return blocks[:num_blocks]

# ==========================================
# 7. UI TABS RESTRUCTURED
# ==========================================
tab_seo_main, tab_wholesale_main = st.tabs(["üìä SEO –ê–Ω–∞–ª–∏–∑", "üè≠ –û–ø—Ç–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä"])

with tab_seo_main:
    col_main, col_sidebar = st.columns([65, 35])
    with col_main:
        st.title("SEO –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")
        if st.button("üßπ –û–±–Ω–æ–≤–∏—Ç—å —Å–ª–æ–≤–∞—Ä–∏ (–ö—ç—à)", key="clear_cache_btn"):
            st.cache_data.clear()
            st.rerun()
        my_input_type = st.radio("–¢–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ["–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç", "–ë–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"], horizontal=True, label_visibility="collapsed", key="my_page_source_radio")
        if my_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            st.text_input("URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã", placeholder="https://site.ru/catalog/tovar", label_visibility="collapsed", key="my_url_input")
        elif my_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏–ª–∏ —Ç–µ–∫—Å—Ç", height=200, label_visibility="collapsed", placeholder="–í—Å—Ç–∞–≤—å—Ç–µ HTML", key="my_content_input")
        st.markdown("### –ü–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å")
        st.text_input("–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞", label_visibility="collapsed", key="query_input")
        st.markdown("### –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        if st.session_state.get('force_radio_switch'):
            st.session_state["competitor_source_radio"] = "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"
            st.session_state['force_radio_switch'] = False
        source_type_new = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫", ["–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API Arsenkin (TOP-30)", "–°–ø–∏—Å–æ–∫ url-–∞–¥—Ä–µ—Å–æ–≤ –≤–∞—à–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"], horizontal=True, label_visibility="collapsed", key="competitor_source_radio")
        source_type = "API" if "API" in source_type_new else "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫"
        if source_type == "–†—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫":
            if st.session_state.get('analysis_done'):
                col_reset, _ = st.columns([1, 4])
                with col_reset:
                    if st.button("üîÑ –ù–æ–≤—ã–π –ø–æ–∏—Å–∫ (–°–±—Ä–æ—Å)", type="secondary", help="–°–±—Ä–æ—Å–∏—Ç—å –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤–≤–µ—Å—Ç–∏ –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫"):
                        keys_to_clear = ['analysis_done', 'analysis_results', 'excluded_urls_auto', 'detected_anomalies', 'serp_trend_info', 'persistent_urls', 'naming_table_df', 'ideal_h1_result']
                        for k in keys_to_clear:
                            if k in st.session_state: del st.session_state[k]
                        st.rerun()
            has_exclusions = st.session_state.get('excluded_urls_auto') and len(st.session_state.get('excluded_urls_auto')) > 5
            if has_exclusions:
                c_url_1, c_url_2 = st.columns(2)
                with c_url_1:
                    manual_val = st.text_area("‚úÖ –ê–∫—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã (–î–ª—è –∞–Ω–∞–ª–∏–∑–∞)", height=200, key="manual_urls_widget", value=st.session_state.get('persistent_urls', ""))
                    st.session_state['persistent_urls'] = manual_val
                with c_url_2:
                    st.text_area("üö´ –ê–≤—Ç–æ-–∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ (–í—ã –º–æ–∂–µ—Ç–µ –≤–µ—Ä–Ω—É—Ç—å –∏—Ö –≤–ª–µ–≤–æ)", height=200, key="excluded_urls_widget_display", value=st.session_state.get('excluded_urls_auto', ""), help="–°—é–¥–∞ –ø–æ–ø–∞–ª–∏ —Å–ª–∞–±—ã–µ —Å–∞–π—Ç—ã. –ï—Å–ª–∏ —Å—á–∏—Ç–∞–µ—Ç–µ, —á—Ç–æ —Å–∞–π—Ç –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π - —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –µ–≥–æ –∏ –≤—Å—Ç–∞–≤—å—Ç–µ –æ–±—Ä–∞—Ç–Ω–æ –≤ –ª–µ–≤–æ–µ –æ–∫–Ω–æ.")
            else:
                manual_val = st.text_area("–°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ (–∫–∞–∂–¥–∞—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)", height=200, key="manual_urls_widget", value=st.session_state.get('persistent_urls', ""))
                st.session_state['persistent_urls'] = manual_val
        if st.session_state.get('analysis_done') and st.session_state.get('analysis_results'):
            results = st.session_state.analysis_results
            if 'relevance_top' in results and not results['relevance_top'].empty:
                st.markdown("<br>", unsafe_allow_html=True)
                with st.expander("üìä –ì—Ä–∞—Ñ–∏–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–ù–∞–∂–º–∏—Ç–µ, —á—Ç–æ–±—ã —Ä–∞—Å–∫—Ä—ã—Ç—å)", expanded=False):
                  graph_data = st.session_state.get('full_graph_data', results['relevance_top'])
                  render_relevance_chart(graph_data, unique_key="main")
                st.markdown("<br>", unsafe_allow_html=True)
        st.markdown("### –°–ø–∏—Å–∫–∏ (Stop / Exclude)")
        st.text_area("–ù–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –¥–æ–º–µ–Ω—ã", DEFAULT_EXCLUDE, height=100, key="settings_excludes")
        st.text_area("–°—Ç–æ–ø-—Å–ª–æ–≤–∞", DEFAULT_STOPS, height=100, key="settings_stops")
        if st.button("–ó–ê–ü–£–°–¢–ò–¢–¨ –ê–ù–ê–õ–ò–ó", type="primary", use_container_width=True, key="start_analysis_btn"):
            st.session_state.analysis_results = None
            st.session_state.analysis_done = False
            st.session_state.naming_table_df = None
            st.session_state.ideal_h1_result = None
            st.session_state.gen_result_df = None
            st.session_state.unified_excel_data = None
            if 'excluded_urls_auto' in st.session_state: del st.session_state['excluded_urls_auto']
            if 'detected_anomalies' in st.session_state: del st.session_state['detected_anomalies']
            if 'serp_trend_info' in st.session_state: del st.session_state['serp_trend_info']
            for key in list(st.session_state.keys()):
                if key.endswith('_page'): st.session_state[key] = 1
            st.session_state.start_analysis_flag = True
            st.rerun()

    with col_sidebar:
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API")
        if not ARSENKIN_TOKEN:
             new_arsenkin = st.text_input("Arsenkin Token", type="password", key="input_arsenkin")
             if new_arsenkin: st.session_state.arsenkin_token = new_arsenkin; ARSENKIN_TOKEN = new_arsenkin 
        if not YANDEX_DICT_KEY:
             new_yandex = st.text_input("Yandex Dict Key", type="password", key="input_yandex")
             if new_yandex: st.session_state.yandex_dict_key = new_yandex; YANDEX_DICT_KEY = new_yandex
        st.markdown("#####‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
        st.selectbox("User-Agent", ["Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "YandexBot/3.0"], key="settings_ua")
        st.selectbox("–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞", ["–Ø–Ω–¥–µ–∫—Å", "Google", "–Ø–Ω–¥–µ–∫—Å + Google"], key="settings_search_engine")
        st.selectbox("–†–µ–≥–∏–æ–Ω –ø–æ–∏—Å–∫–∞", list(REGION_MAP.keys()), key="settings_region")
        st.selectbox("–ö–æ–ª-–≤–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", [10, 20], index=0, key="settings_top_n")
        st.checkbox("–ò—Å–∫–ª—é—á–∞—Ç—å <noindex>", True, key="settings_noindex")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å Alt/Title", False, key="settings_alt")
        st.checkbox("–£—á–∏—Ç—ã–≤–∞—Ç—å —á–∏—Å–ª–∞", False, key="settings_numbers")
        st.checkbox("–ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–ª–∏–Ω–µ", True, key="settings_norm")

    if st.session_state.analysis_done and st.session_state.analysis_results:
        results = st.session_state.analysis_results
        d_score = results['my_score']['depth']
        w_score = results['my_score']['width']
        w_color = "#2E7D32" if w_score >= 80 else ("#E65100" if w_score >= 50 else "#D32F2F")
        if 75 <= d_score <= 88: d_color = "#2E7D32"; d_status = "–ò–î–ï–ê–õ (–¢–æ–ø)"
        elif 88 < d_score <= 100: d_color = "#D32F2F"; d_status = "–ü–ï–†–ï–°–ü–ê–ú (–†–∏—Å–∫)"
        elif 55 <= d_score < 75: d_color = "#F9A825"; d_status = "–°—Ä–µ–¥–Ω—è—è"
        else: d_color = "#D32F2F"; d_status = "–ù–∏–∑–∫–∞—è"
        st.success("–ê–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
        st.markdown(f"""
        <div style='display: flex; gap: 20px; flex-wrap: wrap;'>
            <div style='flex: 1; background:{LIGHT_BG_MAIN}; padding:15px; border-radius:8px; border-left: 5px solid {w_color};'>
                <div style='font-size: 12px; color: #666;'>–®–ò–†–ò–ù–ê (–û—Ö–≤–∞—Ç —Ç–µ–º)</div>
                <div style='font-size: 24px; font-weight: bold; color: {w_color};'>{w_score}/100</div>
            </div>
            <div style='flex: 1; background:{LIGHT_BG_MAIN}; padding:15px; border-radius:8px; border-left: 5px solid {d_color};'>
                <div style='font-size: 12px; color: #666;'>–ì–õ–£–ë–ò–ù–ê (–¶–µ–ª—å: ~80)</div>
                <div style='font-size: 24px; font-weight: bold; color: {d_color};'>{d_score}/100 <span style='font-size:14px; font-weight:normal;'>({d_status})</span></div>
            </div>
        </div><br>""", unsafe_allow_html=True)
        with st.expander("üõí –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ –∏ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è", expanded=True):
            if not st.session_state.get('orig_products'): st.info("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑.")
            else:
                c1, c2, c3 = st.columns(3)
                with c1: render_clean_block("–¢–æ–≤–∞—Ä—ã", "üß±", st.session_state.categorized_products)
                with c2: render_clean_block("–ì–µ–æ", "üåç", st.session_state.categorized_geo)
                with c3: render_clean_block("–ö–æ–º–º–µ—Ä—Ü–∏—è", "üí∞", st.session_state.categorized_commercial)
                c4, c5, c6 = st.columns(3)
                with c4: render_clean_block("–£—Å–ª—É–≥–∏", "üõ†Ô∏è", st.session_state.categorized_services)
                with c5: render_clean_block("–†–∞–∑–º–µ—Ä—ã/–ì–û–°–¢", "üìè", st.session_state.categorized_dimensions)
                with c6: render_clean_block("–û–±—â–∏–µ", "üìÇ", st.session_state.categorized_general)
                st.markdown("<hr style='margin: 15px 0;'>", unsafe_allow_html=True)
                cs1, cs2 = st.columns([1, 3])
                if 'sensitive_words_input_final' not in st.session_state:
                    current_list = st.session_state.get('categorized_sensitive', [])
                    st.session_state['sensitive_words_input_final'] = "\n".join(current_list)
                current_text_value = st.session_state['sensitive_words_input_final']
                with cs1:
                    count_excluded = len([x for x in current_text_value.split('\n') if x.strip()])
                    st.markdown(f"**‚õî –°—Ç–æ–ø-—Å–ª–æ–≤–∞**"); st.markdown(f"–ò—Å–∫–ª—é—á–µ–Ω–æ: **{count_excluded}**")
                with cs2:
                    new_sens_str = st.text_area("hidden_label", height=100, key="sensitive_words_input_final", label_visibility="collapsed", placeholder="–°–ª–æ–≤–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è...")
                    if st.button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä", type="primary", use_container_width=True):
                        raw_input = st.session_state.get("sensitive_words_input_final", "")
                        new_stop_set = set([w.strip().lower() for w in raw_input.split('\n') if w.strip()])
                        st.session_state.categorized_sensitive = sorted(list(new_stop_set))
                        def apply_filter(orig_list_key, stop_set):
                            original = st.session_state.get(orig_list_key, [])
                            return [w for w in original if w.lower() not in stop_set]
                        st.session_state.categorized_products = apply_filter('orig_products', new_stop_set)
                        st.session_state.categorized_services = apply_filter('orig_services', new_stop_set)
                        st.session_state.categorized_commercial = apply_filter('orig_commercial', new_stop_set)
                        st.session_state.categorized_geo = apply_filter('orig_geo', new_stop_set)
                        st.session_state.categorized_dimensions = apply_filter('orig_dimensions', new_stop_set)
                        st.session_state.categorized_general = apply_filter('orig_general', new_stop_set)
                        all_prods = st.session_state.categorized_products
                        count_prods = len(all_prods)
                        if count_prods < 20:
                            st.session_state.auto_tags_words = all_prods
                            st.session_state.auto_promo_words = []
                        else:
                            half = int(math.ceil(count_prods / 2))
                            st.session_state.auto_tags_words = all_prods[:half]
                            st.session_state.auto_promo_words = all_prods[half:]
                        st.session_state['kws_tags_auto'] = "\n".join(st.session_state.auto_tags_words)
                        st.session_state['kws_promo_auto'] = "\n".join(st.session_state.auto_promo_words)
                        st.toast("–§–∏–ª—å—Ç—Ä –æ–±–Ω–æ–≤–ª–µ–Ω!", icon="‚úÖ"); time.sleep(0.5); st.rerun()

        high = results.get('missing_semantics_high', [])
        low = results.get('missing_semantics_low', [])
        if high or low:
            with st.expander(f"üß© –£–ø—É—â–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ ({len(high)+len(low)})", expanded=False):
                if high: st.markdown(f"<div style='background:#EBF5FF;padding:10px;border-radius:5px;'><b>–í–∞–∂–Ω—ã–µ:</b> {', '.join([x['word'] for x in high])}</div>", unsafe_allow_html=True)
                if low: st.markdown(f"<div style='background:#F7FAFC;padding:10px;border-radius:5px;margin-top:5px;'><b>–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞:</b> {', '.join([x['word'] for x in low])}</div>", unsafe_allow_html=True)
        render_paginated_table(results['depth'], "1. –ì–ª—É–±–∏–Ω–∞", "tbl_depth_1", default_sort_col="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è", use_abs_sort_default=True)
        if 'naming_table_df' in st.session_state and st.session_state.naming_table_df is not None:
            df_naming = st.session_state.naming_table_df
            st.markdown("### 2. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ç–æ–≤–∞—Ä–æ–≤")
            if 'ideal_h1_result' in st.session_state:
                res_ideal = st.session_state.ideal_h1_result
                if isinstance(res_ideal, (tuple, list)) and len(res_ideal) >= 2:
                    example_name = res_ideal[0]
                    report_list = res_ideal[1]
                    formula_str = "–§–æ—Ä–º—É–ª–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"
                    for line in report_list:
                        if "—Å—Ç—Ä—É–∫—Ç—É—Ä–∞" in line or "–°—Ö–µ–º–∞" in line:
                            formula_str = line.replace("**–°–∞–º–∞—è —á–∞—Å—Ç–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**", "").replace("**–°—Ö–µ–º–∞:**", "").strip()
                            break
                    with st.container(border=True):
                        st.markdown("#### üß™ –ò–¥–µ–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ –Ω–∞–∑–≤–∞–Ω–∏—è")
                        st.info(f"**{formula_str}**", icon="üß©")
                        st.markdown(f"**–ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:** _{example_name}_")
            st.markdown("##### –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫")
            if not df_naming.empty:
                col_ctrl1, col_ctrl2 = st.columns([1, 3])
                with col_ctrl1: show_tech = st.toggle("–ü–æ–∫–∞–∑–∞—Ç—å —Ä–∞–∑–º–µ—Ä—ã –∏ —Ü–∏—Ñ—Ä—ã", value=False, key="toggle_show_tech_specs_unique")
                df_display = df_naming.copy()
                if not show_tech: df_display = df_display[~df_display['–¢–∏–ø —Ö–∞—Ä-–∫–∏'].str.contains("–†–∞–∑–º–µ—Ä—ã", na=False)]
                if 'cat_sort' in df_display.columns: df_display = df_display.sort_values(by=["cat_sort", "raw_freq"], ascending=[True, False])
                cols_to_show = ["–¢–∏–ø —Ö–∞—Ä-–∫–∏", "–°–ª–æ–≤–æ", "–ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å (%)", "–£ –í–∞—Å", "–ú–µ–¥–∏–∞–Ω–∞", "–î–æ–±–∞–≤–∏—Ç—å"]
                existing_cols = [c for c in cols_to_show if c in df_display.columns]
                df_display = df_display[existing_cols]
                def style_rows(row):
                    val = str(row.get('–î–æ–±–∞–≤–∏—Ç—å', ''))
                    if "+" in val: return ['background-color: #fff1f2; color: #9f1239'] * len(row)
                    if "‚úÖ" in val: return ['background-color: #f0fdf4; color: #166534'] * len(row)
                    return [''] * len(row)
                st.dataframe(df_display.style.apply(style_rows, axis=1), use_container_width=True, hide_index=True, height=(len(df_display) * 35) + 38 if len(df_display) < 15 else 500)
            else: st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
        render_paginated_table(results['hybrid'], "3. TF-IDF", "tbl_hybrid", default_sort_col="TF-IDF –¢–û–ü")
        render_paginated_table(results['relevance_top'], "4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", "tbl_rel", default_sort_col="–®–∏—Ä–∏–Ω–∞ (–±–∞–ª–ª)")

    if st.session_state.get('start_analysis_flag'):
        st.session_state.start_analysis_flag = False
        settings = {'noindex': st.session_state.settings_noindex, 'alt_title': st.session_state.settings_alt, 'numbers': st.session_state.settings_numbers, 'norm': st.session_state.settings_norm, 'ua': st.session_state.settings_ua, 'custom_stops': st.session_state.settings_stops.split()}
        my_data, my_domain, my_serp_pos = None, "", 0
        current_input_type = st.session_state.get("my_page_source_radio")
        if current_input_type == "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ":
            with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
                my_data = parse_page(st.session_state.my_url_input, settings, st.session_state.query_input)
                if not my_data: st.error("–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤–∞—à–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."); st.stop()
                my_domain = urlparse(st.session_state.my_url_input).netloc
        elif current_input_type == "–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ —Ç–µ–∫—Å—Ç":
            my_data = {'url': 'Local', 'domain': 'local', 'body_text': st.session_state.my_content_input, 'anchor_text': ''}
        candidates_pool = []
        current_source_val = st.session_state.get("competitor_source_radio")
        user_target_top_n = st.session_state.settings_top_n
        download_limit = 30 
        if "API" in current_source_val:
            if not ARSENKIN_TOKEN: st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç API —Ç–æ–∫–µ–Ω Arsenkin."); st.stop()
            with st.spinner(f"API Arsenkin (–ó–∞–ø—Ä–æ—Å –¢–æ–ø-30)..."):
                raw_top = get_arsenkin_urls(st.session_state.query_input, st.session_state.settings_search_engine, st.session_state.settings_region, ARSENKIN_TOKEN, depth_val=30)
                if not raw_top: st.stop()
                excl = [d.strip() for d in st.session_state.settings_excludes.split('\n') if d.strip()]
                agg_list = ["avito", "ozon", "wildberries", "market.yandex", "tiu", "youtube", "vk.com", "yandex", "leroymerlin", "petrovich", "satom", "pulscen", "blizko", "deal.by", "satu.kz", "prom.ua", "wikipedia", "dzen", "rutube", "kino", "otzovik", "irecommend", "profi.ru", "zoon", "2gis", "megamarket.ru", "lamoda.ru", "utkonos.ru", "vprok.ru", "allbiz.ru", "all-companies.ru", "orgpage.ru", "list-org.com", "rusprofile.ru", "e-katalog.ru", "kufar.by", "wildberries.kz", "ozon.kz", "kaspi.kz", "pulscen.kz", "allbiz.kz", "wildberries.uz", "olx.uz", "pulscen.uz", "allbiz.uz", "wildberries.kg", "pulscen.kg", "allbiz.kg", "all.biz", "b2b-center.ru"]
                excl.extend(agg_list)
                for res in raw_top:
                    dom = urlparse(res['url']).netloc.lower()
                    if my_domain and (my_domain in dom or dom in my_domain):
                        if my_serp_pos == 0 or res['pos'] < my_serp_pos: my_serp_pos = res['pos']
                    is_garbage = False
                    for x in excl:
                        if x.lower() in dom: is_garbage = True; break
                    if is_garbage: continue
                    candidates_pool.append(res)
        else:
            raw_input_urls = st.session_state.get("persistent_urls", "")
            candidates_pool = [{'url': u.strip(), 'pos': i+1} for i, u in enumerate(raw_input_urls.split('\n')) if u.strip()]

        if not candidates_pool: st.error("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤."); st.stop()
        comp_data_valid = []
        with st.status(f"üïµÔ∏è –ì–ª—É–±–æ–∫–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (–í—Å–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidates_pool)})...", expanded=True) as status:
            with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
                futures = {executor.submit(parse_page, item['url'], settings, st.session_state.query_input): item for item in candidates_pool}
                done_count = 0
                for f in concurrent.futures.as_completed(futures):
                    original_item = futures[f]
                    try:
                        res = f.result()
                        if res:
                            res['pos'] = original_item['pos']
                            comp_data_valid.append(res)
                    except: pass
                    done_count += 1
                    status.update(label=f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {done_count}/{len(candidates_pool)} | –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ: {len(comp_data_valid)}")

            comp_data_valid.sort(key=lambda x: x['pos'])
            data_for_graph = comp_data_valid[:download_limit]
            targets_for_graph = [{'url': d['url'], 'pos': d['pos']} for d in data_for_graph]

        with st.spinner("–ê–Ω–∞–ª–∏–∑ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è..."):
            results_full = calculate_metrics(data_for_graph, my_data, settings, my_serp_pos, targets_for_graph)
            st.session_state['full_graph_data'] = results_full['relevance_top']
            df_rel_check = results_full['relevance_top']
            good_urls, bad_urls_dicts, trend = analyze_serp_anomalies(df_rel_check)
            st.session_state['serp_trend_info'] = trend
            
            bad_urls_set = set(item['url'] for item in bad_urls_dicts)
            if "API" in current_source_val:
                clean_data_pool = [d for d in data_for_graph if d['url'] not in bad_urls_set]
                final_clean_data = clean_data_pool[:user_target_top_n]
            else: final_clean_data = data_for_graph 
            
            final_clean_targets = [{'url': d['url'], 'pos': d['pos']} for d in final_clean_data]
            results_final = calculate_metrics(final_clean_data, my_data, settings, my_serp_pos, final_clean_targets)
            st.session_state.analysis_results = results_final
            naming_df = calculate_naming_metrics(final_clean_data, my_data, settings)
            st.session_state.naming_table_df = naming_df 
            st.session_state.ideal_h1_result = analyze_ideal_name(final_clean_data)
            st.session_state.analysis_done = True
            
            if "API" in current_source_val and 'full_graph_data' in st.session_state: df_rel_check = st.session_state['full_graph_data']
            else: df_rel_check = st.session_state.analysis_results['relevance_top']
            should_auto_filter = True
            is_manual_mode = "–†—É—á–Ω–æ–π" in current_source_val
            has_previous_exclusions = 'excluded_urls_auto' in st.session_state and len(st.session_state.get('excluded_urls_auto', '')) > 5
            if is_manual_mode and has_previous_exclusions: should_auto_filter = False 
            good_urls, bad_urls_dicts, trend = analyze_serp_anomalies(df_rel_check)
            st.session_state['serp_trend_info'] = trend
            
            if should_auto_filter and bad_urls_dicts:
                st.session_state['detected_anomalies'] = bad_urls_dicts
                st.session_state['persistent_urls'] = "\n".join(good_urls)
                excluded_list = [item['url'] for item in bad_urls_dicts]
                st.session_state['excluded_urls_auto'] = "\n".join(excluded_list)
                st.toast(f"üßπ –ê–≤—Ç–æ-—Ñ–∏–ª—å—Ç—Ä: –ò—Å–∫–ª—é—á–µ–Ω–æ {len(bad_urls_dicts)} —Å–ª–∞–±—ã—Ö —Å–∞–π—Ç–æ–≤.", icon="üóëÔ∏è")
            elif not should_auto_filter and bad_urls_dicts:
                all_current_urls = [d['url'] for d in final_clean_data]
                st.session_state['persistent_urls'] = "\n".join(all_current_urls)
                st.toast(f"üõ°Ô∏è –†—É—á–Ω–æ–π —Ä–µ–∂–∏–º: –°–ª–∞–±—ã–µ —Å–∞–π—Ç—ã ({len(bad_urls_dicts)} —à—Ç.) –æ—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –∞–Ω–∞–ª–∏–∑–µ.", icon="üîì")
            else:
                st.session_state['persistent_urls'] = "\n".join(good_urls)
                if should_auto_filter:
                    if 'excluded_urls_auto' in st.session_state: del st.session_state['excluded_urls_auto']
                    if 'detected_anomalies' in st.session_state: del st.session_state['detected_anomalies']

            res = st.session_state.analysis_results
            words_to_check = [x['word'] for x in res.get('missing_semantics_high', [])]
            if not words_to_check:
                st.session_state.categorized_products = []; st.session_state.categorized_services = []
                st.session_state.categorized_commercial = []; st.session_state.categorized_dimensions = []
            else:
                with st.spinner("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏..."):
                    categorized = classify_semantics_with_api(words_to_check, YANDEX_DICT_KEY)
                st.session_state.categorized_products = categorized['products']
                st.session_state.categorized_services = categorized['services']
                st.session_state.categorized_commercial = categorized['commercial']
                st.session_state.categorized_geo = categorized['geo']
                st.session_state.categorized_dimensions = categorized['dimensions']
                st.session_state.categorized_general = categorized['general']
                st.session_state.categorized_sensitive = categorized['sensitive']
                st.session_state.orig_products = categorized['products'] + categorized['sensitive']
                st.session_state.orig_services = categorized['services'] + categorized['sensitive']
                st.session_state.orig_commercial = categorized['commercial'] + categorized['sensitive']
                st.session_state.orig_geo = categorized['geo'] + categorized['sensitive']
                st.session_state.orig_dimensions = categorized['dimensions'] + categorized['sensitive']
                st.session_state.orig_general = categorized['general'] + categorized['sensitive']
                st.session_state['sensitive_words_input_final'] = "\n".join(categorized['sensitive'])

            all_found_products = st.session_state.categorized_products
            count_prods = len(all_found_products)
            if count_prods < 20:
                st.session_state.auto_tags_words = all_found_products
                st.session_state.auto_promo_words = []
            else:
                half_count = int(math.ceil(count_prods / 2))
                st.session_state.auto_tags_words = all_found_products[:half_count]
                st.session_state.auto_promo_words = all_found_products[half_count:]
            
            st.session_state['tags_products_edit_final'] = "\n".join(st.session_state.auto_tags_words)
            st.session_state['promo_keywords_area_final'] = "\n".join(st.session_state.auto_promo_words)
            st.session_state['force_radio_switch'] = True
            if "API" in current_source_val: st.session_state['persistent_urls'] = "\n".join(good_urls)
            st.rerun()

with tab_wholesale_main:
    st.header("üè≠ –ï–¥–∏–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    cat_products = st.session_state.get('categorized_products', [])
    cat_services = st.session_state.get('categorized_services', [])
    structure_keywords = cat_products + cat_services
    count_struct = len(structure_keywords)
    if 'auto_tags_words' in st.session_state and st.session_state.auto_tags_words:
         tags_list_source = st.session_state.auto_tags_words
         promo_list_source = st.session_state.auto_promo_words
    else:
         if count_struct > 0:
            if count_struct < 10: tags_list_source = structure_keywords; promo_list_source = []
            elif count_struct < 30:
                mid = math.ceil(count_struct / 2)
                tags_list_source = structure_keywords[:mid]
                promo_list_source = structure_keywords[mid:]
            else:
                part = math.ceil(count_struct / 3)
                tags_list_source = structure_keywords[:part]
                promo_list_source = structure_keywords[part:part*2]
         else: tags_list_source = []; promo_list_source = []
    
    sidebar_default_text = ""
    if count_struct >= 30 and 'auto_tags_words' not in st.session_state:
         part = math.ceil(count_struct / 3)
         sidebar_default_text = "\n".join(structure_keywords[part*2:])

    tags_default_text = ", ".join(tags_list_source)
    promo_default_text = ", ".join(promo_list_source)
    cat_dimensions = st.session_state.get('categorized_dimensions', [])
    tech_context_default = ", ".join(cat_dimensions) if cat_dimensions else ""
    cat_commercial = st.session_state.get('categorized_commercial', [])
    cat_general = st.session_state.get('categorized_general', [])
    cat_geo = st.session_state.get('categorized_geo', [])
    text_context_list_raw = cat_commercial + cat_general
    text_context_default = ", ".join(text_context_list_raw)
    geo_context_default = ", ".join(cat_geo)

    auto_check_text = bool(text_context_list_raw)
    auto_check_tags = bool(tags_list_source)
    auto_check_tables = bool(cat_dimensions)
    auto_check_promo = bool(promo_list_source)
    auto_check_sidebar = bool(sidebar_default_text.strip())
    auto_check_geo = bool(cat_geo)

    with st.container(border=True):
        st.subheader("1. –ò—Å—Ç–æ—á–Ω–∏–∫ –∏ –î–æ—Å—Ç—É–ø—ã")
        col_source, col_key = st.columns([3, 1])
        use_manual_html = st.checkbox("üìù –í—Å—Ç–∞–≤–∏—Ç—å HTML –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", key="cb_manual_html_mode", value=False)
        with col_source:
            if use_manual_html:
                manual_html_source = st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (HTML)", height=200, placeholder="<html>...</html>", help="–°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Å—é–¥–∞ –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.")
                main_category_url = None
            else:
                main_category_url = st.text_input("URL –ö–∞—Ç–µ–≥–æ—Ä–∏–∏", placeholder="https://site.ru/catalog/...", help="–°–∫—Ä–∏–ø—Ç —Å–æ–±–µ—Ä–µ—Ç —Ç–æ–≤–∞—Ä—ã —Å —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
                manual_html_source = None
        with col_key:
            default_key = st.session_state.get('pplx_key_cache', "AIzaSyALrGas6cbkwNN9lko-Ag4_czO58NS4HIc")
            pplx_api_key = st.text_input("Google AI API Key", value=default_key, type="password")
            if pplx_api_key: st.session_state.pplx_key_cache = pplx_api_key

    st.subheader("2. –ö–∞–∫–∏–µ –±–ª–æ–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º?")
    st.info("‚ÑπÔ∏è **–ê–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞:** –ì–∞–ª–æ—á–∫–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ç–∞–º, –≥–¥–µ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—à–ª–∏—Å—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–ª–æ–≤–∞. –í—ã –º–æ–∂–µ—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –≤—ã–±–æ—Ä –≤—Ä—É—á–Ω—É—é.")
    col_ch1, col_ch2, col_ch3, col_ch4, col_ch5, col_ch6 = st.columns(6)
    with col_ch1: use_text = st.checkbox("ü§ñ AI –¢–µ–∫—Å—Ç—ã", value=auto_check_text)
    with col_ch2: use_tags = st.checkbox("üè∑Ô∏è –¢–µ–≥–∏", value=auto_check_tags)
    with col_ch3: use_tables = st.checkbox("üß© –¢–∞–±–ª–∏—Ü—ã", value=auto_check_tables)
    with col_ch4: use_promo = st.checkbox("üî• –ü—Ä–æ–º–æ", value=auto_check_promo)
    with col_ch5: use_sidebar = st.checkbox("üìë –°–∞–π–¥–±–∞—Ä", value=auto_check_sidebar)
    with col_ch6: use_geo = st.checkbox("üåç –ì–µ–æ-–±–ª–æ–∫", value=auto_check_geo)

    global_tags_list = []
    global_promo_list = []
    global_sidebar_list = []
    global_geo_list = []
    tags_file_content = ""
    table_prompts = []
    df_db_promo = None
    promo_title = "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º"
    sidebar_content = ""
    text_context_final_list = []
    tech_context_final_str = ""
    num_text_blocks_val = 5 

    if any([use_text, use_tags, use_tables, use_promo, use_sidebar, use_geo]):
        st.subheader("3. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥—É–ª–µ–π")
        if use_text:
            with st.container(border=True):
                st.markdown("#### ü§ñ 1. AI –¢–µ–∫—Å—Ç—ã")
                col_txt1, col_txt2 = st.columns([1, 4])
                with col_txt1: num_text_blocks_val = st.selectbox("–ö–æ–ª-–≤–æ –±–ª–æ–∫–æ–≤", [1, 2, 3, 4, 5], index=4, key="sb_num_blocks")
                with col_txt2:
                    ai_words_input = st.text_area("–°–ª–æ–≤–∞ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è (–ö–æ–º–º–µ—Ä—Ü–∏—è + –û–±—â–∏–µ)", value=text_context_default, height=100, key="ai_text_context_editable", help="–≠—Ç–∏ —Å–ª–æ–≤–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –ø–æ—Å—Ç–∞—Ä–∞–µ—Ç—Å—è –≤–Ω–µ–¥—Ä–∏—Ç—å –≤ —Ç–µ–∫—Å—Ç.")
                text_context_final_list = [x.strip() for x in re.split(r'[,\n]+', ai_words_input) if x.strip()]

        if use_tags:
            with st.container(border=True):
                st.markdown("#### üè∑Ô∏è 2. –¢–µ–≥–∏")
                kws_input_tags = st.text_area("–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", value=tags_default_text, height=100, key="kws_tags_auto")
                global_tags_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_tags) if x.strip()]
                if not global_tags_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                st.markdown("---")
                col_t1, col_t2 = st.columns([1, 2])
                with col_t1: u_manual = st.checkbox("–°–≤–æ—è –±–∞–∑–∞ —Å—Å—ã–ª–æ–∫ (.txt)", key="cb_tags_vert")
                with col_t2:
                    default_tags_path = "data/links_base.txt"
                    if not u_manual and os.path.exists(default_tags_path):
                        st.success(f"‚úÖ –ë–∞–∑–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (`links_base.txt`)")
                        with open(default_tags_path, "r", encoding="utf-8") as f: tags_file_content = f.read()
                    elif u_manual:
                        up_t = st.file_uploader("–§–∞–π–ª .txt", type=["txt"], key="up_tags_vert", label_visibility="collapsed")
                        if up_t: tags_file_content = up_t.getvalue().decode("utf-8")
                    else: st.error("‚ùå –§–∞–π–ª –±–∞–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω!")

        def generate_context_aware_headers(count, query, dimensions_list, general_list):
            query_lower = query.lower()
            dims_str = " ".join(dimensions_list).lower()
            gen_str = " ".join(general_list).lower()
            full_context = f"{dims_str} {gen_str} {query_lower}"
            has_sizes_signal = (len(dimensions_list) > 0 or bool(re.search(r'\d+[x—Ö*]\d+', full_context)) or any(x in full_context for x in ['—Ä–∞–∑–º–µ—Ä', '–≥–∞–±–∞—Ä–∏—Ç', '—Ç–æ–ª—â–∏–Ω', '–¥–∏–∞–º–µ—Ç—Ä', '—Ä–∞—Å–∫—Ä–æ–π', '–≤–µ—Å', '–º–∞—Å—Å']))
            has_gost_signal = any(x in full_context for x in ['–≥–æ—Å—Ç', 'din', 'aisi', 'astm', '—Ç—É ', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç'])
            has_grade_signal = any(x in full_context for x in ['–º–∞—Ä–∫', '—Å–ø–ª–∞–≤', '—Å—Ç–∞–ª—å', '—Å—Ç.', '–º–∞—Ç–µ—Ä–∏–∞–ª', '—Ö–∏–º–∏—á', '—Å–æ—Å—Ç–∞–≤'])
            has_usage_signal = any(x in full_context for x in ['–ø—Ä–∏–º–µ–Ω–µ–Ω', '—Å—Ñ–µ—Ä', '–Ω–∞–∑–Ω–∞—á–µ–Ω', '–∏—Å–ø–æ–ª—å–∑'])
            priority_stack = []
            if has_grade_signal: priority_stack.append("–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã")
            if has_sizes_signal: priority_stack.append("–¢–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤")
            if has_gost_signal: priority_stack.append("–ì–û–°–¢—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã")
            if "—Ö–∏–º" in full_context and "—Å–æ—Å—Ç–∞–≤" in full_context:
                 if "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã" in priority_stack:
                     idx = priority_stack.index("–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã")
                     priority_stack.insert(idx+1, "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤")
                 else: priority_stack.append("–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤")
            defaults = ["–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–°–≤–æ–π—Å—Ç–≤–∞", "–°—Ñ–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–¥–µ–ª–∏—è", "–ê–Ω–∞–ª–æ–≥–∏"]
            final_headers = []
            for p in priority_stack:
                if p not in final_headers: final_headers.append(p)
            for d in defaults:
                if d not in final_headers: final_headers.append(d)
            while len(final_headers) < count: final_headers.append("–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏")
            return final_headers[:count]

        if use_tables:
            with st.container(border=True):
                st.markdown("#### üß© 3. –¢–∞–±–ª–∏—Ü—ã")
                raw_query = st.session_state.get('query_input', '')
                found_dims = st.session_state.get('categorized_dimensions', [])
                found_general = st.session_state.get('categorized_general', [])
                col_ctx, col_cnt = st.columns([3, 1]) 
                with col_ctx:
                    tech_context_final_str = st.text_area("–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ç–∞–±–ª–∏—Ü (–ú–∞—Ä–∫–∏, –ì–û–°–¢, –†–∞–∑–º–µ—Ä—ã)", value=tech_context_default, height=68, key="table_context_editable", help="–≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–º–æ–≥—É—Ç AI —Å–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É.")
                with col_cnt:
                    cnt_options = [1, 2, 3, 4, 5]
                    cnt = st.selectbox("–ö–æ–ª-–≤–æ —Ç–∞–±–ª–∏—Ü", cnt_options, index=1, key="num_tbl_vert_select")
                smart_headers_list = generate_context_aware_headers(cnt, raw_query, found_dims, found_general)
                table_presets = ["–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–°–≤–æ–π—Å—Ç–≤–∞", "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–¥–µ–ª–∏—è", "–û–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ", "–¢–∞–±–ª–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–æ–≤", "–°–æ—Ä—Ç–∞–º–µ–Ω—Ç", "–•–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤", "–§–∏–∑–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞", "–ú–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞", "–ú–∞—Ä–∫–∏ –∏ —Å–ø–ª–∞–≤—ã", "–°–æ—Å—Ç–∞–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞", "–ì–û–°–¢—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã", "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –ì–û–°–¢", "–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ", "–°—Ñ–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", "–£—Å–ª–æ–≤–∏—è —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏", "–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è", "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", "–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏", "–ê–Ω–∞–ª–æ–≥–∏", "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π", "–†–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç–∏"]
                table_prompts = []
                st.write(""); cols = st.columns(cnt)
                for i, col in enumerate(cols):
                    with col:
                        st.caption(f"**–¢–∞–±–ª–∏—Ü–∞ {i+1}**")
                        suggested_topic = smart_headers_list[i]
                        try: default_idx = table_presets.index(suggested_topic)
                        except: default_idx = 0
                        is_manual = st.checkbox("–°–≤–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫", key=f"cb_tbl_manual_{i}")
                        if is_manual:
                            selected_topic = st.text_input(f"–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª. {i+1}", value="", key=f"tbl_topic_custom_{i}", label_visibility="collapsed")
                            if not selected_topic.strip(): selected_topic = "–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏" 
                        else:
                            selected_topic = st.selectbox(f"–¢–µ–º–∞ —Ç–∞–±–ª. {i+1}", table_presets, index=default_idx, key=f"tbl_topic_select_{i}", label_visibility="collapsed")
                        table_prompts.append(selected_topic)

        if use_promo:
            with st.container(border=True):
                st.markdown("#### üî• 4. –ü—Ä–æ–º–æ-–±–ª–æ–∫")
                kws_input_promo = st.text_area("–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", value=promo_default_text, height=100, key="kws_promo_auto")
                global_promo_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_promo) if x.strip()]
                if not global_promo_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                st.markdown("---")
                col_p1, col_p2 = st.columns([1, 2])
                with col_p1:
                    promo_presets = ["–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ", "–ü–æ—Ö–æ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã", "–í–∞—Å –º–æ–∂–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å", "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º", "–î—Ä—É–≥–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", "–í–∞–º –º–æ–∂–µ—Ç –ø—Ä–∏–≥–æ–¥–∏—Ç—å—Å—è", "–¢–∞–∫–∂–µ –≤ —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ", "–° —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º –ø–æ–∫—É–ø–∞—é—Ç", "–ß–∞—Å—Ç–æ –ø–æ–∫—É–ø–∞—é—Ç –≤–º–µ—Å—Ç–µ", "–°–æ–ø—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–≤–∞—Ä—ã", "–•–∏—Ç—ã –ø—Ä–æ–¥–∞–∂", "–í—ã–±–æ—Ä –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π", "–õ–∏–¥–µ—Ä—ã —Å–ø—Ä–æ—Å–∞", "–ü–æ–ø—É–ª—è—Ä–Ω–æ–µ —Å–µ–π—á–∞—Å", "–¢–æ–ø —Ç–æ–≤–∞—Ä–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏", "–õ—É—á—à–∞—è —Ü–µ–Ω–∞", "–°–ø–µ—Ü–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è", "–£—Å–ø–µ–π—Ç–µ –∑–∞–∫–∞–∑–∞—Ç—å", "–ù–µ –∑–∞–±—É–¥—å—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å", "–í—ã –Ω–µ–¥–∞–≤–Ω–æ —Å–º–æ—Ç—Ä–µ–ª–∏"]
                    raw_query = st.session_state.get('query_input', '').lower()
                    comm_words = st.session_state.get('categorized_commercial', [])
                    comm_context = f"{raw_query} {' '.join(comm_words)}".lower()
                    target_header = "–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ"
                    is_commercial = any(x in comm_context for x in ["–∫—É–ø–∏—Ç—å", "—Ü–µ–Ω–∞", "–∑–∞–∫–∞–∑", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–ø—Ä–∞–π—Å", "–º–∞–≥–∞–∑–∏–Ω", "–∫–æ—Ä–∑–∏–Ω–∞"])
                    is_promo = any(x in comm_context for x in ["–∞–∫—Ü–∏—è", "—Å–∫–∏–¥–∫", "—Ä–∞—Å–ø—Ä–æ–¥–∞–∂", "–≤—ã–≥–æ–¥–Ω"])
                    is_top = any(x in comm_context for x in ["—Ç–æ–ø", "–ª—É—á—à", "—Ä–µ–π—Ç–∏–Ω–≥", "–ø–æ–ø—É–ª—è—Ä–Ω"])
                    if is_promo: target_header = "–°–ø–µ—Ü–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"
                    elif is_top: target_header = "–õ–∏–¥–µ—Ä—ã —Å–ø—Ä–æ—Å–∞"
                    elif is_commercial: target_header = "–° —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º –ø–æ–∫—É–ø–∞—é—Ç"
                    try: promo_smart_idx = promo_presets.index(target_header)
                    except: promo_smart_idx = 0
                    use_custom_header = st.checkbox("–í–≤–µ—Å—Ç–∏ —Å–≤–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫", key="cb_custom_header")
                    if use_custom_header: promo_title = st.text_input("–í–∞—à –∑–∞–≥–æ–ª–æ–≤–æ–∫", placeholder="–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ", key="pr_tit_vert")
                    else: promo_title = st.selectbox("–í–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–≥–æ–ª–æ–≤–∫–∞", promo_presets, index=promo_smart_idx, key="promo_header_select")
                    st.markdown("<br>", unsafe_allow_html=True)
                    u_img_man = st.checkbox("–°–≤–æ—è –±–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫", key="cb_img_vert")
                with col_p2:
                    default_img_db = "data/images_db.xlsx"
                    if not u_img_man and os.path.exists(default_img_db):
                        st.success("‚úÖ –ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ (`images_db.xlsx`)")
                        try: df_db_promo = pd.read_excel(default_img_db)
                        except: pass
                    elif u_img_man:
                        up_i = st.file_uploader("–§–∞–π–ª .xlsx", type=['xlsx'], key="up_img_vert", label_visibility="collapsed")
                        if up_i: df_db_promo = pd.read_excel(up_i)
                    else: st.error("‚ùå –ë–∞–∑–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")

        if use_sidebar:
            with st.container(border=True):
                st.markdown("#### üìë 5. –°–∞–π–¥–±–∞—Ä")
                kws_input_sidebar = st.text_area("–°–ø–∏—Å–æ–∫ (–¢–æ–≤–∞—Ä—ã + –£—Å–ª—É–≥–∏) - —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏", value=sidebar_default_text, height=100, key="kws_sidebar_auto")
                global_sidebar_list = [x.strip() for x in kws_input_sidebar.split('\n') if x.strip()]
                if not global_sidebar_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—É—Å—Ç!")
                st.markdown("---")
                col_s1, col_s2 = st.columns([1, 2])
                with col_s1: u_sb_man = st.checkbox("–°–≤–æ–π —Ñ–∞–π–ª –º–µ–Ω—é (.txt)", key="cb_sb_vert")
                with col_s2:
                    def_menu = "data/menu_structure.txt"
                    if not u_sb_man and os.path.exists(def_menu):
                        st.success("‚úÖ –ú–µ–Ω—é —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (`menu_structure.txt`)")
                        with open(def_menu, "r", encoding="utf-8") as f: sidebar_content = f.read()
                    elif u_sb_man:
                        up_s = st.file_uploader("–§–∞–π–ª .txt", type=['txt'], key="up_sb_vert", label_visibility="collapsed")
                        if up_s: sidebar_content = up_s.getvalue().decode("utf-8")
                    else: st.error("‚ùå –§–∞–π–ª –º–µ–Ω—é –Ω–µ –Ω–∞–π–¥–µ–Ω!")

        if use_geo:
            with st.container(border=True):
                st.markdown("#### üåç 6. –ì–µ–æ-–±–ª–æ–∫")
                kws_input_geo = st.text_area("–°–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤/—Ä–µ–≥–∏–æ–Ω–æ–≤ (–∏–∑ –≤–∫–ª–∞–¥–∫–∏ –ê–Ω–∞–ª–∏–∑) - —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é", value=geo_context_default, height=100, key="kws_geo_auto")
                global_geo_list = [x.strip() for x in re.split(r'[,\n]+', kws_input_geo) if x.strip()]
                if not global_geo_list: st.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤ –ø—É—Å—Ç!")
                else: st.info(f"–ë—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ç–µ–∫—Å—Ç –¥–æ—Å—Ç–∞–≤–∫–∏ –¥–ª—è –ø–æ–ª—è IP_PROP4819 —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º —ç—Ç–∏—Ö –≥–æ—Ä–æ–¥–æ–≤.")
    st.markdown("---")
    
    ready_to_go = True
    if use_manual_html:
        if not manual_html_source: ready_to_go = False
    else:
        if not main_category_url: ready_to_go = False
    if (use_text or use_tables) and not pplx_api_key: ready_to_go = False
    if use_promo and df_db_promo is None: ready_to_go = False
    if use_geo and not pplx_api_key: ready_to_go = False
    
    if st.button("üöÄ –ó–ê–ü–£–°–¢–ò–¢–¨ –ì–ï–ù–ï–†–ê–¶–ò–Æ", type="primary", disabled=not ready_to_go, use_container_width=True):
        st.session_state.gen_result_df = None
        st.session_state.unified_excel_data = None
        status_box = st.status("üõ†Ô∏è –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...", expanded=True)
        final_data = [] 
        
        tags_map = {}
        all_tags_links = []
        if use_tags:
            if tags_file_content:
                s_io = io.StringIO(tags_file_content)
                all_tags_links = [l.strip() for l in s_io.readlines() if l.strip()]
            elif os.path.exists("data/links_base.txt"):
                with open("data/links_base.txt", "r", encoding="utf-8") as f:
                    all_tags_links = [l.strip() for l in f.readlines() if l.strip()]
            for kw in global_tags_list:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                search_roots = {tr}
                if len(tr) > 5: search_roots.add(tr[:-1]); search_roots.add(tr[:-2])
                elif len(tr) > 4: search_roots.add(tr[:-1])
                matches = []
                for u in all_tags_links:
                    u_lower = u.lower()
                    for root in search_roots:
                        if root in u_lower: matches.append(u); break
                if matches: tags_map[kw] = matches

        p_img_map = {}
        if use_promo and df_db_promo is not None:
            for _, row in df_db_promo.iterrows():
                u = str(row.iloc[0]).strip(); img = str(row.iloc[1]).strip()
                if u and u != 'nan' and img and img != 'nan': p_img_map[u.rstrip('/')] = img
        
        all_menu_urls = []
        if use_sidebar:
            if sidebar_content:
                s_io = io.StringIO(sidebar_content)
                all_menu_urls = [l.strip() for l in s_io.readlines() if l.strip()]
            elif os.path.exists("data/menu_structure.txt"):
                with open("data/menu_structure.txt", "r", encoding="utf-8") as f:
                    all_menu_urls = [l.strip() for l in f.readlines() if l.strip()]

        missing_words_log = set()
        if use_tags:
            for kw in global_tags_list:
                if kw not in tags_map: missing_words_log.add(kw)
        if use_promo:
            for kw in global_promo_list:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                roots = [tr]
                if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                has_match = False
                for u in p_img_map.keys():
                    if any(r in u for r in roots): has_match = True; break
                if not has_match: missing_words_log.add(kw)
        if use_sidebar and global_sidebar_list:
            for kw in global_sidebar_list:
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                roots = [tr]
                if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                has_match = False
                for u in all_menu_urls:
                    if any(r in u for r in roots): has_match = True; break
                if not has_match: missing_words_log.add(kw)

        if missing_words_log:
            missing_list = sorted(list(missing_words_log))
            for w in missing_list:
                if w not in text_context_final_list: text_context_final_list.append(w)
            tech_additions = []
            for w in missing_list:
                if any(char.isdigit() for char in w) or any(x in w.lower() for x in ['–≥–æ—Å—Ç', '—Ç–∏–ø', '—Ñ–æ—Ä–º–∞', '–º–º', '–∫–≥']):
                    tech_additions.append(w)
            if tech_additions: tech_context_final_str += "\n" + ", ".join(tech_additions)
            status_box.markdown(f"""<div style="background-color: #FFF4E5; border-left: 5px solid #FF9800; padding: 15px; border-radius: 4px; margin-bottom: 15px; color: #663C00;"><strong>‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –ß–∞—Å—Ç—å —Å—Å—ã–ª–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞</strong><br><span style="font-size: 0.9em;">–ú—ã –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–ª—è: <b>{', '.join(missing_list)}</b>.<br>‚úÖ <u>–û–Ω–∏ –±—ã–ª–∏ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã –≤ –¢–ó –¥–ª—è –ù–µ–π—Ä–æ—Å–µ—Ç–∏ (–±—É–¥—É—Ç –≤ —Ç–µ–∫—Å—Ç–µ/—Ç–∞–±–ª–∏—Ü–∞—Ö).</u></span></div>""", unsafe_allow_html=True)
            time.sleep(2)

        target_pages = []
        soup = None
        current_base_url = main_category_url if main_category_url else "http://localhost"

        try:
            if use_manual_html:
                status_box.write("üìÇ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º HTML –∫–æ–¥...")
                soup = BeautifulSoup(manual_html_source, 'html.parser')
            else:
                status_box.write(f"üïµÔ∏è –°–∫–∞–Ω–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {main_category_url}")
                session = requests.Session()
                retry = Retry(connect=3, read=3, redirect=3, backoff_factor=0.5)
                adapter = HTTPAdapter(max_retries=retry)
                session.mount('http://', adapter)
                session.mount('https://', adapter)
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
                r = session.get(main_category_url, headers=headers, timeout=30, verify=False)
                if r.status_code == 200: soup = BeautifulSoup(r.text, 'html.parser')
                else: 
                    status_box.error(f"–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞: {r.status_code}")
                    st.stop()
            if soup:
                tags_container = soup.find(class_='popular-tags-inner')
                if tags_container:
                    for link in tags_container.find_all('a'):
                        href = link.get('href')
                        name = link.get_text(strip=True)
                        if href and name:
                            full_url = urljoin(current_base_url, href)
                            target_pages.append({'url': full_url, 'name': name})
                if not target_pages:
                    status_box.warning("–¢–µ–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–∫–ª–∞—Å—Å .popular-tags-inner). –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.")
                    h1 = soup.find('h1')
                    name = h1.get_text(strip=True) if h1 else "–¢–æ–≤–∞—Ä"
                    target_pages.append({'url': current_base_url, 'name': name})
        except Exception as e: 
            status_box.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            st.stop()
            
        urls_to_fetch_names = set()
        promo_items_pool = [] 
        if use_tags:
            for kw, matches in tags_map.items(): urls_to_fetch_names.update(matches)
        if use_promo:
            used_urls = set()
            for kw in global_promo_list:
                if kw in missing_words_log: continue
                tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                roots = [tr]
                if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                matches = []
                for u in p_img_map.keys():
                    if any(r in u for r in roots): matches.append(u)
                for m in matches:
                    if m not in used_urls:
                        urls_to_fetch_names.add(m)
                        promo_items_pool.append({'url': m, 'img': p_img_map[m]})
                        used_urls.add(m)
        sidebar_matched_urls = []
        if use_sidebar:
            if global_sidebar_list:
                for kw in global_sidebar_list:
                    if kw in missing_words_log: continue
                    tr = transliterate_text(kw).replace(' ', '-').replace('_', '-')
                    roots = [tr]
                    if len(tr) > 5: roots.extend([tr[:-1], tr[:-2]])
                    found = []
                    for u in all_menu_urls:
                        if any(r in u for r in roots): found.append(u)
                    sidebar_matched_urls.extend(found)
                sidebar_matched_urls = list(set(sidebar_matched_urls))
            else: sidebar_matched_urls = all_menu_urls
            urls_to_fetch_names.update(sidebar_matched_urls)

        url_name_cache = {}
        if urls_to_fetch_names:
            status_box.write(f"üåç –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è {len(urls_to_fetch_names)} —Å—Å—ã–ª–æ–∫...")
            def fetch_name_worker(u): return u, get_breadcrumb_only(u) 
            with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
                future_to_url = {executor.submit(fetch_name_worker, u): u for u in urls_to_fetch_names}
                done_cnt = 0
                prog_fetch = status_box.progress(0)
                for future in concurrent.futures.as_completed(future_to_url):
                    u_res, name_res = future.result()
                    norm_key = u_res.rstrip('/')
                    if name_res: url_name_cache[norm_key] = name_res
                    else:
                        slug = norm_key.split('/')[-1]
                        url_name_cache[norm_key] = force_cyrillic_name_global(slug)
                    done_cnt += 1
                    prog_fetch.progress(done_cnt / len(urls_to_fetch_names))
            status_box.write("‚úÖ –ù–∞–∑–≤–∞–Ω–∏—è —Å–æ–±—Ä–∞–Ω—ã!")

        full_sidebar_code = ""
        if use_sidebar:
            status_box.write("üî® –°–±–æ—Ä–∫–∞ –º–µ–Ω—é...")
            tree = {}
            for url in sidebar_matched_urls:
                path = urlparse(url).path.strip('/')
                parts = [p for p in path.split('/') if p]
                idx_start = 0
                if 'catalog' in parts: idx_start = parts.index('catalog') + 1
                rel_parts = parts[idx_start:] if parts[idx_start:] else parts
                curr = tree
                for i, part in enumerate(rel_parts):
                    if part not in curr: curr[part] = {}
                    if i == len(rel_parts) - 1:
                        curr[part]['__url__'] = url
                        cache_key = url.rstrip('/')
                        curr[part]['__name__'] = url_name_cache.get(cache_key, force_cyrillic_name_global(part))
                    curr = curr[part]
            
            def render_tree_internal(node, level=1):
                html = ""
                keys = sorted([k for k in node.keys() if not k.startswith('__')])
                for key in keys:
                    child = node[key]
                    name = child.get('__name__', force_cyrillic_name_global(key))
                    url = child.get('__url__')
                    has_children = any(k for k in child.keys() if not k.startswith('__'))
                    if level == 1:
                        html += '<li class="level-1-header">\n'
                        if has_children:
                            html += f'    <span class="dropdown-toggle">{name}</span>\n'
                            html += '    <ul class="collapse-menu list-unstyled">\n'
                            html += render_tree_internal(child, level=2)
                            html += '    </ul>\n'
                        else:
                            target = url if url else "#"
                            html += f'    <a href="{target}">{name}</a>\n'
                        html += '</li>\n'
                    elif level == 2:
                        if has_children:
                            html += '<li class="level-2-header">\n'
                            html += f'    <span class="dropdown-toggle">{name}</span>\n'
                            html += '    <ul class="collapse-menu list-unstyled">\n'
                            html += render_tree_internal(child, level=3)
                            html += '    </ul>\n'
                        else:
                            target = url if url else "#"
                            html += f'<li class="level-2-link-special"><a href="{target}">{name}</a></li>\n'
                    elif level >= 3:
                        target = url if url else "#"
                        html += f'<li class="level-3-link"><a href="{target}">{name}</a></li>\n'
                return html
            inner_html = render_tree_internal(tree, level=1)
            full_sidebar_code = f"""<div class="page-content-with-sidebar"><button id="mobile-menu-toggle" class="menu-toggle-button">‚ò∞</button><div class="sidebar-wrapper"><nav id="sidebar-menu"><ul class="list-unstyled components">{inner_html}</ul></nav></div></div>"""

        has_ai_key = bool(pplx_api_key) and (genai is not None)
        progress_bar = status_box.progress(0)
        total_steps = len(target_pages)
        model_flash = None
        if has_ai_key:
            try:
                genai.configure(api_key=pplx_api_key)
                model_flash = genai.GenerativeModel('gemini-2.0-flash-exp')
            except:
                try: model_flash = genai.GenerativeModel('gemini-1.5-flash')
                except: pass
        
        for idx, page in enumerate(target_pages):
            base_text_raw, tags_on_page, real_header_h2, err = get_page_data_for_gen(page['url'])
            header_for_ai = real_header_h2 if real_header_h2 else page['name']
            row_data = {'Page URL': page['url'], 'Product Name': header_for_ai}
            for k, v in STATIC_DATA_GEN.items(): row_data[k] = v
            current_page_seo_words = list(text_context_final_list)
            
            row_data['Tags HTML'] = "" 
            row_data['Promo HTML'] = ""
            if use_tags:
                tags_html_parts = []
                html_collector = []
                for kw in global_tags_list:
                    if kw in tags_map:
                        valid = [u for u in tags_map[kw] if u.rstrip('/') != page['url'].rstrip('/')]
                        if valid:
                            sel = random.choice(valid)
                            nm = url_name_cache.get(sel.rstrip('/'), kw)
                            html_collector.append(f'<a href="{sel}" class="tag-link">{nm}</a>')
                        else:
                             if kw not in current_page_seo_words: current_page_seo_words.append(kw)
                if html_collector: row_data['Tags HTML'] = '<div class="popular-tags">' + "\n".join(html_collector) + '</div>'
            if use_promo:
                cands = [p for p in promo_items_pool if p['url'].rstrip('/') != page['url'].rstrip('/')]
                random.shuffle(cands)
                if cands:
                    p_html = f'<div class="promo-section"><h3>{promo_title}</h3><div class="promo-grid" style="display:flex;gap:15px;overflow-x:auto;">'
                    for item in cands:
                        p_name = url_name_cache.get(item['url'].rstrip('/'), "–¢–æ–≤–∞—Ä")
                        p_html += f'<div class="promo-card" style="min-width:220px;"><a href="{item["url"]}"><img src="{item["img"]}" style="max-height:100px;"><br>{p_name}</a></div>'
                    p_html += '</div></div>'
                    row_data['Promo HTML'] = p_html

            if use_text and has_ai_key:
                blocks = generate_ai_content_blocks(api_key=pplx_api_key, base_text=base_text_raw if base_text_raw else "", tag_name=page['name'], forced_header=header_for_ai, num_blocks=num_text_blocks_val, seo_words=current_page_seo_words)
                for i, b in enumerate(blocks): row_data[f'Text_Block_{i+1}'] = b
            if use_tables and has_ai_key and model_flash:
                for t_i, t_topic in enumerate(table_prompts):
                    ctx = f"–î–∞–Ω–Ω—ã–µ: {tech_context_final_str}" if tech_context_final_str else ""
                    prompt = f"Create HTML table for '{header_for_ai}'. Topic: {t_topic}. {ctx}. No Markdown."
                    html = call_gemini_safe(model_flash, prompt) 
                    html = html.replace("```html", "").replace("```", "").strip()
                    if "<table" not in html and "Error" not in html: html = f"<table>{html}</table>"
                    row_data[f'Table_{t_i+1}_HTML'] = html
            if use_geo and has_ai_key and global_geo_list and model_flash:
                cities = ", ".join(random.sample(global_geo_list, min(20, len(global_geo_list))))
                prompt = f"Write HTML <p> regarding delivery of '{header_for_ai}' to: {cities}."
                html = call_gemini_safe(model_flash, prompt) 
                row_data['IP_PROP4819'] = html.replace("```html", "").replace("```", "").strip()
            if use_sidebar: row_data['Sidebar HTML'] = full_sidebar_code
            final_data.append(row_data)
            progress_bar.progress((idx + 1) / total_steps)

        df_result = pd.DataFrame(final_data)
        st.session_state.gen_result_df = df_result 
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer: df_result.to_excel(writer, index=False)
        st.session_state.unified_excel_data = buffer.getvalue()
        status_box.update(label="‚úÖ –ö–æ–Ω–≤–µ–π–µ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω! –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã.", state="complete", expanded=False)

    if st.session_state.get('unified_excel_data') is not None:
        st.success("–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω!")
        st.download_button(label="üì• –°–ö–ê–ß–ê–¢–¨ –ï–î–ò–ù–´–ô EXCEL", data=st.session_state.unified_excel_data, file_name="unified_content_gen.xlsx", mime="application/vnd.ms-excel", key="btn_dl_unified")

with tab_wholesale_main: 
    if 'gen_result_df' in st.session_state and st.session_state.gen_result_df is not None:
        st.markdown("---")
        st.header("üëÄ –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
        df = st.session_state.gen_result_df
        page_options = df['Product Name'].tolist()
        selected_page_name = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞:", page_options, key="preview_selector")
        row = df[df['Product Name'] == selected_page_name].iloc[0]
        has_text = any((f'Text_Block_{i}' in row and pd.notna(row[f'Text_Block_{i}']) and str(row[f'Text_Block_{i}']).strip()) for i in range(1, 6))
        table_cols = [c for c in df.columns if 'Table_' in c and '_HTML' in c and pd.notna(row[c]) and str(row[c]).strip()]
        has_tables = len(table_cols) > 0
        has_tags = 'Tags HTML' in row and pd.notna(row['Tags HTML']) and str(row['Tags HTML']).strip()
        has_sidebar = 'Sidebar HTML' in row and pd.notna(row['Sidebar HTML']) and str(row['Sidebar HTML']).strip()
        has_geo = 'IP_PROP4819' in row and pd.notna(row['IP_PROP4819']) and str(row['IP_PROP4819']).strip()
        has_promo = 'Promo HTML' in row and pd.notna(row['Promo HTML']) and str(row['Promo HTML']).strip()
        has_visual = has_tags or has_sidebar or has_geo or has_promo 
        active_tabs = []
        if has_text: active_tabs.append("üìù –¢–µ–∫—Å—Ç")
        if has_tables: active_tabs.append("üß© –¢–∞–±–ª–∏—Ü—ã")
        if has_visual: active_tabs.append("üé® –í–∏–∑—É–∞–ª")

        st.markdown("""<style>.preview-box { border: 1px solid #e0e0e0; padding: 20px; border-radius: 8px; background: #fff; margin-bottom: 20px; }.preview-label { font-size: 12px; font-weight: bold; color: #888; text-transform: uppercase; margin-bottom: 5px; }.popular-tags { display: flex; flex-wrap: wrap; gap: 8px; }.tag-link { background: #f0f2f5; color: #333; padding: 5px 10px; border-radius: 4px; text-decoration: none; font-size: 13px; }table { width: 100%; border-collapse: collapse; font-size: 14px; }th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }th { background-color: #f2f2f2; font-weight: bold; }.sidebar-wrapper ul { list-style-type: none; padding-left: 10px; }.level-1-header { font-weight: bold; margin-top: 10px; color: #277EFF; }.promo-grid { display: flex !important; flex-wrap: wrap; gap: 10px; }.promo-card { width: 23%; box-sizing: border-box; }.promo-card img { max-width: 100%; height: auto; }</style>""", unsafe_allow_html=True)

        if not active_tabs: st.warning("‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç –ø—É—Å—Ç.")
        else:
            tabs_objects = st.tabs(active_tabs)
            tabs_map = dict(zip(active_tabs, tabs_objects))
            if "üìù –¢–µ–∫—Å—Ç" in tabs_map:
                with tabs_map["üìù –¢–µ–∫—Å—Ç"]:
                    st.subheader(row['Product Name'])
                    for i in range(1, 6):
                        col_key = f'Text_Block_{i}'
                        if col_key in row and pd.notna(row[col_key]):
                            content = str(row[col_key]).strip()
                            if content:
                                with st.container():
                                    st.caption(f"–ë–ª–æ–∫ {i}")
                                    st.markdown(f"<div class='preview-box'>{content}</div>", unsafe_allow_html=True)
            if "üß© –¢–∞–±–ª–∏—Ü—ã" in tabs_map:
                with tabs_map["üß© –¢–∞–±–ª–∏—Ü—ã"]:
                    for t_col in table_cols:
                        content = row[t_col]
                        clean_title = t_col.replace('_HTML', '').replace('_', ' ')
                        st.caption(clean_title)
                        st.markdown(content, unsafe_allow_html=True)
            if "üé® –í–∏–∑—É–∞–ª" in tabs_map:
                with tabs_map["üé® –í–∏–∑—É–∞–ª"]:
                    if has_promo:
                         st.markdown('<div class="preview-label">–ü—Ä–æ–º–æ-–±–ª–æ–∫ (–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏)</div>', unsafe_allow_html=True)
                         st.markdown(f"<div class='preview-box'>{row['Promo HTML']}</div>", unsafe_allow_html=True)
                    c1, c2 = st.columns(2)
                    with c1:
                        if has_tags:
                            st.markdown('<div class="preview-label">–¢–µ–≥–∏</div>', unsafe_allow_html=True)
                            st.markdown(f"<div class='preview-box'>{row['Tags HTML']}</div>", unsafe_allow_html=True)
                        if has_geo:
                            st.markdown('<div class="preview-label">–ì–µ–æ-–±–ª–æ–∫</div>', unsafe_allow_html=True)
                            st.markdown(f"<div class='preview-box'>{row['IP_PROP4819']}</div>", unsafe_allow_html=True)
                    with c2:
                        if has_sidebar:
                            st.markdown('<div class="preview-label">–°–∞–π–¥–±–∞—Ä</div>', unsafe_allow_html=True)
                            st.markdown(f"<div class='preview-box' style='max-height: 400px; overflow-y: auto;'>{row['Sidebar HTML']}</div>", unsafe_allow_html=True)
